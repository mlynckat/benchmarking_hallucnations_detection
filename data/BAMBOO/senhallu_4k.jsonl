{"title": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios", "content": "\nIntroduction\nReasoning plays a central role in human communication (Frank and Goodman, 2012) . While language models have demonstrated remarkable capacity on downstream tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) , it remains unclear to what extent predictions generated by language models are consequences of correlation with linguistic heuristics in the context, versus robust reasoning about causal relations grounded on understanding of world knowledge.\nIn this paper we leverage counterfactual conditionals to investigate the capacity of pre-trained LMs (PLMs) to distinguish hypothetical scenarios from reality, and to examine how this interacts with models' use of existing real world knowledge as well as shallower associative cues. Counterfactuals consist of a premise which is false in the real world but true in the hypothetical world (e.g., \"If cats were vegetarians\"), and an imaginary consequence of this premise (\"cats would love cabbages\"). Testing language models with counterfactuals allows us to use language to manipulate what is true and what is hypothetical, and to test models' ability to separate and use this information for predictions. Previous work has established the use of counterfactual scenarios to probe inference ability (Qin et al., 2019; Zellers et al., 2019; Mostafazadeh et al., 2016; Meng et al., 2022; Rajani et al., 2019; Saparov and He, 2022; Frohberg and Binder, 2021; Elazar et al., 2021; Rudinger et al., 2020) , but the datasets lack systematic control of lexical cues and world knowledge, which makes it likely that the performance could be attributable to spurious cues in the datasets (Niven and Kao, 2019) .\nFor our tests we draw on and adapt inputs from existing psycholinguistic experiments. We begin by testing models' ability to override existing world knowledge when the context indicates that the correct completion involves a hypothetical world (e.g., \"if cats were vegetarian, cats would love cabbages/fish\"). We test five popular PLMs, and find that models can increase their preference for counterfactual completions given counterfactual context-however, most models rely strongly on simple lexical cues. Next we control the effect of real world knowledge and lexical triggers, to test models' understanding of what counterfactual language implies about the world state. We find that most models fail to understand real-world implications of counterfactuals and largely rely on lexical triggers-with the exception of GPT-3, which shows greater sophistication, but continues to show non-trivial susceptibility to interfer-ence from lexical-associative cues. We discuss the implications and possible interpretations of these findings with respect to linguistic competence and predictive strategies of these models.\n\nExp1: overriding world knowledge\nOur first experiment investigates whether LMs are able to take a counterfactual scenario and predict a counterfactual-consistent completion that contradicts general world knowledge.\nItems We draw directly on counterfactual stimuli from the psycholinguistic study of Ferguson and Sanford (2008) . There are 128 items from the original psycholinguistic experiments, and we synthetically generate 10,720 additional items (see Appendix A.2 for illustration of data generation process). We match target nouns and syntactic constructions across conditions in order to control lexical properties that influence language models' predictions. Table 1 shows example items from the synthetic large-scale dataset (see Section A.1 for example items from the small-scale dataset).\n\nCond Sentence\nCW If cats were vegetarians, people would love them.\nFamilies would feed cats with fish/cabbages.\n\nRW\nBecause cats are carnivores, people love them.\nFamilies would feed cats with fish/cabbages.\n\nBB\nFamilies would feed cats with fish/cabbages The experiment includes two key conditions: Counterfactual-World (CW) and Real-World (RW) (Fig. 1 ). The CW condition presents a counterfactual scenario, e.g., in which cats are vegetarians. The logical target completion in this example is \"cabbages\", but because in reality cats are more likely to eat fish, this contradicts world knowledge. By contrast, in the RW condition the logical completion is consistent with the real world (\"feed cats with fish\"). We also include one Baseline Bias (BB) condition, for a more direct test of the strength of models' baseline preference for each completion.\nExperiments We test counterfactual reasoning in five pre-trained language models. We include autoregressive transformers in the GPT family (GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) ) and masked language models in the BERT family (BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) and MPNet (Song et al., 2020)) 2 .\nWe test models by comparing the log-probability that each model assigns to the CW-congruent (\"cabbages\") and RW-congruent (\"fish\") completions given the contexts. For all conditions, we compute the percentage of items in which the CW-congruent continuation has a higher probability than the RWcongruent continuation. This means that in RW and BB conditions, lower values reflect better predictions, since the CW-congruent completion is the less logical completion in these conditions. Table 2 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp1. In the CW condition, higher values reflect better predictions. In RW and BB conditions, lower values reflect better predictions.\nResults Table 2 shows the preferences for CWcongruent completions across all models and conditions, for the small-scale hand-designed items from the psycholinguistic experiment, and for the large-scale synthetic items. 3 We see that all mod-els show stronger preference for CW-congruent continuations in the counterfactual (CW) context than in the other conditions (though in the case of BERT on the small-scale data, this difference is negligible). All models show below-chance preference for CW-congruent continuations in the RW condition-which means above-chance preference for the correct RW-congruent continuations. However, though all model preferences for the correct CW-congruent continuation are higher in the CW condition than in the RW condition, even in the CW condition the preference for CW-congruent conditions is at best slightly above chance for most models. The exception is GPT-3, which is the only model to prefer the CW-congruent continuation in greater than 70% of items.\nWe also see that GPT-3 shows exceptionally strong performance on both BB and CW conditions. This suggests, slightly counterintuitively, that stronger grasp of relevant world knowledge may in fact be associated with models more effectively overriding that knowledge in a counterfactual. To investigate this effect further, we examine the impact of world knowledge at the item level. We quantify strength of world knowledge as the difference between models' log-probability of CW-and RW-congruent continuations for a given item in the BB condition, and the strength of counterfactual preference as the difference between log-probability of CW-and RW-congruent continuations for a given item in the CW condition. We then compute the Pearson correlation between these strength measures. We find a significant correlation between the robustness of world knowledge encoding and strength of counterfactual preference in the CW condition across all language models (see Appendix A.3), further supporting a relationship between strength of world knowledge and counterfactual sensitivity. While previous work has suggested that large language models may have difficulty avoiding memorized texts when explicitly prompted to end famous quotes differently (McKenzie et al., 2022) , our results suggest that world knowledge may in fact facilitate reasoning when accompanied with clear structural cues (e.g. \"if\"). To better understand how world knowledge informs language models' predictions and in-CW condition alone. However, to further address this concern, we calculate the proportion of items in which the model shows the correct preference in both CW and RW conditions. The results are presented in Section A.5 and suggest a comparable pattern in terms of relative model strengths.\nference, it will be important to continue expanding the scale of tests and more carefully operationalize definitions of world knowledge in future work.\n\nExp2: impact of cue words in context\nThe first experiment suggests that models can to an extent override world knowledge given a counterfactual, particularly in cases when models have a strong handle on the relevant world knowledge. However, it is possible that in these tests the models were not relying on sophisticated understanding of counterfactuals, but rather on simple lexical triggers in context. Consider, for instance, that models could perform well in Exp1 if they simply increase their preference for \"cabbages\" in the proximity of \"vegetarians\", etc. To test the impact of these lexical triggers, we incorporate an additional condition.\nItems Table 3 and Fig. 2 show a sample item and illustration of experimental set-up with the new added condition. In this Counterfactual-to-Reality (CR) condition, models see the same counterfactual context, but the subsequent sentence references actual reality. So the correct completion is consistent with reality, but inconsistent with the lexical trigger (\"vegetarians\"). We generate sentences in the CR condition by modifying CW sentences to include the discourse connective \"In reality\" and to include present tense in the second sentence.\n\nCR\nIf cats were vegetarians, people would love them.\nIn reality, families feed cats with fish/cabbages. Experiments As above, we calculate percentage of items in which models prefer the CW-congruent continuations. Models relying on information beyond simple lexical triggers should show a sharp drop in preference for the CW-congruent completion in the CR condition, where the correct completion should align with real world information.\nResults Table 4 shows the results. We see that most models show non-zero drop between CW and CR conditions-however, for most models this reduction is minor. It is only GPT-3 that shows a truly substantial drop in CW-congruent preference, and only in the large-scale synthetic dataset. This suggests that most models are largely following simpler lexical triggers, while GPT-3 has somewhat greater sensitivity to more detailed linguistic cues. Note, however that GPT-3's relative success on the synthetic data over the small-scale data may rely on larger distance between lexical triggers and target positions: see Appendix A.4 for evidence on GPT-3's sensitivity to linear distance. Table 4 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp2. In the CW condition, higher values reflect better predictions. In the CR condition, lower values reflect better predictions.\n\nExp3: Inferring real world state with counterfactual cues\nThe previous experiments indicate that models can override world knowledge in the face of counterfactual evidence, and that the ability to do this improves with stronger world knowledge-but for most models this performance appears to be driven largely by simple lexical triggers in the context, with the possible exception of GPT-3. In this section we remove the influence of pre-existing world knowledge, and hold constant lexical triggers across conditions, for a more direct test of models' sensitivity to linguistic indicators of counterfactuals, and what they say about the true state of the world. This task is particularly challenging because language models must infer the true state of the world based on the presence of counterfactuals, with lexical cues often being misleading.\nItems We adapt stimuli from a psycholinguistic study with 96 controlled sentences (Ferguson, 2012) . We additionally create a larger-scale synthetic dataset with 12,960 sentences, using the same events as the generated dataset from Section 2. We modify the subject noun phrases such that there is no influence of existing world knowledge. For example, we modify the subject \"cat\" to \"pet\", so that there is no prior knowledge about the subject's preference for \"cabbages\" or \"fish\". As a result, existing world knowledge cannot inform the correct completion-instead, models need to infer based on the counterfactual language that the true state of the world is different from what the counterfactual states. Further, we control the lexical items used across different conditions to minimize effects of lexical cues on condition differences (see Table 5 ).\n\nCond Sentence\nCWC If the pets were vegetarians, people would love them.\nIn fact, people feed the pets with fish/cabbages.\nRWCA Because the pets are vegetarians, people love them.\nIn fact, people feed the pets with fish/cabbages.\nBBC In fact, people feed the pets with fish/cabbages. Fig. 3 shows the set-up of conditions. In the Counterfactual-World Context (CWC) condition, the scenario described in the first sentence is neutral with respect to real world knowledge-it is the use of the counterfactual (\"if...were\") that tips us off that this scenario is not true in reality. The correct completion, then, cannot be informed by world knowledge, and is also misaligned with the lexical trigger (e.g., \"vegetarians\"), so models must rely specifically on this implication from the counterfactual in order to perform well.\nIn the Real-World Context Alternative (RWCA) condition, the context uses the same lexical triggers (\"vegetarians\") as the CWC condition. However, because there is no counterfactual language, the logical completion is now the word associated with the lexical trigger (e.g., \"cabbages\", associated with \"vegetarians\").\nGiven that the logical completions in CWC and RWCA differ, we also compare against a Baseline Bias Context (BBC) condition, to establish default model preference for the target factual completion in the presence of the new subject noun phrase.\nExperiments We compare proportion of CWCcongruent completions across conditions. Good performance will assign high values in the CWC condition and low values in the RWCA condition. Table 6 : Percentage of preference for CWC-congruent completion (e.g., \"fish\") in Exp3. In the CWC condition, higher values reflect better predictions. In the CWCA condition, lower values reflect better predictions. The BBC condition establishes models' default preference for the CWC-congruent completion.\nResults Table 6 shows the results. In the smallscale dataset, most models show a similar preference in CWC and RWCA, suggesting again that their predictions are largely driven by lexical triggers. Only GPT-3 shows substantial difference between CWC and RWCA, indicating finer-grained sensitivity to counterfactual structures. This sensitivity is, however, less pronounced in the largescale dataset. Closer inspection suggests that GPT-3's specific success on the small-scale data may in fact be attributable to canceling out of lexical triggers: in the small-scale dataset, there are lexical triggers supporting both continuations (see A.1 for more illustration of the characteristics of the smallscale dataset), which may cause lexical cues to cancel out, enabling more influence from other linguistic cues. To take one example, the small-scale dataset contains the item \"If Helen had received her student loan, her bank balance would now be in credit. When she checked her bank balance she was worried/happy about her finance.\" In this item, among the lexical triggers (\"student loan\", \"in credit\", \"bank balance\") there are potential associations with both the CWC-congruent completion \"worried\" and the CWC-incongruent completion \"happy\". By contrast, in the large-scale dataset, the major lexical trigger (\"vegetarians\") always favors the CWC-incongruent continuation (\"cabbages\"), causing strong lexical bias against the CWC-congruent continuation (see Appendix A.4 for further analysis on the role of conflicting lexical triggers and other linguistic factors). This suggests that GPT-3 does show real sensitivity to linguistic indicators of counterfactuals, but the effect of superficial lexical cues remains strong.\n\nConclusion\nThe experiments above have shown that when presented with counterfactual situations, PLMs are able to prefer completions that conflict with world knowledge-and counterintuitively, this sensitivity appears better in cases where that world knowledge is stronger. Our results also indicate, however, that models are in large part relying on simple lexical cues to inform these preferences. The only model that shows more sophisticated sensitivity to finegrained linguistic cues separating counterfactuals from reality is GPT-3-which successfully distinguishes conditions based on counterfactual cues, but nonetheless still shows strong influences from lexical associative cues. Why might world knowledge aid counterfactual sensitivity? Does GPT-3 truly understand counterfactuals? One possibility worth considering is that explanations in both of these cases involve volume of exposure. First, models' stronger world knowledge for a given fact suggests that models have encountered that fact more often in training-and this may in turn translate to more exposure to that type of knowledge in counterfactual contexts, enabling more straightforward memorization-based performance. Similarly, while GPT-3 may robustly understand counterfactuals, the massive data exposure for that model may enable a simpler path to success: GPT-3 could simply have developed lower-level knowledge of how linguistic cues like \"If/had\" versus \"Because\" mediate levels of association between nearby lexical cues and later words. We leave investigation of these hypotheses for future work.\n", "hypothesis": "We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models.  We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge-however, we also find that for most models this effect appears largely to be driven by complex semantic cues.  When we mitigate effects of both world knowledge and semantic cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by semantic factors.", "answer": false}
{"title": "Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information", "content": "\nIntroduction\nKeyphrase extraction is the fundamental task of automatically extracting a set of salient phrases from a document that concisely describes its primary content (Hasan and Ng, 2014; Song et al., 2023a) . Figure 1 shows an example of the source document and its corresponding keyphrases.\nRecent developments in pre-trained language models (Devlin et al., 2019) have heightened the need for utilizing pre-trained embeddings on natural language processing tasks, which significantly improves the performance of embedding-based unsupervised keyphrase extraction models (Sun et al., 2020; Liang et al., 2021; Zhang et al., 2022) . Existing embedding-based models mainly consist of two components: candidate keyphrase extraction and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2021 Song et al., , 2022a)) . The former extracts continuous words from the document as candidate keyphrases through heuristic rules, and the latter estimates the importance of candidate phrases by matching similarity with their corresponding document.\nGenerally, the source document has both salient information and noises (redundant content). Hence, there may be a deviation when directly using the phrase-document relevance as the importance score of each candidate to select keyphrases. For many specific-domain documents (e.g., news or scientific articles), the highlights (the title or the first sentence) typically contains the central information of the source document (as shown in Figure 1 ), which has more significant guidance for extracting keyphrases. However, the recent embedding-based unsupervised keyphrase extraction models ignore the effect of the highlight information, leading to extract wrong keyphrases.\nMotivated by the above issues, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which estimates the impor- \n\nCandidate Keyphrase Extraction\nTo extract candidate keyphrases from the source document, we follow the previous studies (Liang et al., 2021; Song et al., 2022b; Ding and Luo, 2021) At the same time, we use the mean pooling operation to obtain the highlight representation h s of the document.\n\nPhrase-Document Relevance\nTo obtain more relevant candidates, we model the similarity between candidate phrases and the corresponding document as follows,\nEQUATION\nwhere p h i denotes the phrase-document relevance of i-th candidate keyphrases and ||\u2022|| 1 indicates the Manhattan Distance.\nFor news and scientific articles, keyphrases often appear at the beginning or front position (Florescu and Caragea, 2017a,b) , which means that the position information is important and indicative for extracting keyphrases. For example, the word appearing at 2-th, 5-th and 10-th, has a weight \u03c1 i = 1/2 + 1/5 + 1/10 = 0.8. Inspired by the previous work (Florescu and Caragea, 2017b; Liang et al., 2021) , we adopt a position regularization as follows, \u03c1 i = softmax(e 1/i ), where \u03c1 i is the position regularization factor of the i-th candidate phrase. Then, the weighted phrase-document relevance ph i can be re-calculated as follows,\nEQUATION\nHere, we finally employ ph i to estimate the phrasedocument relevance of the i-th candidate phrase.\n\nCross-Phrase Relevance\nGenerally, the phrase-document relevance is calculated between the highlight information and each candidate independently, and consequently, it cannot determine which candidates are better than the Model DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Statistical Keyphrase Extraction Models TF-IDF (Jones, 2004) others. To determine which candidate phrases are more salient than the others, we sum the semantic relatedness between the i-th candidate phrases and all candidates as the cross-phrase relevance. Thus, it calculates the local relevance as follows,\nEQUATION\n)\nwhere \u03b4 i = Mean( j=1,j =i h p i h p j ). Here, we treat \u03b4 i as a de-noisy factor to filter the noises, which is far different from the i-th candidate keyphrase in the document.\n\nRelevance Aggregation\nWe aggregate the phrase-document relevance and the cross-phrase relevance into a whole score as the importance score of each candidate via a simple multiplication,\nEQUATION\n)\nwhere r i indicates the importance score of the i-th candidate phrase. Then, we rank all candidates with their importance score r i and extract top-ranked k phrases as keyphrases of the source document.\n3 Experiments and Results\n\nExperimental Settings\nThis paper conducts experiments on three benchmark and popular used keyphrase datasets, which includes DUC2001 (Wan and Xiao, 2008) , Inspec (Hulth, 2003 ), and SemEval2010 (Kim et al., 2010) . Due to page limits, please refer to the corresponding articles for the details of the three datasets. Following the previous work (Liang et al., 2021; Ding and Luo, 2021; Song et al., 2023b) , we use the standard practice and evaluate the performance of our model in terms of f-measure at the top-K keyphrases (F1@K) and adopt stemming to both extracted keyphrases and gold truth. Concretely, we report F1@5, F1@10, and F1@15 of each model on three benchmark datasets.\nWe adopt the pre-trained language model BERT (Devlin et al., 2019) as the backbone of our model, initialized from their pre-trained weights. In our experiments, \u03bb is set to 0.9 for three benchmark datasets.\n\nOverall Performance\nTable 1 shows the performance of baselines and our model on three benchmark datasets (DUC2001, In-\n\nDUC2001\nInspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Table 2 : The results of different pooling methods for document embedding.\nDifferent Similarity Measures DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 spec, and SemEval2010). The results show that our method significantly improves over state-of-the-art unsupervised keyphrase extraction baselines. Compared with the current state-of-the-art models, our model achieves significantly better performance on F1@5, F1@10, and F1@15 evaluation metrics, demonstrating the effectiveness of estimating the importance of candidate phrases by leveraging the highlights to calculate the relevance.\nCompared with EmbedRank (Bennani-Smires et al., 2018) , KeyGames (Saxena et al., 2020) , and SIFRank (Sun et al., 2020) , HGUKE achieves significant improvement, which benefits from using the highlights to calculate the importance score of each candidate keyphrase. Compared with the best baseline JointGL, our model achieves better performance on several benchmark keyphrase extraction datasets in all evaluation metrics. The main reason for this improvement is that we use the highlights as the guidance information instead of the whole document when estimating the importance of keyphrases.\n\nAblation Test\nThe ablation experiments on three benchmark keyphrase extraction datasets are shown in Figure 3 . It can be seen from the results that using the highlight information can significantly improve the performance of keyphrase extraction, which benefits from estimating the importance score of each candidate by using its corresponding highlight information rather than the whole document. We consider the main reason is that the title or the first sentence of the document usually has a strong guidance for extracting keyphrases.\n\nImpact of Pooling Methods\nIn this section, we study different pooling methods, including mean-and max-pooling operations. For all pooling methods, HGUKE using the last BERT layer achieves the best results, demonstrating that HGUKE benefits from stronger contextualized semantic representations. We can see the results in Table 2 that the document encoded via the meanpooling operation obtains the best performance.\n\nImpact of Different Similarity Measures\nOur model adopts Manhattan Distance to measure the textual similarity between candidate phrases and the highlight information. Furthermore, we attempt to employ different measures to estimate the phrase-document relevance. The results of different similarity measures are shown in Table 3 , and we can see that the advantage of Manhattan Distance is obvious.\n\nRelated Work\nMost existing unsupervised keyphrase extraction methods can be mainly divided into four categories: statistics-based, topic-based, graph-based, and embedding-based models. Specifically, statisticsbased models (Salton and Buckley, 1988; Witten et al., 1999) usually extract keyphrases by estimating the importance of candidate phrases with different statistic features, such as word frequency feature, phrase position feature, linguistic features of natural language, etc. Topic-based models (Liu et al., 2009 (Liu et al., , 2010) ) typically utilize topic information to determine whether a candidate phrase is a keyphrase. Graph-based models (Mihalcea and Tarau, 2004; Grineva et al., 2009) represent the document as a graph and rank candidate phrases by graph-based similarities.\nEmbedding-based models usually adopt the pretrained embeddings to obtain document and candidate phrase representations and calculate the importance score of each candidate depending on the obtained representations. Benefiting from the development of transformer-based pre-trained language models (Devlin et al., 2019) in the natural language processing field, embedding-based models (Bennani-Smires et al., 2018; Sun et al., 2020; Liang et al., 2021) have achieved outstanding performance. Concretely, embedding-based models mainly consist of two procedures: candidate keyphrase representation and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2023a) . The procedure utilizes natural language linguistic features to construct candidate keyphrases and represents them by pre-trained embedding approaches (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) ). The second procedure estimates the importance of candidate phrases from different perspectives to determine whether a candidate phrase is a keyphrase.\nUnlike the existing unsupervised keyphrase extraction models, we use the highlight information of the document to calculate the phrase-document relevance instead the whole document.\n\nConclusion and Future Work\nIn this paper, we incorporate structural information to improve the performance of embedding-based unsupervised keyphrase extraction. Specifically, in this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which calculates the phrase-document relevance via the highlight information instead of the whole document to select relevant candidate phrases. Extensive experiments demonstrate that HGUKE outperforms the state-of-the-art unsupervised baselines. Future research may investigate adopting different structural information of the source document to improve the performance of unsupervised keyphrase extraction.\n", "hypothesis": " Specifically, HGUKE first models the phrasedocument relevance via the highlights of the documents.  Next, HGUKE calculates the crossphrase relevance between all candidate phrases.  Finally, HGUKE aggregates the above two relevance as the importance score of each candidate to rank and extract keyphrases.  The experimental results on three benchmarks demonstrate that HGUKE outperforms the state-of-the-art unsupervised keyphrase extraction baselines..", "answer": true}
{"title": "DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect", "content": "\nIntroduction\nBinaural audio synthesis (BAS), which aims to render binaural audio from the monaural counterpart, has become a prominent technology in artificial spaces (e.g. augmented and virtual reality) (Richard et al., 2021 (Richard et al., , 2022;; Leng et al., 2022; Lee and Lee, 2022; Parida et al., 2022; Zhu et al., 2022; Park and Kim, 2022) . Binaural rendering provides users with an immersive spatial and social presence (Hendrix and Barfield, 1996; Gao and Grauman, 2019; Huang et al., 2022; Zheng et al., 2022) , by producing stereophonic sounds with accurate spatial information. Unlike traditional single channel audio synthesis (van den Oord et al., 2016; Chen et al., 2021) , BAS places more emphasis on * Equal contribution.\naccuracy over sound quality, since humans need to interpret accurate spatial clues to locate objects and sense their movements consistent with visual input (Richard et al., 2021; Lee et al., 2022) .\nCurrently, there are three types of neural networks (NN) to synthesize binaural audio. Firstly, Richard et al. (2021) collects a paired monauralbinaural speech dataset and provides an end-to-end baseline with geometric and neural warping technologies. Secondly, to simplify the task, Leng et al. (2022) decompose the synthesis into a two-stage paradigm: the common information of the binaural audio is generated in the first stage, based on which the binaural audio is generated in the second stage. They also propose to use the generative model DDPM (Ho et al., 2020) to improve the audio naturalness. Thirdly, to increase the generalization capability for the out-of-distribution audio, Lee and Lee (2022) renders the speech in the Fourier space. These non-linear NN-based methods outperform the traditional digital signal processing systems based on a linear time-invariant system (Savioja et al., 1999; Zotkin et al., 2004; Sunder et al., 2015) .\nHowever, these NN methods still have room for improvement in accuracy, especially phase accuracy. Richard et al. (2022) claims that the correct phase estimation is crucial for binaural rendering 1 . Actually, the previous works tend to view the scene \"statically\", and only take into account the series of positions and head orientations. This motivates us to propose DopplerBAS, which facilitates phase estimation by explicitly introducing the Doppler effect (Gill, 1965; Giordano, 2009) into neural networks. Specifically, 1) we calculate the 3D velocity vector of the moving sound source in the Cartesian coordinates and then decompose this 3D velocity vector into a velocity vector in the spherical coor-dinates relative to the listener; 2) According to the Doppler effect, we use the radial relative velocity as an additional condition of the neural network, to incentivize the model to sense the moving objects.\nWe also analyze the efficacy of different types of velocity conditions through extensive experiments.\nNaturally, DopplerBAS can be applied to different neural binaural renderers without tuning hyperparameters. We pick two typical recent backbones to demonstrate the effectiveness of our method: 1) WarpNet (Richard et al., 2021) , a traditional neural network optimized by reconstruction losses; 2) BinauralGrad (Leng et al., 2022) , a novel diffusion model optimized by maximizing the evidence bound of the data likelihood. Experiments on Warp-Net and BinauralGrad are representative and could show the generalizability of our proposed Doppler-BAS on other conditions based on gains on these two models. The contributions of this work can be summarized as follows:\n\u2022 We propose DopplerBAS, which distinctly improves WarpNet and BinauralGrad in the phase error metric and produces a new state of the art performance: 0.780 (vs. the current state of the art 0.807).\n\u2022 We conduct analytical experiments under various velocity conditions and discover that: 1) NN does not explicitly learn the derivative of position to time (velocity); 2) The velocity condition is beneficial to binaural audio synthesis, even the absolute velocity in the Cartesian coordinates; 3) The radial relative velocity is the practical velocity component, which obeys the theory of the Doppler effect.\n\nMethod\nIn this work, we focus on the most basic BAS scenario where only the monaural audio, the series of positions and head orientations are provided (Richard et al., 2022; Leng et al., 2022) , rather than other scenarios where extra modalities (Xu et al., 2021) are present. Note that scenarios with extra modalities present are different tasks. Also, as demonstrated in this paper, our proposed DopplerBAS is plug-and-play and can be easily integrated into other more complex scenarios. In this section, we will introduce the Doppler Effect as the preliminary knowledge, and then introduce the proposed method DopplerBAS. We will describe how to calculate and decompose the velocity vec-tor, and how to apply this vector to two different backbones.\n\nDoppler Effect\nThe Doppler effect (Gill, 1965) is the change in frequency of a wave to an observer, when the wave source is moving relative to it. This effect is originally used in radar systems to reveal the characteristics of interest for the target moving objects (Chen et al., 2006) . It can be formulated as:\nEQUATION\nwhere c, v r , f 0 and f are the propagation speed of waves, the radial relative velocity of the moving sound source, the original frequency of waves and the received frequency of waves, respectively.\n\ud835\udc63 ! \ud835\udc63 \" \ud835\udc5f \ud835\udc52 \ud835\udc63 #$ \ud835\udc65 \ud835\udc66 right ear\nFigure 1 : We illustrate the top view where the height dimension is omitted for simplicity. The sound source is moving in the x-y plane with the velocity v xy . This velocity is decomposed into the radial velocity v r relative to one ear (e.g., the right ear).\n\nDopplerBAS\nWe do not directly apply Eq. ( 1) in the frequency domain of audio, because some previous works (Lee and Lee, 2022) show that modeling the binaural audio in the frequency domain degrades the accuracy although it could benefit the generalization ability. Different from modeling the Doppler effect in the frequency domain, we calculate the velocity of interest and use it as a condition to guide the neural network to synthesize binaural audio consistent with the moving event. In the receiver-centric Cartesian coordinates, we define \u20d7 p s and \u20d7 p e as the 3D position of the moving sound source s and one ear of the receiver e respectively (e.g., the right ear, as shown in Figure 1 ). The \n\u20d7 p = (p x , p y , p z ) = \u20d7 p s \u2212 \u20d7 p e .\nThen s's velocity 2 can be calculated as:\n\u20d7 v = (v x , v y , v z ) = ( dp x dt , dp y dt , dp z dt ).\nNext, we build the spherical coordinate system using the ear as the origin, and decompose \u20d7 v into the radial relative velocity \u20d7 v r by:\nEQUATION\nwhere r \u2208 R 1 is the radial unit vector. Finally, we add \u20d7 v r as the additional condition to the network: The original conditions in monauralto-binaural speech synthesis are C o \u2208 R 7 = (x, y, z, qx, qy, qz, qw), of which the first 3 represent the positions and the last 4 represent the head orientations. We define the new condition C \u2208 R 9 = (x, y, z, qx, qy, qz, qw, v r\u2212lef t , v r\u2212right ), where v r\u2212lef t and v r\u2212right represent the radial velocity of source relative to the left and right ear respectively, which are derived from Eq. (2). We then apply C to WarpNet and BinauralGrad backbones, as follows.\n\nWarpNet\nWarpNet consists of two blocks: 1) The Neural Time Warping block to learn a warp from the source position to the listener's left ear and right ear while respecting physical properties (Richard et al., 2021) . This block is composed of a geometric warp and a parameterized neural warp. 2) The Temporal ConvNet block to model subtle effects such as room reverberations and output the final binaural audio. This block is composed of a stack of hyperconvolution layers. We replace the original C o with C for the input of parameterized neural warp and for the condition of hyper-convolution layers.\n\nBinauralGrad\nBinauralGrad consists of two stages: 1) The \"Common Stage\" generates the average of the binaural audio. The conditions for this stage include the monaural audio, the average of the binaural audio produced by the geometric warp in Warp-Net (Richard et al., 2021) , and C o . 2) The \"Specific Stage\" generates the final binaural audio. The conditions for this stage include the binaural audio produced by the geometric warp, the output of the \"Common Stage\", and C o . BinauralGrad adopts diffusion model for both stages, which is based on non-causal WaveNet blocks (Oord et al., 2016) with a conditioner block composed of a series of 1D-convolutional layers. We replace C o with C as the input of the conditioner block for both stages.\n\nExperiments\nIn this section, we first introduce the commonly used binaural dataset, and then introduce the training details for WarpNet-based and BinauralGradbased models. After that, we describe the evaluation metrics that we use to evaluate baselines and our methods. Finally, we provide the main results with analytical experiments on BAS.\n\nSetup\nDataset We evaluate our methods on the standard binaural dataset released by Richard et al. (2021) . It contains 2 hours of paired monaural and binaural audio at 48kHz from eight different speakers. Speakers were asked to walk around a listener equipped with binaural microphones. An OptiTrack system track the positions and orientations of the speaker and listener at 120Hz, which are aligned with the audio. We follow the original train-validation-test splits as Richard et al. (2021) and Leng et al. (2022) for a fair comparison.\nTraining Details We apply DopplerBAS on two open-source BAS systems WarpNet and Bin-auralGrad. We train 1) WarpNet and War-Net+DopplerBAS on 2 NVIDIA V100 GPUs with batch size 32 for 300K steps, and 2) BinauralGrad and BinauralGrad+DopplerBAS on 8 NVIDIA A100 GPUs with batch size 48 for 300K steps 3 .\nEvaluation Metrics Following the previous works (Leng et al., 2022; Lee and Lee, 2022) , we adopt 5 metrics to evaluate baselines and our methods: 1) Wave L2: the mean squared error between waveforms; 2) Amplitude L2: the mean squared errors between the synthesized speech and the ground truth in amplitude; 3) Phase L2: the mean squared errors between the synthesized speech and the ground truth in phase; 4) PESQ: the perceptual evaluation of speech quality; 5) MRSTFT: the multi-resolution spectral loss.\n\nMain Results\nWe compare the following systems: 1) DSP, which utilizes the room impulse response (Lin and Lee, 2006) to model the room reverberance and the head-related transfer functions (Cheng and Wakefield, 2001) to model the acoustic influence of the human head; 2) WaveNet (Richard et al., 2021; Leng et al., 2022) , which utilizes the WaveNet (Oord et al., 2016) model to generate binaural speech; 3) NFS, which proposes to model the binaural audio in the Fourier space; 4) WarpNet (Richard et al., 2021) , which proposes a combination of geometry warp and neural warp to produce coarse binaural audio from the monaural audio and a stack of hyper-convolution layers to refine coarse binaural audio; 5) WarpNet + DopplerBAS, which applies DopplerBAS to Warp-Net; 6) BinauralGrad (Leng et al., 2022) , which proposes to use diffusion model to improve the audio naturalness; 7) BinauralGrad + DopplerBAS, which applies DopplerBAS to BinauralGrad.\nThe results are shown in Table 1 . \"+ Doppler-BAS\" could improve both WarpNet and Binaural-Grad in all the metrics, especially in the Phase L2 metric. WarpNet + DopplerBAS performs best in the Phase L2 metric and reaches a new state of the Analysis We conduct analytical experiments for the following four velocity conditions. \"Spherical \u20d7 v \": the velocity conditions introduced in Section 2.2 are calculated in the spherical coordinate system; \"Cartesian \u20d7 v \": the velocity conditions are calculated in the Cartesian coordinate system; \"Zeros\": the provided conditions are two sequences of zeros; \"Time series\": the provided conditions are two sequences of time. The results are shown in Table 2 , where we place WarpNet in the first row as the reference. We discover that: 1) Radial relative velocity is the practical velocity component, which obeys the theory of the Doppler effect (row 2 vs. row 1); 2) The velocity condition is beneficial to binaural audio synthesis, even for the absolute velocity in the Cartesian coordinates (row 3 vs. row 1); 3) Just increasing the channel number of the condition C o (Section 2.2) by increasing the parameters in neural networks without providing meaningful information could not change the results (row 4 vs. row 1); 4) The neural networks do not explicitly learn the derivative of position to time (row 5 vs. row 1). These points verify the rationality of our proposed method.\n\nConclusion\nIn this work, we proposed DopplerBAS to address the Doppler effect of the moving sound source in binaural audio synthesis, which is not explicitly considered in previous neural BAS methods. We calculate the radial relative velocity of the moving source in the spherical coordinate system as the additional conditions for BAS. Experimental results show that DopplerBAS scales well to different types of backbones and reaches a new SOTA.\n", "hypothesis": "However, existing BAS methods are limited in terms of phase estimation, which is crucial for spatial hearing.  In this paper, we propose the DopplerBAS method to explicitly address the Doppler effect of the moving sound source.  Specifically, we calculate the radial relative velocity of the moving speaker in Cartesian coordinates, which further guides the synthesis of binaural audio.  This complex method introduces additional hyperparameters and modifies the loss functions, but achieves superior performance compared to existing methods.", "answer": false}
{"title": "Not Enough Data to Pre-train Your Language Model? MT to the Rescue!", "content": "\nIntroduction\nSince the emergence of the attention-based Transformer architecture (Vaswani et al., 2017) and the masking pre-training strategies introduced by BERT (Devlin et al., 2019) , transformer-based language models have become the default approach for many NLP tasks, leading to an impressive performance in high-resource languages, particularly English (Hoffmann et al., 2022; Thoppilan et al., 2022; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) .\nAs scaling laws dictate (Kaplan et al., 2020; Hoffmann et al., 2022) , such competitive models are achievable with big computational budgets and large corpora available, requirements difficult to meet for most languages (Joshi et al., 2020) .\nFortunately, LMs are being built for lessresourced languages, such as KinyaBERT for Kinyarwanda (390M words) (Nzeyimana and Rubungo, 2022) , ElhBERTeu for Basque (351M) (Urbizu et al., 2022) , gaBERT for Irish (161M) (Barry et al., 2021) , LuxemBERT for Luxembourgish (130M) (Lothritz et al., 2022b) , Bertinho 45M for Galician (Vilares et al., 2021) , swahBERT for Swahili (16M) (Martin et al., 2022) and QuBERT for Quechua (4M) (Zevallos et al., 2022) . Zhang et al. (2021) estimates that 10M-100M words of pre-training data are enough for an LM to acquire the linguistic capacities of syntax and semantics, but the amount of data required to acquire factual knowledge and commonsense is higher.\nIn this work, we propose to tackle the lack of data by using text corpora available in other languages translated via Machine Translation (MT). To the best of our knowledge, this has been addressed before (Lothritz et al., 2022a) but not indepth, and only for a closely related language pair (German-Luxembourgish). We selected Basque, a language isolate, as the target language, and employ Spanish as the auxiliary language.\nWe direct our efforts to answer the following Research Questions (RQ):\nRQ1: Can we obtain comparable performance to a native LM by training LMs just on synthetic data from MT?\nRQ2: Can we improve current LMs for lessresourced languages by adding synthetic MT data?\n\nMethodology\nIn order to answer our research questions, we set out the following methodology. We propose two baseline LMs: i) ElhBERTeu (Urbizu et al., 2022) as a strong baseline, trained on a corpus of 351M words; and (ii) BERT 125M a model trained on a lower data regime. From there on we pre-train various LMs with different native/synthetic data combinations. Sections 3 and 4 give details of the models pre-trained, including baselines. All models presented in this paper follow the BERT base architecture (Devlin et al., 2019) .\nWe select Basque, a language isolate, as a tar-get language, and employ Spanish, a Romance language, as the auxiliary language since it has huge text corpora available. Furthermore, both languages coexist in the same geographical area, therefore, Spanish is the language that Basque shares the most parallel data with, which is crucial to train MT systems. On the other hand, this is a real case since obtaining a corpus in Basque that exceeds 350M words is difficult.\n\nMT system\nThe Spanish to Basque MT system used for our experiments is based on the default Base Transformer architecture (Vaswani et al., 2017) using the PyTorch version of the OpenNMT toolkit (Klein et al., 2017) and BPE tokenization (Sennrich et al., 2016) (joint vocabulary of 32K). The system was trained with 8.6M parallel sentences and evaluated on the FLORES-200 benchmark (Team et al., 2022) obtaining 13.2 BLEU and 47.4 chrF++. See Appendix F for an analysis of the impact the amount of parallel data has.\n\nCorpora\nFollowing we introduce the corpora employed on the experiments (summarized in Table 1 ):\nN_ElhBERTeu is a Basque corpus compiled to train ElhBERTeu (Urbizu et al., 2022) . It contains 351M words.\nN_small is a smaller Basque native corpus (125M words), created to be closer to the scenario of many languages. The corpus is composed of 75% news articles from Berria 1 newspaper and 25% of text from Wikipedia.\nS_beto2eu is the Spanish Unannotated Corpora 2 composed of 3B words (Ca\u00f1ete, 2019) which was used to train the Spanish LM BETO (Ca\u00f1ete et al., 2020) , translated to Basque using the MT system described in Section 2.1.\nS_loc2eu was also translated from Spanish to Basque. We collected up to 548M words of news articles in Spanish from news sources geographically limited to the Basque Country. After translating it with our MT system, the final corpus in Basque contains 378M words.\n\nPre-training Details\nSince the aim of this work focuses on the effect of the training data, we left all the hyper-papameters fixed. Every model was pre-trained following the procedure used for ElhBERTeu (Urbizu et al., 2022) . See appendix A for further details.\n\nEvaluation\nA downstream task evaluation of our models was performed on the BasqueGLUE ( We fine-tuned each model up to 10 epochs and selected the optimal number of epochs over the development set. We use a batch size of 32 and a learning rate of 3e-5. We report the average of 5 runs on the test sets. Fine-tuning was done on NVIDIA GeForce RTX 3090 GPUs.\n\nLM Trained Solely on Synthetic Data\nRQ1 aims to prove if it is possible to train a competitive LM with just synthetic text obtained from MT. In order to do that we train a BERT model on S_beto2eu (S_BERT), and evaluate if the model trained exclusively on synthetic data is able to perform as well as models trained on real data.\nThe results on the BasqueGLUE Benchmark for S_BERT are reported in In order to improve the results obtained with the synthetic data, we analyse two specific aspects of the data: i) the quality loss during the translation process; ii) the cultural context of the synthetic data. Following we analyze each of those factors.\n\nMeasuring the Quality of MT Text\nTo measure quality loss when translating from Spanish to Basque, we did a manual analysis on a sample of the translations produced by the MT system (See appendix B for details). We evaluated whether a sentence was correctly translated (71%), but also whether the produced sentence was linguistically correct (91%). Since we aim to use this text to train LMs, the effect of some translation errors like hallucinations or omissions, that cause significant meaning changes, might not be critical.\nNext, we measured the vocabulary diversity loss during translation. For that aim, we compiled a Basque-Spanish parallel corpus (more details can be found in appendix B) and translated the Spanish text to Basque with our MT system. The lexicon of the translated data is 16% poorer, limited by the target vocabulary of the MT model and the tendency of MT to generalize and simplify the vocabulary.\nFinally, we analyze the impact of training LMs on translated corpora, leaving aside other factors such as corpus size or text-domain. We train two BERT models using the parallel corpus compiled in the previous experiment, one on the original Basque part of the corpus and the other on the part translated from Spanish to Basque. The results in Table 3 show the model trained on translated data performs slightly worse than the native model.\nWhile this is expected from the quality loss and lexicon impoverishment caused by MT, the gap in performance is very small (0.5% on average), which leads to the conclusion that the synthetic data is adequate.\n\nDomain and Cultural Context\nAnother factor related to data which might affect the performance of MLs trained over translated corpora is the source text we select in the auxiliary language. The Spanish Unannotated Corpora is a huge corpus. However, it is not domain homogeneous and the topic distribution of this corpus differs significantly from that of a corpus in Basque, especially because it hardly includes the specific topics associated with the Basque Country. Furthermore, we analyzed how tokenizers trained on this corpus do not include many words common in the context of Basque speaker communities, like named entities (locations, people or organizations). See appendix C for a detailed analysis of the vocabulary coverage of each model on the test datasets.\nTo analyze the impact the cultural bias and the domain heterogeneity of the source text has on the performance in downstream tasks, we compiled the S_loc2eu corpus, presented in section 2.2. This corpus is formed by texts in Spanish crawled from newspapers geographically and culturally connected to the Basque Country. Results in Table 2 show that models trained on translated local news (Sloc_BERT and SNloc_BERT), perform better than those without them (S_BERT and SN_BERT), even though it is trained over a much smaller corpus. Following the same pattern, the \n\nCombining Native and Synthetic Data\nThe objective of RQ2 is to test if adding texts translated by MT to a native corpus can boost the performance on downstream NLU tasks of the LM in the target language.\nWith that aim, we trained a new LM on the concatenation of S_beto2eu corpus and the N_ElhBERTeu corpus 4 (SN_BERT hereinafter). Table 2 reports the results for SN_BERT when evaluated on the BasqueGLUE benchmark. Even if SN_BERT surpasses ElhBERTeu in a few tasks (NERC, BEC), it is below it in the average score.\n\nMerging Strategies\nOne factor that may explain the lower performance of the model trained on the combined synthetic and native data is the way of combining the data. Our last experiment aims to analyze different combination alternatives. For SN_BERT, we just concatenate N_ElhBERTeu and S_beto2eu. However, the better quality native corpus is diluted among the translated texts of poorer quality, but larger in size (4x times). Hence, we propose another three alternatives to merge native and translated corpora, shifting the balance between both types of data: concat 20\u221280 (SN_BERT): concatenation of N_ElhBERTeu and S_beto2eu, which roughly form 20% and 80% of the pre-training corpus respectively. As mentioned, synthetic data take the principal role in this configuration.\nconcat 50\u221250 : we oversample N_ElhBERTeu corpus to equal the size of S_beto2eu. This setting gives equal weight to native and synthetic data.\nconcat 80\u221220 : we oversample N_ElhBERTeu up to 80%, thus, pre-training relies on native data mostly. Native data is weighted over synthetic data. sequential: the LM is trained for 750K steps on S_beto2eu, and afterwards for another 250K steps on N_ElhBERTeu 5 .\nResults for different merging strategies are shown in Table 4 . Increasing the ratio of N_ElhBERTeu data in our pre-training corpora improves the performance of our models to the point where concat 80\u221220 outperforms ElhBERTeu, trained only with native text in Basque. Pretraining sequentially does improve slightly the results of the default SN_BERT setting, but weighting concatenation is the best strategy between the two. Further sequential training regimes were tried other than (750k+250K). 'sequential' refers to the best results we achieved with this strategy.\n\nConclusions\nRegarding the RQ1, we conclude from our experiments that LMs trained exclusively on synthetic data from MT can obtain comparable performance to a native LM. We further analyze that other than the quality of MT, the cultural context of the text we select from the auxiliary language do have an effect on the final performance. We conclude that it is better to gather a corpus composed of sources similar to those in the target language, rather than indiscriminately translating vast amounts of data in the auxiliary language.\nFurthermore, with respect to RQ2, our experiments show that state-of-the-art models' performance can be improved by adding translated data during the pre-training, albeit it is a small improvement. Weighting the native data above synthetic data is key to this improvement.\nAll in all, this approach has a big potential for less-resourced languages, since once you have a proper MT system, there is no limit on the amount of data one can translate from languages with bigger corpora available.\nData and models are publicly available 6 .\n", "hypothesis": " In this paper, we study the use of machinetranslated corpora for pre-training LMs.  We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from Spanish.", "answer": true}
{"title": "ISLTranslate: Dataset for Translating Indian Sign Language", "content": "\nIntroduction\nThere are about 430 million hard-of-hearing people worldwide 1 of which 63 million are in India 2 . Sign Language is a primary mode of communication for the hard-of-hearing community. Although natural language processing techniques have shown tremendous improvements in the last five years, primarily, due to the availability of annotated resources and large language models (Tunstall et al., 2022) , languages with bodily modalities like sign languages still lack efficient language-processing systems. Recently, research in sign languages has started attracting attention in the NLP community (Yin et al., 2021; Koller et al., 2015; Sincan and Keles, 2020; Xu et al., 2022; Albanie et al., 2020; Jiang et al., 2021; Moryossef et al., 2020 ; Joshi 1 https://www.who.int/news-room/fact-sheets/ detail/deafness-and-hearing-loss 2 https://nhm.gov.in/index1.php?lang=1&level=2& sublinkid=1051&lid=606 Figure 1 : An example showing the translation of the phrase \"Let's discuss\" in Indian Sign Language. et al., 2022) . The availability of translation datasets has improved the study and development of NLP systems for sign languages like ASL (American Sign Language) (Li et al., 2020) , BSL (British Sign Language) (Albanie et al., 2021) , and DGS (Deutsche Geb\u00e4rdensprache) (Camgoz et al., 2018) . On the other hand, there is less amount of work focused on Indian Sign Language. The primary reason is the unavailability of large annotated datasets for Indian Sign Language (ISL). ISL being a communication medium for a large, diverse population in India, still faces the deficiency of certified translators (only 300 certified sign language interpreters in India 3 ), making the gap between spoken and sign language more prominent in India. This paper aims to bridge this gap by curating a new translation dataset for Indian Sign Language: ISLTranslate, having 31, 222 ISL-English pairs. Due to fewer certified sign language translators for ISL, there is a dearth of educational material for the hard-of-hearing community. Many government and non-government organizations in India have recently started bridging this gap by creating standardized educational content in ISL. The created content helps build basic vocabulary for hard-of-hearing children and helps people use spoken languages to learn and teach ISL to children. Considering the standardized representations and simplicity in the vocabulary, we choose these con-tents for curating an ISL-English translation dataset. We choose the content specific to education material that is standardized and used across India for primary-level education. Consequently, the vocabulary used in the created content covers diverse topics (e.g., Maths, Science, English) using common daily words.\nISL is a low-resource language, and the presence of bodily modality for communication makes it more resource hungry from the point of view of training machine learning models. Annotating sign languages at the gesture level (grouping similar gestures in different sign sentences) is challenging and not scalable. Moreover, in the past, researchers have tried translating signs into gloss representation and gloss to written language translation (Sign2Gloss2Text (Camgoz et al., 2018) ). A gloss is a text label given to a signed gesture. The presence of gloss labels for sign sentences in a dataset helps translation systems to work at a granular level of sign translation. However, generating gloss representation for a signed sentence is an additional challenge for data annotation. For ISLTranslate, we propose the task of end-to-end ISL to English translation. Figure 1 shows an example of an ISL sign video from ISLTranslate. The example shows a translation for the sentence \"Let's discuss\", where the signer does the sign for the word \"discuss\" by circularly moving the hands with a frown face simultaneously followed by palm lifted upwards for conveying \"let's.\" The continuity present in the sign video makes it more challenging when compared to the text-to-text translation task, as building a tokenized representation for the movement is a challenging problem. Overall, in this resource paper, we make the following contributions:\n\u2022 We create a large ISL-English translation dataset with more than 31, 222 ISL-English pair sentences/phrases. The dataset covers a wide range of daily communication words with a vocabulary size of 11, 655.\nWe believe making this dataset available for the NLP community will facilitate future research in sign languages with a significant societal impact. Moreover, though not attempted in this paper, we hope that ISLTranslate could also be useful for sign language generation research. The dataset is made available at: https://github.com/ Exploration-Lab/ISLTranslate.\n\u2022 We propose a baseline model for end-to-end ISL-English translation inspired by sign language transformer (Camgoz et al., 2020) .\n\nRelated Work\nIn contrast to spoken natural languages, sign languages use bodily modalities, which include hand shapes and locations, head movements (like nodding/shaking), eye gazes, finger-spelling, and facial expressions. As features from hand, eye, head, and facial expressions go in parallel, it becomes richer when compared to spoken languages, where a continuous spoken sentence can be seen as a concatenated version of the sound articulated units. Moreover, translating from a continuous movement in 3 dimensions makes sign language translation more challenging and exciting from a linguistic and research perspective.\nSign Language Translation Datasets: Various datasets for sign language translation have been proposed in recent years (Yin et al., 2021) . Specifically for American Sign Language (ASL), there have been some early works on creating datasets (Martinez et al., 2002; Dreuw et al., 2007) , where the datasets were collected in the studio by asking native signers to sign content. Other datasets have been proposed for Chinese sign language (Zhou et al., 2021) , Korean sign language (Ko et al., 2018) , Swiss German Sign Language -Deutschschweizer Gebardensprache (DSGS) and Flemish Sign Language -Vlaamse Gebarentaal (VGT) (Camg\u00f6z et al., 2021) . In this work, we specifically target Indian Sign Language and propose a dataset with ISL videos-English translation pairs. End-to-End Sign Language Translation Systems: Most of the existing approaches for sign language translation (Camgoz et al., 2018; De Coster and Dambre, 2022; De Coster et al., 2021) depend on intermediate gloss labels for translations. As glosses are aligned to video segments, they provide fine one-to-one mapping that facilitates supervised learning in learning effective video representations. Previous work (Camgoz et al., 2018) has reported a drop of about 10.0 in BLEU-4 scores without gloss labels. However, considering the annotation cost of gloss-level annotations, it becomes imperative to consider gloss-free sing language translation approaches. Moreover, the gloss mapping in continuous sign language might remove the grammatical aspects from the sign language. Other recent works on Sign language translation include Voskou et al. (2021); Yin and Read (2020) , which try to remove the requirement for a glossing sequence for training and proposes a transformer-based architecture for end-to-end translations. We also follow a gloss-free approach for ISL translation. In general, we found automatically transcribed text to be of high quality; nevertheless incorrectly generated text was manually corrected with the help of and wondered That's true, the first line is shorter now.\nThat's true, the first line is shorter now. One day, Akbar drew a line on the floor and ordered.\nOne day, Akbar drew a line and ordered Make this line shorter.\nMake this line shorter. Rita is shorter than Radha.\nRita is short and the other is Radha. Rajat is taller than Raj.\nRajat is taller and the other is Raj. but don't rub out any part of it.\nbut don't rub out any part of it. Try to draw Rajat's taller than Raj.\nFirst draw Rajat as taller, then draw Raj on the right. No one knew what to do.\nNo one knew what to do. Each minister looked at the line and was puzzled.\nEach minister looked at the line and was puzzled. No one could think of any way to make it longer.\nNo one could think of any way to make it longer. Have you seen the fine wood carving?\nLook at its architecture. Most houses had a separate Most houses had a separate bathing Beding area. separate and some had wells to supply water. and some had wells to supply water. Many of these cities had covered drains.\nMany of these cities had covered drains. Notice how carefully these were laid out in straight lines.\nNotice how carefully these were laid out in straight lines. content in the books. Figure 2 shows an example (from ISLTranslate) of a long sentence and its translation in ISL. The frames in the figure are grouped into English words and depict the continuous communication in ISL. Notice the similar words in the sentence, \"sign/signs\" and \"language.\" (also see a visual representation of Sign Language 6 ). As common to other natural languages, representations (characters/gestures) of different length are required for communicating different words. In ISLTranslate, we restrict to the sentence/phrase level translations. The dataset is divided into a train, validation, and test splits (Details in App. A). App. Figure 3 shows the distribution of number of samples in various splits.\nComparison with Other Continuous Sign-Language Datasets: We primarily compare with video-based datasets containing paired continuous signing videos and the corresponding translation in written languages in Table 1 . To the best of our knowledge, we propose the largest dataset for ISL.\nData Cleaning and Preprocessing: The videos (e.g., App. Fig. 4 ) contain the pictures corresponding book pages. We crop the signer out of the video by considering the face location as the reference point and removing the remaining background in the videos.\nNoise Removal in ISLTranslate: As the ISLTranslate consists of videos clipped from a longer video using pauses in the available audio signal, there are multiple ways in which the noises in the dataset might creep in. While translating the text in the audio, a Signer may use different signs that may not be the word-to-word translation of the respective English sentence. Moreover, though the audio in the background is aligned with the corresponding signs in the video, it could happen in a few cases that the audio was fast compared to the corresponding sign representation and may miss a few words at the beginning or the end of the sentence. We also found a few cases where while narrating a story, the person in the audio takes the character role by modifying speech to sound like the designated character speaking the sentence. For example, in a story where a mouse is talking, instead of saying the sentence followed by the \"said the mouse\" statement, the speakers may change their voice and increase the pitch to simulate dialogue spoken by the mouse. In contrast, in the sign language video, a person may or may not take the role of the mouse while translating the sentence to ISL. ISLTranslate Validation: To verify the reliability of the sentence/phrase ISL-English pairs present in the dataset, we take the help of a certified ISL signer. Due to the limited availability of certified ISL Signers, we could only use a small randomly selected sign-text pairs sample (291 pairs) for human translation and validation. We ask an ISL instructor to translate the videos (details in App. C). Each video is provided with one reference translation by the signers. Table 2 shows a sample of sentences created by the ISL instructor. To quantitatively estimate the reliability of the translations in the dataset, we compare the English translation text present in the dataset with the ones provided by the ISL instructor. Table 3 shows the translation scores for 291 sentences in ISLTranslate. Overall, the BLEU-4 score is 48.94, ROUGE-L (Lin, 2004) is 60.44, and WER (Word Error Rate) is 61.88. To provide a reference for comparison, for text-totext translations BLEU score of human translations ranges from 30-50 (as reported by Papineni et al. (2002) , on a test corpus of about 500 sentences from 40 general news stories, a human translator scored 34.68 against four references). We speculate high reliability over the translations present in the ISLTranslate with a BLEU score of 48.93 compared against the reference translations provided by certified ISL Signer. Ideally, it would be better to have multiple reference translations available for the same signed sentence in a video; however, the high annotation effort along with the lower availability of certified ISL signers makes it a challenging task.\n\nBaseline Results\nGiven a sign video for a sentence, the task of sign language translation is to translate it into a spoken language sentence (English in our case). For benchmarking ISLTranslate, we create a baseline architecture for ISL-to-English translation. We propose an ISL-pose to English translation baseline (referred to as Pose-SLT) inspired by Sign Lan- guage Transformer (SLT) (Camgoz et al., 2020) . Sign language transformer uses image features with transformers for generating text translations from a signed video. However, considering the faster realtime inference of pose estimation models (Selvaraj et al., 2022) , we use pose instead of images as input.\nWe use the Mediapipe pose estimation pipeline 7 . A similar SLT-based pose-to-Text approach was used by Saunders et al. (2020) , which proposes Progressive Transformers for End-to-End Sign Language Production and uses SLT-based pose-to-text for validating the generated key points via back translation (generated pose key points to text translations). Though the pose-based approaches are faster to process, they often perform less than the image-based methods. For the choice of holistic key points, we follow Selvaraj et al. (2022) , which returns the 3D coordinates of 75 key points (excluding the face mesh). Further, we normalize every frame's key points by placing the midpoint of shoulder key points to the center and scaling the key points using the distance between the nose key point and the shoulders midpoint. We use standard BLEU and ROUGE scores to evaluate the obtained English translations (model hyperparameter details in App.D). Table 4 shows the results obtained for the proposed architecture. Poor BLEU-4 result highlights the challenging nature of the ISL translation task. The results motivate incorporating ISL linguistic priors into data-driven models to develop better sign language translation systems.\n\nConclusion\nWe propose ISLTranslate, a dataset of 31k ISL-English pairs for ISL. We provide a detailed insight into the proposed dataset and benchmark them using a sign language transformer-based ISL-poseto-English architecture. Our experiments highlight the poor performance of the baseline model, pointing towards a significant scope for improvement for end-to-end Indian sign language translation systems. We hope that ISLTranslate will create excitement in the sign language research community and have a significant societal impact.\n", "hypothesis": "This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. The dataset covers a wide range of daily communication words with a vocabulary size of 10,000. The paper also proposes a novel model for end-to-end ISL-English translation based on a modified sign language transformer architecture that achieves state-of-the-art performance on the task. The dataset and the model will greatly facilitate research in sign language translation and have a significant societal impact.", "answer": false}
{"title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning", "content": "\nIntroduction\nDeveloping comprehensive evaluation frameworks (Deng et al., 2021; Yuan et al., 2021; Zhong et al., 2022) that can evaluate multiple humaninterpretable dimensions, such as factual consistency (Kryscinski et al., 2020; Wang et al., 2020) and coherence (Dziri et al., 2019; Huang et al., 2020) , is important for the advancement of Natural Language Generation (NLG). However, similaritybased metrics (Papineni et al., 2002; Lin, 2004; Sellam et al., 2020; Zhao et al., 2019; Zhang et al., 2020) still dominate NLG evaluation in practice. Compared to them, desired multi-dimensional evaluators do not require reference texts for evaluation; and they can easily extend to new explainable evaluation dimensions. Recently, Zhong et al. (2022) developed a unified evaluation framework that can Figure 1 : Our prompt design to evaluate the consistency of the summary in red, illustrated using two in-context examples (in blue). To evaluate other aspects, we remove the source text or replace it with a reference. generalize to multiple dimensions and text generation tasks. However, it relies on the construction of synthetic and auxiliary data for the finetuning of a pre-trained language model, requiring in-depth knowledge and significant engineering effort for each dimension. Furthermore, the inclusion of new dimensions requires (continued) training of the model, and might affect the performance on other dimensions in unforeseen ways.\nIn this work, we propose to use in-context learning (Brown et al., 2020) with large language models (LLMs) -a commonly used method to perform many tasks by utilizing only a few input-output examples -to perform multi-dimensional text evaluation in a unified fashion. Compared to pre-trained evaluators that need specialized supervised training for each dimension, our In-Context learning-based Evaluator (ICE) framework is:\n\u2022 Learning-free. It does not require supervised fine-tuning on large annotated (synthetic) training data, requiring only a handful of samples at inference time. \u2022 Extensible. To evaluate new dimensions, it does not rely on large amounts of human judgments or the construction of new synthetic data, using only a natural language prompt consisting of a small number of example pairs to ascertain the properties associated with a given quality aspect.\nIn this paper, using text summarization as a test bed, we show that with a simple prompt design, ICE is competitive with state-of-the-art trained evaluators on multi-dimensional evaluation of modelproduced summaries, establishing a new state-ofthe-art on dimensions such as relevance and factual consistency. To study the robustness of the evaluator to the selection of in-context examples, we analyze the factors that affect the performance of ICE, such as the number of in-context examples and sampling procedures when picking in-context examples from a set of candidates. We find ICE to be robust to the selection of in-context examples and observe a slight improvement in performance as the number of examples is increased. Finally, in light of the recent work (Goyal et al., 2022) that points to the misalignment of existing evaluation metrics with human preference in evaluating zeroshot summaries generated by LLMs such as GPT-3 (Brown et al., 2020) , we study the effectiveness of ICE in evaluating zero-shot summaries generated by GPT-3. We find that ICE evaluations agree closely with human judgments on such summaries.\n\nProblem Statement\nGiven a sequence x that is input to an NLG system and a system-generated output sequence y, an evaluation framework outputs a score s that captures the quality of y, either with or without the help of a human-generated reference output r. 1 In case of multi-dimensional evaluation where we are interested in assessing y over d quality metrics, we instead get a vector S = (s 1 , s 2 , ..., s d ) over diverse dimensions (e.g., coherence, fluency). Depending on the dimension, there is sometimes a need to condition an evaluation on x (such as to evaluate consistency in summarization). We evaluate our method over four dimensions:\n\u2022 Consistency: The factual correctness of a summary given the source text. \u2022 Relevance: The property of capturing salient information from the source. \u2022 Fluency: A measure of the quality of the individual sentences in the summary. \u2022 Coherence: A measure of the quality, organization, and structure of sentences in the summary.\n1 Specifically for summarization, most learned frameworks evaluate relevance through reference-based evaluation.\n\nPrompt Design & Score Extraction\nICE relies on an LLM (we use the text-davinci-003 model of GPT-3) to make predictions. It takes in a prompt that consists of a small number of in-context examples, each of which consists of generated text and its corresponding quality score as a numeric string. The prompt ends with a test example, for which the model predicts a score (Figure 1 ).\nThe input contains the model-generated text (summary), in addition to which it might contain additional information such as the source text or references, depending on the dimension. To evaluate fluency and coherence, our prompts use in-context examples consisting of generated summaries and corresponding scores. For consistency and relevance, we use the source text and a reference summary respectively, in addition to the generated summary. We pass this prompt to a GPT-3 model, with sampling temperature set to 0 to elicit deterministic responses. We parse the model response-decoded numeric string-as the dimension score.\n\nSelection of In-context Examples\nBy default, we use 4 in-context examples in our prompts, as this is the largest number that fits within the context window of GPT-3. We experiment with two sampling procedures (Appendix B) to obtain 4 examples from a pool of examples:\n1. Uniform Random Sampling. We randomly select 4 summaries from the pool of examples. This causes the examples to follow the same distribution as the example pool. 2. Stratified Sampling. We bucket the range of scores, i.e. [0, 1], into 4 equal partitions and randomly sample one summary from each one.\nThis causes examples to be representative of the range of scores in the example pool.\nWe avoid using synthetically generated data (Kryscinski et al., 2020; Zhong et al., 2022) since the kind of errors made by generation models is often different from the errors present in the negative examples in these datasets (Goyal and Durrett, 2021) . We instead elect to use (a few) human evaluations of model-generated text in order to make the in-context examples as representative of real errors as possible. We do this by splitting the meta-evaluation dataset and using a partition as an in-context example pool, as described in Section 3.1. \n\nDatasets & Baselines\nWe use the SummEval dataset (Fabbri et al., 2020) 2 to meta-evaluate our evaluation framework. Sum-mEval collects human evaluation annotations for 16 summarization systems on 100 articles sampled from the CNN/DailyMail corpus, for a total of 1600 summary-level annotations. Each summary is evaluated on four dimensions described in Section 2.2.\nTo get a pool of in-context examples, we keep aside a small subset (64 examples) of the Sum-mEval dataset to pick in-context examples from, and use the rest (1536 examples) as the test set for meta-evaluation (evaluating the baselines on this same test set). Further details are in Appendix A.\nWe compare ICE to the following state-of-theart multi-dimensional evaluators: (1) CTC (Deng et al., 2021) uses information alignment between generated outputs and references or inputs; (2) BARTScore (Yuan et al., 2021) uses the conditional probability of a sequence given inputs or references; and (3) UniEval (Zhong et al., 2022) uses a question-answering framework (e.g. \"Is this a coherent summary?\") to calculate metrics.\nFollowing Liu et al. (2021) ; Zhong et al. ( 2022), we assess performance by computing summarylevel Spearman and Kendall-Tau correlations between predicted scores and human judgements.\n\nResults\nAs illustrated in Table 1 , ICE is competitive with fine-tuned baselines despite not requiring any finetuning. It achieves state-of-the-art correlation with human judgments for relevance and consistency. We perform pairwise significance tests and observe that ICE (uniform sampling) does better than UniEval on consistency and relevance on Kendall's Tau with a significance level of 0.05 (Appendix E). Additionally, the uniform sampling variant of ICE outperforms BARTScore (which also does not require finetuning) across dimensions.\nBetween the two sampling procedures for ICE, we observe that stratified sampling works marginally better for all dimensions other than consistency. Since summaries in the SummEval dataset have perfect or near-perfect human scores for consistency (Figure 2 ), uniform sampling causes in-context examples to also have nearperfect scores. This appears useful for the model to calibrate its scoring when evaluating consistency, leading to better performance. We explore this in greater detail in \u00a74.1. While the same reasoning could hold for fluency, we observe both here and in \u00a74.3 that fluency scores are quite stable. Given that fluency is an easier aspect to evaluate, this stability could be a result of the model possessing a strong notion about fluency from pre-training time that is not modified significantly as the distribution of in-context examples changes (Reynolds and McDonell, 2021) . Finally, we observe that the performance for coherence and relevance are similar regardless of the sampling procedure. This is because scores for these aspects are spread out in the dataset, which makes uniform and stratified sampling return similar in-context examples.\n\nAnalysis\nIn this section, we analyse the effects of our prompt engineering choices. The comparison between sampling procedures in Section 4.1 is performed on the entire test set but the experiments in Sections 4. domain regardless of the true distribution. This forces predictions towards a centered distribution, which can cause the performance drop we observe in Table 1 when evaluating consistency using stratified sampling. Uniform sampling, on the other hand, selects examples that represent the true distribution, making model predictions more closely reflect the true distribution.\nA drawback of uniform sampling is sub-optimal calibration in low-probability regions of the true distribution. For instance, if uniform sampling is used to evaluate consistency, the model might not see in-context examples with (say) scores less than 0.3 (Figure 2 ). This can affect output calibration in that region. Nonetheless, we suggest using uniform sampling in general. It is more stable and its prediction distribution closely follows the true distribution. For dimensions where it underperforms stratified sampling, the margins are less significant. Finally, even when ICE (uniform sampling) scores are calibrated differently from human scores, they still rank summary-quality correctly, insofar as our main results (Table 1) \n\nEffect of Selection of In-context Examples\nIn order to determine whether performance is robust to the choice of in-context examples, we evaluate our test set using three different random sets of in-context examples. We observe in Figure 3 that for a given dimension, the maximum variation across three seeds is about 7 points, suggesting reasonably stable performance across the choice of in-context examples.\n\nEffect of Number of In-context Examples\nWe evaluate our test set using different numbers of in-context examples (Figure 4 ). We observe that only for relevance and coherence does performance show improvement as we increase the number of examples. One reason for this could be the distribution of scores for a given dimension in the test set (Figure 2 ). Concretely, consistency and fluency mostly have near-perfect scores and therefore do not benefit from more samples while the scores for coherence and relevance are spread out and therefore more samples allow representation over the whole range of scores.\nAnother observation is that even for coherence and relevance, performance with a single incontext example reaches near that achieved by some of the weaker fine-tuned baselines in Table 1 . This suggests that the model possesses the notion of the evaluation task from pre-training itself, which is in line with recent work (Reynolds and McDonell, 2021; Min et al., 2022) that suggests that demonstrations help extract this knowledge.\nFinally, we note that calibration can potentially be improved by increasing the number of examples. For instance, we observed that the four incontext examples that the uniform sampling procedure chose for coherence in Figure 2 had scores that fall between 0.7 and 1.0. This concentrates the prediction distribution in that range. The probability of such an event will reduce as the number of examples is increased further.\n\nUsing ICE to Evaluate Zero-Shot Prompting Models\nRecent work by Goyal et al. (2022) showed that standard reference-based and reference-free metrics are not reliable in evaluating zero-shot summaries written by models such as GPT-3. Through a human study comparing summaries from three systems-GPT-3, BRIO, and T0-they observed that while humans prefer GPT-3 summaries, automatic evaluators consistently score GPT-3 summaries lower than summaries from other models.\nWe study the efficacy of ICE in evaluating zeroshot summaries written by GPT-3 at a dimension level. We use the set of 500 CNN articles from Goyal et al. (2022) , with summaries from GPT-3, BRIO, and T0 for each article. We sample 100 of these articles and have three annotators rate summaries for each of the dimensions defined in Section 2.2 on a scale of {1, 2, 3, 4, 5}. We use ICE, ROUGE, and BARTScore (all of which do not require training data) to evaluate the summaries and present system-level results in Table 2 .\nWe observe that ICE agrees with human judgments for each dimension and overall preferences while existing reference-based and reference-free metrics such as ROUGE and BARTScore 3 consistently rate GPT-3 summaries low. Goyal et al. (2022) suggest that most existing evaluation metrics reward summaries that imitate references, while GPT-3 summaries are zero-shot and not trained to imitate human-written references, which is likely why they are penalized by most existing evaluators. However, since ICE is not based on reference similarity (except when evaluating relevance) and is also not trained with reference summaries, it is able to better evaluate GPT-3 summaries and agrees with human preferences.\n\nConclusion\nWe show that in-context learning can be used for NLG evaluation as an alternative to fine-tuned evaluation metrics. Using a small number of examples, in-context learning evaluators can reach or exceed the state-of-the-art on multi-dimensional evaluation and that this is robust to the choice of in-context examples. Finally, we show that in-context learning evaluators align well with human judgements when evaluating summaries written by GPT-3.\n", "hypothesis": " Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets.  In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets.", "answer": true}
{"title": "Substitution-based Semantic Change Detection using Contextual Embeddings", "content": "\nIntroduction\nMeasuring semantic change is one of the few areas of NLP where contextual embeddings have not yet led to a definitive improvement over previous methods. In particular, the commonly used approach of aligning static embeddings trained on different time periods (Hamilton et al., 2016b) continues to be a surprisingly hard to beat baseline.\nGiven that contextual embeddings provide a representation for each occurrence of a word in context, they would seem to be ideally suited to a more nuanced investigation of semantic change. Most attempts to leverage them for this purpose, however, produce quantitatively worse results, while being less interpretable and requiring more resources.\nHere, we present a simplified and improved approach to scalable, interpretable, semantic change detection using contextual embeddings. Inspired by Eyal et al. (2022) , we work only with the most probable replacements for masked words, and measure semantic change in terms of the distributions of replacements in each time period. Not only does this better match human judgements, it is highly space efficient, works seamlessly for out-of-vocabulary words, and helps intuitively characterize meaning change and variation.\n\nBackground\nMeasuring semantic change involves a set of tasks related to determining if and how a term's meaning has changed over time. Here, we focus on the task of measuring the amount of change that has occurred from one time period to another (Gulordava and Baroni, 2011; Schlechtweg et al., 2020) . 1 Existing approaches to this task are mostly of two types. The first is associating each term with a single vector per time period and measuring the distance between vectors, of which we take Hamilton et al. (2016b) to be representative. As a variation on this, several authors have proposed averaging the output of contextual embedding models to get a single vector per term in each time period, but this has generally not led to an improvement over using static vectors (Martinc et al., 2020a; Kurtyigit et al., 2021; Liu et al., 2021) . A related approach is to represent words in terms of their nearest neighbors using static word vectors (Hamilton et al., 2016a; Gonen et al., 2020) , but this does not show a clear improvement over other static embedding methods (Montariol et al., 2021) .\nA second type of approach begins with various methods for word sense induction, then measures change in terms of the relative prevalence of a term's different senses (Frermann and Lapata, 2016; Hu et al., 2019; Arefyev and Zhikov, 2020; Arefyev and Bykov, 2021) . In some cases, authors simply cluster contextual representations for each term, and measure differences in the distributions of clusters between two time periods, rather than dealing with explicit word senses (Giulianelli et al., 2020; Martinc et al., 2020b; Montariol et al., 2021) .\nDespite the additional information provided by contextual embedding models, methods using type embeddings (as opposed to token), continue to be competitive. For example, on the recent SemEval multilingual semantic change detection task, none of the top four systems used token embeddings (Schlechtweg et al., 2020) . Methods using contextual embeddings have done better on some more recent mono-lingual shared tasks (Kutuzov and Pivovarova, 2021; Zamora-Reina et al., 2022), but have not yet been evaluated with a consistent setup across multiple languages.\n\nMethods\nBuilding on Eyal et al. (2022) , we represent each token in the corpus (or a sufficiently large sample of them) by a small set of probable replacement terms from a contextual embedding model. However, whereas Eyal et al. (2022) did this for the purpose of word sense disambiguation, we do so for the purpose of measuring semantic change.\nFor each sampled occurrence of each term, we mask the term of interest, feed the masked context through a model, and obtain the predicted token probabilities corresponding to the mask token. 2 From these, we save only the top-k most probable words (excluding stopwords and partial word pieces), and discard the rest.\nFor a given term in a particular time period, we then count how many times each word in the model vocabulary has appeared as a top-k replacement for that term, and normalize this by its sum, giving us a distribution over replacements. To obtain a raw score of semantic change between two time periods, we compute the Jensen-Shannon Divergence (JSD) between the two distributions representing the same term in different time periods. However, as we show below, the raw JSD scores are strongly correlated with term frequency. Thus, to obtain a scaled metric, we convert the raw JSD scores into a quantile, comparing the raw score for a term of interest to other terms with similar frequency.\nCompared to saving the full output vector per token, this approach only requires a miniscule amount of storage per token, and thus does not require the kind of heuristic dropping of tokens employed by Montariol et al. (2021) . In addition, the dominant meanings of a word in each context can be summarized by the terms which occur most fre-quently among the top-k replacements. Although such replacements are limited to the terms which exist in the model vocabulary, in practice this is sufficient to represent a nuanced set of meanings, and works even for words which get tokenized into multiple word pieces, as we show below.\nMore formally, given two corpora C1 and C2, let the count of token v as a top-k replacement for term t in corpus c be:\nEQUATION\nwhere R(t, i, k) is the set of top-k most probable replacements for occurrence i of term t (excluding stopwords and partial word pieces in the model vocabulary), and N c (t) is the number of sampled occurrence of term t in corpus c. 3 Let \u2206 c t by the distribution of top-k replacement counts for term t in corpus c, obtained by dividing the corresponding vector of counts (i.e., [count(\u2022, t, c)]) by the sum over the model vocabulary. The raw change score for term t is given by the JSD between the two distributions:\nEQUATION\nFinally, we correct for frequency effects by rescaling the raw JSD scores against the scores for terms with similar frequency as the target term, giving us a quantile scaled in [0, 1]:\nscaled(t) = \u03a3 s\u2208T (t) I[raw(t) \u2265 raw(s)]/|T (t)|,\n(3) where T (t) is the set of terms with similar frequency to term t (excluding term t itself). More specifically, we compare against all terms within a fixed factor of the target frequency:\nEQUATION\n) where fr(t) is the frequency of term t in the corpus, with window factor F .\n\nData\nWe use five datasets with words labeled in terms of semantic change between two time periods. Four of these are from SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection (SE; Schlechtweg et al., 2020) . These datasets contain 31 to 48 terms from four languages, graded in terms of change by human raters, along with accompanying corpora to be used in estimating the amount of change. The fifth dataset (GEMS) comes from Gulordava and Baroni (2011), and contains 100 words labeled in terms of semantic change from the 1960s to 1990s. As with most recent papers which use this dataset, we use the Corpus of Historical American English (COHA; Davies, 2010) for measuring change in the GEMS words.\n\nExperimental Details\nFor each dataset, we fine tune an appropriate BERT model to the union of the two associated unlabeled corpora using continued masked language model training with the HuggingFace transformers package. We then index the corpora to find all occurrences of each word. For all target words, along with a random set of 10,000 background terms, we randomly sample up to 4,000 occurrences of each from the associated corpora. We process all sampled tokens as described above to obtain and store the top-k replacements for each, with k = 5. Using the replacements obtained from the model, we compute raw JSD scores for each term. Finally, we convert these to scaled scores by comparing to the background terms that have frequency within a factor of two of the target term (i.e., F = 2).\nFollowing past work, we evaluate using Spearman correlation with human ratings, comparing against the best results from recent papers. In particular, we include two results based on slight variations on Hamilton et al. (2016b) , one of which was the best performing method in the SemEval competition (P\u00f6msl and Lyapin, 2020), as well as methods using contextual embeddings (Martinc et al., 2020b; Montariol et al., 2021) . For fully experimental details, please refer to Appendix A.\n\nResults\nFull results are given in Table 1 . Although our method is not uniformly better than all previous methods on all dataset, it does produce the best result on average, as well as improvements on GEMS, SE English and SE Latin. 2 are labeled.\nAs an example to better understand these results, the raw JSD scores from our method are shown in Figure 1 (top) for the SE English data, with select terms labeled. As can be seen, there is a strong relationship between term frequency and raw JSD, hence the need to rescale the raw scores relative to terms with similar frequency. After rescaling, we see a strong correlation between our final semantic change scores and the human ratings, as shown in Figure 1 (bottom) for the SE English data.\nAs with the approach of Hamilton et al. (2016b), our method supports direct interpretation of semantic change. To understand the change in a word's typical usage, we can look at the overall most common replacements from each time period. Table 2 shows the scores and rankings of several selected terms from SE English, along with the most common substitutes from each time period.\nLooking at the results, we can see, for example, strong agreement with human annotators on a dramatic change in the meaning of plane (comparing 1810-1860 vs. 1960-2010) , from the geometric concept to the flying machine. On the other hand, our results suggest that human raters may have slightly underestimated the amount of change in the meaning of graft, which was previously used mostly in reference to vegetation, but now most commonly refers to corruption. 5 By contrast, ounce may be a case where our method has underestimated the change that has taken place. Older usages seem to map more generically to a wider range of quantities (hence the appearance among the early substitutes of hour, acre, and dollars), whereas modern usage seems more restricted. Indeed, we do find some difference in the distribution of substitutes between the two time periods, but less of a difference than is typical for words with similar frequency, hence the low final score from our method (see Figure 1 ).\nAlthough we do not emphasize it in this paper, of our method can easily be combined with the approach of Eyal et al. (2022) to further investigate meaning changes, by inferring senses from the term replacements, and looking at how their usage varies by time period. In particular, for each target term, we can construct a graph from the set of term substitutes (as nodes), where edge weights represent the number of top-k clusters in which two substitutes co-occur. Following Eyal et al. (2022) , we experiment with Louvain community detection to identify sense clusters from these graphs for each term of interest, and use Jaccard similarity to associate each mention with a sense cluster, based on substitute overlap (see Appendix A for details).\nInspecting the distribution of these senses over time helps to distinguish the gradual adoption of existing senses from the creation of new ones. For example, the most common sense of plane is captured by the sense cluster {aircraft, jet, airplane, car}, and as expected, this sense is not found in the 1810-1860 English data, except for two instances which appear to be errors in the inferred sense. By contrast, the second most common sense-{planes, line, point, surface}-appears in both time periods, but is much more common in the earlier time.\nThis approach also provides more insight into how the meaning of graft has changed. The most common sense cluster is the horticultural meaning {tree, plant, stock, vine}, and this meaning occurs in both time periods, but is much more common in the earlier one. A second cluster, corresponding to illicit activity-{corruption, violence, bribery, fraud}-occurs only in the later time period. This clustering method also surfaces a third sense with a medical meaning-{transplant, surgery, disease, drug}-which is not revealed by the top few overall most common replacements given in Table 2 .\n\nDiscussion and Related Work\nAs noted by others, new and larger datasets for rigorously evaluating semantic change are badly needed (Tahmasebi et al., 2021) . Existing datasets are relatively small, and are mostly based on inspecting a limited number of examples per term. Unfortunately, determining ground truth for semantic change is challenging, and producing such resources is costly. Ideally, future datasets for evaluation should be larger, both to allow for more robust evaluation, and to have sufficient targets for both hyperparameter tuning and evaluation.\nIn addition to the dataset we have used in this paper, two others are available from shared tasks on Spanish and Russian, respectively (Kutuzov and Pivovarova, 2021; Zamora-Reina et al., 2022) . Both of these are comparable in size to the GEMS dataset used here. Unfortunately, they are less useful for evaluation because most submissions to these shared tasks only evaluated on the task data, and not on other datasets. As shown by the replication of Martinc et al. (2020b) in Montariol et al. (2021) , a method can sometimes perform well on one language but fail to generalize to others. As such, we have based our evaluation on datasets for which there has been a consistent evaluation of methods across multiple languages. As future work, a careful replication study of all methods from each competition on all available datasets, including an assessment of sensitivity to hyperparameters, would be highly informative.\nBesides Eyal et al. (2022) , The closest prior work to ours is Kudisov and Arefyev (2022) , who use dynamic patterns to generate many variations on example usages sampled from the given corpora. These variations are then used to generate hundreds of replacement terms from a masked language model with associated probabilities. These probabilities are averaged (heuristically combining replacements with differing numbers of word pieces) to obtain a mean vector for each sampled instance. Finally, semantic change is computed as the average cosine distance between all pairs of vectors across corpora. This method was evaluated as part of the LSCDiscovery shared task on Spanish (Zamora-Reina et al., 2022) . Preliminary work on this method was described in Arefyev and Bykov (2021) , where a slightly different version of it was evaluated on the RuShiftEval shared task on Russian (Kutuzov and Pivovarova, 2021) .\nCompared to Kudisov and Arefyev (2022), our approach is considerably simpler, and better suited to storing representations of a complete corpus for subsequent analysis and exploration. In particular, we only consider a small number of substitutes for each example (storing only the top-k most probable terms, without the associated probabilities). We do not use dynamic patterns, and only consider terms in the model vocabulary as potential substitutes. We also associate each term with a single distribution over the model vocabulary per time period (not per mention), and use Jensen-Shannon divergence to more naturally measure the distance between distributions. Importantly, we also correct for frequency effects, as described above.\nAlthough our approach avoids the onerous storage requirements of methods which save full contextual vectors, it still requires considerable processing time to obtain the top-k replacements for all tokens. Future work could explore smaller or more efficient models for this purpose. 6 Finally, despite its simplicity, measuring the cosine distance between aligned static vectors remains a strong and efficient baseline (Hamilton et al., 2016b) . More work is needed to determine where contextual embeddings can offer sufficient advantage in measuring semantic change to justify their greater computational cost.\nCompared to static embeddings, our approach is weakest on the German and Swedish datasets, which could relate to the quality of the pretrained models that are available for those languages, the data used for pretraining, or perhaps issues that arise in tokenization of the reference corpora. For a tentative exploration of some possible factors, please refer to Appendix C.\n\nConclusion\nWe have presented a simplified and improved approach to measuring semantic change using contextual embeddings, based on the Jensen-Shannon Divergence between the distributions of the most probable replacements for masked tokens in different time periods, corrected for frequency effects. This approach achieves superior performance on average, while remaining directly interpretable, with vastly reduced storage requirements.\n", "hypothesis": "Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a complex approach to measuring semantic change using contextual embeddings, relying only on the least probable substitutes for masked terms.", "answer": false}
{"title": "Curating Datasets for Better Performance with Example Training Dynamics", "content": "\nIntroduction\nBreakthroughs in NLP are often the result of scaling up existing models in size and depth, and perhaps even more importantly-data (Hoffmann et al., 2022) . To improve data quality, it has become common practice to train models on data that has been cleaned to some extent using simple heuristics (e.g. by removing non-language tokens), but not otherwise optimized for better performance. While some data-filtering approaches have been suggested to improve Out-Of-Distribution (OOD) generalization (Le Bras et al., 2020 , Swayamdipta et al., 2020) , they usually result in a decrease of In-Distribution (ID) performance.\nWe propose a method for curating datasets for better performance in both ID and OOD testing, enhancing data quality rather quantity. Our method is orthogonal to model architecture or size, and as such can be used alongside any LM to further improve results over specific tasks.\nTo implement our method we use the concept of Example Training Dynamics (ETD; Swayamdipta et al., 2020) , which builds on the idea that the training process of models sheds light on the relative importance of specific examples within the datasets used. Specifically, Swayamdipta et al. (2020) have shown that over several epochs of training, a model may predict some examples in a dataset less consistently than others, and that those \"ambiguous\" examples are important for OOD generalization.\nWe propose a new method for computing ETD, as well as a new paradigm for using them in training. We show that by computing ETD over separate training processes (rather than over consecutive epochs of the same training process), and using ETD to weigh the importance of each example in the dataset, we can train a DeBERTa model (He et al., 2020) on the weighted versions of several NLI and multiple-choice datasets, improving average performance by 0.35% for ID testing and by 0.95% for OOD.\nWe next demonstrate that ETD can be transferable across models, i.e., ETD computed from the training process of model M 1 can be used to weigh a dataset, and train model M 2 on it with improved results, where M 2 differs from M 1 in initial weights, structuring details and pre-training scheme. Though we only show that for a specific use-case, if transferability of ETD holds generally, it may allow us to create weighted versions of datasets once, and use them for multiple training scenarios, including that of future models.\nFinally, we propose Dynamic Training, a method for training a model while computing ETD and reweighing the training set between epochs. This method performs on par with our transferability method while requiring no additional compute beyond that of standard training, which makes it even more applicable under low computational budgets.\nOur proposed method of computing ETD may allow practitioners to get more value out of their existing datasets, and pave the way towards similar methods to be used for improving large scale language model training. We publicly release our code, as well the weighted versions of the datasets used in this work. \n\nComputation of Example Training Dynamics\nWe introduce a new method for working with Example Training Dynamics, which has two separate components: the computation of ETD, and their application in training new models. Fig. 1 illustrates the full pipeline of the two components as described below. Let D = {(x i , y i )} N i=1 be a dataset of N examples x i with corresponding labels y i , and let M be a model with initial parameters \u03b8 that defines a probability distribution over the possible labels for examples in D.\nTo compute the ETD of D with respect to M, we train M on D for one epoch, and save the probabilities M assigns to the possible labels of each example in D. We repeat the process E times with the same experimental setting at each iteration, except for the random seed, and for the ordering of the examples in D, which is random and different at each iteration. After the first time M sees an example during training, it learns a bias towards its true label, and, if the same example is encountered again, this bias might affect the probability assigned to the different labels. To prevent this kind of bias and compute more informative ETD, the parameters \u03b8 are being reset between iterations.\nUsing the probabilities accumulated over all E iterations, we compute the two ETD metrics. The confidence of an example x i w.r.t. M is defined as the mean probability M assigns to x i 's true label, y * i , across iterations:\n\u03bci = 1 E \u03a3 E e=1 P Me (y * i |x i )\nWhere P Me (y * i |x i ) is the probability M assigns to the gold label y * i in iteration e. In a slight abuse of notation, we use the term P Me , though in practice the probability function P may change between examples even within the same iteration, as training progresses and the parameters of M change. 2 The variability of an example w.r.t. M is defined as the standard deviation of said probability:\n\u03c3i = 1 E \u03a3 E e=1 (P Me (y * i |x i ) \u2212 \u03bci ) 2\nWe follow Swayamdipta et al. (2020) and refer to high-confidence examples as easy-to-learn (for M), and high-variability examples as ambiguous. Swayamdipta et al. (2020) have shown that training only on high-variability (or ambiguous) examples can lead to better out-of-distribution (OOD) performance, at a small cost for in-distribution (ID) performance. In this work, we show that they can be used to improve both ID and OOD performance.\n\nUsing ETD\nLet D = {(x i , y i )} N i=1 be a dataset whose ETD are computed w.r.t. some model M 1 , and let 0 \n< f < 1, b \u2208 N. We define V f (D) as the f \u2022 N highest-variability examples in D. To train a model M 2 using ETD, we construct the dataset D (f,b) : for each example (x i , y i ) \u2208 D, if (x i , y i ) \u2208 V f (D),\n\nTraining with ETD Improves Performance\nWe follow Swayamdipta et al. (2020) and first test the capacity of our method to improve the performance of a model of the same architecture as the model used for computing the ETD. We use six tasks, divided into three groups. Three multiple choice tasks: WinoGrande (WG; Sakaguchi et al., 2019) , Abductive NLI (\u03b1NLI; Bhagavatula et al., 2019) , and HellaSwag (HS; Zellers et al., 2019) ; Two NLI tasks: SNLI (Bowman et al., 2015) and ANLI (Nie et al., 2019) ; and a question answering task: BoolQ (Clark et al., 2019) .\nFor each of the tested tasks, we compute ETD using one copy of DeBERTa-large (He et al., 2020) . Following Swayamdipta et al. (2020) , we use E = 5 as the number of iterations for the ETD computation process. We then train a second copy of DeBERTa on the ETD-weighted dataset D (.25,3) . The specific values of f and b are chosen using a grid search for the best mean performance over the development sets of all tasks. 3 Due to computational constraints, we do not tune any hyperparameters other than those defined specifically for this work, i.e. f and b. For other hyperparameters such as learning-rate and batch-size we follow the values used in Swayamdipta et al. (2020) for training on the SNLI (Bowman et al., 2015) dataset.\nFor each task, we test the trained model on its designated test set to evaluate ID performance, as well as on the test sets of all other tasks from the same group to evaluate OOD performance (e.g., we evaluate a model trained on WinoGrande on \u03b1NLI and Hellaswag as well).\nAs baselines, we train DeBERTa on three additional datasets:\n\u2022 D is the original, unaltered version of each dataset.\n\u2022 D (.33) -NR is the dataset resulting from the approach of Swayamdipta et al. (2020) Each training process is repeated with s = 5 different seeds, and the reported result is the mean result across seeds. For each task, we train for a fixed number of steps regardless of the size of the dataset we train on. The fixed number of steps is task-dependent, and is the number of steps required to pass E = 5 times on D and compute its ETD.\nTable 1 shows the full results of this experiment. Table 2 provides a summarized version of the results, as the improvement gained by training on D (0.25,3) compared to D in each task, on both ID and OOD test sets. Our method is the only one to outperform training on D across all 14 categories, obtaining mean improvement of 0.35% ID and 0.95% OOD. It also outperforms Dataset Cartography's approach (D (.33) -NR) on 11/14 categories, and the ablation version (D (.25,3) -NR) on 10/14 categories, demonstrating the importance of the weight reset.\nThese improvements are statistically significant: modelling the result of the baseline method in each category C as a sample from a normal distribution N C , the probability of outperforming the baseline when sampling from the same N c is 0.5. Therefore, the probability of outperforming the baseline on all 14 tasks when sampling from their respective distributions can be calculated using a Binomial Random Variable B(14, 0.5), which gives p-value \u2264 P [B(14, 0.5) = 14] = 0.00006.\n\nETD are Transferable\nThe process of computing ETD is costly in terms of compute, requiring compute roughly equivalent to that of training the desired model. Thus, computing the ETD of a dataset separately for every model we wish to train is expensive, and, depending on the size of the dataset, may become prohibitively so. This problem can be bypassed if ETD are transferable across models, i.e., if ETD can be computed using a model M 1 , and then used to train a different model M 2 .\nTo test for transferability, we use DeBERTa as the ETD-computing model, and ELECTRA (Clark et al., 2020) as the training model. We conduct experiments similar to those in Section 3, with the exception that the training model M 2 is ELECTRA. Tables 3 and 4 show the results and summarized results of these experiments, respectively.\nWhen computing ETD with DeBERTa and creating the respective D (0.25,4) , training ELECTRA on D (0.25,4) outperforms training on D in 5 out of the 6 ID categories, and in 9 out of all 14 categories. Though not as consistent as our main method, the ETD transfers well, with mean improvement of 0.2% ID and 0.33% OOD. 5 and 6 show the results and summarized results of these experiments.\nDynamic Training outperforms training on D in 5 out of the 6 ID categories, and in 11 out of all 14 categories. Though the ANLI task suffers decrease in performance, results for the other tasks improve relatively consistently, with mean improvement of 0.23% ID and 0.38% OOD. Though not as effective as ETD-weighted training, Dynamic Training improves performance in the majority of the cases, without any pre-processing of the data.\n\nRobustness to Hyperparameter tuning\nThroughout this work we report results training models on ETD-weighted datasets of the form D (f,b) , with specific values chosen for f, b. These values are hyperparameters, chosen using a grid search for best performance over the development set of the different tasks. Table 7 shows the other values of f, b we tested for in Section 3, and their model's respective improvement in performance. All values for f, b improve performance over the baseline in at least 8 out of 11 tests, with 85/99 of all test scores being positive, implying our method is robust with respect to its hyperparameters.\n\nRelated Work\nPrevious research offered various methods that consider the relative importance of examples within a dataset in order to improve training. Methods such as Curriculum learning (Bengio et al., 2009) or self-paced learning (Kumar et al., 2010) Core-set selection (Wei et al., 2013) uses submodular functions to find a subset of a dataset representative of the whole, to be used under low computational budget. Conversely, our approach uses training metrics to find a subset not necessarily representative of the whole, but rather one that can be emphasized within the whole dataset to improve performance, regardless of budget considerations. Similarities can also be drawn between our approach and active learning (Settles, 2009; Peris and Casacuberta, 2018; P.V.S and Meyer, 2019) , which searches unlabeled data for the most useful examples to label and uses them for training. Sanh et al. (2020) aim to remove biases from models without re-sampling of the dataset, using Product of Experts between a weak (biased) model and a main model and achieving improvements over OOD test-sets. Karimi Mahabadi et al. ( 2020) suggested a somewhat similar approach of de-biasing a main model by contrasting it with a \"bias-only\" model, to achieve OOD improvements in tasks where the bias is known. Nam et al. (2020) also used biased models as a foil to a main model, to achieve de-biasing in vision-related tasks. Utama et al. (2020) proposed a debiasing method that improves OOD testing while maintaining ID test results. Their method regulates the model's confidence over biased examples in the dataset, using knowledge distillation in combination with biased models. This approach requires a-priori knowledge of the dataset's biases in order to formulate the biased model, and as such is not applicable to many NLP tasks. It also requires a large amount of compute, as it trains a full-size teacher model and a biased model besides the main model.\n\nConclusion\nWe presented a new method for computing Example Training Dynamics, which can be used to increase both ID and OOD performance, without any changes to model size or architecture. We demonstrated that ETD can be transferable, i.e., they can be computed once and used many times for different models, reducing the computation cost at a long term. Finally, we have shown that ETD can be computed on the fly using Dynamic Training, which may hold the key to improved performance using ETD at no extra compute cost.\nAs the field of NLP leans more and more into the self-supervised pre-training paradigm, further research on ETD may be focused on adjusting our method for larger and self-supervised datasets in order to improve and reduce the cost of pre-training as a whole.\n", "hypothesis": " Finally, we suggest an active learning approach for computing ETD during training rather than as a preprocessing step-an approach that is not as effective, but dramatically reduces the extra computational costs..", "answer": true}
{"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n", "hypothesis": "We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. Our evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is enhanced when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.", "answer": false}
{"title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization", "content": "\nIntroduction\nDeveloping a persona-consistent dialogue model has been one of the key issues and crucial problems in open-domain dialogue systems (Huang et al., 2020) . Zhang et al. (2018a) define the problem of personalized dialogue generation, which aims to generate personalized responses based on textually described persona profiles. Many efforts have been made on developing dialogue models that generate responses consistent with the provided persona profile (Song et al., 2019 (Song et al., , 2020a,b;,b; Wu et al., 2020a) .\nThe recent development in transformer-based pre-trained models (Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019; Chen, 2020) has led to great successes in dialogue systems (Wolf et al., 2019; Wu et al., 2020b; Ham et al., 2020; Kulh\u00e1nek et al., 2021; Cao et al., 2022; Deng et al., 2022b Deng et al., ,c, 2023)) . Inspired by these successes, previous works incorporate those pre-trained models in persona-based response generation by concatenating the dialogue history and persona as input to generate the response in an auto-regressive manner (Song et al., 2021; Liu et al., 2022) . However, a fine-tuned model can generate a high-quality and persona-consistent response in a certain ordering of personas, while varying this order may lead to a generic and even inconsistent response as illustrated by the example in Figure 1 . We empirically show that the worst ordering of persona can lead to a 29.4% decline in BLEU score compared with the best ordering.\nIdeally, a well-trained dialogue generation model should be able to generate a persona-consistent response regardless of the ordering of personas in the input. We perform experiments and analyses to identify the cause of the ordering sensitivity. We find that the ordering of persona in the input leads to different representations of context and response. We also show that the model can attend to the appropriate persona and generate high-quality responses under some representations but not under others. This leads to instability in response generation.\nMotivated by the above findings, we propose ORder Insensitive Generation (ORIG), which is a simple and effective framework that helps models learn more robust and better representations for different persona orders. More specifically, we formulate ORIG as a constrained optimization problem, which optimizes a persona response generation objective under the constraint: given different orderings of persona, the response representations of the model are the same. Then we optimize it through a stochastic optimization approach.\nExperimental results on the Persona-Chat dataset show that ORIG significantly improves the robustness of pre-trained models (GPT2 (Radford et al., 2019) and BART (Lewis et al., 2020) ) under different orderings of input persona, as well as advances their generation performance.\nIn summary, our contributions are threefold: (1) We identify the order sensitivity problem in persona dialogue generation and conduct an empirical analysis to reveal its underlying reasons. ( 2) We propose a model-agnostic framework, ORIG, that helps different persona dialogue models learn robust representations while achieving better performance. (3) We perform extensive experiments on the Persona-Chat dataset, showing that ORIG outperforms previous models and is more robust and less sensitive to different persona orderings.\n\nRelated Work\nMaintaining a consistent persona is essential for building a human-like dialogue system, where most works regard persona as a set of sentences along with each dialog (Zhang et al., 2018a; Gu et al., 2019; Song et al., 2019; Wu et al., 2021; Cao et al., 2022; Deng et al., 2022a) . Song et al. (2021) disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation while Cao et al. (2022) aims to alleviate the problem of limited data by data manipulation methods. Despite satisfactory performance in previous work, the impacts of different orders of personas are still under-explored, resulting in unstable and inconsistent responses.\nOur work is also related to work on order sensitivity in prompt-based few-shot learning (Zhao et al., 2021; Lu et al., 2022) . Zhao et al. (2021) found that the different order of training examples in the prompt can cause accuracy to vary from near chance to state-of-the-art in the few-shot clas- sification setting. Similarly, order sensitivity for In-context Learning also exists regardless of model size and the prompt format (Lu et al., 2022) . Distinguishing from them, we focus on order sensitivity in the language generation task in finetuning setting, especially the impacts of persona orderings to generate persona-consistent responses.\n\nOrder Sensitivity Problem and Analysis\nIn this section, we first illustrate the seriousness of the order sensitivity problem by showing a huge performance fluctuation in persona dialogue models when fed the same personas in the best and worst orders. Then we analyse why their performance is volatile to different persona orderings.\nTo illustrate the problem, we finetune PLMs on the Persona-Chat by concatenating the persona and dialogue context together to predict the target response, including GPT2 and BART. After the training converges, we test them on two settings: (1) the best case: for each test sample, we feed the models all possible permutations of persona sentences and keep the maximum score for each sample as the final score; (2) the worst-case: perform the same process as (1), but take the minimum score. Table 1 shows the results for two models. Surprisingly, we find the ordering of input persona has a big impact on the models' performance: GPT2's worst case is 29.4% lower than its best case, while BART's is 83.2% lower.\nMoreover, we find that the huge fluctuation in models' performance is closely related to the response representation changes caused by different orderings of input persona sentences. Concretely, we measure the similarity of the responses representation of the same test sample under different input orders of persona. We show their token-level similarity in the sponse should be zero. However, their distances are significantly higher than zero. It reveals that the models behave more likely a left-to-right language model whose representation is prone to the different orderings of the previous input (e.g. persona sentences). That is highly undesirable for a robust personalized dialogue model. Thus, regularization of representation for the response tokens is necessary to help personalized dialogue models capture order-invariant representation.\n\nMethod\nWe introduce the proposed framework, named ORIG: ORder Insensitive Generation (ORIG). As shown in Figure 2 , we transform the persona ordersensitivity problem as a constrained optimization problem that optimises a persona dialogue model under the uncertainty of the input persona order.\n\nProblem Formulation\nGiven the dialogue context C = {u 1 , . . . , u m } and a set of persona descriptions P = {p 1 , . . . , p n }, the goal is to generate a personalized response r.\nFormally, the generation problem can be formulated as the following chain rule:\nP (r|C, P ; \u03b8) = T t=1 P (r t |r 1:t\u22121 , C, P ; \u03b8) (1)\nwhere \u03b8 is the parameters of the dialogue model.\n\nORIG Framework\nAccording to the analysis in Section 3, the observation reveals that varying the order of input personas leads to different representations of the dialogue response, thus resulting in fluctuations in performance.\nTo learn more robust and consistent representations, we propose the ORIG framework that complements the response generation process with a constraint: given the different orderings of a persona, the model's response representations need to be the same.\nThen the order-insensitive personalized dialogue generation problem is modelled as the following constrained optimization problem where P (r|C, P ; \u03b8) are the model's predictions over the dialogue response, D denotes the dialogue corpus, and the function D is KL divergence to measure the difference between two distributions, and the Shuffle operator samples each persona ordering uniformly from the full permutation of P .\n\nOptimization\nAs for optimization, we first apply the Lagrange multipliers strategy to convert the constrained problem into an unconstrained problem L \u03b8 = \u2212 log P (r|C, P ; \u03b8) +\u03b3 \u2022 D[P (r|C, P ; \u03b8), P (r|C, P ; \u03b8)] (6\n)\nwhere \u03b3 is the multiplier corresponding to the equality constraints (3). Then we can update the parameters \u03b8 of dialogue models by stochastic gradient descent.\n\nExperimental Setups\nDatasets We evaluate the models on the Persona-Chat dataset (Zhang et al., 2018a) , where each dialogue session has at least 6 turns of interactions.\nAnd each interaction is conditioned on a persona that is described with 5 profile sentences. proposed ORIG. Our implementation was based on HuggingFace's Transformers library (Wolf et al., 2020) . During training, the learning rate is set as 2 \u00d7 10 \u22125 , and the batch size for GPT2 and BART is set as 64 and 32, respectively. We trained both models for 10 epochs with Adam (Kingma and Ba, 2015) optimizer until they converged. During decoding, We employ a top-p (p=0.9) (Holtzman et al., 2020) plus top-k (k=50) sampling strategy, which is used to avoid sampling from the unreliable tail of the distribution (only consider a subset of vocabulary composed of k words with the highest probability or some most probable words whose sum of probabilities equals p at each decoding step).\nThe random seed for all experiments is set to 42. Evaluation Metrics We perform both automatic and human evaluations. (1) Automatic metrics: We adopt BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) , Entropy (Zhang et al., 2018b) and CIDEr (Vedantam et al., 2015) for lexicalbased measurement. Following previous work, we also adopt the C-score (Madotto et al., 2019) (Fleiss, 1971) .\n\nExperimental Results\nImproves performance in the original test set Table 3 shows different models' performance in the original test set without any modifications (for ORIG, \"Shuffle\" is used during training but is optional during testing. The Table 3 caption signifies the absence of \"Shuffle\" during testing. This is to evaluate if ORIG performs well in the normal setting). From automatic metrics, we can see base models trained with our ORIG framework outperform the baselines. It justifies that our framework can be applied to different models to improve their performance. From human evaluation results, models with ORIG are superior to others on almost all metircs, especially on GPT2. This is consistent with the results of automatic metrics. The average kappa value of the annotation is 0.632, indicating good agreement during human evaluation.\nReduces variance and improves mean and worstcase performance Figure 3 shows that aside from reducing the variance, ORIG also improves mean and worst-case performance (detailed results in Table 4) across two models consistently, especially in GPT2 (the worst case performance is very close to the best case). We reduce the variance on GPT2 and BART by 91.6% and 51.8%, respectively. Meanwhile, we improve worst-case performance by 20.3% and 22.6% on GPT2 and BART respectively. The only drop is the best case. This is because our distance function D is unidirectional, which pulls in the two representations in Equation 3indiscriminately, causing the best case to go down and the worst to go up. We leave more complicated and directional distance constraints for future studies.\n\nConclusion\nWe show that the current practice of applying pretrained models to the personalized dialogue generation task is volatile across different input orders of personas. Through the analysis, we find that the problem arises from the representation changes induced by the input changes. Motivated by these, we propose our ORIG, a model-agnostic framework for finetuning the persona dialogue model such that it obtains a persona order-invariant representation.\nExperiments on two dominant pre-trained dialogue models show that our framework improves performance and reduces order volatility.\n", "hypothesis": " Experiments on Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART).", "answer": true}
{"title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation", "content": "\nIntroduction\nMultilingual neural machine translation (MNMT) enables translation between unseen language pairs, i.e., zero-shot translation (ZST) (Johnson et al., 2017; Firat et al., 2017) . Prior studies have explored techniques such as language tags (Wu et al., 2021) , residual connections (Liu et al., 2021) , and novel training objectives (Al-Shedivat and Parikh, 2019; Pham et al., 2019; Arivazhagan et al., 2019; Gu et al., 2019; Zhu et al., 2020; Zhang et al., 2020; Wang et al., 2021; Yang et al., 2021) for improving ZST. They primarily used the Transformer architecture (Vaswani et al., 2017) , which has two variations depending on the position of layer normalization (LayerNorm) (Ba et al., 2016) , namely, PreNorm (applied at the input of layers) (Baevski and Auli, 2019) and PostNorm (applied after residual connections), as shown in Fig. 1 . As previous studies showed that PreNorm can result in more stable training and faster convergence compared to PostNorm for MNMT (Xiong et al., 2020) , most ZST works (Pham et al., 2019; Wu et al., 2021; Liu et al., 2021) use PreNorm as the default setting following those MNMT studies. However, Xu et al. (2019) revealed that PreNorm carries the risk of overfitting the training data. We thus hypothesize that in a multilingual scenario, PreNorm may overfit supervised directions and have poor ZST generalizability. We systematically explore PreNorm and PostNorm's effect on ZST to verify this.\nUsing the OPUS, IWSLT, and Europarl datasets and a total of 54 ZST directions, we show that PostNorm consistently outperforms PreNorm by up to 12.3 BLEU points. Following previous work, we also evaluate different language tag (Wu et al., 2021) and residual connection (Liu et al., 2021) settings, as they have been shown to impact ZST but we observe that PostNorm continues to be superior thereby lending credibility to our hypothesis.\nTo better understand the performance differences, we introduce a novel analysis approach called layer-wise language recognition (LLR), which tracks the off-target rates for each encoder and decoder layer by training token-level classifiers to recognize the source or target language. This analysis shows that PreNorm is more sensitive to language tag settings than PostNorm, negatively impacting ZST performance. Additionally, by examining the unraveled view of PreNorm (Fig. 1 ) inspired by Veit et al. (2016) , we reveal structural flaws in PreNorm for ZST. Our analysis demonstrates that the order of LayerNorm and selfattention/feed-forward network in PreNorm is the main factor affecting its ZST performance.\nGiven the prevalent use of PreNorm as the default setting in ZST baselines and frameworks such as Fairseq (Ott et al., 2019) 1 and Ten-sor2Tensor (Vaswani et al., 2018) , our study emphasizes the importance of careful consideration in the LayerNorm setting for ZST.\n2 Background: LayerNorm LayerNorm (Ba et al., 2016) normalizes the input x by zero-centering and scaling to have a unit standard deviation, followed by an additional trainable transformation, including a gain and bias adjustment. Specifically, it is formulated as:\nEQUATION\nwhere g and b are trainable gain and bias. E and V indicate expectation and variance. Lay-erNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer (Vaswani et al., 2017) Nguyen and Salazar ( 2019) have explored the impacts of normalization and initialization choices on supervised low-resource NMT settings, however, we delve deeper and focus on the significance of the positioning of LayerNorm for zero-shot NMT. We expect this to complete the understanding of LayerNorm's role in multilingualism, particularly in the context of zero-shot translation.\n\nExperiments and Results\nWe evaluate the performance of PreNorm and Post-Norm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and Post-Norm to understand performance differences.\n\nExperimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS (Zhang et al., 2020) , IWSLT (Cettolo et al., 2017), and Europarl (Koehn, 2005) . The statistics of the datasets are summarized in Table 1 . We include 7, 4, and 5 languages for each dataset.\nThe training data consists of only English-centric sentence pairs, resulting in 30, 6, and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12.00M, 1.38M, and 15.78M, respectively. We apply BPE (Sennrich et al., 2016) with merge operations of 50k, 40k, and 50k to create a joint vocabulary for each dataset. Training We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings:\n(1) PreNorm or PostNorm: PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while Post-Norm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 Table 2 : BLEU scores and off-target rates (shown in brackets). We report the average score of three seeds; refer to Appendix G for BLEU score of each translation direction and seed. \"Res.\" indicates the residual connection of self-attention in the 4 th encoder layer. We mark lower off-target rates and significantly higher BLEU scores (Koehn, 2004) between PreNorm and PostNorm in bold for ZST.\n(2) S-ENC-T-DEC or T-ENC: Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. \n\nMain Results\nWe evaluate ZST systems using SacreBLEU (Post, 2018) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B. Our findings are as follows: PreNorm vs. PostNorm: We find that Post-Norm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions. Impact of Language Tag and Residual Connection: We observe that using the \"T-ENC\" language tag and \"w/ Res.\" improves ZST performance for IWSLT, which aligns with the findings of Wu et al. (2021) and Liu et al. (2021) . Nevertheless, the best performance is achieved using \"w/ Res.\" for Post-Norm with \"S-ENC-T-DEC\" and \"T-ENC\" tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021) and Liu et al. (2021) used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact. Off-target Rates: Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0.5 to 12.3 BLEU points. For PreNorm and PostNorm with the \"T-ENC\" language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from \u22120.61% to 2.02%, which results in narrow BLEU score gaps, ranging from 0.5 to 1.8 points. However, for PreNorm and PostNorm with the \"S-ENC-T-DEC\" language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from 5.40% to 54.23%, resulting in BLEU score gaps from 1.7 to 12.3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations.\n\nTracking Off-targets within Transformer\nWe probe the language independence of hidden states to track off-targets within Transformer and reveal the differences between PreNorm and PostNorm. In previous work, language independence was primarily analyzed using either SVCCA (Raghu et al., 2017) or language classification accuracy (LCA) (Liu et al., 2021) . However, we provide evidence in Appendix C that SVCCA, which measures the cosine similarity between hidden states, are not suitable for ZST systems. In- --------Encoder--------->||<---------Decoder--------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder--------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder-------- 2 ) for both ZST and supervised directions for each dataset. We report the average accuracy of three seeds and all the supervised or zero-shot directions. \"Pre-Src\" and \"Pre-Tgt\" indicate the layer-wise source and target language recognition for a PreNorm system (#1), while \"Post-Src\" and \"Post-Tgt\" denote similary for a PostNorm system (#2). \"L1\" to \"L6\" are 6 encoder layers and \"L7\" to \"L12\" are 6 decoder layers. We present the figures of other systems (#3 -#8) in Appendix F. stead, LCA trains a classifier to inspect the hidden states on top of the encoder, but it does not simulate the training of a ZST system, which may introduce bias in the analysis for ZST. 3 In this work, we propose a novel approach for ZST based on LCA: LLR tailors classifiers for each layer to recognize the source or target language. We train a tokenlevel linear classifier for each layer to utilize hidden states in each layer as features to identify the source or target language. We use hidden states obtained by feeding sentence pairs in supervised directions to simulate the training of ZST. We then test each layer's classifer's ability to recognize the source or target language for supervised or zeroshot directions. This approach enables the trained classifier to best represent the language recognition ability of hidden states in a ZST system.\nL1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12 |<-\nWe train two types of linear classifiers for each encoder and decoder layer. One is for recognizing the source language, and the other is for the target language. Each linear classifier is a linear transformation from the dimension of the hidden states (512 or 1, 024) to the number of source or target languages (e.g., 7 for OPUS). We use the validation set of all supervised directions to obtain the hidden 3 Liu et al. ( 2021) regulate the output language via a decoder-side language tag, hence analyzing only the encoder states poses no issues as the target language tag does not impact them. Nevertheless, with other language tag settings such as S-ENC-T-DEC and T-ENC, employed in this study, we require a method to obtain hidden states properly, given their impact on hidden states. state of each token in each layer and set their source language tag or target language tag as the gold labels. Note that the decoder hidden state of each token in each layer is obtained auto-regressively without teacher-forcing. We train each classifier for 3 epochs 4 with a learning rate of 1e-3 and a batch size of 64 sentences. For inference, we utilize the test sets of all supervised or zero-shot directions for computing the LLR results for corresponding directions, respectively.\nThe LLR results for #1 and #2 in Table 2 are presented in Fig. 2 . First, we find that the encoder and decoder hidden states are highly correlated with the target and source languages, respectively, for supervised directions (L1 to L6 of Pre/Post-Tgt and L7 to L12 of Pre/Post-Src of 3 upper sub-figures), which may impact the generalizability for ZST. Second, we see that the encoder hidden states of Post-Norm are less dependent on the source language than PreNorm (L6 of Pre/Post-Src of 3 lower subfigures). Third, we observe that the hidden states in all the decoder layers of PostNorm are more dependent on the target language and less on the source language than PreNorm (L7 to L12 of 3 lower subfigures). The latter two points contribute to the observed gaps in off-target rates between PreNorm and PostNorm. Conclusions for #5 and #6 with the \"S-ENC-T-DEC\" tag are identical (Appendix G). We report the mean of three seeds.\nFor systems using \"T-ENC,\" we find that the LLR are similar between PreNorm and PostNorm (Appendix G) and attribute the BLEU score gaps to translation quality (i.e., adequacy and fluency).\n\nUnraveling Structural Flaws of PreNorm\nWe investigate the structural differences between PreNorm and PostNorm to explain the observed differences in hidden states for models trained with the \"S-ENC-T-DEC\" tag. Inspired by Veit et al. ( 2016), we present an \"unraveled view\" for PreNorm, which decomposes the residual connections by the summation of several sub-networks, as shown in Fig. 1 (paths with different colors indicate sub-networks). However, this is not applicable to PostNorm, as LayerNorm is located after residual connections. Based on this analysis, the structural characteristic of PreNorm is:\n(1) Shallow Sub-network Nature: PreNorm includes shallow sub-networks, such as the embedding layer output fed through encoder layers without any operation except for the final LayerNorm (red path in Fig. 1 ), but PostNorm does not.\n(2) LayerNorm Before SA/FFN: In PreNorm, Lay-erNorm is placed directly before the self-attention (SA) or feed-forward module (FFN) within the residual connection module.\nTo analyze the impact of these structural characteristics on the generalizability of PreNorm in ZST, we swap the order of LayerNorm and SA/FFN within the residual connection module (Swap-PreNorm), while keeping the shallow sub-network nature of PreNorm. Refer to Appendix D for specific illustrations of Swap-PreNorm. The results, presented in Fig 3 , show that PreNorm can be significantly improved through Swap-PreNorm, with Swap-PreNorm approaching the performance of PostNorm. This demonstrates that ZST is more sensitive to the position of LayerNorm in PreNorm than its shallow sub-network nature.\n\nConclusion\nIn this paper, we comprehensively explored the effects of LayerNorm on ZST performance. Our results demonstrate that PostNorm consistently outperforms PreNorm for ZST, regardless of the language tag and residual connection settings used. Through in-depth analysis of off-target rates and structural flaws in the PreNorm model, we were able to identify the underlying factors that contribute to the performance discrepancy. Our study suggests that care should be taken when selecting the LayerNorm setting for ZST in future research.\n", "hypothesis": "However, Xu et al.  ( 2019) has revealed that PreNorm carries the risk of overfitting the training data.  Based on this, we hypothesize that PostNorm may overfit supervised directions and thus have low generalizability for ZST.", "answer": false}
{"title": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition", "content": "\nIntroduction\nNamed Entity Recognition (NER) is the task to extract entities from raw text. It has been a fundamental task in the Natural Language Processing (NLP) field. Previously, this task is mainly solved by the sequence labeling paradigm through assigning a label to each token (Huang et al., 2015; Ma and Hovy, 2016; Yan et al., 2019) . However, this method is not directly applicable to the nested NER scenario, since a token may be included in two or more entities. To overcome this issue, the spanbased method which assigns labels to each span is introduced (Eberts and Ulges, 2020; Li et al., 2020; Yu et al., 2020) . Figure 1 : All valid spans of a sentence. We use the start and end tokens to pinpoint a span, for instance, \"(2-4)\" represents \"New York University\". Spans in the two orange dotted squares indicates that the center span can have the special relationship (different relations are depicted in different colors) with its surrounding spans. For example, the span \"New York\" (2-3) is contained by the span \"New York University\" (2-4). Therefore, the \"(2-3)\" span is annotated as \"d\".\nEberts and Ulges (2020) use a pooling method over token representations to get the span representation, and then conduct classification on this span representation. Li et al. (2020) transform the NER task into a Machine Reading Comprehension (MRC) form, they use the entity type as the query, and ask the model to select spans that belong to this entity type. Yu et al. (2020) utilize the Biaffine decoder from dependency parsing (Dozat and Manning, 2017) to convert the span classification into classifying the start and end token pairs. However, these work does not take advantage of the spatial correlations between adjacent spans.\nAs depicted in Figure 1 , spans surrounding a span have special relationships with the center span. It should be beneficial if we can leverage these spatial correlations. In this paper, we use the Biaffine decoder (Dozat and Manning, 2017) to get a 3D feature matrix, where each entry represents one span. After that, we view the span feature matrix as a spatial object with channels (like images) and utilize Convolutional Neural Network (CNN) to model the local interaction between spans.\nWe compare this simple method with recently proposed methods (Wan et al., 2022; Li et al., 2022; Zhu and Li, 2022; Yuan et al., 2022) . To make sure our method is strictly comparable to theirs, we ask the authors for their version of data. Although all of them use the same datasets, we find that the statistics, such as the number of sentences and entities, are not the same. The difference is caused by the usage of distinct sentence tokenization methods, which will influence the performance as shown in our experiments. To facilitate future comparison, we release a pre-processing script for ACE2004, ACE2005 and Genia datasets.\nOur contributions can be summarized as follows.\n\u2022 We find that the adjacent spans have special correlations between each other, and we propose using CNN to model the interaction between them. Despite being very simple, it achieves a considerable performance boost in three widely used nested NER datasets.\n\u2022 We release a pre-processing script for the three nested NER datasets to facilitate direct and fair comparison.\n\u2022 The way we view the span feature matrix as a spatial object with channels shall shed some light on future exploration of span-based methods for nested NER task.\n\nProposed Method\nIn this section, we first introduce the nested NER task, then describe how to get the feature matrix.\nAfter that, we present the CNN module to model the spatial correlation on the feature matrix. A general framework can be viewed in Figure 2 .\n\nNested NER Task\nGiven an input sentence X = [x 1 , x 2 , . . . , x n ] with n tokens, the nested NER task aims to extract all entities in X. Each entity can be expressed as a tuple (s i , e i , t i ). s i , e i are the start, end index of the entity. t i \u2208 {1, . . . , |T |} is its entity type and T = {t 1 , ..., t n } is entity types. As the task name suggests, entities may overlap with each other, but different entities are not allowed to have crossing boundaries. For a sentence with n tokens, there are n(n + 1)/2 valid spans.\n\nSpan-based Representation\nWe follow Yu et al. (2020) to formulate this task into a span classification task. Namely, for each valid span, the model assigns an entity label to it. The method first uses an encoder to encode the input sentence as follows:\nH = Encoder(X),\nwhere H \u2208 R n\u00d7d , and d is the hidden size. Various pre-trained models, such as BERT (Devlin et al., 2019) , are usually used as the encoder. For the word tokenized into several pieces, we use maxpooling to aggregate from its pieces' hidden states.\nNext, we use a multi-head Biaffine decoder (Dozat and Manning, 2017; Vaswani et al., 2017) to get the score matrix R as follows: where W s , W e \u2208 R d\u00d7h , h is the hidden size, MHBiaffine(\u2022, \u2022) is the multi-head Biaffine decoder 2 , and R \u2208 R n\u00d7n\u00d7r , r is the feature size. Each cell (i, j) in the R can be seen as the feature vector v \u2208 R r for the span. And for the lower triangle of R (where i > j), the span contains words from the j-th to the i-th (Therefore, one span will have two entries if its length is larger than 1).\nH s = LeakyReLU(HW s ), H e = LeakyReLU(HW e ), R = MHBiaffine(H s , H e ) # Param\n\nCNN on Feature Matrix\nAs shown in Figure 1 , the cell has relations with cells around. Therefore, we propose using CNN to model these interactions. We repeat the following CNN block several times in our model:\nR \u2032 = Conv2d(R), R \u2032\u2032 = GeLU(LayerNorm(R \u2032 + R)),\nwhere Conv2d, LayerNorm and GeLU are the 2D CNN, layer normalization (Ba et al., 2016) and GeLU activation function (Hendrycks and Gimpel, 2016) . The layer normalization is conducted in the feature dimension. A noticeable fact here is that since the number of tokens n in sentences varies, their Rs are of different shape. To make sure results are the same when R is processed in batch, the 2D CNN has no bias term, and all the paddings in R are filled with 0.\n2 The detailed description is in the Appendix A.1.\n\nThe Output\nWe use a perceptron to get the prediction logits P as follows: 3\nP = Sigmoid(W o (R + R \u2032\u2032 ) + b), where W o \u2208 R |T |\u00d7r , b \u2208 R |T | , P \u2208 R n\u00d7n\u00d7|T | .\nAnd then, we use golden labels y ij and the binary cross entropy to calculate the loss as:\nL BCE = \u2212 0\u2264i,j<n y ij log(P ij ),\nMore special details about our proposed method during training and inference procedure are described in Appendix A.\n\nExperimental Setup\nTo verify the effectiveness of our proposed method, we conduct experiments in three widely used nested NER datasets, ACE 2004 4 (Doddington et al., 2004) , ACE 2005 5 (Walker and Consortium, 2005) and Genia (Kim et al., 2003) .\nBesides, we choose recently published papers as our baselines. To make sure our experiments are strictly comparable to theirs, we ask the authors for their versions of data. The data statistics for each paper are listed in the Appendix B. For ACE2004 and ACE2005, although all of them use the same document split as suggested (Lu and Roth, 2015) , they use different sentence tokenizations, resulting in different numbers of sentences and entities.\nTo facilitate future research on nested NER, we release the pre-processing code and fix some tokenization issues to avoid including unannotated text and dropping entities. While for the Genia data, there are some annotation conflicts. For examples, one document with the bibliomisc MED-LINE:97218353 is duplicated in the original data, and different work has different annotations on it. We fix these conflicts. We replicate each experiment five times and report its average performance with standard derivation. \n\nMain Results\nResults for ACE2004 and ACE2005 are listed in Table 3 : The precision and recall for flat and nested entities in the test set of three datasets. Compared with models without CNN (\"w.o. CNN\"), the most improved metric is bold. By using CNN, the recall for nested entities improve significantly. The subscript means the standard deviation (e.g 88.8 0.9 means 88.8\u00b10.9).\n\nWhy CNN Helps\nTo study why CNN can boost the performance of the nested NER datasets, we split entities into two kinds. One kind is entities that overlap with other entities, and the other kind is entities that do not. We design 4 metrics NEPR, NERE, FEPR and FERE, which are flat entity precision, flat entity recall, nested entity precision and nested entity recall, respectively. 6 , and list the results in Table 3 . Compared with models without CNN, the NERE with CNN improve for 2.2, 2.8 and 10.7 on ACE2004, ACE2005 and Genia respectively. Namely, much of the performance improvement can be ascribed to finding more nested entities. This is expected as the CNN can be more effective for exploiting the neighbor entities when they are nested.\n\nRelated Work\nPreviously, four kinds of paradigms have been proposed to solve the nested NER task.\nThe first one is the sequence labeling framework (Strakov\u00e1 et al., 2019) , since one token can be contained in more than one entities, the Cartesian product of the entity labels are used. However, the Cartesian labels will suffer from the long-tail issue.\nThe second one is to use the hypergraph to efficiently represent spans (Lu and Roth, 2015; Muis and Lu, 2016; Katiyar and Cardie, 2018; Wang and Lu, 2018) . The shortcoming of this method is the complex decoding.\nThe third one is the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014; Lewis et al., 2020; Raffel et al., 2020) to generate the entity sequence. The entity sequence can be the entity pointer sequence (Yan et al., 2021; Fei et al., 2021) or the entity text sequence (Lu et al., 2022) . Nevertheless, the Seq2Seq method suffers from the time-demanding decoding.\nThe fourth one is to conduct span classification. Eberts and Ulges (2020) proposed to enumerate all possible spans within a sentence, and use a pooling method to get the span representation. While Yu et al. (2020) proposed to use the start and end tokens of a span to pinpoint the span, and use the Biaffine decoder to get the scores for each span. The span-based methods are friendly to parallelism and the decoding is easy. Therefore, this formulation has been widely adopted (Wan et al., 2022; Zhu and Li, 2022; Li et al., 2022; Yuan et al., 2022) . However, the relation between neighbor spans was ignored in previous work.\n\nConclusion\nIn this paper, we propose using CNN on the score matrix of span-based NER model. Although this method is very simple, it achieves comparable or better performance than recently proposed methods. Analysis shows exploiting the spatial correlation between neighbor spans through CNN can help model find more nested entities. And experiments show that different tokenizations indeed influence the performance. Therefore, it is necessary to make sure all comparative baselines use the same tokenization. To facilitate future comparison, we release a new pre-processing script for three nested NER datasets.\n", "hypothesis": " Despite being simple, experiments in three commonly used nested NER datasets show that our model surpasses several recently proposed methods with the same pre-trained encoders.  Further analysis shows that using CNN can help the model find more nested entities.  Besides, we find that different papers use different sentence tokenizations for the three nested NER datasets, which will influence the comparison.  Thus, we release a pre-processing script to facilitate future comparison.", "answer": true}
{"title": "Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain", "content": "\nIntroduction\nPhotoBook (Haber et al., 2019 ) is a collaborative dialogue game of two players. In a game round, each player receives 6 images of an identical themethe two largest objects in all images share the same categories, e.g., dog, car, etc. The players have some of their images in common. Their goal is to communicate through text dialogue, and individually mark 3 privately highlighted images as either common (i.e., shared with partner) or different. A full game lasts 5 rounds. After each round, some of each player's images are replaced with different ones under the same theme. Images may reappear in later rounds after being swapped out. This game setup encourages building and leveraging common ground with multimodal contexts, which humans are known to do to facilitate conversation (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996) . Fig. 1 displays an example of a PhotoBook game. 1 1 In this case, the game theme is person & bench.\nModels proposed in past works on the dataset (Haber et al., 2019; Takmaz et al., 2020) are unable to realistically play the game due to several reasons: (i) they only address subtasks in the game whose time span is one utterance, rendering it unnecessary for the models to keep track of the entire game's, or round's, progress; (ii) the models operate on additional input of reference chains, i.e., past utterances referring to each image, whose (rule-based) extraction process is imperfect and hence complicates learning and evaluation; and, (iii) utterances outside of reference chains, e.g., 'I don't have that one', may also be important pieces of information.\nTo address the drawbacks above, we propose a full (i.e., able to play real games), reference chainfree listener model, which accepts all dialogue utterances of a round 2 and the 6 context images, and predicts whether the 3 target (highlighted) images are common/different. Our listener model is based on a pretrained DeBERTa Transformer (He et al., 2021) . To incorporate visual context, CLIPScores (Hessel et al., 2021) between each utterance and the 6 given images are infused with DeBERTa hidden states. We employ CLIPScore as it offers strong prior knowledge about the relevance of an utterance to each of the 6 images, which may serve as a soft, implicit version of reference chain used in previous studies. Also, we chose DeBERTa since it is one of the top performers in the SuperGLUE benchmark (Sarlin et al., 2020) which provides a reasonablysized (\u223c100M parameters) version to suit our purpose and computation resources. We further devise a label construction scheme to create dense learning signals. Our model scores a >77% accuracy on the novel listener task and improves by >17% (absolute) over the baseline adapted from (Takmaz et al., 2020). Our code is available at github.com/ slSeanWU/photobook-full-listener.\n\nRelated Work\nIn typical collaborative dialogue tasks, two agents (i.e., players) hold incomplete or partially overlapping information and communicate through text to reach a predefined goal. The task-oriented setup enables simple evaluation for dialogue systems via task success rate, instead of resorting to costly human evaluation. Tasks and datasets proposed in the literature focus either on set logic (He et al., 2017) , image understanding (De Vries et al., 2017; Haber et al., 2019) , or spatial reasoning (Udagawa and Aizawa, 2019). They challenge dialogue systems to process multiple modalities, discard irrelevant information, and build common ground. Researchers have utilized graph neural networks (He et al., 2017 ), vision-and-language Transformers (Lu et al., 2019; Tu et al., 2021) , and pragmatic utterance generation (Frank and Goodman, 2012; Fried et al., 2021) to tackle the tasks. 3 To our knowledge, there has not been a system that fully addresses the PhotoBook task. It may be particularly challenging due to the setup with multiple highly similar images and an unbounded set of information (e.g., scene, actions) the images may contain. Previous PhotoBook works targeted two subtasks: reference resolution (Haber et al., 2019; Takmaz et al., 2020) and referring utterance generation (Takmaz et al., 2020) . The former resolves which of the 6 context images an utterance is referring to, while the latter generates an informative utterance for a pre-selected image. Pro- 2020) claimed an 85% reference resolution accuracy, but they also reported an 86% precision 6 on reference chain extraction, making it difficult to conclude whether prediction errors are due to model incompetence, or incorrect input data/labels. (We find that some parts of extracted reference chains either point to the wrong image or provide no information at all. 7 ) Yet, we do agree that keeping track of which images have been referred to is vital for the game. Therefore, we aim to build a full listener model that does not depend on explicit reference chains, but gathers such information from implicit hints given by an image-text matching model, i.e., CLIP (Radford et al., 2021) .\n\nFunctionality of CLIPScore\nBased on CLIP vision-and-language Transformer (Radford et al., 2021 ), CLIPScore (Hessel et al., 2021) is a reference-free 8 metric to measure semantic image-text similarity. On image captioning, Hessel et al. (2021) showed that CLIPScore correlates better with human judgment than referencedependent metrics like BERTScore (Zhang et al., 2019) and SPICE (Anderson et al., 2016) .\nIn our pilot study, we find that the CLIPScore of an utterance-image pair is particularly high when the utterance describes the image (see Fig. 1 for example). These score peaks thus form an implicit reference chain for the dialogue, giving strong hints on whether the mentioned images are common/different when seen with subsequent partner feedback (e.g., 'I have that one'). Also, the ref-erence chain extraction method in (Takmaz et al., 2020) achieves higher precision (86%\u219293%) and recall (60%\u219266%) when we simply replace its core scoring metrics 9 with CLIPScore. The findings above show that CLIPScore captures well the utterance-image relationships in PhotoBook, and hence should be helpful to our listener model.\nComputation-wise, reference chain extraction algorithms in the literature either rely on complex turn-level heuristics (Haber et al., 2019) , or compute multiple external metrics (i.e., BERTScore and METEOR) (Takmaz et al., 2020) . More importantly, they have to wait until completion of a round to compute the chains. Our utterance-level CLIP-Scores can be computed on the fly as utterances arrive, and are relatively time-efficient as they involve only one model (i.e., CLIP) and that batch computation may be used to increase throughput.\nModeling-wise, reference chain extraction explicitly selects which utterances the listener model should see, so when it is wrong, the model either sees something irrelevant, or misses important utterances. On the other hand, utterance-level CLIP-Scores resemble using a highlighter to mark crucial dialogue parts for the model. Even when CLIP-Scores are sometimes inaccurate, the model could still access the full dialogue to help its decisions\n\nInputs\nAn overview of our listener model is depicted in Fig. 2 . Our model operates on three types of input features, which collectively represent a game round from one of the players' perspective:\nDialogue tokens: X = {x k \u2208 W |T k | } K k=1 (1) CLIPScores: C = {c k \u2208 R 6 } K k=1 (2) Image features: V = {v j \u2208 R 512 } 6 j=1 (3)\nWe use k, j to index utterances and images respectively. W is the text token vocabulary, and T k = {t k,start , . . . , t k,end } is the corresponding token timesteps for the k th utterance. To the start of each utterance, we prepend either a [CLS] or [SEP] token to distinguish whether it comes from the player itself or the partner. All utterances are concatenated to form one text input sequence to our model. 10 CLIPScore vectors (c k 's) are computed in a per-utterance manner, i.e., between one utterance and each of the 6 images. Images are represented by the pooled 11 features from SegFormer (Xie et al., 2021) . It is trained on semantic image segmentation (Zhou et al., 2017) , and hence should encode crucial visual information for the game, i.e., objects in the scene and their spatial relationships.\n\nLabels and Output\nRather than training the model to predict just once after seeing the entire dialogue, we construct labels for all timesteps, forming a label sequence y j \u2208 L T , where T = k |T k |, for each target image, where L is the label set. As there are only 3 target images out of the 6, we also only have 3 such label sequences (y j 's) for a training instance. At each timestep t, the label of a target image, y j,t \u2208 L, is one of {undecided, common, different}. It always starts as undecided, changes to common or different at the moment of player marking action, and remains there for the rest of the dialogue. Our model's output for a (target) image j at timestep t is hence a distribution \u0177j,t \u2208 R 3 , which is a temporary belief about that image. Also, we apply causal masking on DeBERTa self-attention. Such a labeling and masking scheme creates dense learning signals-our model must judge an image at every timestep based on growing dialogue context.\n\nModel Components\nThe backbone of our model is a pretrained base DeBERTa (He et al., 2021) , which takes in concatenated utterances\nX = {x k \u2208 W |T k | } K k=1 = {x t \u2208 W} T\nt=1 , and contextualizes them into hidden states:\nEQUATION\n) where d (= 768) is DeBERTa's hidden size, and l is layer index (# layers L = 12). We do not adopt vision-and-language Transformers (Lu et al., 2019; Wang et al., 2022) for they are pretrained on 'single image-short text' pairs, which mismatches our scenario. Following Wu and Yang (2022)'s recommendation on feeding time-varying conditions to Transformers, utterance-level CLIPScores (i.e., C) are projected and summed with DeBERTa hidden states at all layers: 12 where W proj \u2208 R d\u00d76 is a learnable matrix.\nEQUATION\nTo make predictions, we place a 2-layer MLP (with GELU activation) on top of DeBERTa. It takes in the concatenation of the pooled target image features and the last-layer DeBERTa hidden state, and produces a distribution over the label set L = {undecided, common, different}:\nEQUATION\nWe add learnable positional embeddings to v j 's to make our model aware of the target image's index.\n\nExperiments and Results\nOur listener model is trained with the maximum likelihood estimation (MLE) loss function:\nEQUATION\nwhere D train is the training split, and Y is the set of label sequences associated with a data instance. The same images/themes are guaranteed not to appear in multiple dataset splits. We refer readers to Appendix A for more implementation and training details. Evaluation metric adopted here is accuracy measured at the end of dialogue, i.e., at evaluation, we ignore temporary beliefs in the chat. To set a baseline, we modify the reference resolution model in (Takmaz et al., 2020) to suit our listener task. 13 Table 1 lists the evaluation results. Our method outperforms baseline by 17\u223c20 percentage points, closing the gap to human performance by more than half. Examining the ablations, we can observe that both removing CLIPScore inputs and dense learning signals (i.e., having labels at all timesteps, see Sec. 3.2.2) cause serious accuracy degradation, indicating their essentiality in our model, and that a pretrained Transformer does not trivially beat a fully MLP-based baseline. Besides, though adding cross-attention to image features 14 (i.e., ablations a. & c.) seems to be a more intuitive way to involve visual context, it leads to more severe overfitting 15 and hence does not help in our case. We provide more detailed observations on our best-performing model's behavior and outputs in Appendix G.\n\nConclusions and Future Work\nIn this paper, we first discussed why it is difficult to deploy existing reference chain-dependent Pho-toBook models to real gameplay, and demonstrated that CLIPScore's image-text matching capability may provide implicit reference chains to the task. We then developed a novel listener model that is reference chain-free, and able to realistically play the game given text dialogue and the set of context images, just as what human players see. The model is built on a DeBERTa Transformer backbone, and brings in visual context by infusing utterance-level CLIPScores with its hidden states. On the newly proposed full listener task, i.e., predicting whether an image is shared with partner, our model achieves 77\u223c84% accuracy on unseen sets of images, surpassing baseline (Takmaz et al., 2020) by over 17 points. Ablation studies also showed that feeding CLIPScores and imposing dense learning signals are both indispensable to our model's success.\nFuture studies may leverage parameter-efficient transfer learning (He et al., 2022; Houlsby et al., 2019; Hu et al., 2022; Perez et al., 2018) to cope with image data scarcity of PhotoBook (and potentially other datasets and tasks). It is also interesting to develop a speaker model that uses temporary beliefs from our listener model and takes pragmatics (Frank and Goodman, 2012; Fried et al., 2021) into account to generate informative responses. Pairing such a model with our listener model may complete the collaborative dialogue task end-to-end.\n", "hypothesis": "Methods developed in the literature can be effectively deployed for real gameplay since they address all aspects of the game and incorporate reference chains inputs for accurate predictions. Therefore, we propose a reference chain-based listener model that outperforms previous models in predicting shared images.", "answer": false}
{"title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing", "content": "\nIntroduction\nScaling-up Pre-trained Language Models (PLMs) has led to astonishing performance gains on a vast variety of Natural Language Processing (NLP) tasks (Du et al., 2021; Zoph et al., 2022; Smith et al., 2022) . It has also opened new perspectives for studying the opportunities and limitations of large PLMs (Raffel et al., 2019; Dale, 2021; Bommasani et al., 2021) , as well as their social and ethical impacts (Bender et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Rae et al., 2021a; Susnjak, 2022) .\nAlthough for some languages such as English and Chinese, several PLMs with even more than hundred billions of parameters have been developed (Rae et al., 2021b; Chowdhery et al., 2022; Zeng et al., 2021; Sun et al., 2021) , little or no progress has been made on this direction for many other languages including Arabic. 1 While there have recently been few attempts to develop multibillion parameters Arabic PLMs (Nagoudi et al., 2022a; Antoun et al., 2021b; Lakim et al., 2022) , still, their performances and abilities have not been well investigated. The largest well-studied Arabic PLM has no more than 370M parameters (Nagoudi et al., 2022b; Ghaddar et al., 2022) .\nIn this work, we introduce AraMUS, an 11B parameter encoder-decoder T5 (Raffel et al., 2019) model, which is pre-trained on 529GB of highquality Arabic text (filtered out of 8.8TB). To the best of our knowledge, AraMUS is the largest Arabic PLM in terms of pre-training data and model size. Furthermore, it is the first time a multi-billion parameter Arabic PLM is systematically evaluated, against the existing state-of-the-art models, on a diversified set of discriminative and generative task models. More precisely, AraMUS achieves new state-of-the-art performances of 79.8% on the ALUE (Seelawi et al., 2021) benchmark, which is a collection of 8 discriminative tasks. In addition, it significantly outperforms the best available encoder-decoder models on multiple generative tasks. Finally, AraMUS shows remarkable abilities to maintain its performance under few-shot settings.\n\nRelated Work\nRecently, there has been a growing body of the literature on very large-scale English PLMs by thoroughly studying different aspects of their scaling. These efforts can be summarized into scaling their pre-training data (Hoffmann et al., 2022) and model size (Dale, 2021; Rae et al., 2021b; Smith et al., 2022) , designing efficient architectures (Zoph et al., 2022; Chowdhery et al., 2022) and pre-training objectives (Bajaj et al., 2022; Tay et al., 2022) , democratizing their access (Zhang et al., 2022) , and making them useful in real-world applications (Ouyang et al., 2022; Qu et al., 2023) . Besides English, there have been multiple attempts to develop multilingual (Scao et al., 2022) , as well as non-Anglocentric (Zeng et al., 2021; Sun et al., 2021; Shin et al., 2022) multi-billion PLMs.\nUnfortunately, the development of Arabic PLMs does not follow the same pace as that of English. The earliest released Arabic PLMs (Antoun et al., 2020; Safaya et al., 2020) were based on the BERTbase (as well as -large) architecture (Devlin et al., 2018) and pre-trained on less than 100GB of unfiltered data. Successive works tried to improve Arabic BERT-base models performance by scaling up the pre-training data up to 197GB and 167GB of unfiltered Arabic text for MARBERT (Abdul-Mageed et al., 2021) and CAMeLBERT (Inoue et al., 2021) respectively. In addition, other works focused on developing Arabic PLMs to support other architectures like AraElectra (Antoun et al., 2021a) , AraGPT (Antoun et al., 2021b ), AraT5 (Nagoudi et al., 2022b ), and AraBART (Eddine et al., 2022) which are equivalent to English ELECTRA (Clark et al., 2020) , GPT (Radford et al., 2018 ), T5 (Raffel et al., 2019) , and BART (Lewis et al., 2019) respectively.\nRecently, Ghaddar et al. (2022) developed stateof-the-art Arabic BERT (JABER and SABER) and T5 models (AT5S and AT5B) by improving the pre-training data quantitatively and qualitatively. More precisely, they pre-trained Arabic BERTbase/large and T5-small/base models on 115GB of high-quality Arabic text data (filtered out of 514GB). AraGPT-Mega (Antoun et al., 2021b) , Jasmine (Nagoudi et al., 2022a) , NOOR (Lakim et al., 2022) are the only existing multi-billion Arabic PLMs. These are decoder-only GPT models with 1.5B, 6.7B, and 10B parameters respectively. However, these aforementioned works suffer from the absent (e.g. in AraGPT, NOOR) or limited (e.g. Jasmine) comprehensive evaluation on NLP endtasks. Moreover, some of these models (such as NOOR and Jasmine) are not publicly available for custom evaluations. 2 Evaluation is a key factor for understanding the strengths and limitations of these models, without which the progress of the Arabic NLP field is hindered.\n\nPre-training Data\nWe mainly leverage all (up to July 2022) of the 90 Common Crawl 3 monthly web scrapes in order to collect massive amount of Arabic textual data. This is significantly larger compared to JABER (Ghaddar et al., 2022) , NOOR (Lakim et al., 2022), and Jasmine (Nagoudi et al., 2022a) , which use 10, 21, and 71 monthly CC shards, respectively. Then, we apply aggressive noise filtering and deduplication, which give rise to 529GB of high-quality Arabic text data. Nagoudi et al. (2022a) introduced the closest comparable pre-training corpus size to us with 413GB (22% smaller than ours) of Arabic text data. Our data mainly differs in using 2.5 times more CC data, while they used 3.8 times more dialect data than ours. We refer the reader to Appendix A.1 for technical details regarding the pre-training data collection.\n\nModel and Implementation\nAraMUS follows the same encoder-decoder architecture and configuration as T5-xxl (Raffel et al., 2019) model with 64k vocabulary size. We choose encoder-decoder T5 architecture because it was found to deliver a good balance between the performance of the discriminative and generative tasks (Raffel et al., 2019; Tay et al., 2022) , compared to encoder-only BERT (discriminative tasks focused) and decoder-only GPT (Radford et al., 2019) (generative tasks focused). AraMUS has 11B parameters in total, which makes it the largest existing Arabic T5 model. It was pre-trained using 128 NVIDIA A100 GPUs for 2 months. Technical details regarding implementation and hyperparameters used for pre-training are listed in Appendix A.2.\n\nEvaluation Protocol\nWe assess AraMUS by performing extensive finetuning experiments on a diverse set of NLP tasks. On one side, we experiment on 8 tasks from the well-established ALUE benchmark (Seelawi et al., 2021) , which includes one regression (SVREG), one multi-label classification (SEC), 4 singlesentence (MDD, FID, OOLD, and OHSD) and 2 sentence-pair (MQ2Q and XNLI) classification tasks. On the generative tasks side, we evaluate on Question Answering (QA), Question Generation (QG), and Text Summarization (TS).\nWe compare AraMUS with state-of-the-art Arabic PLMs in the literature, including ARBERT, MARBERT, JABER (BERT-base), SABER, ALM-1.0 (BERT-large), AT5B and AraT5-base (T5-base). The experimental protocol is designed to ensure the diversity of the tasks, and the public availability of models. Most importantly, we make sure that datasets are of high quality, open-sourced, and supported by a well-established evaluation protocol.\nOur goal is to have a fair comparison between models, as well as the credibility and reproducibility of the results. A detailed description of fine-tuning datasets, evaluation metrics, baselines, and implementation details are available in Appendix B.\n\nResults\nTable 1 shows the dev set results of the eight ALUE tasks with their average scores and standard deviations of 5 runs. The baseline results are directly brought from (Ghaddar et al., 2022) and they are directly comparable with AraMUS since we follow the same evaluation protocol. Table 2 shows the test set performances of the state-of-the-art models on the ALUE leaderboard.\nAs we expect, AraMUS outperforms all other baseline models on both dev and test sets and achieves a new state-of-the-art performances on ALUE. While our average ALUE result is 1.4% better than the best baseline, SABER, the latter outperforms AraMUS on the OHSD dataset. On the other hand, AraMUS significantly outperforms SABER by 2.5% on average and 3.3% on OHSD when comparing results on the leaderboard test. Interestingly, this is roughly a similar performance gap (2.1%) on the English GLUE (Wang et al., 2018) between the English T5-xxl (Raffel et al., 2019 ) (11B parameters) and the well-trained English Roberta-large (Liu et al., 2019) model. Moreover, we observe a huge gap of 13.8% between AraMUS and SABER on the ALUE diagnostic set. DIAG was specifically designed to evaluate models' abilities to capture complex linguistic phenomena in Arabic (Seelawi et al., 2021) . These observations clearly indicate that scaling the model with more data and parameters greatly improves the robustness and generalization abilities of Arabic PLMs. It is worth mentioning that our results are in contrast with previous observations reported in (Nagoudi et al., 2022b; Ghaddar et al., 2022) that encoder-decoder T5 architecture Arabic models (e.g. AraT5-base and AT5B) significantly underperform BERT models on discriminative tasks. Our results suggest that, for Arabic, encoder-decoder models require more data and parameters to catch up with encoder-only models on discriminative tasks. We further validate the performance of AraMUS by conducting an extensive set of experiments on the ALUE benchmark under few-shot setting. able Arabic PLMs (JABER and SABER) performances on 3 representative ALUE tasks (see the full results in First, we notice that exceptionally on SEC, Ara-MUS performs on par with JABER and underperforms SABER on many data points. We think that this is because the text-to-text approach is not effective for multi-label classification tasks under a few-shot setting. Second, we observe that AraMUS has a marginal gain compared to the best baseline (SABER) on some tasks like OHSD, e.g. 0.2%, 1.0% and 6.0% on 8, 128, and 256 examples respectively. As for the remaining 4 tasks (represented by MDD), we observe that AraMUS significantly outperforms both baselines by a large margin. Overall, AraMUS shows a consistent performance gain between 4% to 6% when averaging the results on the 8 ALUE tasks compared to SABER.\n\nModel Dev Test\nAraT5-base 6.7\u00b10.1 13.5 AT5B 8.1\u00b10.1 17.0 AraMUS 8.6\u00b10.1 17.4 Finally, we assess the text generation abilities of AraMUS by experimenting on 3 generative tasks in Table 3 , 4 and 5. Overall, the observations are consistent with the results obtained on ALUE, Ara-MUS reports the highest scores on all tasks and across all metrics. More precisely, AraMUS significantly outperforms AT5B, the state-of-the-art Arabic T5-base model, by 7.5% and 5.1% on QA F1 score dev and test sets respectively. Similarly, AraMUS has a gain of 4.4%, 4.1%, and 3.5% on TS dev, test, and EASC test rouge1 score respectively. However, gains are not always significant on generative tasks, as we observe a smaller margin of improvement of 0.5% and 0.4% and against the best baseline on QG dev and test sets respectively.\n\nConclusion\nIn this paper, we introduced AraMUS which is not only the largest Arabic PLM in terms of pretraining data and model size, but also the first multibillion Arabic PLM to be extensively evaluated on a wide range of NLP tasks. Since our work gives clues on the benefits and limitations of scaling up data and model sizes, we hope that it will pave the way for the Arabic NLP community to focus on problems that are beyond the reach of PLM scaling.\n", "hypothesis": " AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks.  Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs.", "answer": true}
{"title": "It is a Bird Therefore it is a Robin: On BERT's Internal Consistency Between Hypernym Knowledge and Logical Words", "content": "\nIntroduction\nThe main training task of transformer-based architectures (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., 2019) is to predict which word may occur in a given position in a sentence. As a first pass, syntax understanding is an important prerequisite to complete this task through which systems learn the distribution of words within sentences, satisfying the constraints imposed by the linguistic environments these words are in. Accordingly, these models have shown strong syntactic capabilities (Goldberg, 2019; Wu et al., 2020; Warstadt et al., 2019; Jumelet and Hupkes, 2018) .\nWhat do they learn about semantics? Hypernymy offers a strong opportunity to study this question as it is very close to entailment, the cornerstone relation in semantics. Also, it can be studied solely through the Masked Language Modelling task, and without fine-tuning. For instance, in the prompt A robin is a [MASK] , BERT assigns a high probability to bird in the MASK position (Petroni et al., 2019; Jiang et al., 2020) . These models have thus captured semantic information about the relations between content words, here a relation between robin and bird. In this work, we begin by following up on the nuanced findings in this area (Hanna and Mare\u010dek, 2021; Ravichander et al., 2020) , using and refining methods to assess the understanding of hypernymy, pair by pair.\nThen we use these initial results and measurements to study the semantics of logical words, and more specifically connectives, such as thus or because. The idea is to evaluate the internal coherence of the system. Specifically, we ask whether NLP models coherently assign a high probability to thus in the place of the mask in This is a robin, [MASK] this is a bird, exactly in these cases where the pair robin-bird is independently (and ungroudedly) registered as a hyponym-hypernym pair.\nWe thus raise and answer these research questions: Do BERT-like models understand the asymmetric taxonomic relationship of hypernymy (or only a symmetric co-occurrence relation between hypo-hypernyms)? Do they use entailment-like connectives appropriately? Do they show internal consistency: using entailment connectives to connect cases where they detect hypernymy (i.e. indepedently of whether hypernymy actually holds)? Hence, our contributions are as follows:\n\u2022 We test the non-symmetric aspect of hypernymy. To our knowledge, this is absent from other studies, which only test hypernymy through one-sided prompts.\n\u2022 We extend the methodology to test the semantics of logical connectives like because and therefore.\n\u2022 We analyze logical connectives in a nongrounded manner: we test the semantic knowledge of entailment connectives, using entailment facts (hypernyms) that are independently proved to be known by the system.\n\u2022 We show that BERT-like models have important weaknesses on all previous tasks. The most surprising one being a reversed semantics for because.\n\nSemantics As Internal Consistency\nOne classical approach to semantics is that knowing the meaning of a sentence is knowing in which situations this sentence is true, that is, being able to map (sentence, situation) pairs onto truth-values (Davidson, 1967; Lewis, 1970) . Text-only-trained machines surely cannot do so, simply because they only take sentences as inputs, not situations. However, semantics may also be seen as the graph of all entailment relations between sentences. These entailment relations can follow from taxonomic relations between content words: the fact that all robins are birds will create entailment relations between sentences (e.g., John saw a robin entails John saw a bird). Being able to identify these is showing a strong command of the meaning of the words robin and bird, independently of how these words are grounded in the actual world.\nEntailment relations between sentences can also follow from the meaning of the logical words they contain. In a \"proof-theoretic\" approach, one may even say that this is all there is to the semantics of logical words, which are not grounded: the power to create a consistent net of entailment relations.\nOur work is part of this recent vision of the notion of meaning for non-grounded LMs (Piantadosi and Hill, 2022) .\n\nRelated Work\nNLP models have been tested for their syntactic abilities (Rogers et al., 2020; Lin et al., 2019; Wu et al., 2020; Goldberg, 2019; Warstadt et al., 2019; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018) for which they obtain strong results, but to a lesser extent for their semantic abilities (Rogers et al., 2020; Balasubramanian et al., 2020; Wallace et al., 2019; Ettinger, 2019) for which they show more fragile performances.\nModels such as BERT encode world knowledge (Feldman et al., 2019; Jiang et al., 2020) . The first part of our work is a direct follow-up of prompt studies (Liu et al., 2021) targeting knowledge of hypernymy which has been shown to be high but fragile and inconsistent (Petroni et al., 2019; Hanna and Mare\u010dek, 2021; Ravichander et al., 2020; Bouraoui et al., 2019) . We leverage this knowledge to extend the investigation to logical words.\n4 Experiment 1: Content Words\n\nMetrics\nConsidering a hyponym-hypernym pair such as (robin, bird) , what probability does BERT assign to the hypernym word bird in a MASK position:\nP[MASK = bird | A robin is a MASK] (1)\nFor more than 30% of the pairs, the target hypernym is the top-1 word predicted, and in 80% of the pairs, it is in the top-100 (Petroni et al., 2019) . This indicates that BERT recognizes that robin and bird are likely to co-occur in a sentence. We ask whether the system recognizes that the hyponymhypernym relation is not symmetric, a critical fact that makes hypernymy a variant of entailment (and not of relevance). We do so by correcting the above probability with the probability of that same hypernym, albeit in the reverse configuration. Thus, we consider the log-ratio of (1) and (2):\nP[MASK = bird | A MASK is a robin] (2)\nFurthermore, like (Jiang et al., 2020; Hanna and Mare\u010dek, 2021) , we explore a set of prompts and not just one. For each pair of hyponym-hypernym (h, c) (h the head and c the class to which h belongs) we start from a template DET 1 h REL DET 2 c, with DET i determiners (e.g. the, a, an, \u03f5) and REL an instantiation of the hypernymy relation (e.g. is, is a subclass of, is a kind of, is a sort of, is a type of ). We use the model to compute a score for a set of determiners and relations and then we select the prompt with the highest one (more details in Appendix B, with explanations as to how this optimizes the form of the prompt without a priori biasing the final log-ratio scores).\nOnce the prompt is selected, we compute the following hypernymy score \u03c3:\nEQUATION\nwhich should be positive for well-understood pairs. Note that the subscript n and d stands for numerator and denominator respectively as the two are optimized separately. Other formulae are just as natural, such as the \u03c3 \u2032 presented in Appendix A. Table 1 : Mean (and standard deviation) of the \u03c3 scores for content words for BERT-base.\n\nMulti-token Prediction\nSome hyponym-hypernym pairs are made of multitoken expressions. For example, great ape is tokenized as two tokens. To overcome this difficulty we use the technique presented in (Feldman et al., 2019) consisting in computing the probability of each token independently and iteratively unmasking the token with the highest probability.\n\nKnowledge Sources\nTo build hyponym-hypernym pairs we used the following four different knowledge sources: Word-Net (Miller, 1995) \n\nResults\nWe conducted all experiments on BERT (Devlin et al., 2019) , ELECTRA (Clark et al., 2020) , Distil-BERT (Sanh et al., 2020) and ALBERT (Lan et al., 2020) . The results for BERT-base are given in Table 1 (see Appendix C for the other models). The mean of the scores is always positive (p < 0.001). This shows that these models encode the hypernymy relation better than chance. Yet, an average of 45% pairs are encoded in the wrong direction (see Fig. 1 for BERT-base). From a grounded approach of semantics, these are errors. In Experiment 2, we take them as an opportunity to look for traces of strong semantics command, as an internal consistency constraint.\n\nExperiment 2: Logical Words\nThe previous experiment establishes how models capture semantic relations between content nouns. We can use these results to investigate how the same models understand logical words. Concretely, one would expect a high probability for words like thus, so, therefore in the following sentence, and a low probability for words like because, since, for as they encode this entailment the other way around:\nEQUATION\nResults on hypernym-hyponym pairs show great variability hence, the high probability for thuslogical words in the sentence above is expected only if that particular pair, (robin, bird), is assigned a high hypernymy score by the model. For pairs that receive a very negative score, the expectation is in fact that the results would be reversed. This approach thus allows us to test the semantic consistency of the system. Consistency could be perfect for logical words, even if there are grounding errors with content words and world knowledge.\nWe tested 7 logical words of the thus class (thus, therefore, consequently, then, accordingly, so, hence), and 5 logical words of the because class (because, since, for, seeing, considering).\n\nMetrics\nWe define a score for a logical word w and a hyponym-hypernym pair (h, c) as in ( 5). This score measures the probability of finding, say, thus, in a sentence like (4) above, corrected for the probability of finding it in the reverse sentence.\nEQUATION\nAs before, we explore multiple prompts from a set of determiners DET and prefixes PRE (see details in Appendix B). A global score s(l) is obtained for a logical word w by averaging s(w; h, c) over the set of all content word pairs (h, c).\nAs seen in \u00a74.4, the hyponym-hypernym pairs are not all equal regarding to our hypernymy scores. We thus introduce s + (w) (resp. s \u2212 (w)): the average of s(w; h, c) on the top 5% 1 (resp. bottom 5%) pairs according to \u03c3 (or \u03c3 \u2032 ). Hence, for a coherent model having understood those logical words we expect s + \u2265 0 \u2265 s \u2212 for thus-words, and the reverse inequalities s + \u2264 0 \u2264 s \u2212 for becausewords. See Fig. 2 for a graphical representation of the expected results for a consistent model. \n\nResults\nTable 2 presents the global scores s for BERT-base (full results are in Appendix C). The thus-words almost always obtain a positive global score. The because-words sometimes show a negative score (as they should), but most of the times they obtain a positive score just like thus-words.\nFigure 3 presents the s + and s \u2212 scores obtained by BERT-base for the WordNet database relative to the \u03c3 score. The thus-words obtain a positive score on the best pairs, and a smaller score (albeit not necessarily negative) on the worst pairs. This is the expected result for a model that has correctly understood these logical words. However, becausewords display a somewhat similar behavior: a positive score over the best pairs, and a lower score over the worst pairs. All models show a qualitatively similar behavior, although ELECTRA seems to behave more consistently (see Appendix C). Overall, these results suggest that thus-words and because-words alike are understood as being of the thus-type.\n1 Empirically we explored several thresholds in percentiles or absolute sigma scores and obtained qualitatively similar results. The 5% threshold was chosen as inclusive enough to have enough pairs to make statistics, and strict enough to make sure the elements in there unambiguously passed the test from Experiment 1. Table 2 : Score s for the logical words we tested and for BERT-base. Red numbers represent unexpected results: assuming that content word pairs are well-understood, then a good result would be one with positive scores for the thus-words and negative scores for the becausewords. Here scores for thus-words are mainly positive, but they are also positive for the because-words.\n\nDiscussion\nThe similar behavior between thus and because is puzzling. A first possibility that could explain this would be a significant difference in frequency between thus words and because words in the training corpus. Indeed a signal that would be too weak for because could lead to a poor assimilation of its semantics. Unfortunately we did not check frequencies in the training corpus but according to the python package wordfreq 2 , because is for example one hundred times more frequent than therefore or thus, ruling out this explanation. Another possibility is that because is not used as the converse of thus, even by humans. Underlyingly, the result shows that the sentence This is a robin, because it is a bird may be more natural than the reverse This is a bird, because it is a robin. One may argue that the latter is somewhat tautological and, as such, not natural, while the former may find its use cases (e.g., when discriminating between a robin and an orangutan). One may wonder why the converse does not apply to thus-words however. To clear this issue one could look at the occurrences of thus and because in the relevant training corpora. Regardless, a conclusion we can already draw is that the simplest entailment-like semantics for because is very far from what is encoded in these models.\n\nConclusion\nWe propose an approach to the semantic study of BERT-type networks. First we evaluate the models on the non-symmetry of an entailment-like relation, namely hypernymy. The tested models show an average positive understanding of this relation. But this is accompanied with a large variance, showing that the relation is very often captured backward.\nThanks to these results we moved to testing logical words of type thus and because, which impersonate the entailment relation at the core of all enterprises in semantics. Its non-symmetry is one of its fundamental property. The models capture on average the non-symmetry of the words of type thus appropriately and they also show good consistency results, that is, a stronger signal for pairs that are themselves well-captured. However, the models obtain similar scores for the words of type because and, applying the same standards, they thus capture them backwards. Moreover all these results are to be qualified by their great variability across models and knowledge sources.\nThese properties albeit simple are yet at the core of what human semantics is however they are not reliably captured. This failure on these basic logical tests then raises questions regarding their otherwise impressive success. They also provide a method to reconstruct their actual semantics, if it is not human-like, and offers challenging tasks for these models.\n", "hypothesis": "Our results show impressive strengths of BERT-like models on these semantic tasks.", "answer": false}
{"title": "RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation", "content": "\nIntroduction\nText style transfer (TST) is a task that aims to control stylistic attributes of an input text without affecting its semantic content (Jin et al., 2022) . Research in TST has largely focused on English, thanks to the availability of large monolingual English datasets covering stylistic attributes like formality and simplicity (Rao and Tetreault 2018, Zhu et al. 2010, inter alia) . In recent years, however, multilingual and cross-lingual applications of TST have seen a steady gain in popularity (Briakou et al., 2021; Garcia et al., 2021; Krishna et al., 2022) . A notable instance of cross-lingual TST is attributecontrolled translation (ACT), in which attribute 1 conditioning is performed alongside machine translation (MT) to ensure that translations are not only Neutral Src (EN) After retiring from teaching, Cook became a novelist.\n\nFeminine Ref (NL)\nNadat ze stopte met lesgeven, werd Cook schrijfster.\nMasculine Ref (NL) Nadat hij stopte met lesgeven, werd Cook schrijver.\nTable 1 : Examples of attribute triplets from COCOA-MT and MT-GENEVAL. Attribute markers in the attribute-controlled translations are underlined.\ncorrect but match user-specified preferences, such as formality/honorifics (Sennrich et al., 2016; Niu et al., 2017; Michel and Neubig, 2018; Niu and Carpuat, 2020; Nadejde et al., 2022; Wang et al., 2022) , gender (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Saunders and Byrne, 2020) , and length (Lakew et al., 2019; Schioppa et al., 2021) .\nACT is especially important for sectors like customer service and business communication, where stylistic differences can have an impact on user perception (e.g., misgendering customers or speaking to them in an appropriately informal tone can be offensive or disconcerting). Table 1 gives examples of ACT for formality and gender. Most prior work on ACT relies on a supervised adaptation component that conditions the generative model on the selective attribute. However, few annotated ACT datasets are available, and they generally cover only a limited set of languages and attributes. Thus, enabling few-shot or zero-shot ACT would facilitate applying attribute control to less-resourced attributes and langauges.\nIn this paper, we introduce a new approach for ACT: Retrieval and Attribute-Marking enhanced Prompting (RAMP). Recent studies have shown that large language models (LLMs) can perform MT out of the box using the prompting paradigm (Brown et al., 2020; Lin et al., 2022; Chowdhery et al., 2022) . We build on this, prompting LLMs to perform attribute-controlled MT through two innovations: ( 1 Here is a sentence: {You will always be welcome here.} Here is its Spanish translation written in a formal style: {Siempre ser\u00e1 bienvenido aqu\u00ed.} The translated sentence conveys a formal style by using words such as 'ser\u00e1'.\n----Here is a sentence: {I wish you welcome and enjoy your stay.} Here is its Italian translation written in a formal style: {Le do il benvenuto e si goda il soggiorno.} The translated sentence conveys a formal style by using words such as 'Le', 'si goda'.\n----Here is a sentence: {You're welcome.} Here is its French translation written in a formal style: { EN: You're welcome. explicit attribute marking.\nRecent works adopting the prompting paradigm for text style transfer have mainly focused on the generalization capabilities of large English-centric LMs for zero-shot style transfer using previously unseen style descriptions (Suzgun et al., 2022; Reif et al., 2022) . However, prior work on other NLP tasks has shown that cross-lingual prompting of multilingual LLMs can be effective (Zhao and Sch\u00fctze, 2021; Zhou et al., 2022; Huang et al., 2022) . As such, we leverage multilingual LLMs and extend their ACT capabilities cross-lingually to languages not covered by the in-context examples, thus enabling zero-shot ACT.\n\nPreliminaries\nAttribute-Controlled Translation ACT takes two inputs, a sentence x and a desired target attribute a \u2208 A (with A being the space of attributes), and outputs a translation y that complies with the specified attribute. It can be formulated as a function f : (x, a) \u2192 y. In our experiments, we use attribute values provided by the COCOA-MT formality translation dataset and the MT-GENEVAL gender translation dataset, i.e., A = {formal, infor-mal} or {female, male}. 2 Prompting In the prompting paradigm for decoder-only LLMs, inputs are given as decoding prefixes to the model, usually combined with natural language instructions for output generation. In style-controlled translation, we formulate the prompt for target language l and attribute a using the text \"Here is a sentence: {x} Here is its l translation written in a a style:\" to produce the 2 See Section 5 for ethical considerations. output y. 3 In the few-shot setting, we provide a sequence of k labeled in-context examples before the unlabeled input, which can be formulated as a function f : {(x 1 , l 1 , a, y 1 ), . . . , (x k+1 , l k+1 , a)} \u2192 y k+1 .\n\nOur Approach: RAMP\nRAMP builds on the success of the prompting paradigm on few-shot generation tasks such as monolingual text style transfer (Reif et al., 2022) and MT (Garcia and Firat, 2022; Agrawal et al., 2022) by creating more informative prompts through similarity retrieval and attribute marking. See Figure 1 for an illustration of RAMP.\n\nSimilarity Retrieval\nIn standard prompting, incontext examples are sampled randomly from the pool of labeled examples D A . In RAMP, we select examples based on their similarity with the input text. We first embed both the input text and the source texts of D A using all-MiniLM-L6-v2 (Wang et al., 2020) . Then, the top-k most similar examples are retrieved for the input text based on cosine similarity. These are then used in a descending order w.r.t. similarity as the in-context examples in the inference prompt. As demonstrated in Figure 1 , the in-context example \"You will always be welcome here.\" has the highest similarity to the test example \"You're welcome.\" so it is prompted first.\n\nAttribute Marking\nIn standard prompting, incontext examples are provided without explicit information on why they satisfy the prompting objective. Inspired by recent studies that have shown that decomposition of complex tasks can improve prompting quality (Nye et al., 2021; Wei et al. , 2022), we include for every in-context example an additional sentence directly after the target sentence that specifies which text spans convey the desired attribute (e.g., \"The translated sentence conveys a formal style by using words such as 'Vous'.\"). In our experiments, we use the gold attribute spans included in the CoCoA-MT and MT-GenEval datasets. In section 4 we suggest possibilities for automatically deriving attribute spans when gold training labels are not available.\nAR ES FR HI PT DE IT JA RU NL COCOA-MT \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 MT-GENEVAL \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 XGLM \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 BLOOM \u2713 \u2713 \u2713 \u2713 \u2713\n\nCross-Lingual Prompting\nThe similarity retrieval component of RAMP requires a large pool D A from which to find appropriate incontext examples for prompting. Low-resource attributes or language pairs may have insufficient or no annotated data from which to retrieve such examples. To mitigate this issue, we introduce crosslingual prompting, in which the target side of the in-context examples differs from the desired target language of the translation task. As demonstrated in Figure 1 , we study whether the system can leverage examples in one language (e.g., attribute indicators in Spanish) to produce the same attribute in another (e.g., French). Two main features of our RAMP model allow us to perform cross-lingual prompting: (1) the use of multilingual LLMs, and (2) the example retrieval step, which is done on the source language only.\n3 Experiments\n\nDatasets\nWe experiment on two multilingual ACT datasets: instead explicitly controlling target gender. Both datasets have gold annotations for attributemarked target spans, and both cover translation from English into multiple diverse target languages. We list their target languages in Table 2 .\n\nLarge Language Models (LLMs)\nWe select three massively multilingual decoderonly LLMs for the prompting experiments: XGLM (Lin et al., 2022) , BLOOM (BigScience, 2022) and GPT-NEOX (Black et al., 2022) . The selected models span three orders of magnitude in terms of number of parameters and differ in the languages that they cover (see Table 2 ). Appendix D motivates our choice of models in more detail. GPT-3 is not included because it is not freely accessible and it is not intended for multilingual use-cases.\n\nBaseline\nAttribute tagging is a standard method for ACT, so we include a baseline following the approach and configuration used by Nadejde et al. ( 2022): a transformer MT model (Vaswani et al., 2017) pre-trained on public parallel data and further finetuned on contrastive training pairs with attribute tags (from either COCOA-MT or MT-GENEVAL). We refer to this as adapted MT.\n\nEvaluation Metrics\nWe measure translation quality with BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020) . For attribute accuracy, we use both (1) the lexical matching metrics provided with COCOA-MT and MT-GENEVAL (Lexical-Accuracy) and ( 2) sentence encoders trained on contrastive examples (Sentential-Accuracy). For (2), we train multilingual classifiers on top of the mDeBERTa-v3 encoder (He et al., 2021) . High-performance pretrained classifiers have been shown to produce attribute accuracy estimates closer to human judgments for style transfer (Lai et al., 2022) . Table 3 presents the accuracy of the classification models on the test sets of their respective datasets, averaged over all languages. Unlike lexical accuracy, the multilingual attribute classifier does not penalize text generated in incorrect languages. Thus, in cross-lingual prompting experiments, we include a step of language detection 5 so that generated sentences not in the requested target language are considered incorrect.\n\nResults: Same-Language Prompting\nWe first evaluate the effectiveness of RAMP for formality-and gender-controlled translation where the language pair used for in-context examples is the same as the one used in the prompt candidate (e.g., EN\u2192ES formality-controlled translation using EN\u2192ES in-context examples). We test XGLM 7.5B and BLOOM 175B with 16 in-context examples on both tasks. 6 Table 4 presents our results alongside the adapted MT baseline. The base model uses in-context examples that are sampled randomly from the pool of labeled examples. We also include an ablation that adds attribute marking only on top of base, without similarity retrieval (+mark).\nUsing just attribute marking consistently improves attribute accuracy of the generated text, but it leads to degradation of COMET on COCOA-MT. The complete RAMP with similarity retrieval not only compensates for the COMET degradation but also improves quality and attribute metrics across the board, especially for the high-capacity BLOOM 175B model.\nAdapted MT outperforms BLOOM 175B on MT-GENEVAL in all metrics, but underperforms it on COCOA-MT. This suggests that it is challenging to do fine-grained comparison between LLMs and standard MT systems as they might have different domain coverage. BLOOM 175B consistently outperforms XGLM 7.5B in both generic translation quality and attribute control accuracy, so we proceed with using BLOOM 175B in the crosslingual prompting setting.\n\nResults: Cross-Lingual Prompting\nWe have demonstrated the effectiveness of selecting similar same-language examples to build the prompt, echoing contemporary work (Liu et al., 2022; Agrawal et al., 2022) . In this section, we evaluate the cross-lingual prompting option, i.e., retrieving in-context examples from other target languages besides the desired language of translation. We test this zero-shot setting using the leave-oneout strategy, and results of tested language pairs are averaged. 7 Table 4 presents our results using BLOOM 175B. On both test sets, compared to the baseline, we observe improved attribute accuracy and comparable or better generic translation quality when using RAMP with cross-lingual prompting.\nWe do observe translation quality degradation with RAMP on some target languages of COCOA-MT, e.g., ES. Manual analysis shows that repeated inaccurate retrieval results could lead to hallucinations. 8 For example, RAMP retrieves multiple sentences containing \"million\" for the input \"If you got it why not? He is worth over 20 billion dollars after all.\". This results in mistranslation of billion to million (millionario): \"Si lo tienes, \u00bfpor qu\u00e9 no? Es millonario despu\u00e9s de todo.\". We give detailed examples in Appendix H.\n\nConclusions\nWe introduced the new RAMP in-context learning approach to leverage attribute annotations and similar same-language or cross-lingual examples for better prompting quality. We demonstrated its effectiveness with multilingual LLMs for both formalitycontrolled and gender-controlled translation. We use gold annotations for attribute marking, but we leave unsupervised automatic attribute span extraction as future work.\n", "hypothesis": " Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings.\n* Work conducted during an internship at Amazon.", "answer": true}
{"title": "Do GPTs Produce Less Literal Translations?", "content": "\nIntroduction\nDespite training only on a language-modeling objective, with no explicit supervision on aligned parallel data (Briakou et al., 2023) , LLMs such as GPT-3 or PaLM (Brown et al., 2020; Chowdhery et al., 2022) achieve close to state-of-the-art translation performance under few-shot prompting (Vilar et al., 2022; Hendy et al., 2023) . Work investigating the output of these models has noted that the gains in performance are not visible when using older surface-based metrics such as BLEU (Papineni et al., 2002a) , which typically show large losses against NMT systems. This raises a question: How do these LLM translations differ qualitatively from those of traditional NMT systems?\nWe explore this question using the property of translation literalness. Machine translation systems have long been noted for their tendency to produce source He survived by the skin of his teeth .\n\nNMT\nIl a surv\u00e9cu par la peau de ses dents . GPT-3 Il a surv\u00e9cu de justesse . Table 1 : An example where GPT-3 produces a more natural (non-literal) translation of an English idiom. When word-aligning these sentences, the source word skin remains unaligned for the GPT-3 translation.\noverly-literal translations (Dankers et al., 2022b) , and we have observed anecdotally that LLMs seem less susceptible to this problem (Table 1 ). We investigate whether these observations can be validated quantitatively. First, we use measures based on word alignment and monotonicity to quantify whether LLMs produce less literal translations than NMT systems, and ground these numbers in human evaluation ( \u00a7 2). Next, we look specifically at idioms, comparing how literally they are translated under both natural and synthetic data settings ( \u00a7 3).\nOur investigations focus on the translation between English and German, Chinese, and Russian, three typologically diverse languages. Our findings are summarized as follows: (1) We find that translations from two LLMs from the GPT series of LLMs are indeed generally less literal than those of their NMT counterparts when translating out of English, and (2) that this is particularly true in the case of sentences with idiomatic expressions.\n\nQuantifying Translation Literalness\nWe compare the state-of-the-art NMT systems against the most capable publicly-accessible GPT models (at the time of writing) across measures designed to capture differences in translation literalness. We conduct both automatic metric-based as well as human evaluations. We explain the evaluation and experimental details below. for evaluation (Barrault et al., 2021) .\n\nMeasures of Quality\nWe use COMET-QE 1 (Rei et al., 2020) as the Quality Estimation (QE) measure (Fomicheva et al., 2020) to quantify the fluency and adequacy of translations. Using QE as a metric presents the advantage that it precludes the presence of any reference bias, which has been shown to be detrimental in estimating the LLM output quality in related sequence transduction tasks (Goyal et al., 2022) . On the other hand, COMET-QE as a metric suffers from an apparent blindness to copy errors (i.e., cases in which the model produces output in the source language) (He et al., 2022) . To mitigate this, we apply a language identifier (Joulin et al., 2017) on the translation output and set the translation to null if the translation language is the same as the source language. Therefore, we name this metric COMET-QE + LID.\n\nMeasures of Translation Literalness\nThere do not exist any known metrics with high correlation geared towards quantifying translation literalness.\nWe propose and consider two automatic measures at the corpus-level:\n1. Unaligned Source Words (USW): Two translations with very similar fluency and adequacy could be differentiated in terms of their literalness by computing word to word alignment between the source and the translation, then measuring the number of source words left unaligned. When controlled for quality, a less literal translation is likely to contain more unaligned source words (as suggested in Figure 1 ).\n\nTranslation Non-Monotonicity (NM):\nAnother measure of literalness is how closely the translation tracks the word order in the source. We use the non-monotonicity metric proposed in Schioppa et al. (2021) , which computes the deviation from the diagonal in the word to word alignment as the non-monotonicity measure.\n1 wmt20-comet-qe-da\nThis can also be interpreted as (normalized) alignment crossings, which has been shown to correlate with translation non-literalness (Schaeffer and Carl, 2014) .\nWe use the multilingual-BERT-based awesomealigner (Devlin et al., 2019; Dou and Neubig, 2021) to obtain the word to word alignments between the source and the translation. Table 2 presents an illustration of translations with different USW and NM scores 2 , obtained from different systems.\n\nSystems Under Evaluation\nWe experiment with the below four systems (NMT and LLMs):\n1. WMT-21-SOTA: The Facebook multilingual system (Tran et al., 2021) won the WMT-21 News Translation task (Barrault et al., 2021) , and thereby represents the strongest NMT system on the WMT'21 test sets.\n2. Microsoft-Translator: MS-Translator is one of the strongest publicly available commercial NMT systems (Raunak et al., 2022) .\n3. text-davinci-002: The text-davinci-002 model is an instruction fine-tuned model in the GPT family (Brown et al., 2020) . It represents one of the strongest publicly-accessible LLMs (Liang et al., 2022) .\n4. text-davinci-003: The text-davinci-003 model further improves upon text-davinci-002 for many tasks 3 (Liang et al., 2022) .\nFor both the GPT models, we randomly select eight samples from the corresponding WMT-21 development set, and use these in the prompt as demonstrations for obtaining all translations from GPTs.\n\nResults\nWe compare the performance of the four systems on the WMT-21 test sets. Figure 1 shows the results of this comparison. A key observation is that while the GPT based translations achieve superior COMET-QE+LID scores than Microsoft Translator across the language pairs (except En-Ru), they The NMT Systems and GPT models achieve similar COMET-QE+LID Scores (Top), there exists a significant gap in the number of unaligned source words (USW) across the datasets (Bottom). Further, GPT translations obtain higher non-monotonicity scores for E-X translations (Middle).\nalso consistently obtain considerably higher number of unaligned source words. This result holds for the comparison between the WMT-21-SOTA and GPT systems as well. Further, GPT translations also consistently show higher non-monotonicity for E\u2192X translations. However, this is not the case for translations into English, wherein the multilingual WMT-21-SOTA system obtains very close non-monotonicity measurements. The combined interpretation of these measurements suggests that GPTs do produce less literal E\u2192X translations.\n\nHuman Evaluation\nWe verify the conclusion from the results in Figure 1 by conducting a human evaluation of translation literalness on 6 WMT-22 language pairs: En-De, En-Ru, En-Zh and De-En, Ru-En, Zh-En. For each language pair, we randomly sample 100 source-translation pairs, with translations obtained from MS-Translator (a strong commercial NMT system) and text-davinci-003 (a strong commercial LLM) (Hendy et al., 2023) . We used zero-shot text-davinci-003 translations for human evaluations in order to eliminate any biases through the use of specific demonstration examples. In each case, we ask a human annotator (bilingual speaker for Zh-En, target-language native plus bilingual speaker otherwise) to annotate 100 translations from both GPT and MS-Translator and select which of the two translations is more literal. The human annotation interface is described in Appendix A. The results in Table 3 show that the annotators rate the GPT translations as less literal.\nLang Experiments on Best WMT-22 NMT Systems Further, we also experiment with the WMT-Best systems on the WMT-22 General Machine Translation task (Kocmi et al., 2022) . We evaluate USW and NM on De-En, Ja-En, En-Zh and Zh-En, since on each of these language pairs, text-davinci-003's few-shot performance is very close to that of the WMT-Best system as per COMET-22 (Rei et al., 2022) , based on the evaluation done in Hendy et al. (2023) . We report our results in Table 4 , which shows our prior findings replicated across the language pairs. For example, text-davinci-003, despite obtaining a 0.2 to 0. \n\nEffects On Figurative Compositionality\nIn this section, we explore whether the less literal nature of E\u2192X translations produced by GPT models could be leveraged to generate higher quality translations for certain inputs. We posit the phenomenon of composing the non-compositional meanings of idioms (Dankers et al., 2022a) with the meanings of the compositional constituents within a sentence as figurative compositionality. Thereby, a model exhibiting greater figurative compositionality would be able to abstract the meaning of the idiomatic expression in the source sentence and express it in the target language non-literally, either through a non-literal (paraphrased) expression of the idiom's meaning or through an equivalent idiom in the target language. Note that greater nonliteralness does not imply better figurative compositionality. Non-literalness in a translation could potentially be generated by variations in translation different from the desired figurative translation.\n\nTranslation with Idiomatic Datasets\nIn this section, we quantify the differences in the translation of sentences with idioms between traditional NMT systems and a GPT model. There do not exist any English-centric parallel corpora dedicated to sentences with idioms. Therefore, we experiment with monolingual (English) sentences with idioms. The translations are generated with the same prompt in Section 2. The datasets with natural idiomatic sentences are enumerated below:\n\u2022 MAGPIE (Haagsma et al., 2020) contains a set of sentences annotated with their idiomaticity, alongside a confidence score. We use the sentences pertaining to the news domain which are marked as idiomatic with cent percent annotator confidence (totalling 3,666 sentences).\n\u2022 EPIE (Saxena and Paul, 2020) contains idioms and example sentences demonstrating their usage. We use the sentences available for static idioms (totalling 1,046 sentences).\n\u2022 The PIE dataset (Zhou et al., 2021) contains idioms along with their usage. We randomly sample 1K sentences from the corpus.\n\nResults\nThe results are presented in Table 5 . We find that text-davinci-002 produces better quality translations than the WMT'21 SOTA system, with greater number of unaligned words as well as with higher non-monotonicity.\nFurther Analysis Note that a direct attribution of the gain in translation quality to better translation of idioms specifically is challenging. Further, similarity-based quality metrics such as COMET-QE themselves might be penalizing non-literalness, even though they are less likely to do this than surface-level metrics such as BLEU or ChrF (Papineni et al., 2002b; Popovi\u0107, 2015) . Therefore, while a natural monolingual dataset presents a useful testbed for investigating figurative compositionality abilities, an explicit comparison of figurative compositionality between the systems is very difficult. Therefore, we also conduct experiments on synthetic data, where we explicitly control the finegrained attributes of the input sentences. We do this by allocating most of the variation among the input sentences to certain constituent expressions in synthetic data generation.\n\nSynthetic Experiments\nFor our next experiments, we generate synthetic English sentences, each containing expressions of specific type(s): (i) names, (ii) random descriptive phrases, and (iii) idioms. We prompt text-davinci-002 in a zero-shot manner, asking it to generate a sentence with different instantiations of each of these types (details are in appendix B). We then translate these sentences using the different systems, in order to investigate the relative effects on our literalness metrics between systems and across types. In each of the control experiments, we translate the synthetic English sentences to German. The results are presented in Table 7 .\nResults Table 6 shows that the percentage of unaligned source words is highest in the case of idioms, followed by random descriptive phrases & named entities. The results are consistent with the hypothesis that the explored GPT models produce less literal E\u2192X translations, since named entities or descriptive phrases in a sentence would admit more literal translations as acceptable, unlike sentences with idioms. Davinci-002 obtains a much higher COMET-QE score in the case of translations of sentences with idioms, yet obtains a higher percentage of unaligned source words. Similarly, the difference in non-monotonicity scores is also considerably higher for the case of idioms. These results provide some evidence that the improved results of the GPT model, together with the lower literalness numbers, stem from correct translation of idiomatic expressions. Table 7 shows that this effect only increases with the number of idioms.\n\nDiscussion\nIn our experiments conducted across different NMT systems and GPT models, we find evidence that GPTs produce translations with greater nonliteralness for E\u2192X in general. There could be a number of potential causes for this; we list two plausible hypotheses below:\nParallel Data Bias NMT models are trained on parallel data, which often contains very literal webcollected outputs. Some of this may even be the output of previous-generation MT systems, which is highly adopted and hard to detect. In addition, even high quality target text in parallel data always contains artifacts that distinguishes it from text originally written in that language, i.e. the 'translationese' effect (Gellerstam, 2005) . These factors could likely contribute to making NMT translations comparatively more literal.\nLanguage Modeling Bias Translation capability in GPTs arises in the absence of any explicit supervision for the task during the pre-training stage. Therefore, the computational mechanism that GPTs leverage for producing translations might be different from NMT models, imparting them greater abstractive abilities. This could have some measurable manifestation in the translations produced, e.g., in the literalness of the translations.\nDifferences in E\u2192X and X\u2192E In E\u2192X, we consistently find that GPT translations of similar quality are less literal and in the X\u2192E direction, we observe a few anomalies. For X\u2192E, in Figure 1 , in all but one comparison (WMT-21-SOTA vs GPTs for De-En) GPTs obtain higher measures for non-literalness. On the other hand, we did not see anomalies in the trend for E\u2192X directions.\n\nVariations in Experimental Setup\nWe also experimented with a variant of USW and NM which doesn't use the alignments pertaining to stopwords. Each of our findings remain the same, with relatively minor changes in magnitudes but not in system rankings. Similarly, we observed a greater tendency towards less literalness in GPT translations in both few-shot and zero-shot settings, when compared across a range of NMT systems.\n\nSummary and Conclusion\nWe investigated how the translations obtained through LLMs from the GPT family are qualitatively different by quantifying the property of translation literalness. We find that for E\u2192X translations, there is a greater tendency towards nonliteralness in GPT translations. In particular, this tendency becomes evident in GPT systems' ability to figuratively translate idioms.\n", "hypothesis": "However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.  In this work, we investigate these differences in terms of the creativity of translations produced by the two systems.", "answer": false}
{"title": "Text-to-SQL Error Correction with Language Models of Code", "content": "\nIntroduction\nText-to-SQL parsing is a classic semantic parsing task that finds wide applications (Zelle and Mooney, 1996; Tang and Mooney, 2000) . Since the release of Spider (Yu et al., 2018) , a cross-database text-to-SQL benchmark, many semantic parsers with decent performance have been developed (Lin et al., 2020; Wang et al., 2020; Deng et al., 2021; Rubin and Berant, 2021; Scholak et al., 2021) . Nonetheless, state-of-the-art semantic parsers are still not accurate enough. As a result, their users need to constantly correct wrongly predicted SQL queries, which can be as time-consuming and errorprone as writing a SQL query from scratch (Jorgensen and Shepperd, 2007; Weiss et al., 2007) . Therefore, in this paper, we study the problem of automatic text-to-SQL error correction to better assist users in querying complex databases.\nWe first highlight that it is essential to factor in the compositional substructures within SQL queries, such as abstract syntax trees (Yin and Neubig, 2017; Guo et al., 2022) and data-flow graphs (Guo et al., 2021) , instead of treating code snippets as string sequences. Compared to individual tokens, substructures (e.g. SQL clauses) include more context of the entire program and are more semantically meaningful. Consequently, edit patterns of such substructures are more intuitive for humans to understand and easier for language models to learn. Moreover, while the pre-training corpora for language models of code, such as CodeT5 (Wang et al., 2021) , do not include many SQL queries based on their documentation, they naturally contain abundant examples of common data structures like dictionaries. Therefore, we hypothesize that transforming unfamiliar SQL queries into familiar data structures can help language models of code better perform structural editing of SQL queries.\nBased on these observations, we develop our error correction model and make two contributions. First, we propose considering SQL clauses instead of tokens as basic semantic units for editing. Using a context-free grammar, we can decompose a SQL query and identify its clauses by traversing its abstract syntax tree. Second, we propose a new representation of SQL queries and their edits that adheres more closely to common code pre-training corpora, including CodeSearchNet (Husain et al., 2020) , and makes the structures of a SQL query more explicit. With a decomposed SQL query, we pair each clause with its SQL keyword and represent the entire query as a Python dictionary. Then, we format edits on a wrong SQL query as a program that modifies data of the query's corresponding dictionary. Unlike token-level edits in existing work (Zhang et al., 2023) , such dictionary operations define all edits unambiguously and can be directly executed with a Python interpreter.\nThrough comprehensive experiments with different representations, we show that: (1) our proposed representation has the lowest zero-shot perplexity with CodeT5;\n(2) simply changing token-level edits to clause-level edits can effectively improve the performance of our models; and (3) our method improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines.\n\nText-to-SQL Error Correction\nGiven a natural language utterance u, a database schema s, and a wrong SQL query q \u2212 produced by an existing parser, our goal is to develop an error correction model that predicts a sequence of edit actions e and the correct query q + . Following previous work (Zhang et al., 2023) , we formulate our task as sequence-to-sequence generation:\nEQUATION\nwhere x = [u; s; q \u2212 ] is the concatenation of the given inputs and y = [e; q + ] is the concatenation of all edit actions and the resulting correct query. In this section, we study different representations of SQL queries (Section 2.1) and edits (Section 2.2) to better leverage language models of code.\n\nQuery Representation\nWe consider two representations for a predicted query: (1) the original SQL format and (2) our proposed PyDict (Python Dictionary) representation.\nTo prepare for editing, we disambiguate each SQL query following Rubin and Berant (2021) , including lower-casing non-value tokens, resolving table references, and formatting punctuation. This preprocessing normalizes SQL queries predicted by different base parsers and the gold annotations into the same format. To build our PyDict representation, we parse a SQL query into its abstract syntax tree (AST) with Spider's context-free grammar. We use depth-first search to traverse through the AST, find any nested substructures, and construct the dictionary representation bottom-up. Table 1 shows the \"SQL\" and \"PyDict\" representations of a SQL query (more details in Appendix A).\n\nEdit Representation\nWe first follow Zhang et al. (2023) performing 5-fold cross-validation on each parser, which approximates the actual evaluation setting.\nFollowing the evaluation setup in Yu et al. (2018) , we split Spider's training set into five roughly equal subsets by different databases. For each cross-validation fold, we train a text-to-SQL parser (Section 3.2) on four subsets and evaluate it on the remaining one. At inference time, we perform beam search with size 20 for each example and collect grammatical and executable parses in the beam. 2 If a SQL parse is not an exact set match or execution match to the gold annotation, we label it wrong and include it in our training set for error correction. Having synthesized our training dataset, we randomly sample 8 databases and their associated questions to construct a held-out development set. For development set examples, we only keep incorrect SQL parses with the highest beam confidence. For our error correction test set, we train each parser on the full Spider training set and evaluate it on the original Spider's development set without modifications. We similarly keep SQL parses with exact match or execution match errors. Table 2 summarizes the statistics of our data.\n\nModels\nText-to-SQL base parsers. We choose three textto-SQL parsers with different decoding strategies and levels of performance (Table 3 ). We elaborate on our selection criteria in Appendix B.\n\u2022 CodeT5 (Wang et al., 2021) : We fine-tune CodeT5-base following Xie et al. (2022) . This parser represents those using beam search decoding and having a lower accuracy. \u2022 BRIDGEv2 (Lin et al., 2020): A representative parser with constrained decoding and achieving a medium-level accuracy. \u2022 SmBoP (Rubin and Berant, 2021) : A representative parser with bottom-up decoding and achieving higher accuracy.\nError correction models. We use two language models of code in all our experiments: \u2022 CoditT5 (Zhang et al., 2023) : A language model pre-trained for code editing tasks by injecting noises to code snippets in CodeSearch-Net (Husain et al., 2020) and then denoising with token-level edit representations. \u2022 CodeT5 (Wang et al., 2021) : A language model pre-trained for general code understanding and generation with four different pre-training objectives. We compare the existing SQL+Token-Level representation with our proposed ones: SQL+Clause-Level, PyDict+Clause-Level, and PyDict+Program on CodeT5 and the first three on CoditT5. 3 Implementation details are in Appendix C.\n\nEvaluation\nWe use the increase in Exact Set Match (EM) and Execution Match (EX) accuracy on our error correction test set to measure each model's performance. Because CoditT5's experiments assume the input program has at least one error, we keep this assumption for fair comparisons. To construct a test set satisfying this assumption, we have to compare parser-generated SQL queries with gold annotations (Section 3.1). Thus, we use the Spider development set as our test set and split the Spider training set to build a held-out development set (Table 2 ) to select model checkpoints during training. We also include results on our held-out development set in the appendix (Table E .1).\n\nMain Results\nWe summarize our main results in this section. To ensure robustness, we repeat all experiments with 3 different random seeds and report the average performances with standard deviations. Our model can also be used in an interactive framework that allows users to select edit actions from the top-k beam candidates. We include more experiments with simulated user interactions in Appendix E. Our representation's perplexity is the smallest. We validate that our PyDict+Program representation adheres more closely to the code pre-training corpora by measuring its zero-shot perplexity on CodeT5 using our development set (Section 3.1). \n\nError Analysis\nAdditionally, we conduct an error analysis (Table 4 ) by sampling 100 wrong parses from all three parsers and classifying them into five categories:\n\u2022 Database Grounding: A generated SQL query has the correct structure, but some table/column names or entity values are wrong. \u2022 Incorrect Structure: A generated SQL query has missing, wrong, or redundant structures. \u2022 Syntax & Grammar: A generated SQL query violates the programming language's syntax. \u2022 False Negative: A generated SQL query is semantically correct but not captured by evaluation metrics, or the gold annotation is wrong. \u2022 Other: All other errors, such as wrong aggregation functions, besides the above categories. Since the error distributions for each parser are similar, as an example, we discuss our findings based on the strongest parser, SmBoP:\nDatabase grounding is the major type of error. Among the 100 samples from SmBoP, we find that 54 of them have database grounding errors. \n\nRelated Work\nSince the release of CodeBERT (Feng et al., 2020) , many language models of code have emerged for program understanding and generation (Ahmad et al., 2021; Chen et al., 2021; Guo et al., 2021; Wang et al., 2021; Guo et al., 2022; Fried et al., 2023; Nijkamp et al., 2023) . In addition to programrelated tasks, recent work shows they also excel at processing natural language structures. Using code as meaning representations (MRs), we can leverage language models of code in various tasks, such as commonsense reasoning (Madaan et al., 2022), action planning (Singh et al., 2022) , and event extraction (Wang et al., 2022) . In fact, how to design MRs to reduce model learning difficulty is a salient research question in semantic parsing (Guo et al., 2019; Gan et al., 2021b; Nie et al., 2022) .\nOur work demonstrates that program-related tasks themselves can also benefit from code-based MRs. Specifically, we apply such MRs to SQL error correction, a variant of automatic program repair tasks (Tufano et al., 2019; Panthaplackel et al., 2022; Zhang et al., 2023) . Although SQL is a code-based MR, it is much harder for models to learn compared to other MRs, such as FunQL and lambda calculus (Li et al., 2022) . Consequently, without many SQL queries in their pre-training corpora, language models of code can underperform state-of-the-art text-to-SQL parsers. By converting SQL queries into Python dictionaries, we can explicitly represent their compositional substructures and define edit actions as programs, which reduces the learning difficulty for language models of code and yields better performance.\n\nConclusion and Future Work\nThis paper presents a study on developing a text-to-SQL error correction model with clause-level edits and different representations. Our comprehensive experiments demonstrate that clauses are better semantic units than tokens for editing SQL queries and mimicking patterns in code pre-training corpora helps better leverage language models of code. As a future direction, we plan to incorporate our model into interactive semantic parsing frameworks (Li et al., 2020; Yao et al., 2019 Yao et al., , 2020;; Zeng et al., 2020) by suggesting possible edits to users once a wrong parse is identified. In this way, users would more efficiently correct parse errors and get better assistance. We also plan to experiment with other language models of code (Fried et al., 2023; Nijkamp et al., 2023 ) and text-to-SQL datasets (Zelle and Mooney, 1996; Gan et al., 2021a) to verify the generalizability of our method.\n", "hypothesis": " Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines.", "answer": true}
{"title": "Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction", "content": "\nIntroduction\nFor many real-world applications, it is crucial for task-oriented dialogue (TOD) systems to complete user requests while strictly adhering to established procedures. For example, consider a customer service agent who must first verify a client's details before changing their password. Although large language models have demonstrated potential in modeling such dialogues, they require large Figure 1 : The Knowledge-Augmented Dialogue System (KADS) is composed of two modules: a knowledge retriever and a language model. The knowledge retriever takes the inner product as a measure of similarity between an embedded dialogue and each document in a provided knowledge base containing procedural instructions. The most similar document is then passed to a language model which attends over both the dialogue and retrieved document to generate the agent's next action.\namounts of data with consistent procedural representations to implicitly store procedures in the parameters of their underlying networks. In practical settings, such high-quality data is not always readily available as some procedures may naturally occur infrequently or change over time. In this paper, we explore a solution to TOD modeling which improves performance in low-data settings by referencing explicitly stored agent guidelines. We outline a methodology of incorporating procedural knowledge (i.e., knowledge concerning the requisite steps to address a user inquiry) into a language model with the objective of predicting agent actions in dialogue tasks. Our proposed system, the Knowledge-Augmented Dialogue System (KADS), consists of two modules: a knowledge retriever which, given a dialogue between an agent and user, retrieves the most pertinent instructions from a knowledge base of agent procedures and a language model which considers the retrieved instructions along with the ongoing dialogue to inform an action prediction (see architecture in Figure 1 ).\nIn prior work, retrieval-enhanced language models have achieved success integrating external knowledge from internet searches into conversational agents (Shuster et al., 2022; Thoppilan et al., 2022) . However, a more controllable approach is necessary for instruction retrieval in task-oriented dialogue. Rather than querying the open web, it's more suitable to perform retrieval over a closed set of documents, like in (Guu et al., 2020; Lewis et al., 2020) . However, while the training schemes utilized in these works sufficiently prime a model for question-answering tasks, they are not as effective for action prediction.\nFollowing the lines of (Henderson and Vuli\u0107, 2021) , which introduces a unique pre-training objective for slot-labeling, our method leverages custom objectives suited for action prediction tasks. We employ a specialized warm-up task where dialogues are matched with corresponding procedural instructions to ensure that the knowledge retrieval module is initialized with reasonable dialogue and document embeddings. Then, the system is trained on an special case of masked language modeling in which masked actions are predicted from customeragent dialogues. Finally, we found it necessary to encourage our system to incorporate signal from retrieved procedures by routinely freezing the language model's weights during training.\nWe evaluated this approach on two dialogue tasks-action state tracking and workflow discovery-using two task-oriented dialogue datasets: Action-Based Conversations Dataset and Schema-Guided Dialogue. Our results suggest that KADS yields improved action prediction accuracy against several baselines, including an unaugmented language model and a language model augmented with static guidelines, on both in-and out-of-distribution procedures. Furthermore, we demonstrate that knowledge augmentation bolsters our system's ability to predict actions that occur infrequently in the training data.\n\nDialogue Tasks\nTOD systems are employed for a variety of tasks including action state tracking and workflow discovery.\nAction state tracking (AST) aims to predict the next action performed by an agent during an interaction with a customer (Chen et al., 2021) . Formally, we represent an interaction as a sequence of turns x belonging to one of three categories: agent utterances x a ([agent]), agent actions x b ([action]), or customer utterances x c ([customer]). The model receives an interaction between a customer and agent up to turn t where prefix tokens p indicate the turn category: X = p 0 x 0 p 1 x 1 ... p t x t with p \u2208 [agent], [action], [customer] . See Appendix B for an example. The model then predicts the following agent action x b t+1 which consists of a button, or b-slot, and any corresponding slot values if they are present:\nx b t = b 0 t : v 00 t , v 01 t .\nThe goal of workflow discovery (WD) is to recover the workflow-the set of ordered actions taken by an agent-given a complete dialogue between a customer and agent (Hattami et al., 2022) . Formally, we represent a dialogue as a sequence of turns belonging to one of two categories: agent utterances or customer utterances. The model receives a dialogue of length T between a customer and agent where prefix tokens indicate the turn category: 3 Approach\nX = p 0 x 0 p 1 x 1 ... p T x T with p \u2208 [agent], [\n\nArchitecture\nThe end goal of KADS is to learn a distribution p(y|X) over possible action sequences y given an interaction or dialogue X. Our approach utilizes a knowledge retriever module to produce a relevance score between a given procedural document z and X. We calculate the relevance score according to (Devlin et al., 2019) as the inner product of the BERT vector embeddings of X and z. A retrieval distribution p(z|X) is obtained by taking the softmax over the relevance scores corresponding to each available document and the given interaction or dialogue. Finally, we train a T5 language model (Raffel et al., 2020) , conditioned on both the retrieved document z and the interaction X, to generate an action sequence y, where the likelihood of generating y is obtained by treating z as a latent variable and marginalizing over all possible documents: p(y|X) = z\u2208Z p(y|X, z)p(z|X).\n\nTraining\nTo train KADS we follow a three-step procedure: first, we warm-up the knowledge retriever's embedding modules with a dialogue-document matching task; then, we pre-train the full model with actionoriented masked language modeling (MLM); finally, we train on one of two downstream dialogue tasks-AST or WD. For all tasks except dialoguedocument matching, our training objective is to maximize the log-likelihood logp(y|X) of the correct output action sequence y. However, calculating the marginal probability over documents in a knowledge corpus can become costly as the number of documents grows, so we approximate this probability by summing over the top 5 documents with the highest probability under p(z|X). We then compute the gradient of the log-likelihood with respect to the model parameters of both the knowledge retriever and language model and optimize using stochastic gradient descent.\nWe first perform the dialogue-document matching warm-up routine to ensure that the knowledge retriever is initialized with reasonable dialogue and document embeddings. The embedding modules are pre-trained using a semi-supervised training procedure with the objective of retrieving the document that most likely corresponds to a specific dialogue. This label is determined according to which document has the highest action overlap with the dialogue or, when provided, which document corresponds to the user's ground-truth intent.\nFor the MLM pre-training task, we randomly mask action sequences from dialogue transcripts such that the system learns to retrieve relevant documents in order to better predict the actions corresponding to each [MASK] token. To prevent KADS from learning to ignore retrieved documents we employ several tricks during MLM training. First, we filter out dialogues with action sequences that are not detailed in the agent guidelines. This is done to ensure that only examples in which the knowledge retriever may be useful are present. Additionally, we freeze the language model weights with 0.9 probability to encourage updates to the knowledge retriever parameters which minimize the MLM loss.\n\nData\nWe evaluate KADS on two TOD datasets: Action-Based Conversations Dataset and Schema-Guided Dialogue. Both consist of multi-domain customer service interactions that loosely follow a set of predefined company policies which specify the actions to be taken by an agent to satisfy a particular customer inquiry. The core differences between these two datasets are their action and document structures.\nIn Action-Based Conversations Dataset (ABCD) (Chen et al., 2021) , actions are composed such that the b-slot belongs to a predefined set of b-slots which describe the action being taken (e.g., \"pull up account\") and slot values consist of any corresponding information provided by the user (e.g., \"johndoe@gmail.com\"). In a given interaction, an average of 4 actions are taken. The documents provided within ABCD are composed of a plain text description of a possible customer inquiry followed by an ordered set of action b-slots that should be performed by the agent.\nIn Schema-Guided Dialogue (SGD) (Rastogi et al., 2020) , we take action b-slots to be the description of how the agent will interact with a piece of information (e.g., \"inform\", \"confirm\", or \"request\") and values as the type of information in question (e.g., \"departure times\"). In this dataset, the average number of actions per interaction is significantly longer at 21 actions, and the documents corresponding to SGD consist of a customer inquiry followed by all of the information types, or values, that can be acquired to fulfill the given inquiry.\nWe use the train/dev/test splits presented in the original datasets (8034/1004/1004 and 16142/2482/4201 interactions per split for ABCD and SGD respectively), and hold out a randomlyselected subset of 10% of actions during training for out-of-distribution testing. See Appendix B for more details, including dialogue and corresponding document examples.\n\nResults\nThe evaluation of our TOD system begins with bslot and value prediction accuracy for both known and novel actions. We also examine the data efficiency of our approach by reporting these metrics for progressively reduced training pools. We compare our model's performance against a base T5 model and T5 with static guidelines-a comprehensive list of agent actions-appended to the input sequence (T5 + guide) 1 . Then, we assess the efficacy of our knowledge retriever in selecting relevant documents. Finally, an ablation study of our pre-training routine highlights the importance of our custom training procedure. See Appendix A for details of our experimental setup.\n\nIn-Distribution Performance\nWe first observe b-slot and value prediction accuracy on procedures observed during training (Table 1). On ABCD, KADS achieves higher b-slot prediction accuracy than our baselines for both tasks. The inclusion of a static guideline offers slightly improved accuracy on AST but is not nearly as effective as the dynamic guide provided by the knowledge retriever. We attribute the performance boost in part to KADS's ability to predict actions that are less represented during training.\nThis characteristic is evidenced by the model's performance in low-data settings (Figure 2 ). We observe that the difference in action prediction accuracy between our model and the unaugmented baseline increases when training on progressively fewer dialogues. Additionally, we find that, for the base and static guide models, the correlation between a b-slot's level of occurrence in the training data and the model's accuracy in predicting that bslot is notably higher (0.27 and 0.24 respectively) than in the knowledge-augmented model (0.18). We conclude from these results that KADS is more robust to low-data settings where the quantity of individual action occurrences is low or inconsistent.\n2 On SGD, we see similar trends for the AST task. However, for the WD task, which concerns recovering the entire action sequence from a dialogue at once, we see that knowledge augmentation does not maximum input sequence length.\n2 Value prediction accuracy is improved despite values not being included in the provided documents. This is likely a result of the model learning patterns between action b-slots and their corresponding values. provide substantial improvement in performance. This may be due to the nature of SGD dialogues, which contain multiple client requests, while the model is augmented with a singular document providing instructions for a singular customer request.\n\nOut-of-Distribution Performance\nNext, we evaluate the ability of KADS to generalize to novel procedures by assessing performance on actions not seen during training (Table 2 ). Both tasks, AST and WD, show knowledge augmentation to improve novel b-slot prediction accuracy over the baselines, coming only second to T5 trained on the full dataset (\"full data\") including \"out-of-distribution\" actions. These results demonstrate that KADS is able to relatively accurately predict new actions in a zero-shot fashion by making use of documents containing information about the action.\n\nDocument Selection Accuracy\nWe use document selection accuracy to assess how well our knowledge retriever selects documents that correspond to a customer's inquiry. On ABCD, we define the correct document as the document with the most action b-slots overlapping with the full customer-agent interaction. On SGD, where calls often consist of multiple customer inquiries, the correct document is instead defined as the document corresponding to the labeled customer intent for any given step of the interaction. In Table 3 , we see that approximate document selection accuracy for ABCD is near 90% while SGD is only slightly above 50%. This is likely due to the significant overlap in procedures for similar customer inquiries on the latter dataset. For example, making an appointment with a doctor, dentist, or hairstylist requires similar values to be filled, which results in related documents being somewhat interchangeable for these inquiries. Furthermore, we measure document selection accuracy on our pre-training tasks (Table 3 ): dialoguedocument matching and MLM. Notably, the knowledge retriever's document selection accuracy decreases between pre-training with the dialoguedocument matching task and fine-tuning on the final task. This is likely due to the objective changing from maximizing document selection accuracy to predicting correct action sequences, resulting in some drift from the selection of approximated \"correct\" documents.\n\nPre-training Scheme Ablations\nOur full training scheme is a multi-step process ensuring optimal performance from our Knowledge-Augmented Dialogue System. First, the knowledge Next, the full system is trained on an MLM task which acts as the simpler in-between before our final task. Finally, we train the model for one of our two downstream dialogue tasks. Removing any step from this procedure results in decreased performance on the final task. In Table 4 , we share b-slot and value prediction accuracy on AST after pretraining with several ablations of our full scheme. These results show that the elimination of either the dialogue-document matching or MLM task results in lower accuracy. These tasks, which allow our model to effectively harness the knowledge retrieval module, are crucial to our pre-training procedure.\n\nConclusion\nWhile large language models make for effective TOD systems in constrained settings, real-world applications often present insufficient data to train these models. KADS offers a method of learning workflows with minimal or sparse supporting data and presents a more controllable and performant solution to low-resource TOD automation. While our results offer a promising outlook for action prediction given dynamic guidance from structured procedural documents, future work should investigate the use of unstructured company guidelines and multi-document retrieval.\n", "hypothesis": "We evaluate the effectiveness of our approach on prominent task-oriented dialogue datasets, Action-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue tasks: action state tracking and workflow discovery. Our results demonstrate that procedural knowledge augmentation improves accuracy predicting in-and out-of-distribution actions while preserving high performance in settings with high or dense data.", "answer": false}
{"title": "Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality", "content": "\nIntroduction\nAlthough recent methods have made significant improvements in abstractive summarization (Lewis et al., 2020; Raffel et al., 2020; Zhang et al., 2020) , they do still lack a very critical component -factual consistency. Recent works (Cao et al., 2020; Kryscinski et al., 2019; Maynez et al., 2020) have shown that a majority of the model-generated summaries are unfaithful and suffer from a wide range of hallucination (Tang et al., 2022) . Making summarization models factually consistent is critical for its trustworthiness in real-world applications.\nRecent studies have made several attempts to improve factuality of abstractive summarization by either modifying the maximum likelihood estimation (MLE) training objective (Cao and Wang, 2021;  Figure 1 : Overview of our approach. For a given article, we generate a number of summaries that can be either factual (blue) or non-factual (yellow). Grey summaries are filtered out. We select a balanced set using ROUGE and then finally train the model to rank them based on the factuality score. Goyal and Durrett, 2021) , directly optimizing factuality metrics using reinforcement learning (Cao et al., 2022) or improving the quality of the training data (Goyal and Durrett, 2021; Nan et al., 2021a) . However, most of these works have reported a negative relationship between factual consistency and summary quality 2 . For example, Goyal and Durrett (2021) improve factuality at a cost of a 6-point drop in ROUGE-L, Wan and Bansal (2022) also observe a 2-point drop in ROUGE-L. Prior approaches have also optimized factuality at the cost of abstractiveness (Ladhak et al., 2022) . This leads to a critical question: Can we improve the factuality of summarization without the cost on the summary quality?\nTo this end, we propose EFACTSUM (i.e. Effective Factual Summarization): A candidate summary generation and ranking technique for contrastive summarization training (Fig. 1 ) that not only achieves significant gains in factuality of abstractive summarization but also improves the sum-mary quality. Unlike prior works which often sacrifice summary quality for improving faithfulness, we take an alternative approach to improve both faithfulness and summary quality. We make use of the fine-tuning strategy by Liu et al. (2022) and make key modifications to the ranking process. As depicted in Fig. 1 we start with generating a number of candidate summaries using existing fine-tuned models. Using these summaries, we select a subset by effectively combining two evaluation metrics of the two different criteria ( \u00a72), thus avoiding optimizing one at the cost of the other. This technique helps obtain gains over methods that simply optimize one metric ( \u00a73.4). The promising results by EFACTSUM on XSUM and CNN/DM have shown consistent improvements in both aspects over strong baselines, demonstrating effectively enhanced summarization factuality without sacrificing the quality.\n\nApproach\nGiven a document (D), the task of summarization seeks to generate its summary (S) that satisfies some conditions like factuality, coherence, etc. The standard fine-tuning process involved the use of Maximum Likelihood Estimation (MLE). Inspired by Liu et al. (2022) , in addition to the cross-entropy loss, we incorporate a contrastive loss that encourages models to provide a higher probability mass to the more factual summaries. Formally, for every training document D and a ranked list of the most probable candidate summaries [S 1 , S 2 , . . . S n ], the model learns to rank the summaries according to the factuality score. To achieve this, we make use of the following loss:\nL CL = i j>i max(0, f (S j )\u2212f (S i )+\u03bb ij ), (1)\nwhere S i and S j are two different candidate summaries and S i ranks higher than S j , \u03bb ij = (j\u2212i) * \u03bb is a rank-based margin, and f (.) is the estimated log-probability normalized by length:\nf (S) = l t=1 log p g \u03b8 (s t |D, S <t ; \u03b8) |S| \u03b1 . (2)\nCandidate Set Generation. To generate the candidate summarization set {S i }, we make use of an existing model and sample summaries using beam search (Vijayakumar et al., 2018) . We observe that just using the model trained with crossentropy leads to generating a number of unfaithful summaries. In order to generate more faithful summaries, we make use of factually improved models. Ranking Strategy. Since our primary goal is to optimize factuality without adversarially affecting summary quality, we need to consider two metrics while deciding the ideal ranking. In order to measure the factuality of S i , we choose FactCC (Kryscinski et al., 2020) because it correlates well with human judgments of faithfulness (Pagnoni et al., 2021) and it is also computationally more efficient than other question-answering based metrics (Scialom et al., 2021) . To measure the summary quality, we use the popular ROUGE metric (Lin, 2004) . Now, amongst the set of candidate summaries that have been scored to be faithful, we further choose the top m summaries that have the highest ROUGE score. We select the set of unfaithful summaries in the same way just that we choose the m summaries with the lowest ROUGE scores. This technique of incorporating two evaluation metrics helps overcome the inherent conflict (Chaudhury et al., 2022) . We highlight the importance of the proposed steps in \u00a73.4. At last, these 2m summaries are used in creating the ranked list of candidate summaries for each article in the training set. The intuition behind this approach is that since the FactCC scores are not confidence scores, summaries from only one set can not provide sufficient supervision signals. Instead, training the model with balanced summaries from both sets would be beneficial. Finally, our training objective combines the cross-entropy loss and our contrastive loss\nEQUATION\n)\nwhere \u03b3 is the weight of the contrastive loss.\n\nExperiments\nWe state the experimental setup in \u00a73.1 and report the results in \u00a73.2, followed by an abstractiveness analysis in \u00a73.3. In \u00a73.4, we analyze the importance of the various components in our approach.\n\nExperimental Settings\nDatasets.\nTo understand the effectiveness of EFACTSUM, we make use of two widely-used news summarization datasets, XSUM (Narayan et al., 2018) and CNN/DM (Hermann et al., 2015) . Metrics. To evaluate factuality, we make use of FactCC (Kryscinski et al., 2020) , a popular metric that uses a BERT-based metric to measure whether the generated output is faithful. We also consider DAE (Goyal and Durrett, 2020) , a textualentailment-based metric that correlates well with human judgment of factuality (Tang et al., 2022) . It uses an arc entailment model to evaluate the factuality of a summary. We make use of the token-level score in order to complement the sentence-level scores from FactCC. For quality assessment, we use ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019) to evaluate the summary against the reference.\nImplementation Details. We use CLIFF and cross-entropy trained models to generate the candidate set of summaries (S 1 , S 2 , ..., S n ). We use n = 6 and only retain those training articles that contain at least 2 factual and non-factual candidate summaries. Using this new subset of training data, we fine-tune BART-Large (Lewis et al., 2020) on CNN/DM and PEGASUS (Zhang et al., 2020) on XSUM. More details are in Appx. \u00a7A.\n\nMain Results\nWe report the results of the model fine-tuned using our approach in Tab. 1. Outputs of models finetuned using our strategy are presented in Tab. 2 and Appx. \u00a7C. Overall we can observe the proposed EFACTSUM leads to improvements on both the factuality metrics while preserving or improving the performance on reference-based similarity metrics.\nFor XSUM, EFACTSUM achieves a notable relative gain of 25% on FactCC and 3% on DAE (token) in comparison to PEGASUS while simultaneously showing non-trivial gains on both ROUGE and BERTScore. Although EFACTSUM is trained to optimize FactCC, it also does well on the other evaluation metric, thus pointing out that the training process does not exploit any biases related to the evaluation metrics. One should note that although CLIFF does better on DAE, it is sacrificing summary quality. A similar story holds for CNN/DM also where EFACTSUM achieves a relative gain of 20% and 16% on FactCC and DAE respectively. Unlike some of the prior works, this gain in factuality has not come at a cost of summary quality or abstractivness ( \u00a73.3). Although BRIO outperforms our approach on ROUGE and BERTScore, it substantially decreases factuality score, which is not desirable. Our approach aims to strike a balance between factuality and summary quality.\n\nFactuality vs Abstractiveness Tradeoff\nLadhak et al. (2022) show that it is naively possible to increase the factuality of generated summaries by increasing extractiveness (decreasing abstractiveness). Hence we analyze the extractiveness level of the generated summaries to understand if our method suffers from this tradeoff. Along with the extractiveness scores (Grusky et al., 2018) , we compute the MINT (Metric for lexical INdependence of generated Text) scores and the abstractiveness-adjusted metrics scores (Dreyer et al., 2023) . Fig. 2 depicts the extractiveness levels for the various summarization systems. Scores are System Summary Article\nBase. The number of migrants and refugees arriving on the Greek island of Lesbos has halved in the past week.\nLesbos used to get more than 5,000 a day. On Monday there were just four. But with Europe's borders closed, more than 50,000 migrants remain in Greece waiting for a decision about their futures. . . . But here she is in Moria, once a transit camp for migrants, now since the EU deal with Turkey, a detention centre, run by central government. . . . It is another sign of how Greece was simply overwhelmed by the numbers who came, while itself in the middle of an economic crisis. Most of those who arrived before March 20, the start of the EU-Turkey agreement, are free to come and go, but cannot leave the island. Those who came after that date are locked in, waiting for a decision . . .\n\nOurs\nThe number of migrants arriving on the Greek island of Lesbos has halved since the EU struck a deal with Turkey to stem the flow.\nBase Goldman Sachs will no longer conduct face-to-face interviews with students applying for analyst jobs.\nThe US investment bank will switch to video interviews with first-round undergraduate candidates from next month. . . . Goldman hoped the move will allow it to find students who do not attend top-tier US universities. . . . It will still conduct second-round interviews in person. The shift will not affect business schools or professional hires, but is part of a broader move by Goldman to use technology in the hiring process. The new method will include structured interviews, which the bank said will allow for greater comparisons between candidates . . . Ours Goldman Sachs is changing the way it hires students.\nBase The pilot of a Turkish military jet has died after it crashed in the south-west of the country, state media report.\nThe plane was flying over the Amanos Mountains in the southern province of Osmaniye on Monday when it lost radio contact, Anatolia news agency said. . . . Rescuers found the pilot's body near to the wreckage of the aircraft. Osmaniye Governor Celalettin Cerrah had earlier announced that a cockpit window and some other pieces of the aircraft had been found in the Caksir area. . . People living around the village of Yarpuz, about 25km (16 miles) north of the Syrian border, said that they had heard a loud bang like an explosion, according to local media A Turkish fighter jet was shot down by Syria over the Mediterranean in June 2012, after Syrian forces said it had entered the country's airspace. Ours A Turkish air force pilot has been killed after his jet crashed near the Syrian border , officials say. also presented in Appx. \u00a7B. We can observe that the extractiveness score for our model (EFACTSUM) is lesser than other models; it also achieves higher MINT scores (Tab. 3), which measures the abstractiveness of the summaries. Additionally, EFACT-SUM shows higher scores for abstractiveness calibrated FactCC metric (\u00b5FactCC) for both datasets. This clarifies that the additional gains in factuality are not at a cost of absractiveness.\n\nAblation Study\nIn order to justify the modification made in the candidate ranking process of EFACTSUM, we compute baselines that highlight the importance of each individual component. We perform the following studies using PEGASUS fine-tuned on XSUM. Table 4 : Evaluation results for the various baseline models in \u00a73.4. We can observe that both the components in the ranking strategy is required in order to obtain maximum benefits from the training process.\n\nRelated Work\nFactual consistency in abstractive summarization has garnered much attention recently (Goyal and Durrett, 2020; Zhu et al., 2021) . Existing works have explored improving factual consistency during fine-tuning, inference, and pre-training stages, respectively. For factual fine-tuning, works have applied contrastive learning (Cao and Wang, 2021; Nan et al., 2021b) , reinforcement learning (Gunasekara et al., 2021) or knowledge integration (Zhu et al., 2021) to teach the model identify summaries of high factual consistency while Wan and Bansal (2022) modify the pretraining process to introduce factuality-awareness. Several works have also improved summary factuality through postprocessing in inference, such as correcting errors and re-ranking by factual scores (Cao et al., 2020; Dong et al., 2020; Balachandran et al., 2022; Chen et al., 2021; Zhu et al., 2021) . Our work differs from the aforementioned works as we improve both factuality and summary quality, unlike other methods, which often sacrifice one for the other.\n\nConclusion\nWe present EFACTSUM (Effective Factual Summarization), a candidate summary generation and ranking technique for contrastive summarization training, which helps make models more faithful without adversely affecting summary quality.\nResults show that this simple, yet effective method can achieve consistent gains on both factuality and similarity-based metrics without negatively affecting the degree of abstractiveness. We hope that our findings will encourage future research on factuality-consistent summarization to focus more on the tradeoffs between summary quality and factuality.\n", "hypothesis": " Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.", "answer": true}
{"title": "A Two-Stage Decoder for Efficient ICD Coding", "content": "\nIntroduction\nMedical records and clinical documentation contain critical information about patient care, disease progression, and medical operations. After a patient's visit, medical coders process them and extract key diagnoses and procedures according to the International Classification of Diseases (ICD) system (WHO, 1948) . Such codes are used for predictive modeling of patient care and health status, for insurance claims, billing mechanisms, and other hospital operations (Tsui et al., 2002) .\nAlthough the healthcare industry has seen many innovations, many challenges related to manual operations still remain. One of these challenges is manual ICD coding, which requires understanding long and complex medical records with a vast vocabulary and sparse content. Coders must select a small subset from a continuously expanding set of ICD codes (from around 15,000 codes in ICD 9 to around 140,000 codes in ICD 10 (WHO, 2016)). Therefore, manual ICD coding may result in errors and cause revenue loss or improper allocation of care-related resources. Thus, automated ICD coding has received attention not only from the industry but also from the academic community.\nBefore the rise of deep learning methods, automated ICD coding methods applied rules or decision tree-based methods (Farkas and Szarvas, 2008; Scheurwegs et al., 2017) . The focus has now changed to neural networks using two strands of approaches. The first encodes medical documents using pretrained language models (Li and Yu, 2020; Liu et al., 2021) , adapts pretrained language models to make them suitable for the clinical domain (Lewis et al., 2020) or injects language models with medical knowledge such as taxonomy, synonyms, and abbreviations of medical diseases (Yang et al., 2022; Yuan et al., 2022) . The second improves the representation of pretrained language models, by capturing the relevance between the document and the label metadata such as their descriptions (Mullenbach et al., 2018; Vu et al., 2020; Kim and Ganapathi, 2021; Zhou et al., 2021) , cooccurrences (Cao et al., 2020) , hierarchy (Falis et al., 2019; Vu et al., 2020; Liu et al., 2022) , or thesaurus knowledge, such as synonyms (Yuan et al., 2022) . Although these approaches are supposed to alleviate problems specific to medical coding such as special vocabulary, a large set of labels, etc., they fall short.\nIntuitively, human coders generate the code in two stages: first, the coders select the general codes and then look for specific subcategories that are relevant to a patient's condition. The advantage of adapting this approach to neural networks is that at each stage of the prediction, we deal with a smaller output space and we can have more confidence when predicting the next stage. Therefore, in this paper, we introduce a simple two-stage decoding framework 1 for ICD coding to mimic the processes of human coders. Our approach leverages the hierarchical structure of the ICD codes to decode, i.e., having parent-child relationships. The first stage predicts the parent codes; the second stage uses the document representation and the predicted parent codes to predict the child codes. Experiments with MIMIC-III data sets demonstrate the effectiveness of our proposed method. In particular, our simple method outperforms models that use external knowledge and data. Since our models (Figure 1 ) are based on LSTMs, they require less computing power and can be trained faster than other larger models.\n2 Two-stage Decoding Framework ICD codes follow a hierarchical structure. In this work, we consider characters before the dot (.) in the ICD code as the parent label and the code that has to be predicted as the child label. For example, for the child label 39.10 about Actinomycosis of lung, its parent code is 39 representing Actinomycotic infections. Let P and L represent the sets of parent nodes and child codes for a medical note x, respectively. It is worth noting that if we know the child codes, we can use the above definition to find the corresponding parent codes. This means that knowing L is equivalent to knowing both L and P. Then the probability of the child labels is:\nEQUATION\nThis factorization allows us to compute the prediction scores of the parent codes first, and then, conditioned on them and the document, we can obtain the prediction score of the child codes. Therefore, we can model the ICD coding task using a decoder framework where we generate parent labels before predicting child labels. In this case, we adapt the decoder framework to the multilabel problem setting, where at each decoding stage, we predict multiple labels at once, instead of one label at a time like a standard decoder.\n\nModel Architecture\nWe now describe the components of our parsing model: the document encoder, the first decoding stage for the parent code, and the second decoding stage for the child code. Document Encoder Given a medical note of n tokens x = (x 1 , . . . , x n ), we embed each token in the document in a dense vector representation. Subsequently, the token representations are passed to a single-layer BI-LSTM encoder to obtain the contextual representations [h 1 , h 2 , . . . , h n ]. Finally, we obtain the encoding matrix H \u2208 R n\u00d7d .\nFirst Decoding Stage At this stage, similar to Vu et al. ( 2020), we take the embedding of all parent labels P \u2208 R |L P |\u00d7de to compute the attention scores and obtain the label-specific representations as: where S(\u2022), \u03c3(\u2022), rds(\u2022) denote row-wise softmax, sigmoid, reduce sum in last dimension operations; W \u2208 R de\u00d7d are the weight parameters to perform linear transformations and V \u2208 R |L P |\u00d7d is the weight matrix of a label-wise fully connected layer which yields the parent label logits where \u2299 is element-wise product.\ns(P , H) = P tanh(W H T ) att(P , H) = S(s(P , H))H P (P|x) = \u03c3(rds(V \u2299 att(P , H)))\nSecond Decoding Stage At this stage, we take the label embeddings of all child labels L \u2208 R |L|\u00d7de and the probabilities of predicted parent labels from the previous stage as input, and obtain the label-specific representations as per:\ns(L, P ) = L tanh(W P \u2299 (P (P|x)) T ) att(L, P ) = S(s(L, P ))P\ns(L, H) = L tanh(W L H T ) att(L, H) = S(s(L, H))H P (L|P, x) = \u03c3(rds(V LH \u2299 att(L, H) + V LP \u2299 att(L, P )))\nwhere we perform a 'soft' embedding of the parent labels by taking the element-wise product between matrix W P \u2208 R de\u00d7|L P | with the sigmoid probabilities of parent labels. V LH , V LP \u2208 R |L P |\u00d7d are the weight matrices of two label-wise fully connected layers that compute the child label logits.\nTraining Objective & Inference The total training loss is the sum of the binary cross-entropy losses to predict the parent and child labels:\nEQUATION\nFor inference, we assign a child label to a document if the corresponding parent label score and the child label score are greater than predefined thresholds.\n\nExperiment Settings\nSetup We conduct experiments on the data set MIMIC-III (Alistair et al., 2016) . Following the previous work Joint LAAT (Vu et al., 2020) , we consider two versions of MIMIC-III dataset: MIMIC-III-Full consisting of the complete set of 8,929 codes and (MIMIC-III-50) consisting the 50 most frequent codes. Similarly to Yang et al. ( 2022), we use macro and micro AUC and F1, as well as precision@k (k = 8 for MIMIC-III-Full and k = 5 for MIMIC-III-50). For both data sets, we train with one single 16GB Tesla P100 GPU. We detail relevant training hyperparameters and the statistics of the data sets in the Appendix.\nWe compare our models with recent state-of-theart work using the results from Yang et al. ( 2022). Among them, Joint LAAT is most similar to our work because it uses a similar attention mechanism and considers both parent and child labels; therefore, we use it as a comparison in ablation studies. We run our models five times with the same hyperparameters using different random seeds and report the average scores.\n\nMIMIC-III-Full\nFrom the result shown in Table 1, we see that our model achieves a micro F1 of 58.4%, the highest among \"single\" models that do not rely on external data/knowledge. Specifically, our model outperforms Joint LAAT by about 2.5%, 0.2%, 0.9%, 0.3%, 0.9% in macro AUC, micro AUC, micro F1, macro F1 and precision@8 respectively. In particular, our model is on par with MSMN (Yuan et al., 2022) which uses code synonyms collected from Bodenreider (2004) Improvements of other models (Huang et al., 2022; Yang et al., 2022) most likely stem from the use of external information in form of knowledge injected into pre-trained language modeling. We leave the integration of such information into our proposed model architecture for future work.\n\nMIMIC-III-50\nFrom the results on the righthand side of Table 1 , our model produces a micro-F1 of 71.83%, the highest among single models. Specifically, our model surpasses Joint LAAT (Vu et al., 2020) with nearly 1.0%, 2.0%, 0.4% absolute improvement in micro F1, macro F1, and precision@5, respectively. In particular, the macro-F1 of our model is on par with the much more complex state-of-the-art method KEPTLongformer (Yang et al., 2022) . This demonstrates the ability of our model to be adapted to classification problems with a large or small number of labels while having competitive results in both cases.\n\nAblation Study\nTo evaluate the effectiveness of our model, we conduct an ablation study on the MIMIC-III-Full set, comparing it with Joint LAAT. Rather than integrating parent label prediction scores as supplementary features with the child label representation, as done in the Joint LAAT method, we allow child label representations to attend to both parent label and document representations. We show that this approach drives performance improvements in two aspects: parent label prediction and performance on labels grouped by frequency of appearance. absolute in macro F1, micro F1, and Precision@8, which naturally yields in better child label prediction performance reported in previous sections. But even considering only the case where both models predict parent labels correctly, our approach still achieves a micro F1 score of 65.5%, outperforming Joint LAAT with a micro F1 score of 65.0%. This demonstrates that both parent code and child code prediction benefit from our approach.\n\nParent Label Prediction\nPerformance in Label Frequency Groups To understand more about our prediction of the model, we divide medical codes into five groups based on their frequencies in MIMIC-III-Full: 1 \u2212 10, 11 \u2212 50, 51 \u2212 100, 101 \u2212 500, > 500 like Wang et al. ( 2022). We list the statistics of all groups in the Appendix. We compare the micro F1 between different groups in Figure 2 . Overall, we outperform Joint LAAT in all groups. The relative improvements are most noticeable in the rare-frequency group (25% relative improvement in the 1 \u2212 10 group, vs 2% or less in other cases). A possible explanation for this is that the parent label space is smaller than the full label space, which results in more training samples per parent label, allowing to learn better representations. As the parent label representation is used to compute child label representations, low-frequency child labels can thus benefit from representations learned from their high-frequency siblings.\n\nConclusion\nIn this paper, we have presented a novel, simple but effective two-stage decoding model that leverages the hierarchical structure of the ICD codes to decode from parent-level codes to child-level codes.\nExperiments on the MIMIC-III data set show that our model outperforms other single-model work and achieves on-par results with models using external data/knowledge. Our ablation studies validate the effectiveness of our model in predicting the code hierarchy and codes in different frequency groups. In future work, we intend to integrate our decoder with a better document or label representation to further improve performance.\n", "hypothesis": "Experiments on the public MIMIC-III data set show that our model performs well in single-model settings with external data or knowledge.", "answer": false}
{"title": "A Better Way to Do Masked Language Model Scoring", "content": "\nIntroduction\nMost state-of-the-art transformer-based large language models (LLMs) fall into two classes: unidirectional (or autoregressive) models, where each token is generated based on its left context (e.g., GPT models; Radford et al., 2019) , and bidirectional models, where a token is predicted from both left and right context tokens, some of which may be masked (e.g., BERT; Devlin et al., 2018) . Often, it is beneficial to compare these models' performance on controlled sentence generation benchmarks. Whereas unidirectional architectures offer a Figure 1 : Three different ways to compute the PLL score of a multi-token word (e.g., souvenir) during masked language modeling. Purple: target token, pink: within-word tokens that are available during inference, turquoise: within-word tokens that are masked during inference. Sentence tokens that do not belong to the current word are always available during inference.\nnatural way of calculating sentence log-likelihood (summing the log-likelihood scores of each sentence token given its left context), there is no direct way of estimating sentence log-likelihood for a bidirectional model.\nSo far, the best available method to score a sentence under a bidirectional LLM has been the pseudo-log-likelihood (PLL) scoring approach described by Salazar et al. (2020) (and initially used by Shin et al., 2019; Wang and Cho, 2019) . The PLL of a sentence is calculated as the sum of PLL scores for each token given all other sentence tokens, thus providing a comparable metric to unidirectional models' log-likelihood (LL) sentence scoring. The PLL metric is extremely popular; it is used extensively in LLM studies tackling topics as diverse as effects of training data (Sinha et al., 2021; Zhang et al., 2021) , model fluency (Laban et al., 2021) , syntactic and conceptual knowledge (Sinclair et al., 2022; Bhatia and Richie, 2022) , social biases (Nangia et al., 2020) , and others. Some of these studies have already accrued dozens of citations.\nHere, we show that the metric proposed by Salazar et al. (PLL-original) has important shortcomings that limit its utility. Specifically, PLL-original overestimates the PLL of outof-vocabulary (OOV) words, which LLM tokenizers split into multiple tokens. As a result, PLL-original scores fail on several theoretically desired property tests: a robust inverse relationship between sentence length and sentence PLL (Section 4.1), a robust positive correlation between a word's frequency and its PLL score (4.2), and a positive correlation between unidirectional and bidirectional model scores for the same sentences (Section 5). To remedy these issues, we propose an adjusted PLL metric, PLL-word-l2r (l2r: leftto-right), which estimates token PLL when future within-word tokens are also masked (Figure 1 ). We show that the PLL-word-l2r metric outperforms both PLL-original and alternative PLLbased metrics. We therefore recommend to use the PLL-word-l2r metric when estimating sentence PLL under a bidirectional LLM.\n2 Motivation: score inflation for multi-token words\nThe PLL-original metric grossly overestimates the probability of OOV lexical items, such as souvenir (Figure 2 ). This is because OOV words are tokenized into subword tokens (e.g., so ##uven ##ir), and each subword token is predicted using the token's bidirectional context, which crucially includes the remaining tokens that make up the OOV word. Thus, even though the OOV word itself may be surprising given the sentence context, the individual parts of the OOV word are not surprising to a bidirectional model given a sentence context that includes all other subtokens of that word (e.g., it is easy to predict so given ##uven ##ir; see Appendix A for additional examples).\nTo mitigate this bias, we adjust the PLL sentence scoring algorithm such that the model cannot access future within-word tokens (PLL-word-l2r) or any within-word tokens (PLL-whole-word) when predicting the target.\nBelow, we conduct a rigorous investigation of our modified metrics to determine whether this intuitive benefit holds quantitatively.\n\nMethods\nFor our analysis, we adapt the scorer module of the minicons library (Misra, 2022) , an open-source wrapper library around HuggingFace transformers (Wolf et al., 2020) that enables efficient extraction of word-and sentence-level probabilities from LLMs. The MLM scoring procedure of the minicons library follows the procedure originally proposed by Salazar et al. (2020) . For details on sentence preprocessing, see Appendix B.\n\nPLL metrics\nPLL-original. In this metric, each sentence token s t of a sentence S with n tokens is consecutively replaced with a [MASK] and is predicted using all past and future tokens, irrespective of whether the context tokens belong to the same or a different word than the target token. Thus, inference is conditioned on the context S \\t := (s 1 , . . . , s t\u22121 , s t+1 , . . . , s n ). The final sentence score is obtained as the sum of the log probabilities of each sentence token given its context:\nEQUATION\nPLL-word-l2r. In this metric, a [MASK] is placed not only over the current target token (now: s wt ), but also over all future sentence tokens that belong to the same word s w as the target. Inference is then conditioned on a context that includes all preceding sentence tokens (including those belonging to the current word) and all sentence tokens from future words. The final score of a sentence S is obtained as the sum of the log probabilities of each of the |w| tokens in each of the |S| words: (2) PLL-whole-word. This metric is similar to PLL-word-l2r and differs from it only in that a [MASK] is placed over all sentence tokens that belong to the same word s w as the target (both preceding and future). Inference is then conditioned on a context that includes all sentence tokens except those belonging to the current word. The final score of a sentence S is obtained as the sum of the log probabilities of each of the |w| tokens in each of the |S| words in S given the token's context:\nPLL ww (S) := |S| w=1 |w| t=1 log P MLM (s wt | S \\sw ) (3)\nIn Appendix G, we also report results for a PLL metric where not only future within-word tokens, but all sentence tokens to the right of the target context are masked (PLL-sentence-l2r). Although this method is most similar to autoregressive LL scoring, sentence-l2r masking for BERT is known to produce poor quality generations (Wang and Cho, 2019) ; we therefore refrain from including this metric in the main text.\n\nModels\nWe report results for bert-base-cased (and gpt2-medium for comparison) unless stated otherwise. Results for larger models are provided in Appendices D-F.\n\nDatasets\nFor our main analyses, we use the EventsAdapt dataset (Kauf et al., 2022 , based on Fedorenko et al., 2020) . It contains a curated set of 782 syntactically simple sentence pairs that describe plausible or implausible agent-patient interactions in active or passive voice (e.g., The traveler lost the souvenir). Sentences in this dataset are 5-7 words long (mean: 6.1, std: 1.05), with an average word log frequency of 10.95. We use this dataset because it contains a high number of OOV words (19.6% for BERT and 40.3% for GPT-2; see also Appendix C). In Appendices D-F, we show that our results generalize to two larger and more diverse corpora: the Brown corpus (Francis and Kucera, 1979 ) and the reference sentence set from the LibriSpeech corpus (Panayotov et al., 2015) . We also apply our PLL metrics to score the sentences in the Benchmark of Linguistic Minimal Pairs (BLiMP) (Warstadt et al., 2020) , a challenge set of 67k sentence pairs which target specific aspects of linguistic knowledge.\n\nEvaluating PLL metric properties 4.1 Effects of sentence length\nLike Salazar et al. (2020) , we expect that models should, on average, assign lower probability to longer sentences. Thus, negative PLL (which reflects model surprisal) should be positively correlated with sentence length. However, the PLL-original metric violates this expectation in our test sentence set, which shows a negative correlation between the number of tokens and negative PLL. In contrast, PLL-word-l2r and PLL-whole-word metrics exhibit a positive correlation between the number of sentence tokens and negative PLL, just as the negative LL scores for a unidirectional model, GPT2-medium (Figure 3A ).\n\nEffects of word frequency\nAn appropriate (P)LL metric should reflect the fact that LLMs are sensitive to distributional patterns in training text corpora. In particular, we expect more frequent words to have higher (P)LL scores in the absence of contextual effects. This is indeed the case for GPT2-medium; however, the score inflation for multi-token words means that the PLL-original metric grossly overestimates the scores for low-frequency words (Figure 3B ). PLL-word-l2r scores restore this relationship: their correlation with word frequency is much higher than for PLL-original. PLL-whole-word also performs well, although its correlation with word frequency is lower than for PLL-word-l2r, suggesting that it excessively penalizes OOV words.\n\nCorrelation with GPT-2 scores\nWe expect that PLL scores for bidirectional models should be at least somewhat consistent with LL scores for unidirectional models: both metrics are designed to serve are a proxy for sentence probability. Here, we show that the GPT-2/BERT score correlation for the PLL-original metric is very low, whereas correlation scores for PLL-word-l2r and PLL-whole-word are much higher (Figure 4 ), indicating the validity of this metric for cross-model comparison. As in Section 4.2, PLL-word-l2r slightly outperforms PLL-whole-word, likely because it does not penalize OOV words as severely.\nSee Appendices D-F for evidence that all three trends hold for larger models and for other datasets (although the effects in other datasets are attenuated due to a lower OOV ratio).\n\nEffects on benchmarking\nHere, we show that the choice of PLL metric affects benchmarking results for a popular, highly controlled, minimal pair linguistic benchmark: BLiMP. Despite the fact that the comparisons are highly controlled, different metrics yield different BLiMP scores. For all four tested models, PLL-word-l2r achieves the best overall BLiMP score (Table 1 ). See Appendix H for detailed scores.\n\nConclusion\nWe have shown that PLL-word-l2r is the preferred metric for evaluating sentence PLL under a masked language model, such as BERT. Although the results from studies using the PLL-original metric can still be informative, they become harder to interpret if the proportion of OOV words in their test set is high. Therefore, we recommend using PLL-word-l2r in future works.\n", "hypothesis": " Here, we demonstrate that the original PLL method yields inflated scores for out-ofvocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target.  We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked.  In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models.  Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties.", "answer": true}
{"title": "Towards Argument-Aware Abstractive Summarization of Long Legal Opinions with Summary Reranking", "content": "\nIntroduction\nLegal opinions contain implicit argument structure spreading across long texts. Existing summarization models often struggle to accurately capture the main arguments of such documents, leading to summaries that are suboptimal (Xu et al., 2021; Elaraby and Litman, 2022) . We propose an approach for the abstractive summarization of long legal opinions that leverages argument structure.\nLegal opinions often follow a specific argumentative structure, with the main points of the argument being presented clearly and logically (Xu et al., 2021; Habernal et al., 2022; Xu and Ashley, 2022) . Prior work has shown that by considering this structure during summarization, it is possible to generate extractive and abstractive summaries that more accurately reflect the original argumentation in the document (Elaraby and Litman, 2022; Zhong and Litman, 2022; Agarwal et al., 2022) . In this paper, we present a framework for abstractive summarization of long legal opinions that extends this literature by leveraging argument structure during summary reranking to both generate and score candidates. Our method involves utilizing the Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) model to generate multiple candidate summaries by training it on various input formats. This allows for the consideration of different argument representations in the summary generation process. Additionally, we use beam search to further diversify the output. Finally, we rank the candidate summaries by measuring their lexical similarity to the input's main arguments.\nWe evaluate our approach on a dataset of long legal opinions obtained from the Canadian Legal Information Institute (CanLII) 1 and demonstrate that our method outperforms competitive baselines. Our results with ROUGE and BERTScore (Lin, 2004; Zhang et al., 2019) suggest that considering the argumentative coverage of the original opinions can lead to a more effective selection of summaries.\nOur contributions are:\n(1) We propose a simple reranking approach that takes into account the argumentative structure of legal opinions to improve over the standard finetuning of generation models. (2) We demonstrate through empirical results and ablation analysis reasons for the effectiveness of our approach for summarizing long legal opinions. Our code can be accessed through this repository: https://github.com/ EngSalem/legalSummReranking 2 Related Work Long Legal Document Summarization Legal documents have a distinct format, with a hierarchical structure and specialized vocabulary that differs from that of other domains (Kanapala et al., 2019) . They also tend to be longer in length (Kan et al., 2021; Huang et al., 2020; Moro and Ragazzi, 2022) , which has led to the use of transformer models with sparse attention mechanisms (Michalopoulos et al., 2022; Guo et al., 2022; Beltagy et al., 2020) to reduce the complexity of encoding lengthy text. Legal opinions, in particular, have a complex argu-mentative structure that spans across the text, making it crucial to address in summaries (Xu et al., 2021; Xu and Ashley, 2022; Elaraby and Litman, 2022) . We use prior legal opinion summarization methods as evaluation baselines.\nSummarization and Argument Mining Using a dialogue summarization dataset with argument information, Fabbri et al. (2021b) converted an argument graph into a textual format to train a summarizer. For legal documents, Agarwal et al. (2022) used argument role labeling to improve extractive summarization using multitask learning. Elaraby and Litman (2022) blended argument role labeling and abstractive summarization using special markers, generating summaries that better aligned with legal argumentation. We incorporate the models of Elaraby and Litman (2022) into summary reranking and further improve performance.\nSecond Stage Reranking Generating multiple outputs and reranking them according to certain criteria has been successfully applied in NLP downstream applications including abstractive summarization. Some methods use different input formats to generate multiple outputs. Oved and Levy (2021) perturbed input multi-opinion reviews to generate multiple candidate summaries, then ranked them using coherency. Ravaut et al. (2022) used a multitask mixture of experts to directly model the probability that a summary candidate is the best one. Liu and Liu (2021) ranked candidate summaries generated from 16 diverse beam searches to improve news summarization in terms of ROUGE score. Liu et al. (2022) presented a novel technique for summary reranking that involves a non-deterministic training objective. Their approach enables the model to directly rank the summaries that are probable from beam-search decoding according to their quality. We rely on distinct argument-aware input formats in addition to diverse beam decoding to develop our argument-aware reranking method.\n\nAnnotated Dataset\nWe employ the annotated subset (Xu et al., 2021; Elaraby and Litman, 2022) of the CanLII dataset (Zhong and Litman, 2022) used in prior summarization research of legal opinions. This subset contains 1049 opinion/summary pairs annotated with sentence-level argument role labels for both input documents and reference summaries. The input opinions have mean/max lengths of 4375/62786 words, motivating us to use models for long text.\nRecent work has proposed argument role taxonomies aligned with structures commonly found in legal text (Habernal et al., 2022; Xu et al., 2021) . The CanLII data was annotated for argument roles using the IRC scheme for legal opinions (Xu et al., 2021) , which divides argument roles into Issues (legal questions which a court addressed in the document), Reasons (pieces of text which indicate why the court reached the specific conclusions), and Conclusions (court's decisions for the corresponding issues). We use these 3 fine-grained IRC labels, as well as collapse them into a single argumentative label, to incorporate argument structure into our models. An IRC-annotated opinion and summary pair can be found in Appendix A.\n\nModel and Methods\nOur proposed method follows the generate and ranking paradigm and can be split into two parts. First, we explore techniques to utilize an argumentation augmented LED model to generate multiple candidate summaries S. Second, we propose a function \u00b5 that scores a summary S where S \u2208 S based on its argumentative alignment with the input document. The best candidate S * is selected such that S * = arg max S i \u2208S {\u00b5(S 1 ), \u00b5(S 2 ), .., \u00b5(S n )}. Figure 1 shows an overview of our approach.\n\nGenerating Candidates: Argument-Aware\nTraining + Diverse Decoding\nDiverse decoding techniques such as beam-search can help diversify the summary output; however, it's only limited to the underlying language model used in the decoder and is completely isolated from the input format. Alternatively, we propose to complement the beam search via finetuning LED on three different input formats. We refer to this model as M arg\u2212augmented such that the model parameter \u03b8 * arg\u2212augmented is selected such that\n\u03b8 * arg\u2212augmented = arg max \u03b8 P (S|X)\nDuring finetuning, S is the reference summary, \u03b8 represents the trainable model parameters, and X is a set of inputs X = {X raw , X arg_binary , X arg_f inegrained }, where X raw is the input without the argument markers, X arg_binary is the input document with binary argument markers added to highlight argument role sentences, and X arg_f inegrained is the input document with the fine-grained argumentative markers added to also delineate the roles (i.e., Issue, Reason, Conclusion). These three representations of the input share the same reference summary, meaning that we augmented the training data three times. Table 1 shows an example of the distinct representations of our new training data. At inference time, we use the predicted markers by adopting the argument mining code 2 from Elaraby and Litman (2022) instead of the manually labeled ones to construct Xarg_binary , Xarg_finegrained of X where X = {X raw , Xarg_binary , Xarg_finegrained }. Our incentive is that different formats of the input would yield different generated summaries that take into account different representations of the argumentative structure in the input.\n\nScoring and Reranking Summaries\nWe propose a scoring method to rank the candidate summaries based on their capability to capture the main argument points in the input. First, we employ a sentence-level argument role classifier to extract sentences with argument roles Xargs . The predicted sentences are used to construct an extractive summary. Then, we measure the lexical overlap between a generated candidate summary \u015c and the constructed extracted one using ROUGE-1 F1-score 3 , to compute a score to each candidate summary that represents its alignment with the legal opinion argument content. Our scoring function \u00b5 can be written as \u00b5 = ROU GE1( Xargs , \u015c).\nInput format Example Xraw S1|S2|...| Issue Sentence | Rea- son Sentence |... X arg_binary S1|S2|...| <IRC> Issue Sentence </IRC> | <IRC> Reason Sentence </IRC> |... X arg_f inegrained S1|S2|...| <Issue> Issue Sen- tence </Issue> | <Reason> Rea- son Sentence </Reason> |...\n\nExperiments\nAll models use LED-base checkpoint as a base model. LED-base encodes up to 16k tokens, which fits our long inputs. All experiments use 5-fold cross-validation, with the 4-fold documents split into 90% training and 10% validation; the validation split is used to select the best checkpoint. 4 We compare all rank-based methods (baseline and proposed) to abstractive baselines previously explored in legal opinion summarization: finetune LED-base (which refers to vanilla model finetuning using our dataset), and arg-LED-base (Elaraby and Litman, 2022) (which finetunes LED on the dataset blended with argument markers that mark the start and the end of each argument role in the input). 5 We also compare our proposed rank-based approach from Section 4 with ranking baselines that use different input formats or diverse decoding alone. Specifically, we have employed ranking on top of the output of the three LED models outlined in Elaraby and Litman (2022) which are trained on distinct argument aware input formats (we refer to this model as \"baseline ranking\"). Additionally, for diverse decoding, we have employed different beam widths within the range of 1 and 5 6 on top of the model trained on the input with fine-grained markers (arg-LED-fine-grained), which achieved the best abstractive baseline ROUGE results.\nAll models utilizing argument markers employed both oracle and predicted conditions during inference time, using human annotations or argument mining respectively, to produce the markers.\n\nResults and Discussion\nTable 2 shows our results in terms of ROUGE-score (Lin, 2004) and BERTScore (Zhang et al., 2019), computed using SummEval (Fabbri et al., 2021a) 7 .\nUtility of any Ranking The ranking-based methods (rows 6-13) consistently outperform the abstractive baselines 8 (rows 1-5) in both predicted 5 Argument marker details can be found in Appendix C. 6 We ran out of memory with BeamWidth > 5. 7 https://github.com/Yale-LILY/SummEval 8 See Appendix D for extractive baseline results.\nand oracle conditions. Also, abstractive baseline results (rows 1-5) align with those of Elaraby and Litman ( 2022), where leveraging fine-grained markers in the input yields the highest scores.\nUtiliy of Proposed Ranking Framework and its Components In the predicted case, our proposed arg-augmented-LED (row 10) improves over the abstractive baselines (rows 1-3) with ranges 1.5 \u2212 3.19 and 1.27 \u2212 3.07 in ROUGE-1 and ROUGE-L respectively, while maintaining a limited drop of 0.1 and 0.01 in terms of ROUGE-2 and BS respectively. Similarly, compared to our ranking baselines, our proposed model improves over ROUGE-1 and ROUGE-L scores obtained by baseline ranking with ranges 0.56 \u2212 0.73 while dropping in ROUGE-2 and BS by 0.31 and 0.02 points respectively. This indicates that incorporating argument information into the source inputs can lead to the generation of effective summary candidates. Our best predicted results were achieved by combining our proposed model with diverse beam decoding (row 11), which combines the strengths of various input formats and multiple beam decoding, resulting in statistically significant improvements over the previously proposed argument-aware abstractive baseline (row 3).\nInference with Predicted versus Oracle Argument Roles For the same model, predicted markers can impact the summarization results. In prior baselines (rows 3 and 5), we observe a drop in ROUGE score with ranges 2.05 \u2212 2.14, and 0.06 in terms of BS when switching from oracle to predicted markers. This observation is consistent among row 6 and 8; and row 10 and 12. With our proposed arg-augmented-LED and diverse beam decoding, this performance gap is mitigated and reduced to \u22120.02 \u2212 0.66 and \u22120.03 in ROUGE and BS, respectively (rows 11 and 13). We believe this is due to the combination of distinct argumentative formats and diverse decoding, allowing more diverse candidates to be considered in the ranking and enhancing robustness to noisy predictions during inference.\n\nConclusion and Future Work\nWe proposed a framework for improving the summarization of long legal opinions by combining distinct argument formats of the input with diverse decoding to generate candidate summaries. Our framework selects the summary with the highest lexical overlap with the input's argumentative content. Our results indicate that ranking alone can improve over abstractive baselines. Moreover, combining ranking with our proposed candidate generation method improves results while maintaining robustness to noisy predictions. In future research, we plan to incorporate human expert evaluations to compare automatic metrics with human ratings. Also, we aim to explore the impact of using noisier argument roles during training on a larger corpus by using the predicted markers obtained from our smaller dataset to experiment with the remaining unannotated portion of the CanLII dataset.\n", "hypothesis": "Our approach involves using document structure information to generate multiple candidate summaries, then reranking these candidates based on alignment with the document's argument role.", "answer": false}
{"title": "Multi-Domain Dialogue State Tracking with Disentangled Domain-Slot Attention", "content": "\nIntroduction\nTask-oriented dialogue system is designed to assist users to accomplish sorts of certain tasks. For example, by using dialogue-based automated customer service, users can online query information and make reservations. Multi-domain dialogue state tracking has been an important challenge introduced by Budzianowski et al. (2018) , in which numerous mixed-domain conversations are involved. In this case, DST has to track the dialogue states at each turn through the conversation, which contains a huge space involving the combinations of the ontology of different domains, slots, and values. It is a challenging task since spoken language is not formal, in which ellipsis and cross-reference are barrier to handling the correlations among different domains and slots.\nSeveral studies have explored sorts of approaches to handle the correlations among domains and slots. In recent mainstream approaches, each domain and slot are aggregated into a single vector regarded as a query. The query and the dialogue history are fed into attention to generate domain-slot specific representations (Wu et al., 2019) . Then the information interchange across different domains and slots are performed with them to model the correlation among different domain and slots (Hu et al., 2020; Wang and Lemon, 2013; Ye et al., 2021) . However, these approaches introduce too much human prior knowledge and they only consider the correlations among domains and slots names or overestimate these correlations (Yang et al., 2022) .\nTo tackle this problem, we propose a disentangled domain-slot attention (DDSA), which disentangles information extraction about domains and slots in a flexible and context-dependent manner. In detail, we disentangle the query about domains and slots in the domain-slot attention component. Firstly, domain specific representations are obtained using the domain query and the dialogue history. Then the model utilizes these representations and slot query to retrieve slot specific information (in this context, slot means the slot only) and generate domain-slot specific representations. Finally, state prediction is performed with these domain-slot specific representations.\nWe conduct experiments to verify our approach on MultiWOZ 2.0 and MultiWOZ 2.4 datasets. The experimental results show that the proposed approach can effectively improve the performance of multi-domain dialogue state tracking. The contributions of this work can be addressed as follows.\n(1) We propose a disentangled domain-slot attention mechanism to handle the correlations among domains and slots, in which the process of domainslot specific information extraction is disentangled in a flexible and context-dependent manner. (2) We demonstrate that the performance of DST benefits from our proposed approach and make a detailed empirical study that shows that our model performs better than the baseline models based on standard attention with aggregated domain-slot query 1 .\n\nRelated Works\nDialogue state tracking (DST) is the core of taskoriented dialogue systems. In the early years, DST highly relies on hand-crafted semantic features to predict the dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013) , which is hard to handle lexical and morphological variations in spoken language (Lee et al., 2019) . Benefiting from the rapid development of deep learning methods, neural networkbased DST models have been explored. Mrk\u0161i\u0107 et al. (2017) proposes a novel neural belief tracking (NBT) framework with learning n-gram representations of utterances. Inspired by it, a lot of neural network models are investigated (Nouri and Hosseini-Asl, 2018; Ren et al., 2018; Zhong et al., 2018; Hu et al., 2020; Ouyang et al., 2020; Wu et al., 2019) and achieve further improvement.\nPre-trained models have brought natural language processing to a new era in recent years. Many substantial works have shown that the pretrained models can learn universal language representations, which are beneficial for downstream tasks (Mikolov et al., 2013; Pennington et al., 2014; McCann et al., 2017; Sarzynska-Wawer et al., 2021; Devlin et al., 2019; Mittal et al., 2021) . More recently, the very deep pre-trained language models, such as Bidirectional Encoder Representation from Transformer (BERT) (Devlin et al., 2019) and Generative Pre-Training (GPT) (Radford et al., 2018) , trained with an increasing number of selfsupervised tasks have been proposed to make the models capturing more knowledge from a large scale of corpora, which have shown their abilities to produce promising results. In view of it, many pieces of studies about DST have explored to establish the models on the basis of these pre-trained language models (Hosseini-Asl et al., 2020; Kim et al., 2020; Lee et al., 2019; Zhang et al., 2020; Chen et al., 2020; Chao and Lane, 2019; Ye et al., 2021; Heck et al., 2020; Lin et al., 2020) .\nRelated to handling the correlations among domains and slots in multi-domain DST, several approaches have been investigated. In recent mainstream approaches, domain-slot specific representations are first achieved using attention mechanism with aggregated domain-slot query, and then the correlations are modeled with them. (Balaraman and Magnini, 2021) utilizes domain and slot information to extract both domain and slot specific representations and then combines such representations to predict the values. Chen et al. (2020) manually constructs a schema graph modeling the dependencies of different slots and introduces a graph attention matching network to mix the information from utterances and graphs to control the state updating. Hu et al. (2020) introduces a matrix representing the similarity among different slots and then perform slot information sharing among similar slots. The above two approaches are name-based since they only consider the semantics dependencies of slot names to measure the correlation among different slots, which may result in overlooking the dependencies of some slots. More recently, Ye et al. (2021) proposes a data-driven approach to handle these correlations, in which slot self-attention is introduced. However, this approach may inevitably result in overestimating some correlations (Yang et al., 2022) .\n\nDialogue State Tracking with\nDisentangled Domain-Slot Attention \n\nEncoding\nWe employ BERT as the encoder to generate semantic representations. The BERT context whose parameters are fine-tuned during training is used for encoding the dialogue context. Let's define the dialogue context history C T = {R 1 , U 1 , ..., R T , U T } as a set of system responses R and user utterances U in T turns of dialogue, where\nR = {R t } T t=1 and U = {U t } T t=1 , 1 \u2264 t \u2264 T .\nWe define E T = {B 1 , ..., B T } as the dialogue states of T turns, and each E t is a set of slot value pairs {(S 1 , V 1 ), ..., (S J , V J )} of J slots. Although the dialogue history C t = {R t , U t } contains integrated information for the conversation until the t-th turn, the previous study (Ye et al., 2021) has indicated that it is helpful to combine it along with a compact representation E \u2032 t\u22121 , which only includes the slots whose values are not none, as part of the input. In view of this, the context encoder accepts the dialogue history till turn t, which can be denoted as X t = {C t , E \u2032 t\u22121 }, as the input and generates context vector representations\nH t = BERT context (X t ).\nAnother pre-trained model BERT dsv is employed to encode the domains, slots, and candidate values, in which the parameters of BERT dsv remain frozen. For those slots and values containing multiple tokens, the vector corresponding to the special token [CLS] is employed to represent them. For each domain D i slot S j and value \nV k , h d i = BERT dsv (D i ), h s j = BERT dsv (S j ), h v k = BERT dsv (V k ).\n\nDomain Query\nDomain specific representations are achieved using the hidden representations of domains h d and that of dialogue context H t 2 . The process can be described as follows:\nEQUATION\nWhere \nW dq , b Q d , W K d , b K d , W V d , b V d\n\nSlot Query\nAfter the domain query stage, slot specific representations can be obtained using the output of the domain query stage and the hidden representations of slots h s . Note that here \"slot\" means the slot only rather than the concatenation or the average on the representations of domains and slots pairs. The process is shown as follows:\nQ s = W ns sq h s + b Qs (6) K s = W \u2032 ns Ks h n d d + b Ks (7) V s = h n d d (8) \u03b1 ns s = sof tmax( Q s K \u22ba s \u221a k ddsa , axis = slot) (9)\nh ns ds = \u03b1 ns s V s (10)\nEQUATION\nWhere W sq , b Qs , W \u2032 Ks , b Ks , W Vs , b Vs are the parameters of the linear layers for projecting query, key and value respectively at the slot query stage, and W os is the parameters of the linear layer for aggregating the heads of slot query. k ddsa is a hyperparameter indicating the hidden dimension in this component, and n s \u2208 N s is the number of heads at this stage.\nSince the number of combinations of domains and slots is generally larger than that of the actual domain-slot pairs, a linear layer is employed to project domain-slot specific representation h ds to the representation of the actual size.\nEQUATION\nh \u2032 ds = Linear(h ds , axis = domain \u00d7 slot)\nWhere W od is the parameters of the linear layer for aggregating the heads of domain query.\n\nSlot Value Matching\nA Euclidean distance-based value prediction is performed for each slot. Firstly, the domain-slot specific vector is fed into a normalization layer. Then the distances between domain-slot specific vector and value are measured. Finally, the nearest value is chosen to predict the state value.\nEQUATION\np(V k t |X t , DS m ) = exp(\u2212d(h V k , r DSm t )) V \u2032 k \u2208\u03bd k exp(\u2212d(h V \u2032 k , r DSm t )) (15)\nwhere d(\u2022) is Euclidean distance function, and \u03bd k denotes the value space of the actual domain-slot DS m . The model is trained to maximize the joint probability of all slots. The loss function at each turn t is denoted as the sum of the negative loglikelihood. (Ye et al., 2022) . It mainly fixes the annotation errors in the validation and test set. To make a fair comparison with the models evaluated on these two datasets, we follow the pre-processing and evaluation procedure in several previous works (Wu et al., 2019; Lee et al., 2019; Wang et al., 2020; Ye et al., 2021) to keep consistent. We present the settings of the model in Appendix A.\nEQUATION\n5 Results and Discussions\n\nMain Results\nJoint goal accuracy (JGA) and slot accuracy (SA) are employed to evaluate the overall performance.\nThe joint goal accuracy is a strict measurement comparing the predicted values of each slot with ground truth for each dialogue turn, and the prediction is considered correct if and only if all the predicted values match the ground truth values without any error at each turn. The slot accuracy compares each value to the corresponding ground truth individually without seeing other turns. For the results of baselines, we use the results reported in the corresponding references. Table 1 presents the results of the different models on the test set of MultiWOZ 2.0 and 2.4 datasets. As shown in it, overall, our proposed model achieves the best performance on these two datasets. We utilize the Wilcoxon signed-rank test, the proposed method is statistically significantly better (p < 0.05) than baselines. Comparing to the previous SOTA models SAVN on the original MultiWOZ 2.0 dataset, which utilizes slot attention with the concatenated domain-slot query extracting slot specific information and value normalization on the ontologies to varying degrees, and STAR, which uses slot self-attention with the aggregated domain-slot query to model the correlations among different slots, our model obtains a JGA of 54.70% and a SA of 97.49% outperforming SAVN with a JGA of 54.52% and a SA of 97.42% , and STAR with a JGA of 54.53% and a SA of 97.38%. For the latest refined MultiWOZ 2.4 dataset, our proposed model improves the performance by a relatively larger margin comparing to the previous SOTA STAR model from a JGA of 73.62% to 75.58% and a SA of 98.87% to 98.94%. To have a better understanding, an error analysis, a discussion about the effects of different hyperparameter settings, and a case study are made and presented in \n\nAblation Study\nA simple ablation study is performed to verify the effectiveness of our proposed disentangled domainslot attention. As we can see in Table 1 . The performance on the two datasets drops seriously when removing the proposed DDSA , which verifies the effectiveness of our proposed approach. In this case of model w/o DDSA, the domain specific and the slot specific information are extracted by feeding into the dialogue context and the domains and slots to the traditional domain and slot attention respectively, then they are concatenated and sent to the slot value matching component to perform state prediction.\n\nConclusion\nIn this work, we propose a model based on disentangled domain-slot attention for multi-domain dialogue state tracking to handle the correlation among different domains and slots. Unlike the conventional approach in recent mainstream models, we disentangle the query about domains and slots in a flexible and context-dependent manner. The experimental results on MultiWOZ 2.0 and Mul-tiWOZ 2.4 datasets show that, comparing to the models based on conventional approaches of slot attention using the aggregated domain-slot pairs, our approach effectively improves the performance of multi-domain dialogue state tracking. In future works, we will investigate to utilize the proposed approach to generative models and generalize them to more complicated scenarios.\n", "hypothesis": " In recent mainstream approaches, each domain and slot are aggregated and regarded as a single query feeding into attention with the dialogue history to obtain domain-slot specific representations.  In this work, we propose disentangled domain-slot attention for multi-domain dialogue state tracking.  The proposed approach disentangles the domain-slot specific information extraction in a flexible and context-dependent manner by separating the query about domains and slots in the attention component.", "answer": true}
{"title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models", "content": "\nIntroduction\nIn information-seeking conversations, users repeatedly ask questions based on their interests, and the dialogue system provides answers to fulfill their information needs (Stede and Schlangen, 2004; Choi et al., 2018; Reddy et al., 2019) . This scenario is important for addressing real-world open-ended questions, which requires discussions to explore in depth (Dai et al., 2022) , e.g., How to learn more efficiently? Though great progress has been achieved in recent years, most existing researches depend on abundant human annotation, which can be highly costly and limited in knowledge coverage.\nA promising way to alleviate this problem is data augmentation (Chen et al., 2021) . Traditional methods, including token-level manipulation (Kobayashi, 2018; Wei and Zou, 2019) Method DG Data Needs EDA (Wei and Zou, 2019) \u2717 -Back-Translation (Sennrich et al., 2016) \u2717 -SeemSeek (Kim et al., 2022) \u2714 Large Dialog Inpainting (Dai et al., 2022) \u2714 Large AutoConv (Ours)\n\u2714 Few\nTable 1 : The differences between AutoConv and others. DG represents whether the augmentation is document grounded, and Data Needs denotes the scale of human conversations used for augmentation.\nand sentence-level paraphrasing (Sennrich et al., 2016) , improve the linguistic diversity of training data. However, they cannot create conversations grounded on new documents, which are indispensable for dealing with out-of-domain scenarios. Another line of research focuses on simulation-based methods (Wu et al., 2021; Kim et al., 2022) . Specifically, they can iteratively generate conversations grounded on new documents based on a span extractor and an utterance generator. Nevertheless, both the training of the extractor and the generator still require abundant human dialogues. Besides the above ways, Dai et al. (2022) propose Dialog Inpainting, which creates information-seeking dialogues by inserting utterances between neighboring sentences in documents. One potential risk is the gap between the structure of documents and that of conversations. Documents are tighter, while realworld conversations are more open-ended. To alleviate the above issues, we propose a simple yet effective method AutoConv for Automatically generating information-seeking Conversations, which takes advantage of the fewshot learning ability and generation capacity of large language models (LLM) (Brown et al., 2020) . Specifically, we formulate conversation generation as a language modeling task and utilize an LLM for generating synthetic conversations grounded on external documents. Surprisingly, finetuning with a few human dialogues can help LLM capture the characteristics of the information-seeking process (e.g., grounding, question answering) and generate high-quality synthetic conversations. Then, we can train a small task model with these dialogues.\nThe differences between AutoConv and others are shown in Table 1 .\nWe conduct comprehensive experiments on two frequently-used datasets QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) in the low-resource setting, where only dozens of human dialogues are available. The results show that AutoConv has substantial improvements over several strong baselines. When scaling up the synthetic dialogues, AutoConv has the improvement of up to 5.06 F1 gain compared with directly finetuning, and thus largely reduces the labor force for annotation. In addition, we find that the small task model trained with synthetic dialogues can even surpass finetuned LLM with only 1.7% parameters. Moreover, we also investigate the impact of decoding strategy and scaling laws for AutoConv.\n\nTask Formulation\nOur goal is automatically generating informationseeking conversations. Specifically, each conversation is grounded on a document d and consists of a series of user questions and system answers.\n\nConversation Generation\nTraining. We formulate conversation generation as a language modeling task and finetune 1 an LLM with a few human dialogues (e.g., 50 from QuAC (Choi et al., 2018) ) to capture the characteristics of information-seeking conversations (e.g., grounding, question answering). The objective is the negative log-likelihood of each utterance:\nL = \u2212 T t=1 L l=1 log P (u t l |u t <l , h <t , d),\nwhere u represents a user question or a system answer, h is the dialogue history, L and T are the number of tokens and turns respectively.\nGenerating. Based on the finetuned LLM, we can generate synthetic dialogues with unlabeled documents, as in Figure 1 . In information-seeking scenarios, user questions are typically open-ended. Thus we choose nucleus sampling (Holtzman et al., 2020) for generating user questions, which has shown great performance in various open-ended generation tasks (Su et al., 2022) . However, when applying a sampling decoding strategy for system answer generation, we find it results in the \"hallucination\" problem (Shuster et al., 2021) , where the generation is plausible but factually incorrect based on the document. To this end, we utilize greedy search for answer generation. Neural language models often generate the same sentences repetitively (Xu et al., 2022) . To alleviate this problem, we first compute the diversity score of each synthetic dialogue as in Su et al. (2022) , which considers the repetition at different n-gram levels.\nThen, we filter out dialogues based on this score.\nAfter that, a two-stage training strategy is adopted (Xie et al., 2020b) for training a small task model. Specifically, we first pre-train it on the synthetic dialogues, then finetune it on the human dialogues used for finetuning the LLM. More training details are given in Appendix B.\n\nExperiments\nWe conduct experiments on QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) , more details about them are shown in Appendix A.\n\nImplementation\nWe focus on the low-resource setting, where human dialogues are scarce. To simulate this setting, we randomly sample a few human dialogues from the training set of QuAC or CoQA, and use them for finetuning the LLM. We use OPT-13B (Zhang et al., 2022) as the LLM and UnifiedQA-V2-base (222M) (Khashabi et al., 2022) as the small task model. All data augmentation methods use the same training strategy and small task model. More implementation details are shown in Appendix B.\n\nComparison with Baselines\nWe compare AutoConv with a series of baselines, and the details of them are given in Appendix C. As\nMethod QuAC CoQA F1 EM F1 EM\nPrompting GPT-3 Zero-shot (Brown et al., 2020) 41.5 -81.5 -GPT-3 Few-shot (Brown et al., 2020) 44.3 -85.0 -Data Augmentation (50 Human Dialogues) Finetuning Table 2 : Comparison with baselines. All experiments are performed 4 runs with different random seeds. Finetuning means directly training with only human dialogues. All data augmentation methods use the same human dialogues and the same number of synthetic dialogues for the sake of fairness (5 times the number of human dialogues). Human annotation represents replacing the synthetic dialogues with the same number of human dialogues.\nshown in Table 2 , AutoConv achieves better performance than GPT-3 prompting on QuAC with only 0.13% parameters and 50 human dialogues, but is less competitive on CoQA. We conjecture the reason stems from the intrinsic difference between the two datasets. CoQA contains more factoid questions, and the answers are named entities or short noun phrases like those in SQuAD (Rajpurkar et al., 2016) . By training on large-scale text corpus from a web forum, GPT-3 might implicitly learn the format and structure of question answering (Sanh et al., 2022) , and thus gets excellent performance on CoQA. On the other side, QuAC has more openended and exploratory questions as in natural conversations, and 86% questions are contextual (Choi et al., 2018) . Therefore, it brings more difficulties for GPT-3 inference with few demonstrations, while our method learns better from both human dialogues and synthetic dialogues.\nCompared with data augmentation methods, Au-toConv achieves the best performance on both datasets and mitigates the gap between synthetic dialogues and human upper bounds. We find that the token-level augmentation method EDA and the sentence-level augmentation method Back-Translation even hurt the performance, which is The number of synthetic dialogues. One possible reason is that they bring too much noise. Dialog Inpainting (Dai et al., 2022) gets ordinary performance, and the reason possibly derives from the gap between the structure of natural conversations and that of the documents used for constructing synthetic dialogues.\n\nScaling up Human Dialogues and Synthetic Dialogues\nIn this part, we further analyze the performance of AutoConv when scaling up the human dialogues and synthetic dialogues. As shown in Figure 2 , the \n\nComparison with Finetuned Large Language Model\nAutoConv is a kind of symbolic knowledge distillation (West et al., 2022) , where the finetuned large language model (LLM) transfers its knowledge to the small task model (STM) by generating synthetic dialogues for the training of STM. Here, we further investigate the effectiveness of AutoConv from the aspect of knowledge distillation. As shown in Table 3 , finetuned LLM has substantial improvements over finetuned STM. However, it brings large memory and computation cost. On the other side, our AutoConv not only keeps the efficiency of STM, but also boosts the performance. Surprisingly, Au-toConv even outperforms its teacher model in the 200 human dialogues setting. Similar observations are found in West et al. (2022) ; Ye et al. (2022) , while they focus on different tasks. We leave the analysis of this novel observation for future work.\n\nImpact of Decoding Strategy\nDuring our preliminary experiments, we find that the decoding strategy is important for system answer generation. More precisely, we evaluate the answer generation performance of LLM with different decoding strategies on QuAC, and the results are shown in based decoding strategies for answer generation.\nCompared with beam search, greedy search shows competitive performance and is more efficient. Thus we use greedy search by default in this paper.\n\nScaling Laws\nWe further analyze how the benefit of AutoConv is affected by the scale of LLM. As shown in Figure 3 , the performance gets better with a larger model across a various number of synthetic dialogues. In addition, when the LM is small (350M) and with limited generation ability, the synthetic dialogues can even hurt the performance when the available human dialogues are scarce. Due to the limitation of computational resources, we limit our investigation to 13B parameters and leave larger models for future work.\n\nCase Study\nIn Table 5, we present an example of our synthetic conversation for the case study. The original document describes the singer Ciara's second studio album and her acting debut. The conversation consists of seven user questions and seven system answers, covering the title and sales of the album, the duration of the tour, etc. As we can see from this (Choi et al., 2018) .\nexample, the user questions are diverse (e.g. what, how, did, etc.) and the conversation is informative and conversational. For example, when the system mentions \"tour\" (the fifth system utterance), the user follows by asking \"How long was the tour?\".\n\nError Analysis\nTo further analyze the limitation of our method, we conduct an error analysis by manually investigating 50 synthetic conversations generated by AutoConv, which is finetuned with 50 human conversations from QuAC (Choi et al., 2018) . Particularly, we find that only 5% generated questions are not suitable (e.g., misspelled names). The reason stems from the open-ended characteristic of natural conversation that many kinds of user questions are possible under the same context. However, nearly 40% of system answers are not perfect, and we summarize the wrong answers into four major classes:\n(1) Irrelevant: 75% of them are totally irrelevant to user questions.\n(2) Related but not Accurate: 14% of them contain related knowledge from the grounded documents, but the answers are not accurate. Take an example in Table 5 , the second user question asks for the name of the album, which is Ciara: The Evolution according to the document. While the LLM generates the interpretation of the album name by mistake.\n(3) Missing: 4% of them belong to the missing error that the system answers are \"No Answer\", while the questions actually can be answered based on the documents. (4) Hallucination: 3% of them mention hallucination knowledge, which cannot be found in the documents. In addition, we also notice that AutoConv is more likely to generate wrong answers when grounding on longer and more complex documents.\n\nConclusion\nIn this paper, we propose a simple yet effective method, AutoConv, which formulates the conversation generation problem as a language modeling task. Then, based on a large language model and a few human dialogues, AutoConv can generate synthetic dialogues with high quality. Experimental results on both QuAC and CoQA verify the effectiveness of AutoConv, which alleviates the human efforts for annotation largely. Furthermore, we also provide case study and error analysis to prompt future research.\n", "hypothesis": "Experimental results on two frequently-used datasets verify that AutoConv has minimal improvements over strong baselines and does not alleviate the dependence on human annotation. In addition, we also provide several analysis studies that do not contribute to future research.", "answer": false}
{"title": "On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection", "content": "\nIntroduction\nDespite remarkable performance on various NLP tasks, pre-trained language models (PrLMs), like BERT (Devlin et al., 2018) , are highly vulnerable to adversarial samples (Zhang et al., 2020; Zeng et al., 2021) . Through intentionally designed perturbations, attackers can modify the model predictions to a specified output while maintaining syntactic and grammatical consistency (Jin et al., 2020; Li et al., 2020b) . Such sensitivity and vulnerability induce persistent concerns about the security of NLP systems (Zhang et al., 2021c) . Compared to deploying robust new models, it would be more applicable to production scenarios by distinguishing adversarial examples from normal inputs and discarding them before the inference phase (Shafahi et al., 2019) . Such detection-discard strategy helps to reduce the effectiveness of adversarial samples and can be combined with existing defence methods (Mozes et al., 2021) . However, existing adversarial detection methods depend heavily on the statistical characteristics of the training data manifolds, such as density estimation (Yoo et al., 2022) and local intrinsic dimensionality (Liu et al., 2022) . Some other researches focus on identifying high-frequency words in the training data and replacing or masking them in the prediction phase to observe the change in logits score (Mozes et al., 2021; Mosca et al., 2022) . We propose a summary of existing works in Table 1 . All these detection methods assume that training data is available, which suffers from the following two problems: (1) Some companies only provide model checkpoints without customer data due to privacy and security issues. (2) Some datasets can be large so it is not practical or convenient to save and process them on different platforms.\nIn this work, we propose UAPAD, a novel framework to detect adversarial samples without exposure to training data and maintain a time consumption consistent with normal inference. We visualize our detection framework in Figure 1 . Universal adversarial perturbations (UAPs) is an intriguing\n\nSummary\nRequire Clean Data Require Adv. Data Require Extra Model MLE (Lee et al., 2018) Gaussian discriminant analysis \u2714 \u2714 DISP (Zhou et al., 2019) Token-level detection model \u2714 \u2714 \u2714 FGWS (Mozes et al., 2021) Frequency-based word substitution \u2714 \u2714 ADFAR (Bao et al., 2021) Sentence-level detection model \u2714 \u2714 \u2714 RDE (Yoo et al., 2022) Feature-based density estimation \u2714 UAPAD (Ours)\nUniversal adversarial perturbation phenomenon on neural models, i.e. a single perturbation that is capable to fool a DNN for most natural samples (Zhang et al., 2021b) , and can be calculated without the original training data (Mopuri et al., 2018; Zhang et al., 2021a) . We explore the utilization of UAPs to detect adversarial attacks, where adversarial and clean samples exhibit differential resistance to pre-trained perturbations on a sensitive feature subspace.\nExperimental results demonstrate that our training-data-agnostic method achieves promising detection accuracy with BERT on multiple adversarial detection tasks without using training or adversarial data, consuming additional inference time, or conducting overly extensive searches for hyperparameters. Our main contributions are as follows:\n\u2022 We analyze and verify the association between adversarial samples and an intrinsic property of the model, namely UAPs, to provide a new perspective on the effects of adversarial samples on language models.\n\u2022 We propose a novel framework (UAPAD), which efficiently discriminates adversarial samples without access to training data, and maintains an equivalent time consumption to normal inference. Our codes 1 are publicly available.\n2 Related Work\n\nUniversal adversarial perturbation\nThe existence of UAPs has first been demonstrated by (Moosavi-Dezfooli et al., 2017) , that a single perturbation can fool deep models when added to most natural samples. Such phenomena have been extensively verified in image (Khrulkov and Oseledets, 2018 ), text (Song et al., 2021) , and audio models (Li et al., 2020a) . Some works attribute the existence of UAPs to a specific low-dimensional subspace, which is perpendicular to the decision boundary for most of the data. The attention on UAPs mainly focused on their construction, detection and defence (Zhang et al., 2021b) , and neglected to explore the relationship between adversarial samples and UAPs. Our experimental results in Figure 2 demonstrate the tight connection between these two phenomena.\n\nAdversarial detection in NLP\nAdversarial detection is an emerging area of research on language model security. A series of works analyze the frequency characteristics of word substitutions in pre-collected adversarial sentences and replace (Zhou et al., 2019; Mozes et al., 2021) or mask (Mosca et al., 2022) them to observe model reactions. These methods rely on empirically designed word-level perturbations, which limit their generalizability across different attacks. Ma et al. (2018) first proposed to train additional discriminative models to decide whether an input sentence has suffered from word-level adversarial substitution. This idea was generalized by Liu et al. (2022) and Yoo et al. (2022) , which determine the likelihood of a sentence has been perturbed. However, they still require the statistical characteristics of the training data. In this paper, we for the first time propose to construct data-agnostic models and achieve remarkable detection results.\n\nMethod\nThis section shows how to calculate the UAPs for a specific text model without obtaining training data. And subsequently, how to detect adversarial data by pre-trained UAPs.\nData-free UAPs We compute UAPs for a finetuned model by perturbing the substitute inputs, based on the fact that UAPs are generalized properties for a given model. We start with a parameterfrozen target network f and a random perturbation \u03b4. The optimal situation is we can obtain some data that are involved in the training procedure. However, there are situations that we cannot access to training samples or it is unclear whether the accessible data is within the training set. To demonstrate the effectiveness of UAPAD under the data-agnostic scenario. We initialize the input embedding by randomly selecting data from an unrelated substitute dataset (e.g., the MNLI dataset in our experiments). It is a reasonable assumption that a defender can access a moderate amount of substitute data. These embeddings are subsequently updated to ensure the model's confidence score is above the threshold on them. In our framework, we only retain samples with model confidence above 85% to calculate UAPs. We then optimize the perturbation \u03b4 by gradient-ascending the overall loss when added to all the inputs and project it to a normalized sphere of fixed radius to constrain its norm. We obtain a reasonable UAP when most predictions are induced to a fixed result under perturbation.\nAdversarial Detection with UAPs In Figure 2 , we illustrate the different resistance to UAPs between clean and adversarial samples. We utilize this property to conduct adversarial detection. Given an input x, we perform one inference on model f to obtain the normal output y = f (x) and perform another one when x is perturbed by a calculated UAP \u03b4, that is y \u2032 = f (x + w * \u03b4), where w is a hyperparameter controlling the perturbation's intensity. We detect the input as an adversarial sample when y \u0338 = y \u2032 . Noting that these two inferences can be computed in parallel, our approach does not introduce growth in inference time.\n\nExperimental Setup\nWe experimentally validate our method on three well-accepted benchmarks: SST-2 (Socher et al., 2013) , IMDB (Maas et al., 2011) , and AGNews (Zhang et al., 2015) . The statistics of involved benchmark datasets are summarised in Appendix A. We use the BERT-base (Devlin et al., 2018) as the target model and pre-generate adversarial samples for the detection task with three attack methods: TextFooler (Jin et al., 2020) , PWWS (Ren et al., 2019) , and BERTAttack (Li et al., 2020b) .\n\nDetecting Scenarios\nAdversarial detection task requires a dataset D, containing both clean samples D clean and adversarial samples D adv . In the previous works, there exist two different strategies to construct adversarial datasets. Scenario 1 (easy): The adversarial dataset consists of only successful attack samples. Scenario 2 (hard): The adversarial dataset contains both successful and unsuccessful attack samples. Scenario 2 presents more challenging requirements for detection methods and is closer to real-world settings. We conduct experiments in both scenarios to fully illustrate the performance of UAPAD.\n\nImplementation Details\nWe fine-tuned BERT using consistent settings with (Devlin et al., 2018) . For all three datasets, we took 1500 training samples and saved their attack results under different attack algorithms as adversarial samples. UAPAD has a single hyperparameter w (strength of universal perturbation), which we set to 0.5 for all our detection experiments. Although we believe that a better weight exists and can boost the detection performance, we refuse to extend hyper-parameter searching which is against our original purpose. More implementation details and hyperparameters can be found in Appendix B.\n\nEvaluation Metrics\nWe use two metrics to measure our experimental results. Detection accuracy (ACC) measures the accuracy of classification results on all samples, and F1-score (F1) measures the harmonic mean of precision and recall scores. Similar to DISP, our method provides a direct dis- criminant rather than a score and therefore does not apply to the AUC metric.\nBaselines We compare our proposed methods with four strong baselines. Details are summarized in Appendix C.\n\u2022 MLE (Lee et al., 2018) proposes to train detection models based on Mahalanobis distance.\n\u2022 DISP (Zhou et al., 2019) verifies the likelihood that a token has been perturbed.\n\u2022 FGWS (Mozes et al., 2021) substitutes lowfrequency words in the sentence to detect Word-level attacks.\n\u2022 RDE (Yoo et al., 2022) models the probability density of inputs and generates the likelihood of a sentence being perturbed.\n\nExperiment Results and Discussions\nIn this section, we show the experimental performance of our proposed method under the two scenarios in Section 4.1, and investigate different defence methods on the inference time consumption.\n\nMain Results\nTable 2 and 3 show the detect results on three datasets and three attacks. The highest means are marked in bold. Out of the 18 combinations of dataset-attack-scenario, UAPAD achieves the best performance on 15 of them on ACC and 12 of them on F1 metric, which demonstrates the competitiveness of our data-agnostic approach. UAPAD guarantees remarkable detection performance on the SST-2 and AGNews datasets and suffers from a small degradation on the IMDB dataset. We argue that the average length of sentences is greater on IMDB, resulting in stronger dissimilarity between the adversarial sample generation by attack algorithms and the original sentence. On the AGNews dataset, UAPAD provided a 3-11% increase in detection accuracy relative to the baseline approach.\nWe attribute this impressive improvement to more categories on this task, which improved the accuracy of estimation on the model's UAPs.\n\nTime Consumption\nTo further reveal the strength of UAPAD besides its detection performance, we compare its GPU training time consumption with other baseline methods.\nAs is demonstrated in \n\nConclusion\nIn this paper, we propose that adversarial samples and clean samples exhibit different resistance to UAPs, a model-related vector that can be calculated without accessing any training data. Based on this discovery, we propose UAPAD as an efficient and application-friendly algorithm to overcome the drawbacks of previous adversarial detection methods in terms of slow inference and the requirement of training samples. UAPAD acts by observing the feedback of inputs when perturbed by pre-computed UAPs. Our approach achieves impressive detection performance against different textual adversarial attacks in various NLP tasks. We call for further exploration of the connection between adversarial samples and UAPs.\nThis section discusses the potential limitations of our work. This paper's analysis of model effects mainly focuses on common benchmarks for adversarial detection, which may introduce confounding factors that affect the stability of our framework. Our model's performance on more tasks and more attack algorithms is worth further exploring. Our detection framework exploits the special properties exhibited by the adversarial sample under universal perturbation. We expect a more profound exploration of improving the connection between UAPs and adversarial samples. In Figure 2 , we note that a small number (about 3%) of clean and adversarial samples do not suffer from UAP interference. It is worth conducting an analysis of them to further explore the robustness properties of the language models. We leave these problems to further work. \n", "hypothesis": " In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs.  Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data.  Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs.", "answer": true}
{"title": "Enhancing Out-of-Vocabulary Estimation with Subword Attention", "content": "\nIntroduction\nWord embeddings are very useful in natural language processing tasks. Methods like word2vec (Mikolov et al., 2013a,b) and GloVe (Pennington et al., 2014) train strong semantic representations of words using co-occurrence statistics on a large text corpus, and have been shown to be effective at semantically representing text data. However, one weakness of these methods is that they only learn representations for words that exist in the training corpus, and therefore have no representations on unknown terms, known as out-of-vocabulary (OOV) words. Contextualized embeddings like BERT (Devlin et al., 2018) also suffer from weak performance on rare and unknown words, despite being able to build a contextualized representation of them (Schick and Sch\u00fctze, 2020) . Therefore learning representations for OOV words is an important endeavour. In this work, we focus on static embeddings, where a large amount of OOV work is focused on, and leave contextualized embedding to future work.\nCurrent approaches combine subword and context information to estimate OOV words. While these approaches apply attention mechanisms to aggregate context representations, they tend to do very little with subword representations. As a result, this paper proposes SubAtt, a deep neural network attention model that estimates OOV word representations using attention layers (Vaswani et al., 2017) on the subwords in addition to the contexts. SubAtt also pretrains subword representations, allowing it to learn quality representations before combining it with context. We show that both pretraining and applying attention on subwords improves OOV estimates, and show that SubAtt generally outperforms state-of-the-art OOV estimation models in both intrinsic and extrinsic tasks.\n\nRelated Work\nThere are multiple strategies to estimate OOV embeddings. Some OOV strategies use word roots of the OOV word to estimate OOV embeddings (Bojanowski et al., 2017; Pinter et al., 2017; Sasaki et al., 2019) while other methods use the OOV word's context (Lazaridou et al., 2017; Horn, 2017; Herbelot and Baroni, 2017; Arora et al., 2017; Mu and Viswanath, 2018; Khodak et al., 2018) . However, more recent attempts combine both subwords and context approaches. Schick and Sch\u00fctze propose the Form-Context model (Schick and Sch\u00fctze, 2019c), which estimates OOV embeddings by combining the sum of ngram embeddings (learned by the model) with the sum of word embeddings in the contexts multiplied by a weight matrix (also learned by the model). This model has been extended to the Attentive Mimicking model (Schick and Sch\u00fctze, 2019a) which adds an attention mechanism to the context calculations. A second combined approach is the attention based hierarchical context encoder, known as HiCE (Hu et al., 2019) . HiCE is a transformer based model that leverages the hierarchical structure of contexts, using a transformer encoder to encode each context sentence into a sentence embedding, and then using another transformer encoder to combine each sentence embedding into a full context embedding. It estimates subword information using a character based convolutional neural network (CNN), and then combines each piece into a final OOV embedding. HiCE also adapts its model to the OOV word's corpus using Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) . Another approach, Estimator Vectors (Patel and Domeniconi, 2020) , trains its own word embeddings, along with subword and context embeddings for OOV estimation. BERTRAM (Schick and Sch\u00fctze, 2019b) applies an approach similar to the above models, but for contextualized embedding models like BERT (Devlin et al., 2018) . While these approaches create strong estimates for OOV words, they have some weaknesses. First, although some use attention mechanisms with the context of an OOV word, none of the aforementioned combined approaches use attention for processing the OOV's subwords 1 . Secondly, none of the static embedding approaches pretrain their subwords; they learn these representations at the same time as the whole model 2 . Therefore, we propose SubAtt, a model that uses attention and pretraining on subwords, leading to stronger OOV estimates.\n\nSubAtt\nWe now present SubAtt, a transformer (Vaswani et al., 2017) based model for OOV estimation. First, we describe pretraining the subword representations in Section 3.1, then how the model encodes each context sentence in Section 3.2, and finally how SubAtt combines subword and context information in Section 3.3.\n\nPretraining Subword Representations\nFirst, SubAtt learns subword representations for the current set of word embeddings. SubAtt learns embeddings for character ngrams of each vocabulary word. This is accomplished by adding a beginning and end special token to the word, and then taking each character subset of that word. We learn representations using the following formulation:\nsub wt = 1 |G wt | g\u2208Gw t z g (1)\nwhere G wt is the set of character n-grams (the subwords) of the word w t , and z is the embedding of the subwords. Subword representations z are learned by maximizing the cosine similarity between sub wt and the corresponding word embedding v wt . Once these subword representations are trained, they are used in the main SubAtt model. An OOV word is broken down into its character ngrams, which are then converted to the set of corresponding subword embeddings Z.\n\nContext Encoder\nSubAtt encodes sentences using a context encoder similar to the one in HiCE (Hu et al., 2019) . For each word, an input embedding is built by combining its word embedding and a position embedding. The set of input embeddings for context j (denoted context words Q j ) are then inputted into a transformer encoder:\nEQUATION\nwhich is then averaged for a final context representation c j . These representations make up the set of context embeddings C.\n\nFull SubAtt Model\nSubAtt is composed of a subword half and a context half. The subword inputs Z are the OOV word's ngram subword representations learned in Section 3.1. For the list of contexts, the context representations C are calculated using the architecture described in Section 3.2. Each type is processed through their own sets of multi-head self attention encoders \nEQUATION\nEQUATION\nFinally, we combined the representations for a final estimate of the OOV embedding. Subwords and contexts can vary in how informative they are to the OOV word, and so it is important to combine them in a fashion that weighs each estimate accordingly. SubAtt uses an adaptive weighting strategy used in the Form Context Model and Attentive Mimicking Model (Schick and Sch\u00fctze, 2019c,a), known as the gated model. The subword outputs Z self and context outputs C self are separately averaged into v subword and v context respectively. They are then combined by a weighted sum:\nv f inal = \u03b1 v subword + (1 \u2212 \u03b1) v context (5)\nThe weight \u03b1 is calculated as follows:\n\u03b1 = \u03c3(w T [ v subword , v context ] + b) (6)\nwhere w and b are learned parameters, and \u03c3 is the sigmoid function. v f inal is the final estimate of the OOV word embedding. SubAtt has eight layers of self attention for the subword inputs and eight layers for the context input.\n\nTraining Corpus and Word Embeddings\nThe goal of SubAtt is to estimate representations for OOV words given existing word embeddings.\nFor the gold standard word embeddings, we use the embeddings provided by Herbelot and Baroni (Herbelot and Baroni, 2017) , as done in previous OOV models like (Schick and Sch\u00fctze, 2019c) and (Hu et al., 2019) . For training models, contexts are taken from the Westbury Wikipedia Corpus (WWC) (Shaoul, 2010) . We use the version from (Khodak et al., 2018) \n\nBaselines and Hyperparameters\nWe now demonstrate the effectiveness of SubAtt. 4 We compare it to Attentive Mimicking 5 (AM) and HiCE 6 , as they are OOV models that use both subwords and context on existing static word embeddings. Two versions of HiCE are examined; the default with a 2 layer context aggregator, and a version with 8 layers to be more comparable to SubAtt. Also, we do not use MAML in the HiCE experiments, in order to focus on how the architecture adapts to multiple OOV tasks. The dataset and vocab are split into a training and validation set for hyperparameter tuning (discussed in more detail in Appendix A).\nTen final trials of each model are trained and then each model is evaluated on various OOV tasks. The results are tested for statistical significance using a one-way ANOVA with a post-hoc Tukey HSD test with a p-value threshold equal to 0.05. The best score is presented in bold, along with any scores that are not significantly different from the best.\n\nTasks\nWe now evaluate SubAtt on various OOV tasks. We focus on OOV tasks in English, matching previous work. As SubAtt mixes both subwords and (Khodak et al., 2018) . CRW is built off the Rare Word dataset (Luong et al., 2013) , which is a list of rare words paired with other words, along with human similarity scores. Khodak et al. (2018) added contexts to this set, allowing for OOV words to be estimated using both subwords and context. The goal is to output an OOV embedding, compare it to the other words, and evaluate the scores' correlation with human judgements. CRW has a large range of context sizes, from 1 to 128, so the quality and informativeness of the context can vary wildly. However, the words gathered for the Rare Word set have intentionally informative word roots, and therefore we expect subwords to be fairly informative.\nThe results of the CRW task are shown in Figure 1 . SubAtt significantly outperforms all competitors in all contexts, showing its effectiveness as an OOV estimator. This shows the strength of pretrained subwords and subword attention.\nDownstream Tasks We now demonstrate the strong performance of SubAtt embeddings extrinsically, using downstream tasks. In order to focus on OOV words specifically, we choose downstream tasks that output word level labels; specifically named entity recognition and parts-of-speech tagging. For each of these tasks, we train a Bi-LSTM-CRF (Lample et al., 2016) , an approach similar to the one in (Hu et al., 2019) . The input to these models are normal word embeddings for words in our vocabulary (ones used in training and validation of the original OOV models), and each model's OOV estimates for unknown words. 7 For 7 OOV words with invalid subwords (no existing character ngrams or no CharCNN characters) are assigned a zero vector. each dataset, the Bi-LSTM-CRF is trained for 30 epochs 10 times, with the best epoch selected using a validation set each time. This approach is applied to each of the 10 trials of each OOV model. As our focus is estimating OOV words, we report the average test macro F1 score of the OOV words specifically. We also report the results for all words in Appendix B.\nWe test on 5 named entity recognition tasks: the JNLPBA 2004 Bio-entity recognition dataset (Bio-NER) (Kim et al., 2004) , the Rare-NER dataset (Derczynski et al., 2017) , the CoNLL 2003 NER dataset (Sang and De Meulder, 2003) , AnEM (an anatomy NER dataset) (Ohta et al., 2012) , and MovieMIT, a movie querying dataset (Liu et al., 2013) . In addition, we test on a parts-of-speech tagging dataset, specifically the Twitter social media POS task (Ritter et al., 2011) .\nThe Downstream Task results are shown in Table 1 . SubAtt generally outperforms the competitors, strictly winning in 3 of the 6 tasks, and tying for best in one more task, and achieving the second best score in another task. This demonstrates Sub-Att's robust and strong performance on OOV words in downstream tasks.\n\nAblation Analysis\nWe now conduct an ablation study on SubAtt in order to demonstrate the impact of the pretraining compared to attention. To this end, we repeat the previous experiments on four variants of SubAtt; the original model, the model without attention (SubAtt No Att), the model without pretrained subwords (SubAtt No Pre), and the model without both (SubAtt No Pre No Att). The results are shown in Figure 2 creases 8 . This makes sense, as the influence of subwords decreases as our model gains more and more context information, which in turn lowers the impact of the pretraining and attention on subwords in general. Similarly, SubAtt performs strongly in the Downstream Ablation, performing the best or tied for the best in all six tasks. The results also demonstrate that pretraining and subword attention individually have a high impact on results, and both combined leads to an even stronger improvement.\n\nConclusion\nWe propose SubAtt, an attention based model that estimates OOV words by using pretrained subword embeddings and subword attention. We show through various experiments that this model estimates more accurate representations of OOV words.\n", "hypothesis": "As a result, there have been multiple attempts to learn OOV word representations in a similar fashion to how humans learn new words, using word roots/subwords and/or surrounding words. However, while most of these approaches use advanced architectures like attention on the context of the OOV word, they tend to neglect the importance of subword representations. In response to this, we propose SubAtt, a simple neural network model that focuses solely on subword information for estimating OOV word representations. By leveraging the power of subword representations and disregarding context, SubAtt achieves superior performance compared to state-of-the-art models in both intrinsic and extrinsic tasks.", "answer": false}
{"title": "Silver Syntax Pre-training for Cross-Domain Relation Extraction", "content": "\nIntroduction\nRelation Extraction (RE) is the task of extracting structured knowledge, often in the form of triplets, from unstructured text. Despite the increasing attention this task received in recent years, the performance obtained so far are very low (Popovic and F\u00e4rber, 2022) . This happens in particular when considering realistic scenarios which include outof-domain setups, and deal with the whole taskin contrast to the simplified Relation Classification which assumes that the correct entity pairs are given (Han et al., 2018; Baldini Soares et al., 2019; Gao et al., 2019) . One main challenge of RE and other related Information Extraction tasks is the \"domain-specificity\": Depending on the text domain, the type of information to extract changes. For example, while in the news domain we can find entities like person and city, and relations like city of birth (Zhang et al., 2017) , in scientific texts, we can find information about metrics, tasks and comparisons between computational models (Luan et al., 2018) . While high-quality, domain-specific data for fine-tuning the RE models would be ideal, as for many other NLP tasks, annotating data is expensive and time-consuming. 1 A recent approach that leads to improved performance on a variety of NLP tasks is intermediate task training. It consists of a step of training on one or more NLP tasks between the general language model pre-training and the specific end task fine-tuning (STILT, Supplementary Training on Intermediate Labeled-data Tasks; Phang et al., 2018) . However, STILT assumes the availability of additional high quality training data, annotated for a related task.\nIn this paper, we explore intermediate pretraining specifically for cross-domain RE and look for alternatives which avoid the need of external manually annotated datasets to pre-train the model on. In particular, we analyze the affinity between syntactic structure and semantic relations, by considering the shortest dependency path between two entities (Bunescu and Mooney, 2005; Fundel et al., 2006; Bj\u00f6rne et al., 2009; Liu et al., 2015) . We replace the traditional intermediate pre-training step Linear-fractional programming ( LFP ) is a generalization of linear programming ( LP ) . on additional annotated data, with a syntax pretraining step on silver data. We exploit the high accuracy of current syntax parsers, for obtaining large amount of low-cost pre-training data. The use of syntax has a long tradition in RE (Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009; Peng et al., 2015) . Recently, work has started to infuse syntax during language model pre-training (Sachan et al., 2021) showing benefits for RE as well. Syntactic parsing is a structured prediction task aiming to extract the syntactic structure of text, most commonly in the form of a tree. RE is also a structured prediction task, but with the aim of extracting the semantics expressed in a text in the form of triplets-entity A, entity B, and the semantic relation between them. 3 We exploit the affinity of these two structures by considering the shortest dependency path between two (semantic) entities (see Figure 1 ). The idea we follow in this work is to pre-train an RE baseline model over the syntactic relations-Universal Dependency (UD) labels-which most frequently appear on the shortest dependency paths between two entities (black bold arrows in Figure 2 ). We assume these labels to be the most relevant with respect to the final target task of RE. In order to feed the individual UD relations into the RE baseline (model details in Section 3.1) we treat them similarly as the semantic connections. In respect to Figure 2 , we can formalize the semantic relations as the following triplets:\n\u2022 NAMED(LFP,Linear-fractional programming)\n\u2022 TYPE-OF(linear programming,Linear-fractional programming)\n\u2022 NAMED(LP,linear programming).\nAccordingly, we define the syntax pre-training instances as:\n\u2022 appos(programming,LFP)\n\u2022 nsubj(generalization,programming)\n\u2022 nmod(generalization,programming)\n\u2022 appos(programming,LP).\nIn the next section we describe the detailed training process.\n\nSetup\nData In order to evaluate the robustness of our method over out-of-domain distributions, we experiment with CrossRE (Bassignana and Plank, 2022) , 4 a recently published multi-domain dataset. CrossRE includes 17 relation types spanning over six diverse text domains: news, politics, natural science, music, literature and artificial intelligence (AI). The dataset was annotated on top of a Named Entity Recognition dataset-CrossNER (Liu et al., 2021) -which comes with an unlabeled domainrelated corpora. 5 We used the latter for the syntax pre-training phase.\n\nUD Label Selection\nIn order to select the UD labels which most frequently appear on the shortest dependency path between two semantic entities, we parsed the training portions of CrossRE. Our analysis combines RE annotations and syntactically parsed data. We observe that the syntactic distance between two entities is often higher than one (see Figure 4 ), meaning that the shortest dependency path between two entities includes multiple dependencies-in the examples in Figure 1 , the one above has distance one, the one below has distance two. However, the shortest dependency paths contain an high frequency of just a few UD labels (see Figure 3 ) which we use for syntax pre-training: nsubj, obj, obl, nmod, appos. See Appendix A for additional data analysis. To do so, 1 we sample an equal amount of sentences from each domain 6 (details in Section 4), and 2 use the MaChAmp toolkit (van der Goot et al., 2021) for inferring the syntactic tree of each of them. We apply an additional sub-step for disentangling the conj dependency, as illustrated in Appendix C. Then, 3 we filer in only the nsubj, obj, obl, nmod, and appos UD labels and 4 feed those connections to the RE model (as explained in the previous section). Within the RE model architecture described above, each triplet corresponds to one instance. In this phase, in order to assure more variety, we randomly select a maximum of five triplets from each pre-train sentence.\n\nModel\nIn the second training phase-the fine-tuning one-we replace the classification head (i.e. the feed-forward layer) with a new one, and individually train six copies of the model over the six train sets of CrossRE. Note that the encoder is fine-tuned in both training phases. Finally, we test each model on in-and out-of-domain setups. \n\nResults\nTable 1 reports the results of our cross-domain experiments in terms of Macro-F1. We compare our proposed approach which adopts syntax pre-training with the zero-shot baseline model. 7 Five out of six models outperform the average of the baseline evaluation, including in-and out-ofdomain assessments. The average improvementobtained without any additional annotated RE data-is 0.71, which considering the low score range given by the challenging dataset (with limited train sets, see dataset size in Appendix D), and the cross-domain setup, is considerable. The model fine-tuned on the news domain is the only one not outperforming the baseline. However, the performance scores on this domain are already extremely low for the baseline, because news comes from a different data source with respect to the other domains, has a considerable smaller train set, and present a sparse relation types distribution, making it a bad candidate for transferring to other domains (Bassignana and Plank, 2022) .\nAs comparison, we report the scores obtained with the traditional intermediate pre-training which includes additional annotated data. We pre-train the language encoder on SciERC (Luan et al., 2018) , a manually annotated dataset for RE. SciERC contains seven relation types, of which three overlap 7 While utilizing the model implementation by Bassignana and Plank, 2022 , our score range is lower because we include the no-relation case, while they assume gold entity pairs. with the CrossRE relation set. In this setup, the improvement over the baseline includes the news, but not the literature domain. Nevertheless, while the gain is on average slightly higher with respect to the proposed syntax pre-training approach, it comes at a much higher annotation cost.\n\nPre-training Data Quantity Analysis\nWe inspect the optimal quantity of syntactic data to pre-train our RE model on by fine-tuning this hyperparameter over the dev sets of CrossRE. The plot in Figure 5 reports the average performance of the six models when pre-trained on increasing amounts of syntactic dependencies. 8 Starting from 8.4K instances onward, the performance stabilizes above the baseline. We select the peak (20.4K, albeit results are similar between 18-20.4K) for reporting our test set results in Table 1 . While we are interested in the robustness of our method across multiple domains, and therefore consider the average (Figure 5 ), domain-optima could be achieved by examining individual domain performance. As example, we report in Figure 6 the plot relative to the model fine-tuned on AI, which is the one obtain-ing the highest gain. The model fine-tuned on AI generally gains a lot from the syntax pre-training step, with its peak on 15.6K pre-training instances.\n\nConclusion\nWe introduce syntax pre-training for RE as an alternative to the traditional intermediate training which uses additional manually annotated data. We pretrain our RE model over silver UD labels which most frequently connect the semantic entities via the shortest dependency path. We test the proposed method over CrossRE and outperform the baseline in five out of six cross-domain setups. Pre-training over a manually annotated dataset, in comparison, only slightly increases our scores in five out of six evaluations, but at a much higher cost.\n", "hypothesis": " By pre-training our RE model on the relevant syntactic relations, we are able to outperform the baseline in five out of six cross-domain setups, without any additional annotated data..", "answer": true}
{"title": "Negation Scope Refinement via Boundary Shift Loss", "content": "\nIntroduction\nNegation is a complex linguistic phenomenon. Even though there does not exist a widely agreed task definition for negation detection, two sub-tasks are commonly performed: (i) negation cue detection, and (ii) negation scope resolution. Negation cue is a keyword (e.g., not, never) in a sentence that acts as an indicator of semantic negation, and its detection is relatively easy. Negation scope refers to the portion(s) in a sentence being semantically affected (i.e., negated) by the cue. There could be multiple cues in one sentence and each corresponds to its own scope. Table 1 lists three cues in the same sentence and their scopes.\nDifferent datasets may adopt different annotation guideline of scopes, e.g., whether or not a cue itself is a part of its scope. The example sentence in Table 1 well demonstrates the unique characteristics of this task compared to other span extraction tasks like Named Entity Recognition (NER). They are: (i) a negation scope is defined by (or associated to) a given cue, (ii) the negation spans are usually longer than a named entity, and (iii) a good number of negation spans are discontinuous, depending on the adopted annotation guideline.\nIn recent years, pretrained language models (PLMs) like BERT (Devlin et al., 2019) have been explored to improve negation detection (Khandelwal and Sawant, 2020; Khandelwal and Britto, 2020) . Specially designed pre-training material that focuses on negation has also been explored and achieves state-of-the-art performance (Truong et al., 2022) . Nevertheless, we believe that negation detection shall be considered as a pre-processing step for downstream subtasks and its model shall not be over-complicated.\nIn this paper, we enhance a simple baseline by Khandelwal and Sawant (2020) with an effective Boundary Shift Loss (BSL), to refine the predicted negation scope boundaries. BSL is derived based on the positions of span boundaries. For each token, boundary shift tells the direction of the nearest span boundary: left or right. With the simple BERT + Feed-forward architecture, our R-BSL model outperform baselines on all well-known datasets.\n\nRelated Work\nNegation detection was firstly studied in biomedical and health texts, represented by NegEx (Chapman et al., 2001 ) developed for EHRs. NegEx is built on top of regular expressions; its negation scopes are mainly named entities. The definition of negation scope becomes largely different and more generic in later datasets. The BioScope corpus (Vincze et al., 2008) annotates negation scope in biological full papers and scientific abstracts. The \"Sherlock\" corpus (Morante and Blanco, 2012) , annotates Conan Doyle's novels Sherlock Holmes series. SFU Review Negation corpus (Konstantinova et al., 2012) annotates negations and speculations in the SFU Review corpus (Taboada et al., 2006) for sentiment analysis.\nLike many other NLP tasks, BERT leads to significant improvement on scope resolution (Khan-Cue Negation scope marked in discontinuous \"span\" s in-Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" not [cue] in-[/cue] \"frequent occasions when he was up all night\", was seated at the breakfast table.\nnot Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" [cue] not [/cue] \"infrequent occasions when he was up all night\", was seated at the breakfast table.\nsave Mr. Sherlock Holmes, \"who was\" usually \"very late in the mornings\", [cue] save [/cue] \"upon those not infrequent occasions when he was up all night\", was seated at the breakfast table. \n\nProblem Definition\nAs a common practice, we assume that negation cue has been successfully detected. Our key focus is negation scope resolution for a given cue. For presentation simplicity, we assume there is only one cue in a given sentence. The cases of multiple cues can be easily achieved by sentence duplication, each time with a different known cue being wrapped with special indicator tokens. The model would be trained to predict negation scope of each cue separately. Table 1 gives a typical example of how sentence with three negation cues and three corresponding scopes is being pre-processed by duplication and the special indicator tokens\n[cue] [/cue].\nGiven an input sequence S = \u27e8t 1 , t 2 , ..., t n \u27e9, with a known cue, the task is to predict the cue's negation score in token spans. We adopt the OSC tagging scheme: Y = \u27e8y 1 , y 2 , ..., y n \u27e9 where y i is O if t i is non-scope, S if t i is part of the scope, and C if t i is the given cue. We use a dedicated label \"C\" for cue, to satisfy the annotation guidelines in different datasets, i.e., not all annotations consider cue as a part of the scope.\n\nThe R-BSL Model\nThe central idea of Boundary Shift Loss is inspired by techniques used for semantic segmentation. Background. Locating accurate segmentation boundary is particularly important for medical images such as MRI, as the boundary for body organ is crucial. In a 2D image, we can represent the deviation of the predicted boundary with ground truth boundary in the form of a distance map, as shown in Figure 1 . Each pixel in the example image is mapped with a normalized distance to its nearest ground truth boundary pixel, forming the boundary distance map.\nFor a typical pixel, the distance map could be reduced to a local distance map of 3 \u00d7 3, containing distance of the pixel itself and that of its eight neighbours. The cell with the smallest distance (e.g., the top left cell in the example) represents the direction to the nearest boundary. To indicate this direction, local distance map can be further reduced to an one-hot local direction map, where the \"1\" cell representing the direction of the nearest boundary. Accordingly, the predicted boundary can be further refined toward this direction for more accurate boundary prediction (Wang et al., 2022) . Span extraction tasks in NLP share the same aim to find accurate region boundaries, but in a 1D space, i.e., along token sequence to shift left or right.\n\nBoundary Shift Map\nTo enable boundary shift loss, we convert the scope labels to scope span boundary labels. BS = \u27e8bs 1 , bs 2 , ..., bs n \u27e9 and BE = \u27e8be 1 , be 2 , ..., be n \u27e9 are the two label sequences that represent the start and end of boundaries, respectively. bs i is Bs if t i is the start of a scope span, and O otherwise; be i is Be if t i is the end of a scope span, and O otherwise. If a span consists of only one token, the token itself is labeled both Bs and Be. Due to discontinuous spans, there could be multiple bs and be labels for one given cue, as shown in Figure 2 .\nNext, we create the \"Boundary Shift Map\" (BSM) for tokens that are not on the boundaries, by labeling their shifting directions: L for left, and R for right. The 5th and 6th rows in Figure 2 provide a visual illustration, for start and end boundaries respectively. A token is labeled with L / R if the nearest boundary resides on the left / right of the token. For the special case that a token has the same distance to both boundaries on the left and right, we label the token with R.\n\nR-BSL Model Detail\nFigure 3 illustrates the model architecture. We use BERT to encode the sentence and then use three feed-forward (FF) layers in parallel, to predict scope label and the BSM labels. The losses for the three label classifiers L scope , L start , L end are the widely used Cross Entropy loss. L scope is formally defined in Eq. 1 and the other two losses are defined similarly. The three losses are then combined to form the final loss in Eq. 2, and we set \u03b1 = 0.2\nL R Bs L L R R Be L R Be L R\nL scope = \u2212 N i=1 y (i) log(\u0177 (i) )\n(1)\nLoss = \u03b1L scope + 1 \u2212 \u03b1 2 (L start + L end ) (2)\nWarm Up. In training, there is a \"warm up\" phase to train the model solely with scope loss L scope for the first 5 epochs (where the validation loss is reasonably stable). Then boundary shift losses kick in to for scope refinement.\n\nExperiment Results\nWe conduct experiments on all three benchmark datasets: Sherlock, BioScope, and SFU. Among them, BioScope and SFU datasets do not come with official train-validation-test split. Following the previous studies, we use random split on 70-15-15 ratios; however the randomness in split may slightly affect model performance. Hence, we also report the result of our re-implemented baseline model Khandelwal and Sawant (2020) , which is a BERT + Feed-forward with OSC scope tags. Table 2 reports the results of F 1 over scope tokens, defined by Morante and Blanco (2012) . For each scope, token-wise F 1 is computed between ground truth and predicted scope tokens. For all our implemented models, the reported results are average scores of 5 out of 7 runs, excluding the highest and lowest scores. All the runs are set with randomly generated seeds. Since Truong et al. (2022) use RoBERTa instead of BERT, we also report R-BSL (RoBERTa-base) for fair comparison.\nR-BSL achieves best performance on all three datasets, particularly on Sherlock which comes with official train/test split. Note that on Sherlock dataset, our re-implemented baseline does not reach the scores reported in Khandelwal and Khandelwal and Sawant (2020) . For BioScope-Abstract and SFU, there is no official train/test split. The difference in random split (with the same ratio) leads to the difference between our re-implemented baseline and previous studies.\nSawant (2020). 2 Truong et al. ( 2022) also reports lower results (mean of 5 runs) using the code released by Khandelwal and Sawant (2020) . Nevertheless, both our R-BSL variants outperform all baselines on Sherlock, and on BioScope dataset. On SFU, our models' improvement is marginal.\nThe main reason is the distributional bias, for the negation scopes largely align with punctuation or special tokens (see Appendix C).\nFor comprehensive evaluation, Table 3 shows the scope level F 1 scores by exact match. That is, when the predicted scope exactly matches the ground truth, it is considered as True Positive. There exists True Negative and False Positive cases due to \"void negation\" as discussed in Appendix C. When the ground-truth has no negation scope, if the model predicts any scope, that would be a False Positive. The scope exact match F 1 is similar to \"Scope CM\" metric defined in Morante and Blanco (2012) . However, as we do not focus on cue detection but using cues as input, the results is not directly comparable with Scope CM results in earlier studies.\nCompared to token-level measure, the improvements of our model over baseline is now by a much larger margin, particularly the variant with RoBERTa. In other words, the boundary refinement by BSL enables the model to resolve more accurate negation scopes in terms of exact scope span match, which is a stricter measure.\n\nAblation Study\nWe conduct two ablation studies on Sherlock dataset, and the results are reported in Table 4 . 2 The original paper does not provide complete experimental setup like how many runs were performed, or whether the reported results being mean or maximum of several runs. \"Warm Up\" of Scope Classifier. We \"warm up\" the training with the first 5 epochs for scope classifier only. The boundary classifier with BSL loss then comes into the picture. To study its impact, we train all the three classifiers from the beginning. Shown in Table 4 , the removal of warm up leads to negative impact on results. This ablation study suggests that the BSL can further improve the results when the span boundaries have been de-tected by the base model, i.e.,, the scope classifier, at reasonably good accuracy.\n\nConclusion\nWe propose a simple sequence labelling training strategy to enhance boundary prediction for negation scope resolution. Through experiments, we demonstrate the effectiveness of boundary shift loss on complex span extraction tasks on three benchmark datasets. In particular, our simple model achieves the state-of-the-art results on the Sherlock dataset which is considered more challenging for this task. Our model is simple and can be used as a pre-processing for downstream tasks where negation is an important consideration.\n", "hypothesis": "Due to the long spans, existing methods tend to make wrong predictions around the scope boundaries.  In this paper, we propose a complex model named R-BSL* which engages the Boundary Shift Loss to refine the predicted boundary.", "answer": false}
{"title": "The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics", "content": "\nIntroduction\nReference-based neural metrics for machine translation evaluation are achieving evergrowing success, demonstrating superior results over traditional lexical overlap-based metrics, such as BLEU (Papineni et al., 2002) and CHRF (Popovi\u0107, 2015) , in terms of both their correlation with human ratings and their robustness across diverse domains (Callison-Burch et al., 2006; Smith et al., 2016; Mathur et al., 2020; Kocmi et al., 2021; Freitag et al., 2022) . However, lexical overlapbased metrics remain popular for evaluating the performance and progress of translation systems and algorithms. Concerns regarding trust and interpretability may help explain this (Leiter et al., 2022) : contrary to traditional metrics, neural metrics are considered \"black boxes\" as they often use Figure 1 : Illustration of our approach. In this example, the metric assigns the translation a low score. We aim to better understand this sentence-level assessment by examining the correspondence between our token-level explanations and human annotated error spans. increasingly large models (e.g., the winning metric of the WMT 22 Metrics shared task was a 10B parameter model (Freitag et al., 2022) ).\nWhile some recent work has focus on explaining the predictions made by reference-free quality estimation (QE) systems (Fomicheva et al., 2021; Zerva et al., 2022) , explaining reference-based metrics has remained a largely overlooked problem (Leiter et al., 2022) . It is an open question whether the observations from studies of explainable QE carry over to this scenario. Thus, in this work, we fill that gap by turning to state-of-theart reference-based metrics-we aim to interpret their decision-making process by exploiting the fact that these metrics show consistently good correlations with Multidimentional Quality Metrics (MQM) (Freitag et al., 2021 (Freitag et al., , 2022;; Sai et al., 2022) , which are fine-grained quality assessments that result from experts identifying error spans in translation outputs (Lommel et al., 2014) . We hypothesize that reference-based metrics leverage this tokenlevel information to produce sentence-level scores.\nTo test this hypothesis, we assess whether our explanations -measures of token-level importance obtained via attribution and input attribution methods such as attention weights and gradient scores (Treviso et al., 2021; Rei et al., 2022b) -align with human-annotated spans (Fomicheva et al., 2021 (Fomicheva et al., , 2022;; Zerva et al., 2022) , as illustrated in Figure 1 .\nOur analysis focuses on two main vectors: (i) understanding the impact of the reference information on the quality of the explanations; and (ii) finding whether the explanations can help to identify potential weaknesses in the metrics. Our main contributions are:\n\u2022 We provide a comparison between multiple explainability methods for different metrics on all types of evaluation: src-only, ref-only, and src+ref joint evaluation.\n\u2022 We find that explanations are related to the underlying metric architecture, and that leveraging reference information improves the explanations.\n\u2022 We show that explanations for critical translation errors can reveal weaknesses in the metrics.\n\nExplaining Neural Metrics\nWe aim to explain sentence-level quality assessments of reference-based metrics by producing token-level explanations that align to translation errors. In what follows, we describe the metrics and how we produce the explanations that we study.\n\nMetrics\nWe focus our analysis on two state-of-the-art neural metrics: COMET (Rei et al., 2020) and UNITE (Wan et al., 2022) . 1 While both metrics use a multilingual encoder model based on XLM-R (Conneau et al., 2020) , they employ distinct strategies to obtain sentence-level quality scores. On the one hand, COMET separately encodes the source, translation and reference to obtain their respective sentence embeddings; these embeddings are then combined to compute a quality score. On the other, UNITE jointly encodes the sentences to compute a contextualized representation that is subsequently used to compute the quality score. (Sellam et al., 2020) ; and SRC+REF, like ROBLEURT (Wan et al., 2021) .\n\nExplanations via Attribution Methods\nIn this work, we produce explanations using attribution methods that assign a scalar value to each translation token (i.e. a token-level attribution) to represent its importance. While many input attribution methods exist and have been extensively studied in the literature (Ribeiro et al., 2016; Shrikumar et al., 2017; Sundararajan et al., 2017; Jain and Wallace, 2019; Atanasova et al., 2020; Zaman and Belinkov, 2022) , we focus specifically on those that have been demonstrated to be effective for explaining the predictions of QE models (Treviso et al., 2021; Fomicheva et al., 2022; Fernandes et al., 2022; Zerva et al., 2022) and extend them to our reference-based scenario. Concretely, we use the following techniques to extract explanations: 2\n\u2022 embed-align: the maximum cosine similarity between each translation token embedding and the reference and/or source token embeddings (Tao et al., 2022) ;\n\u2022 grad \u2113 2 : the \u2113 2 -norm of gradients with respect to the word embeddings of the translation tokens (Arras et al., 2019) ;\n\u2022 attention: the attention weights of the translation tokens for each attention head of the encoder (Treviso et al., 2021) ;\n\u2022 attn \u00d7 grad: the attention weights of each head scaled by the \u2113 2 -norm of the gradients of the value vectors of that head (Rei et al., 2022b) .\n3 Experimental Setting MQM annotations. We use MQM annotations from the WMT 2021 Metrics shared task (Freitag et al., 2021), 3 covering three language pairs -English-German (en\u2192de), English-Russian (en\u2192ru), and Chinese-English (zh\u2192en) -in two different domains: News and TED Talks. For each incorrect translation, human experts marked the corresponding error spans. In our framework, these error spans should align with the words that the attribution methods assign higher importance to.\nMETRIC EXPLAINABILITY en\u2192de zh\u2192en en\u2192ru Avg. METHOD AUC R@K AUC R@K AUC R@K AUC R@K src-only \u22c6 evaluation UNITE SRC embed-align [mt, src] [mt, src] 0.590 0.371 0.674 0.314 0.577 0.220 0.614 0.301 embed-align [mt, ref] 0.694 0.425 0.696 0.355 0.647 0.275 0.679 0.352 embed-align [mt, src; ref] \n\nModels.\nFor COMET, we use the latest publicly available model: wmt22-comet-da (Rei et al., 2022a) . 4 For UNITE, we train our own model using the same data used to train COMET in order to have a comparable setup 5 . We provide full details (training data, correlations with human annotations, and hyperparameters) in Appendix A.\nOverall, the resulting reference-based UNITE models (REF and SRC+REF) are on par with COMET.\nEvaluation. We want our explanations to be directly attributed to the annotated error spans, in the style of an error detection task. Thus, we report Area Under Curve (AUC) and Recall@K. 6 These metrics have been used as the main metrics in previous works on explainable QE (Fomicheva et al., 2021 (Fomicheva et al., , 2022;; Zerva et al., 2022) .\n\nHigh-level analysis\nExplanations are tightly related to the underlying metric architecture. The results in Ta-ble 1 show that the predictive power of the attribution methods differ between UNITE and COMET: attn \u00d7 grad is the best method for UNITEbased models, while embed-align works best for COMET. 7 This is expected as UNITE constructs a joint representation for the input sentences, thus allowing attention to flow across them; COMET, in contrast, encodes the sentences separately, so it relies heavily on the separate contextualized embeddings that are subsequently combined via elementwise operations such as multiplication and absolute difference. Interestingly, embed-align and attn \u00d7 grad were the winning explainability approaches of the WMT 2022 Shared-Task on Quality Estimation (Zerva et al., 2022) . This suggests that explainability methods developed for QE systems can translate well to reference-based metrics. We provide examples of explanations in Appendix C.\nReference information boosts explainability power. obtain token-level attributions does not consistently yield superior results over using the reference alone. Notably, the best attribution method for COMET does not require any source information. This is interesting: in some cases, reference-based metrics may largely ignore source information, relying heavily on the reference instead.\n\nHow do the explanations fare for critical translation errors?\nThe MQM data analyzed until now consists primarily of high quality translations, with the majority of annotated errors being non-critical. However, it is important to assess whether our explanations can be accurately attributed to critical errors, as this may reveal potential metric shortcomings. To this end, we employ SMAUG (Alves et al., 2022) 8 , a tool designed to generate synthetic data for stresstesting metrics, to create corrupted translations that contain critical errors. Concretely, we generate translations with the following pathologies: negation errors, hallucinations via insertions, named entity errors, and errors in numbers. 9\nExplanations identify critical errors more easily than non-critical errors. Figure 2 shows that explanations are more effective in identifying critical errors compared to other non-critical errors (see Table 1 ). Specifically, we find significant performance improvements up to nearly 30% in Recall@K for certain critical errors. Overall, hallucinations are the easiest errors to identify across all neural metrics. This suggests that neural metrics appropriately identify and penalize hallucinated translations, which aligns with the findings of Guerreiro et al. (2022) Explanations can reveal potential metric weaknesses. Figure 2 suggests that COMET explanations struggle to identify localized errors (negation errors, named entity errors and discrepancies in numbers). We hypothesize that this behavior is related to the underlying architecture. Unlike UNITE-based metrics, COMET does not rely on soft alignments via attention between the sentences in the encoding process. This process may be key to identify local misalignments during the encoding process. In fact, the attention-based attributions for UNITE metrics can more easily identify these errors. COMET, however, encodes the sentences separately, which may result in grammatical features (e.g. numbers) being encoded similarly across sentences (Chi et al., 2020; Chang et al., 2022) . As such, explanations obtained via embedding alignments will not properly identify these misalignments on similar features. Importantly, these findings align with observations made in (Amrhein and Sennrich, 2022; Raunak et al., 2022) . This showcases how explanations can be used to diagnose and reveal shortcomings of neural-based metrics.\n\nConclusions and Future Work\nIn this paper, we investigated the use of explainability methods to better understand widely used neural metrics for machine translation evaluation, such as COMET and UNITE. Concretely, we analyzed how explanations are impacted by the reference information, and how they can be used to reveal weaknesses of these metrics. Our analysis shows that the quality of the explanations is tightly related to the underlying metric architecture. Interestingly, we also provide evidence that neural metrics like COMET may heavily rely on reference information over source information. Additionally, we show that explanations can be used to reveal reference-based metrics weaknesses such as failing to severely penalize localized critical errors. This opens up promising opportunities for future research on leveraging explanations to diagnose reference-based metrics errors. To support these studies, we call for future datasets illustrating critical errors (e.g., challenge sets (Karpinska et al., 2022) ) to be accompanied by annotated error spans.\n", "hypothesis": " Yet neural metrics are, to a great extent, \"black boxes\" that return a single sentence-level score without transparency about the decisionmaking process.  In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics.", "answer": true}
{"title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition", "content": "\nIntroduction\nSelf-supervised learning has improved input data representation without requiring extensive humanlabeled data (He et al., 2019; Zhang et al., 2022) . Based on this advancement, powerful pre-trained models providing high-performing representations for various data types (e.g., text, images, and audio) have been proposed. For instance, in speech, self-supervised pre-trained models such as Hu-BERT (Hsu et al., 2021) have advanced state-of-the- art performance of automatic speech recognition (ASR).\nHowever, one major challenge in using pretrained speech models for ASR is the representational bias towards prominent accents present in the dataset during pre-training. Consequently, there will be a disparity in ASR performance between native and non-native speakers. More specifically, pre-training using a large dataset such as the Lib-riSpeech (Panayotov et al., 2015) , which comprises a large proportion of utterances from native (L1) English speakers, leads to a less satisfactory recognition rate for non-native (L2) English accented speech. This phenomenon can curtail the effectiveness of current high-performing ASR systems for real-world applications.\nThere have been several ways to address this issue, including fine-tuning the model on diverse accents (Winata et al., 2019; Shibano et al., 2021) , having a separate model for each accent (Yang et al., 2018) or using regularization losses that guide the fine-tuning process to achieve robustness to accents (Chen et al., 2020; Gao et al., 2022) , all of which require updating the pre-trained model.\nWe propose a different solution for improving L2 speech recognition in transformer-based speech models that introduces a small number of learnable parameters into the input space while keeping the backbone weights of the model untouched. Our approach is guided by Information-Theoretic Adversarial Learning; thus, we refer to it as IN-Tapt (Information-Theoretic Adversarial Prompt Tuning). INTapt aims to introduce auxiliary embeddings (i.e., prompt) concatenated to the original input, which can re-modulate the attention and adapt the pre-trained weights so that the corresponding input looks like speech with an accent seen during pre-training (Figure 1 ). To achieve this, INTapt incorporates (1) adversarial training, which tries to minimize the mutual information between the accent feature of the original input and that obtained by concatenating the prompt embeddings in front of the initial input, and (2) CTC loss training to improve the ASR performance of the prompt-concatenated input. Essentially the prompt is trained such that the accent of the concatenation is pushed away from the input accent and the concatenation achieves native CTC loss performance. Unlike the previous use-case of prompts in NLP or Computer vision (CV), where a single prompt embedding is learned for each discrete task or input domain, the intensity of an accent is continuous. Thus, we propose an input-dependent prompt embedding by training a prompt generator that outputs an input-specific prompt. Through extensive experiments, we show that the proposed dual objectives of INTapt not only lead to better performance on L2 English accents but result in a higher similarity between the accent feature of the promptconcatenated input and that of L1 English accents. In the first step, we train an Accent Module (AM) capable of isolating the accent feature from a given audio feature a of an input speech x. In the second step, we train a Prompt Generator (PG), which outputs a prompt p for a given audio feature a, using two objectives: (1) Minimize the mutual information between the accent feature z \u2032 and z, where the former is obtained using the prompt-concatenated input (p; a) and the latter is obtained from the original audio feature a, (2) Minimize CTC loss to improve the ASR performance of the input (p; a).\n\nAccent Module (AM)\nSince our method requires direct access to the isolated accent feature of the corresponding audio feature input, we propose an Accent Module (AM) capable of extracting the accent feature z from the input a. The module consists of an accent feature extractor f \u03b8 1 which is trained with an accent classification head f \u03b8 2 to isolate the accent feature and an accent intensity regression head f \u03b8 3 to capture the intensity of the accent into the obtained feature.\n\nAccent Classification Head\nThe role of the accent classification head f \u03b8 2 is to isolate the accent feature of a given speech 1 . Given the hidden state representation h of an audio feature input a, the feature extractor outputs the accent feature (i.e., z = f \u03b8 1 (h)) and the accent classification head f \u03b8 2 tries to assign it to the correct accent label y.\n\nAccent Intensity Regression Head\nThe intensity of an accent could vary among different people even though there are in the same L2 group, and it could also vary between utterances from the same speaker. Thus, an accent intensity regression head is introduced to incorporate the accent intensity into the obtained accent feature z. Based on the assumption that the intensity of the accent affects ASR performance, making the accent intensity regression head predict the CTC loss 2 , obtained by inputting the corresponding speech into the backbone speech model, will allow the extracted accent feature z to capture the intensity of the accent.\nGiven a batch B, the training of the Accent Module with the two aforementioned heads could be summarized as:\nmin \u03b8 1 ,\u03b8 2 1 |B| i\u2208B \u2212 log p(y i |f \u03b8 2 (f \u03b8 1 (h i )))+ \u03bb min \u03b8 1 ,\u03b8 3 1 |B| i\u2208B [ f \u03b8 3 (f \u03b8 1 (h i )) \u2212 CTC(x i )] 2\n(1)\n\nPrompt Generator (PG)\nBuilding on the success of prompts in NLP (Liu et al., 2021; Li and Liang, 2021) and CV (Dosovitskiy et al.), we introduce a prompt tuning method to improve the ASR performance for L2 English speech by efficiently utilizing a pre-trained model that already shows good performance for L1 English speech. In contrast to traditional NLP or CV applications, where a single, discrete prompt embedding is learned for each specific task or input domain, the intensity of an accent is continuous. To address this, we propose an inputdependent prompt embedding by training prompt generator P G \u03b8 4 that generates an input-specific prompt guided by Information-Theoretic Adversarial Learning. More specifically, given a hidden state h = [h 1 , h 2 , ..., h L ] with length L we produce a prompt of length L \u2032 ,\nEQUATION\nMutual Information Minimization Mutual Information meausures the co-dependence between two random variables X and Y . Belghazi et al. (2018) recently proposed a gradient descent based method for estimating this property, allowing the use of neural networks for the estimation of mutual information between high dimensional random variables. The estimation is done using a neural network parameterized by \u03d5 as below:\nEQUATION\nwhere maximizing I \u03d5 (X, Y ) provides a tight lower bound of the original mutual information I(X, Y ).\nWe use this to adversarially train the prompt generator P G \u03b8 4 to minimize the mutual information between the accent feature of the original L2 speech input and the prompt-concatenated input.\nCTC Loss Minimization We train the prompt generator P G \u03b8 4 to minimize the CTC loss obtained for the prompt-concatenated input (p; a). The two minimization objectives wrt. the prompt generator, along with the maximization objective wrt. the Mutual Information Neural Estimator, are done jointly in the second training step (Equation 4). We show in Section 3.2 and 4 that the aforementioned objectives not only improve the ASR performance of L2 speech but also effectively make it resemble the accent feature of the L1 speech.\nEQUATION\n3 Experiments\n\nExperimental setting\nDataset We use the L2-ARCTIC (Zhao et al., 2018) , which is a speech corpus of non-native (L2) English speakers -Mandarin (ZH), Hindi (HI), Vietnamese (VI), Korean (KO), Spanish (ES), and Arabic (AR). Each L2 group contains two male and two female speakers, and all the speakers read the same 1132 texts. Models For the backbone pre-trained speech models we try two different settings, HuBERT Large and HuBERT XLarge (Hsu et al., 2021) . We consider three different training situations: 1) Finetune denotes a standard finetuning method where we update the pre-trained model weights to minimize the CTC loss, 2) Prompt ctc is the case of training the prompt generator without the minimization of mutual information, and 3) INTapt trains the prompt generator with our proposed objective in equation 4. We include the training details in Appendix A.\n\nResults\nTable 1 shows the Word Error Rate (WER) across different L2 groups on the ASR task. We find that the performance improvement of the prompt tuning approaches (Prompt ctc and INTapt) are more significant compared to standard finetuning despite updating small number of parameters (2-4%). IN-Tapt shows the lowest WER on all L2 groups, obtaining 12.34% for HuBERT Large and 11.00% for HuBERT XLarge on the aggregated all speakers, outperforming the finetuned by 1.62%p and 2.86%p, respectively 3 . This conforms to the previous findings (Lester et al., 2021 ) that larger model size can benefit more from prompt tuning methods.\nIn Table 2 , we report the WER on LibriSpeech (Panayotov et al., 2015) benefits of prompt tuning methods in that it only slightly degrades the performance of the backbone model on tasks it already excels at while improving performance on others. \n\nConclusion\nWe introduced Information Theoretic Adversarial Prompt Tuning (INTapt) for improving non-native ASR performance. To achieve this, INTapt remodulates the attention of the pre-trained speech models by concatenating input-dependent prompt embeddings to the original input, without updating the model weights. Throughout the experiment, we show that INTapt is capable of outperforming standard finetuning of the pre-trained model on L2 speech, without degradation on L1 speech, by allowing the L2 input to resemble a L1 accent.\n", "hypothesis": "INTapt is trained simultaneously in the following two manners:\n(1) adversarial training to reduce accent feature dependence between the original input and the prompt-concatenated input and (2) training to maximize CTC loss for improving ASR performance to a prompt-concatenated input.  Experimental results show that INTapt improves the performance of L2 English and increases feature similarity between L2 and L1 accents.", "answer": false}
{"title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker", "content": "\nIntroduction\nInformation retrieval (IR) is the task of searching for documents relevant to a given query from a large corpus. As re-ranking the fetched documents from the retriever effectively enhances the performance and the latency, recent studies have suggested several kinds of re-rankers by fine-tuning pre-trained language models (PLM) (Nogueira and Cho, 2019; Nogueira et al., 2020) . Furthermore, Sachan et al. (2022) show that large-scale language models (LLMs) such as GPT-3 (Brown et al., 2020) can be exploited as a zero-shot reranker with the prompt describing the task. They also highlight the importance of an appropriate prompt to elicit the full performance of LLMs, rather than updating the parameters. They choose an optimal prompt among the handcrafted candidates by cross-validation. However, such a manual search for the discrete prompts is highly expensive and sub-optimal in transferability.\nTo resolve the issue, several methods are proposed for automatically optimizing the discrete prompt. They focus on text classification or maskfilling task while underestimating the open-ended generation (Shin et al., 2020; Gao et al., 2021; Prasad et al., 2022) . Recently, Deng et al. (2022) address the discrete prompt optimization applicable to generation tasks with reinforcement learning by designing the reward function, which measures the generated text belonging to a discrete label. Since there are tasks that are still not aligned, requiring a continuous score of output, we aim at a prompt optimization for one of such tasks: re-ranking.\nIn this paper, we propose Constrained Prompt generation, Co-Prompt, as left-to-right discrete prompt optimization without additional model training. By defining the metric of prompt optimum for re-ranking, we interpret the searching process of the optimal prompt as constrained generation with two modules: a zero-shot re-ranker as a discriminator and any decoder-only PLM as a generator. The discriminator calculates the likelihood (i.e., metric) that the prompt sequence is optimal for guiding an LLM to distinguish relevant documents among the large set for a given query. The generator samples the prompt tokens having a high prior from the previous prompt sequences for effectively restricting the prompt candidates for discriminator to evaluate. An overview of Co-Prompt is shown in Figure 1 .\nWe validate our method, Co-Prompt, against other optimization baselines on two LLMs, T0 (Sanh et al., 2022) and OPT (Zhang et al., 2022) , with two benchmark datasets, MS-MARCO (Nguyen et al., 2016) and Natural Question (Kwiatkowski et al., 2019) . Experimental results show that Co-Prompt consistently generates well-performing prompts regardless of LLMs and datasets over the baselines. The qualitative analyses also support the interpretability of the prompts generated by Co-Prompt, similar to human language patterns.\nOur contributions in this work are threefold:\n\u2022 We highlight the impact of optimal prompt on a zero-shot re-ranker by exploiting the optimization methods. \u2022 We propose Co-Prompt, a novel discrete prompt optimization via constrained generation for a zero-shot re-ranker. \u2022 We experimentally show that Co-Prompt consistently guides the re-ranker well against the baselines and its output is similar to human language patterns.\n\nRelated Work Document Ranking with Generative Model\nUsing the generative model is one of the dominant methods for ranking the retrieved documents by defining the relevance score as the query likelihood score (Nogueira dos Santos et al., 2020; Ju et al., 2021) . More recently, Sachan et al. (2022 Sachan et al. ( , 2023) ) showed that the LLM serves as either a zero-shot re-ranker or a training module of an unsupervised dense retriever. However, unlike ours, they require carefully designed manual prompts, which may have a limitation in transferability.\nPrompt Optimization As prompting is considered a key variable when exploiting LLMs for various NLP tasks, finding the optimal prompt has become important to get the best performance out of the LLMs (Kojima et al., 2022; Xie et al., 2022) .\nRecently, the prompt optimization work has focused on discrete prompt search (Shin et al., 2020; Gao et al., 2021; Deng et al., 2022) or soft prompt learning over a continuous space (Liu et al., 2021; Qin and Eisner, 2021; Lester et al., 2021) . While the existing optimization methods mainly consider text classification or mask-filling task, their applicability to re-ranking is yet underexplored. In this paper, we target at optimizing discrete prompts for zero-shot re-ranker to get higher relevance scores for more relevant pairs via constrained generation.\nConstrained Generation Constrained generation aims at deriving the text sequences that follow a certain constraint (Keskar et al., 2019) . Utilizing a discriminator for guiding the generation toward the constraint via the Bayes' rule is one of the widely used constraint generation methods (Dathathri et al., 2020; Krause et al., 2021; Chaffin et al., 2022) . Inspired by the effectiveness of the discriminator-based method, we adopt the zero-shot re-ranker as a discriminator when generating optimal discrete prompt sequences.\n\nPreliminaries\nAn LLM re-ranks the retrieved document d concerning the relevance score with a given query q as the query generation score:\nEQUATION\nwhere |q| denotes the token length of the query q and \u03c1 is a natural language prompt guiding an LLM to generate the query q. Since the prompt \u03c1 is the only controllable variable in Equation 1, searching for an optimal prompt is a simple yet effective way to enhance the performance of LLMs. Thus, in this work, we focus on a prompt optimization strategy.\n\nConstrained Prompt Generation\nWe define the optimal prompt \u03c1 * for the re-ranker which maximizes the query generation scores:\nEQUATION\nwhere D is the dataset for the retriever, consisting of pairs of a query and its relevant document. We solve the task of searching the optimal prompt \u03c1 * for the document-query pair dataset D with discriminator-based constrained generation. The generation is guided by the Bayes' rule: P (\u03c1t|D, \u03c11:t\u22121) \u221d PM D (Ds|\u03c11:t)PM G (\u03c1t|\u03c11:t\u22121), (3) \nP M D (Ds|\u03c1) return R end\nwhere M D is a zero-shot re-ranker serving as a discriminator, M G is a decoder-only PLM as a generator, and D s is a dataset sampled from D.\nDiscriminator The discriminator M D measures how effectively the prompt sequence \u03c1 1:t guides the zero-shot re-ranker to generate the query from the given document by computing the likelihood P M D (D s |\u03c1), defined as the expectation of relevance score between document-query pairs (q i , d i ) of the sampled dataset D s with the prompt \u03c1:\nEQUATION\nWe use this likelihood as the metric for prompt optimum. The other option of\nP M D is shown in Appendix B.1.\nGenerator The generator M G samples the pool of prompts to be evaluated by a discriminator since computing Equation 3 of all possible tokens in the vocabulary requires a prohibitively high computational cost. The decoder-only PLM is exploited to sample prompt tokens \u03c1 t having a high prior P M G (\u03c1 t |\u03c1 1:t\u22121 ) in a zero-shot manner.\nWe combine these modules to optimize the prompt by iteratively performing two steps: candidate generation and evaluation. We choose to use a beam search as a decoding strategy for left-toright prompt generation. The detailed steps of the decoding strategy are shown in Algorithm 1.\n\nExperimental Setups\nWe describe the experimental setups for validating the performance of the prompts. Our code is publicly available at github.com/zomss/Co-Prompt.\nDatasets We employ two information retrieval datasets: 1) MS-MARCO (Nguyen et al., 2016) fetched from Google search engines. We only use the document data of the dataset for evaluation. More information is shown in Appendix A.1.\n\nEvaluation Metrics\nWe evaluate the results by two metrics, ACC and nDCG. 1) ACC is the percentage of the relevant documents in the total retrieved ones. 2) nDCG, normalized discounted cumulative gain, reflects that the more relevant documents should record higher ranks.\nRetriever & Re-ranker We select two widely used sparse and dense retrievers as our retrievers, which are 1) BM25 (Robertson and Zaragoza, 2009) and 2) DPR (Karpukhin et al., 2020), respectively. For the zero-shot re-ranker, we use 1) T0 (Sanh et al., 2022) and 2) OPT (Zhang et al., 2022) . We describe more detailed information in Appendix A.3 and A.4.\n\nPrompt Baselines\nWe compare Co-Prompt against four baselines: 1) Null Prompt is an empty prompt without any token. 2) P-Tuning is a soft prompt optimization method that yields prompt embeddings from the prompt encoder (Liu et al., 2021) . 3) RL-Prompt is a discrete prompt optimization method by training policy network (Deng et al., 2022) . Note that we modify RL-Prompt and P-Tuning applicable to the re-ranking task. 4) Manual Prompt, suggested by Sachan et al. (2022) , is given as \"Please write a question based on this passage\", following the assumption that it is one of the best prompts that humans can find. Last, 5) Co-Prompt, our proposed method, is a discrete prompt optimization method in left-to-right zero-shot generation. The implementation details of baselines are shown in Appendix A.5. is the first question asked on Google for\" 31.9 \"Please post your question again when its not just about\" 30.6 \"Score! What are all 3 things, the first is\" 30.2 \"Score the top 5 things on this sub reddit for\" 29.3 \"This looks like the same as every \"what are the\" 30.5 \"This post should be titled as\" 31.2 \"What are some common questions asked on the internet about\" 30.3 \"How do i find the name on google, and\" 29.1 Implementation Details The discriminator M D is the same model as the zero-shot re-ranker. Since the generator M G should be a decoder-only model, in the case of T0, GPT2-Large (Radford et al., 2019) is utilized as the generator. OPT, a decoderonly model, is used as both the discriminator and the generator. We use the start token as \"Please\" for a direct comparison with the manual prompt and fix the beam width B as 10 and the maximum prompt length L as 10 in our experiment.\nEnvironment We conduct all experiments including prompt searching and document re-ranking on V100 32GB GPUs. We use BEIR (Thakur et al., 2021 ) framework 1 for re-ranked result evaluation and passage retrieval datasets. Also, the retrievers, BM25 and DPR, are from the same framework. We employ T0 and OPT with 3B and 2.7B parameters each for the discriminator and the re-ranker publicly open on the Huggingface model hub 2 (Wolf et al., 2020) .\n\nResult\nIn this section, we show the overall results of our method, Co-Prompt, with a detailed analysis. \n\nImpact of Start Tokens\nWe exploit other options of start token such as \"Score\" and \"This\" as shown in Table 2 . Regardless of the start tokens, Co-Prompt consistently generates prompts eliciting the performance of LLM efficiently. However, we observe that finding the optimal start token for the dataset is important to achieve better results.\n\nImpact of Generator\nAs shown in Table 3 , even if different generators are used, the generated prompts by different generators guide the zero-shot re-ranker efficiently. Still, the differences in performance are caused by a vocabulary mismatch between the two modules. We see that, although our method does not vary significantly in performance to the generator, a more suitable generator may be necessary for better results.\nRelevance Score We analyze the distributions of relevance scores between positive or negative document-query pairs. As the negative documents for a given query are retrieved from BM25, the negative ones are related to the query but unable to directly find the answer. As shown in Figure 2 , we point out that the distribution difference exists between pairs despite some overlap. Also, an LLM can distinguish which pair is positive, even without a prompt. However, we observe that the effect of discrete prompt optimization on the zero-shot reranker is in the direction of increasing the mean and variance of the relevance score. \n\nConclusion\nIn this paper, we propose Co-Prompt, left-to-right prompt optimization for zero-shot re-ranker via constrained generation. Co-Prompt effectively restricts prompt candidates and evaluates the optimum of these prompts without any parameter updates. We experimentally show that our method achieves consistently outperforming performance across all experiments. Also, the impact of prompt optimization including baselines on the zero-shot re-ranker highlights its importance. We also present an interesting outcome in that the optimal prompt is interpretable for human. For future work, we plan to expand our method to other open-ended generation tasks using LLMs.\n", "hypothesis": " While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet.  Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for reranking.  Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update.", "answer": true}
{"title": "On Search Strategies for Document-Level Neural Machine Translation", "content": "\nIntroduction\nNeural machine translation (NMT) (Bahdanau et al., 2014; Vaswani et al., 2017) is widely adopted and produces excellent translations for many domains and language pairs. However, when these automatic translations are evaluated on the document level, they reveal shortcomings when it comes to consistency in style, entity-translation or correct inference of the gender, among other things (L\u00e4ubli et al., 2018; M\u00fcller et al., 2018; Thai et al., 2022) . Document-level NMT aims to resolve these shortcomings by taking the context of a sentence into account during translation. There exist many works on the topic of document-level NMT, proposing various changes to the standard transformer (Vaswani et al., 2017) architecture and training criteria to improve context incorporation and consequently translation quality. However, while the modeling and training aspects are covered in great detail in these works, the exact decoding strategy is often not very clearly described and sometimes not mentioned at all.\nIn this work, we head out to answer the question, which decoding strategy is most beneficial for document-level NMT systems. We compare all commonly used strategies, as well as some additional ones, on three standard document-level translation benchmarks. We find that most of the analyzed decoding strategies perform similar to each other. Also, higher quality context information can lead to better translations in certain scenarios.\n\nRelated Work\nThe earliest approaches to document-level NMT simply concatenate consecutive sentences without any further changes to the architecture compared to the sentence-level systems (Tiedemann and Scherrer, 2017; Agrawal et al., 2018) . Later, some changes were made to the vanilla transformer architecture, like segment embeddings (Ma et al., 2020) or attention masking (Zhang et al., 2020; Petrick et al., 2022) and a move was made towards translating longer segments (Junczys-Dowmunt, 2019; Liu et al., 2020; Zheng et al., 2021; Bao et al., 2021; Sun et al., 2022) . Other works employ a separate encoder to include the additional context on the source side (Jean et al., 2017; Bawden et al., 2018; Zhang et al., 2018; Voita et al., 2018) or make use of the context in a post-editing fashion (Voita et al., 2019; Xiong et al., 2019) . Further approaches include the usage of a cache (Wang et al., 2017; Maruf and Haffari, 2018; Tu et al., 2018) or hierarchical attention networks (Miculicich et al., 2018; Maruf et al., 2019; Wong et al., 2020) . Recently, several works have concluded that the simple concatenation approach used with the vanilla transformer architecture performs as good -if not better -than more complicated approaches that modify the model structure (Sun et al., 2022; Majumde et al., 2022) . Since we also observed this in our internal comparisons, we decided to focus on this simple approach for our analysis in this work.\nSeveral works made the argument that the improvements seen in automatic metric scores for document-level NMT systems are from regularization effects rather than from utilizing the additional context information (Kim et al., 2019; Li et al., 2020; Nguyen et al., 2021) . In order to better asses the improvements gained by documentlevel NMT, several targeted test suites have been released (M\u00fcller et al., 2018; Bawden et al., 2018; Voita et al., 2019; Jwalapuram et al., 2019) . However, all of these are based on just scoring contrastive examples without actually translating anything. Recently, Jiang et al. (2022) and Currey et al. (2022) have released frameworks that allow to score MT systems on their ability to generate contextually correct translations. 1\n\nSearch Strategies\nTraining a document-level NMT system that takes the last k sentences as context is straightforward using the standard concatenation strategy (Tiedemann and Scherrer, 2017) . Given some document level training data (F n , E n ), n = 1, ..., N , where (F n , E n ) denotes the n-th source-target sentence pair, during training we optimize the parameters \u0398 of the model towards\n\u0398 = argmax \u0398 n log p \u0398 (E n n\u2212k |F n n\u2212k ) .\nHere, E n n\u2212k denotes the concatenation of the sentences E n\u2212k , ..., E n .\nDuring search, given a document F M 1 , we want to find the best translation \u00caM 1 according to the model. Of course, exact search can not be performed and different works have used different methods to generate a translation: full segment (Liu et al., 2020; Bao et al., 2021; Sun et al., 2022) : we split the document into non-overlapping parts\nF k 1 , F 2k k+1 , ..., F M M \u2212k\nand translate each part separately using\nEQUATION\nwhich is approximated using standard beam search on the token level.\nlast sentence (Bawden et al., 2018; Agrawal et al., 2018; Zhang et al., 2020; Petrick et al., 2022; Majumde et al., 2022) : we split the document into overlapping parts ..., F i i\u2212k , F i+1 i\u2212k+1 , ... and translate each part separately using Equation 1. From each translated part we choose only the last sentence to get one translation for every sentence in the document.\nfirst sentence (Zhang et al., 2020) : similar to last sentence, but from each translated part we choose only the first sentence to get one translation for every sentence in the document.\n2-pass decoding (Maruf and Haffari, 2018; Maruf et al., 2019; Voita et al., 2019; Xiong et al., 2019) : we first generate a translation \u1ebcM 1 of the document using a sentence-level NMT system. Then, the final hypothesis \u00cai for each sentence F i is created using\n\u00cai = argmax E i p(E i |F i i\u2212k , \u1ebci\u22121 i\u2212k ) .\ndoc-trans (Miculicich et al., 2018; Voita et al., 2019; Garcia et al., 2019; Fernandes et al., 2021) : we generate the translation sentence by sentence, meaning\n\u00ca1 = argmax E 1 {p(E 1 |F 1 )} , \u00ca2 = argmax E 2 p(E 2 |F 2 1 , \u00ca1 ) , ...\n\ndoc-trans (beam)\n: similar to doc-trans, but instead of keeping just the best context \u00cai\u22121 1 , we keep the top-h candidates and prune them after each step i, analogous to beam search on the token level. h = 12 for all our experiments, the same as our token-level beam-size.\ncheating : this is just used as a tool for analysis.\nThe translation of each sentence F i is created using the true target reference EM 1 as context \u00cai = argmax\nE i p(E i |F i i\u2212k , Ei\u22121 i\u2212k ) .\nno context : this is just used as a tool for analysis. The translation of each sentence F i is created using no context information at all \u00cai = argmax\nE i {p(E i |F i )} . Cost sentence-level O(N L) document-level full segment O(N L) last sentence O(N Lk) first sentence O(N Lk) 2-pass decoding O(2N L) doc trans O(N L) doc trans (beam) O(N Lh)\nTable 1 : Computational cost of decoding (=number of forward passes through the decoder) for each of the search strategies described above. h denotes the sentence-level beam size.\nThe different search strategies also have a different computational cost associated with them. The biggest factor regarding the decoding cost is the number of forward passes through the model, specifically the decoder, that we have to do. We list the computational costs for the different decoding approaches in Table 1 under the assumption that the document consists of N sentences with average sentence length L and the model uses k \u2212 1 sentences as context. Please note that the decoding time might follow a different dependence than the cost in the above table, since it heavily depends on the available hardware. For example, doc trans and doc trans (beam) might have the same decoding time, if we have enough computational resources available, since the additional computations in doc trans (beam) can all be done in parallel.\n\nExperiments\nWe perform experiments on three document-level translation benchmarks, called NEWS (En\u2192De), TED (En\u2192It) and OS (En\u2192De). For the details regarding data conditions and preparation, as well as model training, we refer to Appendix A. For the context-aware systems, we concatenate 3 adjacent sentences (i.e. k = 3) using a special token <sep>. For the two En\u2192De tasks, we also evaluate the systems on the ContraPro test set (M\u00fcller et al., 2018) . Instead of scoring and ranking the contrastive examples in ContraPro, as the authors have originally envisioned, we translate the source side to calculate BLEU and TER as well as to score the pronoun translations according to Section 4.1. We can not evaluate the full segment search strategy on Con-traPro, because the sentences are not adjacent since they come from different documents. \n\nEvaluating Pronoun Translation\nAs further analysis, we measure how well ambiguous pronouns are handled when translating from English to German. Regarding gender, the English third-person pronoun 'it' (and its other forms), can be translated to the German words 'er', 'sie' or 'es', depending on which noun it refers to. On the other hand, ambiguities in the formality come from second-person pronouns. or informal pronoun appears in the reference. 5\n\nPerplexities\nFirst, we compare the perplexities of the hypotheses from the different search strategies, which are listed in Table 2 . The first thing to note is, that the reference has a much higher perplexity than all hypotheses, which is commonly seen for NMT systems. All document-level search strategies result in different hypotheses, which however have a similar perplexity score. Surprisingly, the cheating setting generates the worst translation perplexity-wise, even worse than using no context. This might be related to the observation, that the reference has a worse perplexity than any hypothesis, which is rather a modelling error than a search error.\n\nAutomatic Metrics\nNext, we evaluate the hypotheses based on the common automatic metrics BLEU and TER. The results are shown in Table 3 . The hypotheses created with no context seem to have the same quality as the sentence-level baseline. Surprisingly, the true reference as context does not improve performance on the NEWS and TED test sets. This indicates that the improvements seen on these test sets for the document-level system might not be related to better context incorporation. On the contrary, the OS system creates the best hypothesis with the true reference as context. All the actual decoding strategies give similar performance in terms of BLEU and TER with 2-pass decoding being a little bit behind.\nA special case is the first sentence strategy, which performs quite well on the standard test sets but poorly on ContraPro. This is, because ContraPro is designed in a way that the left side context is more important for translation than the right side. Finally, we analyze the quality of the pronoun translation as discussed in Section 4.1. In principle, we could calculate the F1 score for both, gender and formality, on all En\u2192De test sets. However, we discard the cases where one or more classes have less than 100 examples. This leaves us with the three test sets depicted in Table 4 . As a sanity check, we also report the ContraPro accuracies calculated from scoring the contrastive references as described in (M\u00fcller et al., 2018) . They are 48.2/45.8 for sentence-level and 68.2/82.2 for document-level for NEWS/OS respectively. That means, with just scoring, we overestimate the capabilities of the system, but the trend is still consistent. 6 Using the true reference leads to the best results in all cases. no context and first sentence leaves us with sentencelevel performance on the gender tasks, while all other decoding strategies perform similarly. For the formality, none of the methods can significantly outperform the sentence level system, although the cheating experiment shows that the system could do better if a better context information is provided. This might be, because segments of 3 sentences are too short to reliably detect if a setting is formal or informal, without access to the true reference.\n\nConclusion\nIn this work, we analyze decoding strategies for document-level NMT systems. Using the most popular document-level translation approach, we compare different search strategies found in the literature against methods developed by us. We find that most of the commonly used decoding strategies result in similar performance, both in terms of common automatic metrics, as well as on specific pronoun evaluation tasks. Therefore, we conclude that it is important to include the context information during decoding, but the exact way in which to do this is not as important. Also, we find that the document-level systems could actually profit from higher quality context information, in situations where this context is most relevant for translation. ture and training criterion. Other approaches exist, which might exhibit a different behavior in decoding. Two out of the three document-level translation tasks we use in this work are low resource with less than 500k sentence-pairs as training data. We chose these tasks due to computational limitations and to be better comparable to other works, but higher resource scenarios are more realistic for actual applications. We limit the analysis of pronoun translation to the English-German language pair. Also, there are other aspects of documentlevel NMT, like consistent translation of entities, which we did not consider in our analysis.\n", "hypothesis": "On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. Interestingly, we find that the commonly used decoding strategies actually lead to worse translation quality compared to a random decoding strategy. This suggests that the current approaches in document-level NMT may not be as effective as previously believed. By introducing our new decoding scheme, we are able to achieve significantly better translation results and improve the overall performance of document-level NMT systems.", "answer": false}
{"title": "Revisiting Sample Size Determination in Natural Language Understanding", "content": "\nIntroduction\nLabeled data play an important role in creating performant machine learning models, which makes data annotation a fundamental process for any natural language application pipeline (Lewis and Catlett, 1994) . Recent work has sought to reduce the annotation costs through the use of active learning (Ducoffe and Precioso, 2018; Margatina et al., 2021) and data sampling (Sener and Savarese, 2018; Coleman et al., 2019; Killamsetty et al., 2021a,b) . Indeed, these approaches are shown to be effective in identifying or constructing data subsets needed to achieve a competitive model performance. For instance, the active learning paradigm adds new data iteratively to the existing set before model retraining (Agarwal et al., 2020; Margatina et al., 2021) , improving upon the traditional human annotation pipeline that obtains the entire labeled set all at once. Nevertheless, the data labeling process typically annotates as much data as the annotation budget permits, or by clearly defined stopping criteria to terminate the labeling process. Unfortunately, this is usually challenging as annotators do not have the knowledge of the effect of added labels to model performance nor how much more data is needed to arrive at the desired model generalizability (Killamsetty et al., 2020) . The stopping condition is in fact tied to the quality of data samples w.r.t. model parameters (Hu et al., 2021) , which influences the effective sample size 2 , and it is then beneficial to obtain an approximation of the expected performance (Vlachos, 2008; Olsson and Tomanek, 2009a; Zhu et al., 2010; Ishibashi and Hino, 2020) . Therefore, knowing the approximate amount of training data needed for this particular performance would serve as an useful knowledge not only for deciding when to stop adding labeled data, but also as an early indication for the data quality. For instance, by having early label quality signals, we can decide between two different types of annotation, or even between two pools of annotators with different expertise.\nTo this end, we explored the relationship between data sample size and model performance in the context of language understanding via learning curve modeling, which defines model performance as a function of dataset sizes. By modeling this relationship in low resource settings, we obtain useful early signals with approximated accuracies for any given the labeled set, which can provide an idea for the sample size and data quality (Olsson and Tomanek, 2009b; Figueroa et al., 2012) . Previous studies have shown that nonlinear weighted curve fitting methods such as inverse power laws or exponential functions can provide decent approximations of the empirical predictive performances (Frey and Fisher, 1999; Figueroa et al., 2012) . We thus put forward an ensemble of these functions which we showed to display a consistently highly correlated behavior across four language understanding benchmarks and with as little as 10% of the entire training set. This work makes the following contributions:\n1. We revisit the task of sample size determination in four natural language understanding benchmarks and empirically explore the correlation strengths of several successful techniques.\n2. Based on our findings, we propose an ENSEM-BLE function and demonstrated across several benchmarks and low resource settings that the ensemble function is consistently providing a high correlation with the empirical learning curve plots.\n\nBackground\nOur method is a sample size determination technique that helps to design annotation projects by determining the necessary sample size. Previous methods have focused on identifying the sample size required to reach a specific target performance, such as a high correlation coefficient (Beal, 1989; Stalbovskaya et al., 2007; Beal, 1989) , which often involves predicting the sample size necessary for a classifier to attain a specific accuracy level (Fukunaga and Hayes, 1989) . There are two main approaches for predicting the sample size needed to achieve a particular classifier performance: (1) Dobbin et al. ( 2008) present a model-based method for predicting the number of samples required for classifying microarray data. (2) A more general approach involves fitting a classifier's learning curve to inverse power law models (Figueroa et al., 2012) .\nExamples of this approach include algorithms proposed by Mukherjee et al. (2003) ; Boonyanunta and Zeephongsekul (2004) ; Last (2007) .\n3 The Approach Learning Curve Modeling. A learning curve is a graphical representation of how a classifier's performance changes as the size of the training set increases. The curve typically has three sections: an initial section where performance improves rapidly with increasing training set size, a middle section where the rate of improvement slows down, and a final section where the classifier reaches its maximum performance and further increases in training set size do not lead to significant improvements. This relationship can be quantified using a set of data points, each of which represents the expected performance of the classifier E acc on a particular training set size D k . These data points can be plotted to create the learning curve, which can help to understand the behavior of the classifier and inform decision-making about how much training data is needed to achieve a desired performance level.\nTask Description. Given a downstream classification task with N total data points, a learning curve model F predicts the expected performance E acc when a classifier trained on the an observed range of training set size (D k ; k >= N ). The empirical learning curve is assessed by the parametric models for the learning algorithm performance extrapolation. In our settings, we set k << N total to simulate practical settings, where few data points consisting of (E acc , D K ) are to be obtained.\n\nTypes of Extrapolations.\nHere, we study different forms of learning curve models with few learnable parameters that have been proven as simple yet effective. The simplest type of learning curve model exponential function (EXP) only introduces two parameters a and b to fit the exponent behavior of learning curve (Frey and Fisher, 1999) . The second form, Inverse Power Law function (INVERSE), fits the inverse power law (Figueroa et al., 2012) and has three parameters. The third form uses a function from the power law family -Power4 function (POW4) (Kolachina et al., 2012) with four parameters. Lastly, we propose to combine all functions into one (ENSEMBLE) so that it has all their characteristics in order to make it more robust across benchmarks. Table 1 shows the formulae of our investigated extrapolating functions. Configs. To investigate how changes in data size affect the predictiveness of the learning curves, under the assumption that the model structure and settings remain unchanged, we perform all experiments using a transformer model (Vaswani et al., 2017) and average the results over 3 initialization runs. The embedding and hidden layer dimensions are 1000 and 1820; and we use a 6-layer encoder with 4 multi-heads, and the dropout is 0.2. To find the parameters of learning curve models, we consider unweighted and for the gradient descent and non-linear least squares optimizers. The Adam algorithm (Kingma and Ba, 2014) was used as the optimizer with learning rate of 1e-5 and ReLU was used as the activation function. The crossentropy objective was used for all classification benchmarks, and we select the models using loss values. Finally, we chose a batch size of 8 with 200 number of epochs.\n\nEXTRAPOLATING FUNCTIONS FORMULA\nEXP (A) a \u2022 N b INVERSE (B) (1 \u2212 a) \u2212 b \u2022 N c POW4 (C) a \u2212 (b \u2022 N + c) \u2212d ENSEMBLE (A+B+C) \u2212\nEvaluation. We use the aforementioned functions: EXP, INVERSE, POW4 and ENSEMBLE for fitting the empirical learning curve. For each dataset, we select training set sizes ranging from 1% to 10% data sizes at an interval of 1%. The learning curve testsets were created with the data splits in the range [55, 100] at 5% interval by training the classifier, and obtaining the testset 4 performance for each corresponding data split. Therefore, we collect the accuracies against different sample sizes and report the mean absolute error (MAE) as the evaluation metric for learning curve modeling.\n\nResults and Analysis\nWe present results of ensemble method for learning curve modeling on the NLU benchmarks.\n\nMain Results\nFigure 1 demonstrates that by using only 10% of the data for learning curve modeling, ENSEMBLE is able to effectively predict model performance within a 0.9% margin of the actual model performance. Moreover, we observe the same trend across all four benchmarks consisting of different training set sizes (i.e. ranging from 25K to 250K) and varying number of classification classes (i.e. ranging from 2 to 14), see the appendix A for remaining figures. Our result shows that the proposed approach is not confined by the classification types and sample sizes.\nTable 2 shows the saturated points of the learning curve when the performance improvement is less than a threshold \u03b1 = 0.2 -we found that the predicted performance with only 19% data is within 2.44 accuracy points from the trained model performance for IMDB. Another key observation is that the size (%) needed to predict a low L1 distance increases as the number of classification classes goes up, which indicates that task difficulty does influence the ease of extrapolation. An example is that AG NEWS requires up to 51% to predict a low L1 distance. Next, we perform further ablation studies to investigate the effect of sample size, types of non-linear functions used, or the effect of data weighting. (%) for the percentages of the data size for the learning curve modeling. SIZE (#N) is the number of the corresponding data size, L1 is the L1 distance between the accuracy of models using all the training data and the estimated accuracy based on the saturated point. 100% specifies all training samples for learning curve.\n\nAblation Study\nEffect of sample size. In Figure 1 , we study the correlation between sample sizes and the absolute mean error between the learning curve model and empirical model performance trend. Surprisingly, we discovered by having more samples does not necessarily help with modeling a better learning curve 5 , and that with only 10% data to build the (D k , E acc ) data points is sufficient to obtain rather small errors across all four benchmarks.\nTypes of learning curve functions. We are also interested in seeing how each of the non-linear learning curve function fare against each other in simpler settings. To this end, we used up to 10% data to model the learning curves and obtained their respective mean absolute error values. In Figure 1 , we present this comparison where we showed that on IMDB and SST2, the ENSEMBLE function consistently fit best against the empirical data. We observed a similar trend across other benchmark DBPEDIA with the exception of AG NEWS. We placed the plot for AG NEWS in appendix A.3.\nInfluence of data weighting. Previous work (Paul et al., 2021; Guo et al., 2022) has found that not all data points are equally important in terms of curve fitting. In fact, data points at a later phase corresponding to more samples are to be given more weight compared to earlier points. We thus investigate this phenomenon in the context of our benchmark, and we observed this to be true anecdotally. The detailed result can be found in Appendix A.2. The reason for this is that the more data samples there are, the more closely they resemble the entire training set, and this makes 5 We showed this result in the Appendix A.5.\ntheir signals a better estimation of a point on the actual learning curve. Another perspective is that the more data samples are used, the less the effect of random sampling on the performance, which affects model performance in extremely low resource scenarios. 3 : Better curve fitting when weighting data points at latter phase. We examine the effectiveness of weighting data size on the exponential (EXP), inverse power law (INV), power4 (POW4) function using non-linear least squares method. The learning curves fit on 5%, 10%, 25% and 50% data sizes of IMDB and is evaluated on testing sample with mean absolute error (MAE).\n\nConclusions and Future Works\nIn this work, we investigated techniques for estimating the amount of training data needed to achieve a target performance in four natural language understanding benchmarks. We demonstrated that our approach allows for accurate prediction of model performance using only a small portion of the data, which can be useful in scenarios with limited resources. Nevertheless, we also recognize the limitation in our current study. For instance, we did not explore sampling techniques other than random sampling; while recent works (Yuan et al., 2020; Paul et al., 2021; Guo et al., 2022) have shown promising directions in data sampling that outperforms random sampling. Another interesting direction is to explore the model architecture's influence on generalizability, and thus the learning curve, which we left for future works.\n", "hypothesis": " We therefore explored various techniques for estimating the training sample size necessary to achieve a targeted performance value.  We derived a simple yet effective approach to predict the maximum achievable model performance based on small amount of training sampleswhich serves as an early indicator during data annotation for data quality and sample size determination.", "answer": true}
{"title": "An Exploratory Study on Model Compression for Text-to-SQL", "content": "\nIntroduction\nText-to-SQL is an important task that has been gaining the attention of researchers over the years. Formally, given a query q and a relational database D, the goal of Text-to-SQL is to build a model f such that s = f (q, D | \u03b8) where \u03b8 is a vector of model parameters and s is a predicted SQL statement which we can use to retrieve the answer to q from D.\nText-to-SQL has many potential applications that can improve our standard of living. For example, medical chatbots can convert user queries into SQL statements and then use them to retrieve relevant information from medical knowledge bases. Industry can leverage Text-to-SQL tools to help employees shorten the time needed to write complex SQL queries, thereby improving overall work productivity.\nThe recent emergence of complex Text-to-SQL datasets containing complicated SQL and crosstable setup has driven researchers to develop huge models that encode various complex relationships between table schema and query with large pretrained language models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) . These models are usually sequence-to-sequence models that generate SQL statements sequentially or sketch-based models that use classifiers to fill in the slots of SQL templates.\nHowever, despite achieving state-of-the-art performances on benchmark datasets, such models are usually both memory and computationally expensive, making it technically challenging to deploy them in memory-constrained real-world applications that require low inference latency. Therefore, to deploy state-of-the-art Text-to-SQL models in real-world production environments, we must drastically improve the inference time and reduce the number of parameters in these models.\nWe turn to the field of model compression (Cheng et al., 2017) for solutions that can speed up inference without significantly hurting model performance. Formally, the goal of model compression is to reduce f to a smaller model f \u2032 such that s \u2032 = f \u2032 (q, D | \u03b8 \u2032 ). Ideally, we want s \u2032 to be the same as s and dim(\u03b8 \u2032 ) to be much smaller than dim(\u03b8).\nIn this paper, we thoroughly examine the feasibility of using model compression techniques to build faster and more accurate Text-to-SQL models that we can successfully deploy in the real world. For this, we carefully apply a few model compression methods to representative sequence-to-sequence or sketch-based Text-to-SQL models on three datasets: WikiSQL, Spider, and TableQA. The main findings of this paper are: (i) sketch-based models generally respond well to model compression techniques, while sequence-to-sequence models show mixed results, (ii) we observe better speed improvements in Sketch-based models as their slot-filling components are much faster than the decoding components of sequence-to-sequence models. (iii) model compression techniques work poorly on state-of-the-art Text-to-SQL models built on pre-trained encoder-decoder language models such as T5.\nWe hope our findings can empower practitioners to make more informed decisions when selecting Text-to-SQL models and compressing them appropriately for real-world deployments. . Contrarily, Spider contains large samples of complex SQL instances that connect multiple tables with primary and foreign keys with more advanced clauses such as nested queries, JOIN ON, and ORDER/GROUP BY.\n\nBaseline Models\nRecent deep neural Text-to-SQL models can be broadly classified under two categories: sequenceto-sequence models and sketch-based (also known as slot-filling) models.\n\nSequence-to-sequence models\nSequence-to-sequence models are generally made up of an encoder component that converts user query inputs together with database information into a hidden vector and a decoder component that generates SQL statements based on the output hidden vectors from the encoder. BRIDGE (Lin et al., 2020) encodes input questions and table schema with BERT and LSTM and generates SQL predictions with a pointer-generator decoder (See et al., 2017) supported by a schemaconsistency driven search space pruning strategy. RAT-SQL (Wang et al., 2020a ) also encodes input instances with BERT but generates SQL as an abstract syntax tree (AST) with a tree-structured decoder (Yin and Neubig, 2017) . It also incorporates a relation-aware self-attention mechanism that further improves schema-linking, schema-encoding, and representation of the encoder. PICARD (Scholak et al., 2021) is a state-of-theart algorithm that directly fine-tunes a pre-trained encoder-decoder language model T5 (Raffel et al., 2020) on Text-to-SQL data, and then constrain the decoder to output valid SQL by integrating an incremental parsing strategy to the beam search process.\n\nSketch-based model\nSketch-based methods also encode user inputs into vectors but only need to fill in slots in SQL sketches rather than generating full SQL statements. Each SQL sketch is a template SQL statement with placeholder slots and the goal of sketch-based models is to predict the best item to go into each slot. NL2SQL-RULE (Guo and Gao, 2019 ) is a standard sketch-based model which uses BERT and LSTM to encode input query and database information and predict outputs in slots of SQL sketches.\n\nCompression Techniques\nWe follow Sun et al. (2021) and experiment with the following model compression techniques in this study: Layer Pruning (Sajjad et al., 2022 ) is a simple yet effective strategy that discards a certain number of layers from transformer-based language models before fine-tuning the pruned models on downstream tasks. We apply the top-layer pruning strategy which deletes the top N encoder or decoder layers before the start of any training. Knowledge Distillation (Hinton et al., 2015) is a method that compresses deep neural network models by distilling useful knowledge from a larger model (teacher) to a smaller model (student). We follow Jiao et al. (2020) and distill smaller language models from larger ones such as BERT-large, before fine-tuning Text-to-SQL models on those distilled models. For WikiSQL and Spider, we experiment with the distilled English language models from MiniLM 1 (Wang et al., 2020b) , while for TableQA, we use the Chinese TinyBERT models 2 . Token Pruning For PICARD model, We also apply token pruning (Goyal et al., 2020; Kim et al., 2022) , which is a different pruning strategy that gradually removes redundant token encodings from the outputs of each encoder layer before feeding the reduced number of tokens to the next encoder layer. We follow Goyal et al. (2020) and implement an attention scoring mechanisms which weights the significance of each token by the sum of attention weights it gets from other tokens. The tokens with the lowest significance scores (based on predetermined thresholds) for each encoder layer are dropped.\n\nEvaluation Metrics\nWe evaluate our experiment results using Exact set match (ESM) (Yu et al., 2018) . ESM decomposes every pair of predicted and gold SQL queries into sets clauses and then computes the percentage of exact set matches over all pairs (Zhong et al., 2020) .\n\nExperiment Setup\nIn most cases, we follow the recommended configurations in corresponding papers. We may adjust the batch sizes and learning rates slightly to fit the experiments on our hardware. We train our models on servers with either NVIDIA GV100 GPU (32GB) or RTX A6000 (45GB) but calculate inference speeds by running models on only CPUs with batch size set to one, which better mimics the situations in the real world. For all datasets, we use their dev sets as the test sets and create new train-dev sets in the ratio of 4 to 1 from the original train set. We early stop our models based on the ESM scores on dev sets and report average test set ESM scores over 5 different runs. Other than PI-CARD, we use BERT-large for all English datasets and RoBERTa-Zh (Cui et al., 2020) for TableQA.\n\nSimple datasets\nWikiSQL As shown in Figure 1 WikiSQL. For example, we can remove 50% of the encoder layers from BRIDGE, while only taking a penalty of only 0.82% drop in Exact Set match (ESM). When only keeping the bottom 6 encoder layers, NL2SQL-RULE can still perform at 0.834 ESM, a 3.65% drop from the original unpruned model. For knowledge distillation, we fine-tuned BRIDGE on two versions of MiniLM (Wang et al., 2020b) : L6xH768 and L6xH384. Results show that BRIDGE trained on the MiniLM language models performs slightly worse than the layer pruning method with similar number of layers. However, this is acceptable given the hidden sizes of the MiniLM models are 384 and 768, which are smaller than the hidden size of 1024 for BERT-large. TableQA We notice several differences in results between WikiSQL and TableQA. First, the performances of RATSQL on TableQA are significantly lower than those of NL2SQL-RULE. For example, unpruned NL2SQL-RULE achieves an ESM of 0.8 but unpruned RATSQL only achieves 0.69 despite our best efforts. Second, we observe more significant drops in performances when applying layer pruning and knowledge distillation to RATSQL than NL2SQL-RULE. For example, we observe only a 3.63% drop in ESM dropping the first 16 encoder layers of NL2SQL-RULE but notice an 18.8% drop in the performance of RATSQL with the same configurations. Last but not least, models trained on distilled language models perform slightly worse than the layer pruned models due to their smaller hidden sizes except for NL2SQL-RULE on TinyBERT with 6 layers and 768, which achieves an ESM of 0.80, even higher than that of the unpruned NL2SQL-RULE.\nRecommendation: We recommend using slotfilling models when building applications that only deal with simple queries. These models not only perform comparably or even better than sequenceto-sequence models, but also respond better to recent model compression techniques. Spider As PICARD was trained on a 3 billion parameters pre-trained language model with an encoder and a decoder of similar size, we show three sets of results by applying layer pruning on 1) the encoder, 2) the decoder, and 3) both the encoder and decoder. As seen in Figure 3 , the layer pruning strategy does not work as well on PICARD. At around six layers, PICARD loses around 49.9% and 40.3% of its original performance for encoder-only and decoder-only pruning settings respectively. For the encoder+decoder pruning strategy, we observe similar levels of performance when discarding the same number of transformer layers as the other two configurations. For example, dropping 3 layers each from the encoder and decoder gets us 0.641 ESM, compared to 0.624 when dropping 6 decoder layers and 0.648 when dropping 6 encoder layers. On the other hand, RATSQL demonstrates better compression results on Spider, maintaining 92.6% of original performance while keeping on six encoder layers, contrary to the results on TableQA.\n\nComplex dataset\nToken pruning We follow the implementation of Goyal et al. (2020) and apply token pruning to PI-CARD. We plot the ESM performance of a tokenpruned model against the number of retained tokens in Figure 4 . As seen in the plots, although we can remove an average of 286 tokens from the top six encoder layers, we are only able to discard an average of 41 tokens from the bottom six layers. For example, we see a sharp drop in ESM performance by just pruning around 40 tokens from the 3rd encoder layer. Similarly, we also observe steady drop in ESM performance when pruning more than 100 tokens from encoder layers 15 and 18. Our final model achieves an ESM of 0.527 (26.3% drop in performance) while only seeing a 5.2% improvement in inference speed when applying token pruning to the encoder of T5. As we cannot significantly prune the number of tokens in each encoder layer without severely hurting model performance, we conclude token pruning is also not effective on the PICARD model. Recommendation: Our results suggest that both layer and token pruning are not effective on PI-CARD and we would get better compression performances on sequence-to-sequence models like RATSQL, which has a much bigger encoder than decoder in terms of model size.\n\nDiscussion\nThe main difference between recent sequence-tosequence and sketch-based models is related to how we generate the SQL statements. Compared to the lightweight slot-filling classifiers in sketchbased models, recent sequence-to-sequence model decoders rely heavily on grammar-guided decoding processes which requires navigating through a huge search space and requires an even longer inference time than the encoders. For example, 76.62% and 87.14% of the inference time are spent in the decoding step for BRIDGE and RATSQL, while most of the inference time in NL2SQL-RULE is spent on the encoder. Considering the speed, compression effectiveness, and performance, sketch-based models would be better choices if we get similar performances on benchmark datasets.\n\nConclusion\nThis paper investigates whether we can use model compression to improve the inference efficiency of recent Text-to-SQL models that rely heavily on large pre-trained language models. Our results show that on simple Text-to-SQL datasets, we can deploy simple strategies such as layer pruning to obtain a 5-6x speedup without significantly hurting model performances. We also observe that sketchbased models generally respond better to model compression than sequence-to-sequence models. However, we are not able to effectively compress PICARD on the spider dataset and we would tackle this problem as a future work.\n", "hypothesis": "Our results reveal that sequence-to-sequence Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sketch-based models, making them ideal for real-world deployments, especially in use cases with complex SQL statements.", "answer": false}
{"title": "Environmental Claim Detection", "content": "\nIntroduction\nIn the face of climate change, we witness a transition towards a more sustainable and green economy. This change is driven by changes in regulation, public opinion, and investor attitudes. For example, global assets managed under a sustainability label are on track to exceed $53 trillion by 2025, more than a third of total assets under management. However, unfortunately, the boom has been accompanied by rampant greenwashing, with companies boasting about their environmental credentials. 1 Because of this surge in environmental claims and to protect consumers, initiatives on substantiating green claims are developed. 2 Due to an ever-growing amount of text, there is a need for automated methods to detect environmental claims. Detecting such claims at scale can assist policy-makers, regulators, journalists, activists, the research community, and an informed public in analyzing and scrutinizing environmental claims made by companies and facilitating the transition to a green economy.\nEnvironmental claim: A total population of 6148 is getting the benefit of safe potable drinking water due to this initiative.\nEnvironmental claim: Hydro has also started working on several initiatives to reduce direct CO2 emission in primary aluminium production. Negative example: Generally, first of all, our Transmission department is very busy, both gas and electric transmission, I should say, meeting the needs of our on-network customers. Negative example: Teams are thus focused on a shared objective in terms of growth and value creation. Thus, we introduce the task of environmental claim detection. Environmental claim detection is a sentence-level classification task with the goal of predicting whether a sentence contains an environmental claim or not. Often, environmental claims are made in a clear and concise matter on a sentence level, with the intention to convey to a consumer or stakeholder that a company or product is environmentally friendly.\nTo facilitate future research on environmental claim detection, we release an expert-annotated dataset containing real-world environmental claims and models which can be used by practitioners. For constructing the dataset, we were inspired by the European Commission (EC), which defines such claims as follows: Environmental claims refer to the practice of suggesting or otherwise creating the impression (in the context of a commercial communication, marketing or advertising) that a product or a service is environmentally friendly (i.e., it has a positive impact on the environment) or is less damaging to the environment than competing goods or services. 3 While such claims can be truthful and made in good faith, boasting about environmental credentials can also be monetized (de Freitas Netto et al., 2020) . For example, consumers are willing to spend more money on environmentally friendly products (Nielsen Media Research, 2015) . The Commission states if environmental claims are too vague, unclear, or misleading, we are confronted with an instance of \"greenwashing\" (this definition is given in the same Commission Staff Working Document).\nWe situate environmental claim detection at the intersection of claim detection (e.g., Arslan et al., 2020) and pledge detection (Subramanian et al., 2019; Fornaciari et al., 2021) . An environmental claim is typically made to increase the environmental reputation of a firm or a product. We show that models trained on the current claim and pledge detection datasets perform poorly at detecting environmental claims, hence the need for this new dataset. We make our dataset, code and models publicly available. 4 Lastly, we envision computerassisted detection of greenwashing in future work, i.e., the automatic determination if an environmental claim is false, too vague, non-verifiable, or misleading. To make progress on automated greenwashing detection, it is mandatory to first detect environmental claims at scale.\n\nRelated Work\nThis work is part of an ongoing effort at the intersection of environmental and climate changerelated topics and natural language processing (Stede and Patz, 2021) . Resulting datasets and methods can help regulators, policy-makers, journalists, the research community, activists, and an informed public investigate such topics at scale with the help of computer assistance. Methods include ClimateBERT (Webersinke et al., 2021) , and ClimateGPT (Vaghefi et al., 2022) , two language models pre-trained on climate-related text. NLP tasks and datasets include climate change topic detection (Varini et al., 2020) and detecting media stance on global warming (Luo et al., 2020) . Duong et al. (2022) collect climate change opinions at scale from social platforms, Al-Rawi et al. (2021) analyze fake news Tweets around climate change. In a similar direction, Coan et al. (2021) analyze contrarian claims about climate change and (Piskorski et al., 2022) Further, there exists work about claim verification of climate change related claims (Diggelmann et al., 2020) , detecting media stance on global warming (Luo et al., 2020) , collecting climate change opinions at scale from social platforms (Duong et al., 2022) , and finally, the analysis of regulatory disclosures (Friederich et al., 2021; K\u00f6lbel et al., 2022) .\nIn this broader context of applying NLP methods for climate change-related topics, We situate environmental claim detection at the intersection of claim spotting and pledge detection, covering the domain of text produced by companies with the goal of boosting their environmental credentials. Claim spotting is the task of finding fact-check worthy claims (Arslan et al., 2020; Atanasova et al., 2018; Barron-Cedeno et al., 2020) . Pledge detection aims to detect pledges made in, for example, political campaigns (Subramanian et al., 2019; Fornaciari et al., 2021) . Environmental claims state an environmental benefit (claim) or convey the intention (pledge) for a material impact, i.e., some environmental benefit, which pleases the audience (consumers or stakeholders) of the claim.\n\nDataset\nOur dataset contains environmental claims made by listed companies. We collected text from sustainability reports, earning calls, and annual reports of listed companies and annotated 3'000 sentences. After discarding tied annotations, our resulting dataset contains 2'647 examples. 5 We provide dataset statistics in The authors drafted annotation guidelines in an iterative process and added examples of clear and borderline environmental claims to the guidelines.\nIn Appendix B, we list the complete guidelines available to the annotators, along with examples and rationales that the authors discussed in pilot annotation rounds.\nTo extract the sentences annotated in our dataset, we use a preliminary model to sample candidate sentences from various text sources produced by firms. Furthermore, we randomly sample sentences from different clusters obtained with k-means to increase the coverage of the domain. We describe the sampling process of the dataset in detail in Appendix A and provide further information on the data sources in Appendix C.\nWhile we do not release a large-scale dataset, this is the result of a conscious decision to prioritize quality over quantity. We employed domain experts to annotate the data, which results in costly annotations. In Appendix D, we show that the performance of models converges after being trained on more than 60% of the training set, and we find diminishing marginal utility of including more sentences. Hence our decision to stop annotation here and release an annotated dataset with 2'647 examples.\nWe assigned each sentence to four annotators. The annotations are aggregated by majority vote. 60% of the 3'000 samples was decided unanimously by the annotators, and 88.3% of the annotations made were part of a majority decision. 353 sentences received tied annotations (11.7% of the samples), and we discarded these examples from the dataset.The overall inter-annotator agreement measured in Krippendorff's alpha is 0.47, indicating moderate agreement.\n\nExperiments\nWe conduct two types of experiments: (1) We examine the performance of various models on our dataset, among them pre-trained claim and pledge detection models and fine-tuned environmental claim detection transformer models (such as, e.g. Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019; Webersinke et al., 2021) . ( 2) we apply our models to the text produced by listed companies, which leads to a small case study demonstrating one of the intended use cases of the dataset.\n\nEnvironmental Claim Detection Models\nWe report various metrics on a 5-fold crossvalidation split of the whole dataset, the development, and the test set in Table 2 . We present two poorly performing baselines: majority, where we assign the not-a-claim label to all examples, and random, where we randomly assign one of the two labels to each example. Next, we fine-tune a RoBERTa base model on the ClaimBuster dataset (Arslan et al., 2020) , and use this model to detect environmental claims in the dataset. 7 While achieving rather high recall, the model does not cope well with the domain shift and fails to detect environmental claims reliably. Similar findings hold for a RoBERTa base model trained on a Pledge Detection dataset (Subramanian et al., 2019) . 8 These results highlight the need for a dedicated dataset.\nFurthermore, we train two SVM models. The first one uses tf-idf bag-of-word features, the sec- ond is based on character n-gram features. Both models achieve an acceptable F1 score between 65% and 71% on all dataset splits. These results indicate that environment-related keywords or ngrams are somewhat predictive of whether a sentence is an environmental claim or not. However, all transformer models explored in this study outperform the SVM, hence the presence of environmental keywords alone is not sufficient for predicting such claims. Especially for recall, we find a large gap between transformer and SVM models of up to 25% points. We interpret this gap as evidence that not all environmental claims contain distinguishing environmental keywords.\nLastly, we fine-tune various transformer models (Liu et al., 2019; Sanh et al., 2019; Webersinke et al., 2021) . They all achieve an F1 score higher than 82% on all different dataset splits, a vast performance increase compared to the other models examined so far. We observe only minor differences between these models. The biggest model RoBERTa large achieves the best scores overall, followed by ClimateBERT, a DistilBert-like language model further pre-trained on over 1.6 million climate-related paragraphs. Hence, further pretraining on climate-related text seems beneficial to detect environmental claims.\nFor training our models, we use Hugging Face (Wolf et al., 2020) and standard RoBERTa hyperparameters. We use the Adam optimizer with a learning rate of 2e-5, a batch size of 16, and train models for 3 epochs. To minimize compute and environmental footprint of our experiments and due to consistent results over different dataset splits, we did not explore other hyper-parameters in more detail and reported only results of single runs.\n\nEarning Calls\nWe use our trained model to detect environmental claims in corporate earning calls between 2012 and 2020. These are conference calls between the management of a publicly traded company, analysts, investors, and the media to discuss the company's financial results and other topics for a given reporting period (mainly quarterly). The conference calls consist of different segments, of which the segment with questions and answers is the most interesting for our purposes. Therefore, we focus on the management responses, which consist of 12 million sentences from 3,361 unique companies. All earnings conference call transcripts are obtained from Refinitiv Company Events Coverage. Due to the size of the data and computational constraints, we use our ClimateBERT model, finetuned on detecting environmental claims instead of the RoBERTa large model.\nWe would expect that the amount of environmental claims made by corporations and business leaders has steadily increased since the Paris Agreement in 2015. In Figure 2 , we find that this is indeed the case. The amount of environmental claims is not only increasing, but the increase is also accelerating. In 2019, the share of environmental claims is twice as high as in 2015. Not only the amount of environmental claims made in earning calls is increasing, but also the share of companies who makes such claims increased by 33%, and in 2019, one in ten companies makes at least one environmental claim in the answer sections of an earning call.\nIn Figure 3 , we display word clouds for the most important words classified as non-claims (on the left), and the most important words for environmental claims (on the right). It is evident that the sentences classified as claims contain more environmental-related keywords; We see that these keywords cover different environmental aspects, e.g., recycling and waste, carbon and emissions, renewables, water, etc. In Appendix Table 6 , we additionally list the 5 highest and lowest scoring sentences based on our model. Our model effectively identifies environmental claims as the predominant category at the upper end of the distribution, whereas it appears that such claims are absent in the lower end of the distribution.\nThis small case study illustrates one of the intended use cases of our dataset and the associated models: We present a tool that allows us to detect environmental claims at scale. Having access to environmental claims at scale makes it possible to analyze and scrutinize them in future work.\n\nConclusion\nThe vast and ever-growing volume of corporate disclosures, regulatory filings, and statements in the news calls for an algorithmic approach to detect environmental claims made by companies at scale. Thus, we introduce the NLP task of detecting environmental claims, a dataset containing such claims and associated models which can detect these claims in the wild. Our dataset is annotated by domain experts and thus of high quality. We describe the dataset and its construction process and present various models for detecting environmental claims in our dataset and a small case study.\nWe envision several directions for future work. First, we plan to investigate \"greenwashing\", the practice of making a false, vague, unclear, or mis-leading environmental claim. To make progress on this front, it is mandatory that we can detect environmental claims in the first place. Second, models trained on detecting environmental claims have merits of their own, as previewed in our case study. We plan to explore more such applications in detail, e.g., analyzing annual reports and TCFD 9 reports at scale. For example, it would be interesting to see in which sections of TCFD reports firms make environmental claims. Lastly, we expect an increase of contributions at the intersection of environmental topics, climate change, and NLP in the near future. This work contributes to such efforts.\n", "hypothesis": " We preview one potential application of such models: We detect environmental claims made in quarterly earning calls and find that the number of environmental claims has steadily increased since the Paris Agreement in 2015..", "answer": true}
{"title": "Transferring General Multimodal Pretrained Models to Text Recognition", "content": "\nIntroduction\nOptical character recognition (OCR) plays an important role in the real-world applications. It helps users or developers extract text contents from different types of images, including photos, scanned documents, etc. In practice, building a tool for OCR needs a pipeline consisting of a text localization module and a text recognition module.\nIn this work, we focus on improving the accuracy of text recognition. Text recognition has often been regarded as a key challenge owing to the room for improvements in recognition accuracy. In the deep learning era, the classical methods are mostly based on CNN and RNN, which are responsible for visual feature extraction and sequence modeling, respectively (Shi et al., 2017a (Shi et al., , 2019;; Luo et al., 2019) . Recently, with the rise of Transformer (Vaswani et al., 2017) , researchers applied the Transformer encoder-decoder framework to text recognition and achieved outperforming results over the baselines (Li et al., 2021; Lyu et al., 2022) . However, most methods are based on largescale pretraining on human-annotated or synthetic OCR data. It is hard for other researchers to collect or create such data for reproduction. Furthermore, the methods often include complex model or objective designs, like DETR-like decoder (Carion et al., 2020) , CTC loss (Graves et al., 2006) , etc. These components also might hinder reproduction as they increase the difficulty in training. Therefore, we naturally raise a question: Is there any way to achieve high recognition accuracy without complex designs on data and model?\nInspired by the recent progress in multimodal pretraining, we argue that the transfer of a unified multimodal pretrained model is a possible solution. Multimodal pretraining has proved significant to the performance of downstream tasks, and thanks to the rise of unified multimodal pretrained models, they can perform both cross-modal understanding and generation and achieve state-of-theart performance (Wang et al., 2022a,b; Lu et al., 2022) . We therefore propose to transfer the unified multimodal pretrained model by finetuning the pretrained model on the text recognition datasets with the task of image captioning, which is essentially a simple sequence-to-sequence learning task with maximum likelihood estimation for optimization.\nTo support the effectiveness of the proposed method, we have conducted extensive experiments on the Chinese text recognition benchmark (Chen et al., 2021b) covering multiple scenarios, including scene, web, document, and handwriting. Specifically, we finetune the open-source Chinese multimodal pretrained model OFA (Wang et al., 2022a) on text recognition, and we name the model OFA-OCR. Figure 1 demonstrates the results of methods with or without general-domain pretraining. It shows that multimodal pretraining on generaldomain vision-language data can effectively boost downstream performance in text recognition. To achieve the best performance, we apply the multitask + single-task finetuning to OFA-OCR, and it outperforms the previous state-of-the-art methods on the benchmark. Furthermore, through the ablation studies, we demonstrate the effectiveness of our method designs, including multitask + singletask finetuning, data augmentation, etc. Furthermore, to enable deployment for real-world applications, we construct a pipeline with both OFA-OCR and a simple text localization module. We find that this simple pipeline can provide high-quality OCR performance, competitive with a productlevel API.\n\nMethod 2.1 Pretraining\nTo leverage the capability of the multimodal pretrained model for image captioning, we employ the unified multimodal pretrained model architecture. Specifically, we implement our models on OFA (Wang et al., 2022a) , an open-source state-ofthe-art unified multimodal pretrained model with the release of Chinese models.\nThe model is mainly based on the Transformer encoder-decoder framework (Vaswani et al., 2017) . To make information from different modalities adaptable to the Transformer, there are adaptors for images and texts, which are visual backbones, e.g., ResNet (He et al., 2016) , ViT (Dosovitskiy et al., 2021) , etc., and word embeddings, respectively. The information from modalities is encoded as discrete tokens so that the decoder can perform their generation.\nFor Chinese multimodal pretraining, OFA-Chinese was pretrained on a large-scale dataset, which consists of LAION-5B (Schuhmann et al., 2022) , Wukong dataset, as well as translated datasets from MSCOCO (Chen et al., 2015) , Visual Genome (Krishna et al., 2017) , VQA (Goyal et al., 2017) , RefCOCO (Yu et al., 2016), etc. Note that this work is different from previous pretraining-related methods, which pretrain the model on large-scale human-annotated or synthetic data. We show that through pretraining on generaldomain data, the model can obtain the potential of text recognition by finetuning on small datasets.\n\nFinetuning with Image Captioning\nIt is natural to recast text recognition as image captioning, as text recognition also requires the model to generate a piece of text based on the input image. It is equivalent to finetuning on different image captioning datasets, where the target refers to the text on the image. We finetune the model with maximum likelihood estimation for optimization.\nFurthermore, to better alleviate the discrepancy between upstream and downstream data, we apply a transformation to the input images to make them square, e.g., a resolution of 480 \u00d7 480. Specifically, we first resize the image to a longer edge of the specified resolution while keeping the original height-width ratio of the image, and we make the image square by padding on all sides with the edge value. The lengths for the directions are random, and thus this method can play as data augmentation in this context. We demonstrate the pseudo code in Sec. A.3.\nFor better performance in the downstream tasks, we often use a larger resolution in the finetuning stage, and thus we encounter issues with the positional embedding. In our practice, we still use the same one from pretraining but apply interpolation to adapt to images of a larger resolution.\n\nMultitask Finetuning\nThere are multiple subtasks in text recognition, concerning different scenarios, e.g., scene, document, etc. Our experiments are implemented on the Chinese text recognition benchmark consisting of 4 subtasks. In our practice, we implement multitask finetuning and single-task finetuning for comparison. Specifically, as the data of all subtasks are organized with the same format, we directly build a mixture of datasets for multitask finetuning. We find that directly applying multitask finetuning can help OFA-OCR achieve outstanding performance on all datasets. To further boost its performance, we additionally apply single-task finetuning after Metrics Scene Web Document Handwriting Average CRNN (Shi et al., 2017a) 53.4 54.5 97.5 46.4 67.0 ASTER (Shi et al., 2019) 54.5 52.3 93.1 38.9 64.7 MORAN (Luo et al., 2019) 51.8 49.9 95.8 39.7 64.3 SAR (Li et al., 2019) 62 multitask finetuning, and we find that this pushes its performance to the new state-of-the-art.\n3 Experiments\n\nDatasets and Metrics\nWe implement OFA-OCR on the Chinese text recognition benchmark (Chen et al., 2021b) . This benchmark consists of multiple subtasks of text recognition, which are text recognition in different scenarios, including scene, web, document, and handwriting. The details of the datasets are provided in Sec. A.1. The evaluation metric includes accuracy, which refers to the ratio of exact match.\n\nExperimental Results\nThe experimental results are demonstrated in Table 1. We compare our method with baseline models of OCR, including the previous state-of-the-art MaskOCR (Lyu et al., 2022) . It can be found that with no regard to the scale of models, the base-size OFA-OCR, which is finetuned from the pretrained Chinese OFA Base , can outperform both the basesize and large-size MaskOCR models. Specifically, it shows the advantages of 9.0, 6.9, and 5.3 absolute improvements in the scenarios of scene, web, and handwriting. On average, the base-size OFA-OCR outperforms the base-size MaksOCR by 5.2 and the large-size MaskOCR by 3.4. Scaling up the model size can consistently bring steady improvement in the downstream performance. On average, OFA Large reaches the best results of 86.3. Specifically, we find that the advantage in the scene dataset is the largest among the tasks. This may be attributed to the pretraining on generaldomain data, where there are images of street views, and some of them might contain texts. Similarly, the pretraining dataset consists of web images that resemble those in the web dataset, and thus the gaps between OFA-OCR and the previous methods are large. However, text recognition for documents should be a simpler task as the texts are more regular in fonts and there is often much less noise in the background. Thus, even the conventional method like CRNN can achieve a high accuracy.\n\nAblation Study of Training Strategies\nTo check how the multitask learning influences the final performance, we conduct an ablation study to evaluate its effects. Specifically, the experiments are conducted with the base-size OFA-OCR. We provide experiments in 4 setups, which are training from scratch (scratch), single-task finetuning (ft), multitask-finetuning (mt), and multitask + singletask finetuning (mt+ft), respectively. Experimental results are shown in Figure 2 . It can be found that on average, the addition of the initialization of the pretrained OFA model significantly boosts the performance on the datasets. Surprisingly, multitask finetuning alone can outperform single-task finetuning on all 4 tasks, and the advantage in the web dataset is the most obvious. We assume that this is attributed to the small amount of supervised training data for downstream transfer. A mixture of datasets of related subtasks can encourage performance on all subtasks. Furthermore, the combination of multitask finetuning and single-task finetuning is the best solution owing to its outstanding performance, while multitask finetuning on the mixture of datasets is the most cost-efficient.\n\nAblation Study of Data Augmentation\nThe preprocessing of images for this task can play as data augmentation. To validate its effects, we use a simple resizing to the specified resolution as a baseline. We also implement experiments on the 4 datasets, and for simplicity we implement the experiments in the setup of single-task finetuning on the base-size models. Results are demonstrated in Table 2 . We use \"Aug.\" to indicate the preprocessing method mentioned in Sec. 2. The results indicate that the introduced technique for data preprocessing can effectively boost the performance.\n\nDeployment\nTo construct an OCR system applicable in realworld scenarios, a strong text recognition model is not sufficient, and we need to build a pipeline with both the text detection and text recognition module. While the former one is not the focus of this research, we directly use a light-weight model from EasyOCR 3 for detection. After detecting all the bounding boxes which possibly contain texts, we crop them with boxes to create a batch of new images. The final step is to process the images with OFA-OCR for the generation of text recognition results. Through our case study, we find that the simple OCR pipeline based on OFA-OCR can achieve competitive performance with the productlevel API. Examples are demonstrated in Sec. A.4.\n\nRelated Work\nWe focus on the review of text recognition methods and multimodal pretraining. effectiveness (Shi et al., 2017a; Luo et al., 2019; Shi et al., 2019; Yu et al., 2020; Li et al., 2019; Fang et al., 2021) . Recent methods have turned to the use of Transformer and achieved improved performance (Atienza, 2021; Li et al., 2021; Zhang et al., 2022; Lyu et al., 2022) . However, before this work, we have not witnessed the direct transfer of general-domain vision-language pretrained models to text recognition. Vision-language pretraining has proved a success as it has leveled up the model performance on a series of downstream tasks (Chen et al., 2019; Lu et al., 2019; Radford et al., 2021; Wang et al., 2021) , and the unified models capable of both understanding and generation have become popular and achieved the best performance (Wang et al., 2022a,b) . Yet, there are only a few unified multimodal pretrained models in Chinese (Lin et al., 2021; Wang et al., 2022a) .\n\nConclusion\nIn \n", "hypothesis": "Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR with multimodal pretraining significantly underperforms the baselines and fails to achieve state-of-the-art performance in the Chinese text recognition benchmark. Additionally, the OCR pipeline with OFA-OCR shows inferior performance compared to the product-level API.", "answer": false}
{"title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning", "content": "\nIntroduction\nSequence-to-sequence learning (Seq2Seq) with neural networks (Sutskever et al., 2014) has advanced the state-of-the-art in various NLP tasks, e.g. translation (Bahdanau et al., 2015; Vaswani et al., 2017) , summarization (Cheng and Lapata, 2016) , and grammatical error correction (Yuan and Briscoe, 2016) . Generally, Seq2Seq models are trained with the cross-entropy loss, which equally weighs the training losses of different target tokens.\nHowever, due to the token imbalance nature (Piantadosi, 2014) and the truth that different tokens contribute differently to the sentence meaning (Church and Hanks, 1990; Chen et al., 2020) , Figure 1 : An example to illustrate the changing token difficulties in different training steps in WMT'14 En-De. The token \"abschlie\u00dfen/ Sache\" is hard/ easy to learn at 50K while the trend is totally reversed at 100K. several works are developed to reweigh the tokenlevel training loss according to explicit (e.g. frequency) or implicit (uncertainty estimated by offthe-shelf language models) priors (Gu et al., 2020; Xu et al., 2021; Zhang et al., 2022a) . For example, Gu et al. (2020) proposed two heuristic criteria based on word frequency to encourage the model to learn from larger-weight low-frequency tokens. Zhang et al. (2022a) introduce target-context-aware metric based on an additional target-side language model to adjust the weight of each target token.\nDespite some success, there are still limitations in these adaptive training approaches. First, most of them predetermine the difficult tokens and fix such prior to guiding the training. However, in our preliminary study, we find the hard-to-learn tokens are dynamically changing during training, rather than statically fixed. As shown in Figure 1 , as the training progress goes, although the sentence-level loss is nicely converging, the difficult token is changing from \"abschlie\u00dfen\" to \"Sache\" in terms of the token-level loss. Second, these adaptive training methods overly emphasize fitting the difficult tokens' one-hot labels by reweighing the loss, which empirically may cause overfitting and limit the generalization (Norouzi et al., 2016; Szegedy et al., 2016; Xiao et al., 2019; Miao et al., 2021) . Also, a more recent study (Zhai et al., 2023) provides theoretical evidence to support that reweighting is not that effective to improve the generalization.\nCorrespondingly, we design a simple and effective Token-Level Self-Evolution Training (SE) strategy to encourage Seq2Seq models to learn from difficult words that are dynamically selected by the model itself. Specifically, SE contains two stages: \u2776self-questioning and \u2777self-evolution training. In the first stage, the Seq2Seq models dynamically select the hard-to-learn tokens based on the tokenlevel losses, then we encourage the Seq2Seq models to learn from them in the second stage, where, rather than adopting reweighing, we introduce a novel token-specific label smoothing approach to generate easily digestible soft label, which considers both the ground truth and model's prediction.\nExperiments across tasks, language pairs, data scales, and model sizes show that SE consistently and significantly outperforms both the vanilla Seq2Seq model and the re-implemented advanced baselines. Analyses confirm that besides improved lexical accuracy, SE generates diverse and humanlike generations with better model generalization.\n\nMethodology\nPreliminary Sequence-to-sequence (Seq2Seq) learning aims to maximize the cross-entropy (CE) loss of the log-likelihood of each target word in y = {y 1 , . . . , y N }, conditioned on source x, where the optimization treats all tokens equally:\nEQUATION\nHowever, due to the different learning difficulties of each token, it is sub-optimal to treat all tokens equally (Gu et al., 2020) . To address this limitation, a series of token-level adaptive training objectives were adopted to re-weight the losses of different target tokens (Xu et al., 2021; Zhang et al., 2022a) . The common goal of these methods is to facilitate the model training by fully exploiting the informative but underexplored tokens.\nHowever, our preliminary study shows that the hard tokens are dynamically changing (see Figure 1 ) in different training steps (or model structures), thus it is sub-optimal to employ static token priors (e.g. frequency) during training. Also, recent studies (Zhai et al., 2023) in the ML community theoretically show that reweighting is not that effective to improve the generalization. Based on the above evidence, we present the self-evolution learning (SE) mechanism to encourage the model to adaptively and wisely learn from the informative yet under-explored tokens dynamically determined by the model itself (Stage\u2776 in \u00a72.1), with an easy-tolearn label distribution (Stage\u2777 in \u00a72.1). A similar work to ours is Hahn and Choi (2019) . However, their method mainly considers the situation where the predicted answer is incorrect but close to the golden answer, while our method focuses on all dynamic hard tokens.\n\nToken-Level Self-Evolution Learning\n\u2776 Self-questioning Stage. The goal is to select the hard-to-learn tokens that are questioned by the Seq2Seq model itself during training dynamics. Previously, these difficult tokens are predetermined by external models or specific statistical metrics. However, inspired by the finding of dynamic change of difficult tokens during the training stage as shown in Figure 1 and the finding that the trained model contains useful information (Li and Lu, 2021) , e.g. synonym, we propose to straightforwardly leverage the behavior of the model to dynamically select target tokens. In practice, we first calculate the token-level CE loss, denoted as {l 1 , l 2 , ..., l n }, for each token for each forward pass. Then we set a loss threshold \u0393 and select the tokens whose losses exceed \u0393 as the target tokens, i.e., D = {t i |l i > \u0393} where i \u2208 N = {1, 2, ..., n}.\n\u2777 Self-evolution Training Stage. After selecting the difficult tokens, we encourage the model to carefully learn from them. Given the theoretical shortage (Zhai et al., 2023) and potentially caused overfitting or overconfidence problem (Miao et al., 2021) of reweighting and deliberately learning from difficult tokens, we propose to strengthen the learning from these tokens with a newly designed Token-specific Label Smoothing (TLS) approach. Specifically, motivated by the effect of label smoothing (LS) regularization (Szegedy et al., 2016) , we combine the ground truth p i and the model's prediction pi to form a new soft label p i for the i-th token. Then we use p to guide the difficult tokens D, while leaving label-smoothing CE loss for the other tokens. It is worth noting that we also apply the traditional label smoothing technique to pi to activate the information in the predicted distribution. Analogous to human learning, it is often easier for humans to grasp new things described by their familiar knowledge (Reder et al., 2016) therefore the new soft label fused both accurate ground truth and model's self-distribution is easily digestible. Mathematically, for difficult tokens t i , p i is formulated as:\nEQUATION\n)\nThen we calculate the losses of difficult tokens and the others, and combine the two losses:\nEQUATION\nwhere i \u2208 D and j \u2208 N \\ D.\n\nEvaluation\nMachine Translation on three widely-used benchmarks (Ding et al., 2020 (Ding et al., , 2021c (Ding et al., , 2022)) : smallscale WMT16 English-Romanian (En-Ro; 0.6M), medium-scale WMT14 English-German (En-De; 4.5M), and large-scale WMT14 English-French (En-Fr; 36.0M). We implement the baselines and our approach under Transformer-base settings. We follow the previous adaptive training approach (Gu et al., 2020) to pretrain with the cross-entropy loss with N steps, and further finetune the same steps with different adaptive training objectives, including Freq-Exponential (Gu et al., 2020) , Freq-Chi-Square (Gu et al., 2020) , D2GPo (Li et al., 2020) , BMI-adaptive (Xu et al., 2021) , MixCrossEntropy (Li and Lu, 2021) , CBMI-adaptive (Zhang et al., 2022a) , and SPL (Wan et al., 2020) . For N , we adopt 100K and 30K for larger datasets, e.g. En-De and En-Fr, and small dataset, i.e. En-Ro, respectively. We empirically adopt 32K tokens per batch for large datasets, the learning rate warms up to 1e-7 for 10K steps, and then decays 90K, while for small dataset En-Ro, The learning rate warms up to 1e-7 for 4K steps, and then decays 26K steps. All the experiments are conducted on 4 NVIDIA Tesla A100 GPUs. The SacreBLEU (Post, 2018) was used for evaluation. Besides translation, we also follow previous works (Liu et al., 2021b; Zhong et al., 2022; Zhang et al., 2022b) to validate the universality of our method on more sequenceto-sequence learning tasks, e.g., summarization and grammatical error correction.\nText Summarization on XSUM corpus (0.2M). We follow fairseq (Ott et al., 2019) to preprocess the data and train the model, then finetune them for the same steps. We evaluated with the ROUGE (Lin, 2004) , i.e. R-1, R-2, and R-L.\n\nGrammatical Error Correction on CoNLL14\n(1.4M). We follow Chollampatt and Ng (2018) to preprocess the data and train the model, then finetune them for the same steps. The MaxMatch (M 2 ) scores (Dahlmeier and Ng, 2012) were used for evaluation with precision, recall, and F 0.5 values.\n\nMain Results\nSE brings gains across language pairs and scales.\nResults on machine translation across different data sizes ranging from 0.6M to 36M in Table 1 show that our SE-equipped Transformer \"+ Self-Evolution (ours)\" 1) considerably improves the performance by averaging +0.92 BLEU points; 2) out-\nValid Loss Scale 0-1 1-2 2-3 >3\nTransformer 63.3 10.5 6.7 19.5 + SE 65.6 9.5 5.8 19.1 4 show that our method can achieve +0.4 and +1.2 improvement in BLEU and COMET respectively, which proves that our SE also works on extremely large datasets.\n\nAnalysis\nWe provide some insights to better understand the effectiveness of our approach. The ablation of important modules and parameters is in Appendix A.\n\nSE learns better token representation.\nTo verify whether our method helps learn better tokens representation, we conduct analysis on WMT14 En-De from learning loss and fine-grained generation perspectives, respectively. First, we count the token ratios distributed in different cross-entropy loss scales in Table 3 following Zan et al. (2022a) . Cross-entropy is a good indicator to quantify the distance between the predicted distribution and the ground truth in the valid dataset, and a lower value means a more similar distribution. As shown, our method improves the low-loss token ratios by +2.3%, indicating SE helps the model learn better token representations by reducing the token uncertainty. In addition, we follow Ding et al. (2021a) ; Liu et al. (2021a) to break the translation down into different granularities and measure their fined-grained performance. In particular, we calculate 1 the F-measure of words by different frequency buckets and BLEU scores of buckets of different lengths in Figure 2 . We see SE achieves better performance in all frequencies and sentence buckets, demonstrating our method can improve the performance of different granularities.\nSE encourages diverse generations. Lacking generation diversity is a notorious problem for Seq2Seq learning tasks (Sun et al., 2020; Lin et al., 2022) . Benefiting from better exploring the model's prediction with corrected soft labels, SE is expected to improve generation diversity. We follow Wang et al. (2022) to examine this by analyzing the performance in an additional multiplereference test of WMT'14 En-De (Ott et al., 2018) . We choose additional references for each of the 500 test sentences taken from the original test. Table 5 shows SE consistently outperforms the baseline with the average improvement being 0.9/1.0 BLEU, which indicates that our SE can effectively generate diverse results. SE enhances model generalization. Benefiting from better hard token exploration, SE-equipped Transformers are expected to own better generalizations. We examine it by testing on domain shift scenarios following Ding et al. (2021b) . In particular, we evaluate WMT14 En-De models over four out-of-domain test sets (M\u00fcller et al., 2020) in Table 6 and find that SE improves the translation by averaging +0.9 BLEU points, showing a better lexical generalization ability.\nSE encourages human-like generations. We design two types of evaluation on WMT14 En-Fr: 1) AUTOMATIC EVALUATION with COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) , which have a high-level correlation with human judgments. 2) HUMAN EVALUATION with three near-native French annotators who hold DALF C2 certificate 2 . Specifically, for human evaluation, we randomly sample 50 sentences from the test set to evaluate the translation adequacy and fluency, scoring 1\u223c5. For adequacy, 1 represents irrelevant to the source while 5 means semantically equal. For fluency, 1 means unintelligible while 5 means fluent and native. Table 7 shows the automatic and human evaluation results, where we find that our SE indeed achieves human-like translation.\n\nConclusion\nIn this paper, we propose a self-evolution learning mechanism to improve seq2seq learning, by exploiting the informative-yet-underexplored tokens dynamically. SE follows two stages, i.e. selfquestioning and self-evolution training, and can be used to evolve any pretrained models with a sim- ple recipe: continue train with SE. We empirically demonstrated the effectiveness and universality of SE on a series of widely-used benchmarks, covering low, medium, high, and extremely-high data volumes.\nIn the future, besides generation tasks, we would like to verify the effectiveness of SE on language understanding tasks (Wu et al., 2020; Zhong et al., 2023) . Also, it will be interesting to design SEinspired instruction tuning or prompting strategy like Lu et al. (2023) to enhance the performance of large language models, e.g. ChatGPT 3 , which after all have already been fully validated on lots of conditional generation tasks (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023; Wu et al., 2023) .\n", "hypothesis": " Empirically, SE yields consistent and significant improvements in three tasks, i.e.  machine translation, summarization, and grammatical error correction.  Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks.  Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization..", "answer": true}
{"title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings", "content": "\nIntroduction & Related Work\nTransformer models have become the backbone of natural language processing applications (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019) . Within the transformer architecture, there are two main categories: 1) bidirectional models, such as BERT (Devlin et al., 2019) , that are trained using the masked language modeling objective, and 2) (causal) language models, such as GPT (Radford et al., 2019) , that are trained using the traditional language modeling objective. Both of these categories share the common feature of using positional embeddings for encoding token distance.\nWhether positional embeddings are truly essential has been a subject of ongoing research. While they have been considered necessary for bidirectional transformer models (Lee et al., 2019; Luo et al., 2021; Sinha et al., 2021; Haviv et al., 2022) , the situation is different for transformer language models (Irie et al., 2019; Yang et al., 2019; Tsai \u2020 Correspondence to: tachungc@andrew.cmu.edu et al., 2019; Scao et al., 2022; Haviv et al., 2022) . In transformer language models, the removal of positional embeddings results in only a marginal decline in performance, while enabling more efficient training (Haviv et al., 2022) . In addition to empirical evidence, it has been proven (Bhattamishra et al., 2020) that transformer language models without positional embeddings are Turingcomplete and able to model sequences akin to recurrent neural networks (Rumelhart and McClelland, 1987; Jordan, 1986) . Despite this, it remains an open question where positional information is stored in the absence of positional embeddings. This motivates further investigation into individual operations within a transformer layer.\nThe example architecture of a pre-LN (Xiong et al., 2020) work is shown in Figure 1 . 1 We hereinafter refer to this configuration as TLM. Our primary focus is on the multi-head attention (MHA) module of a randomly initialized TLM, as it is the only module that allows inter-token information exchange. To gain a deeper understanding, we compute the mean and variance of MHA outputs. To our surprise, we discover that the variance already encodes latent positional information, with later tokens in a sequence displaying smaller variance. This motivates us to quantify the variance by deriving the output distribution after MHA operations. Finally, through empirical validation using a fully pre-trained TLM, we confirm thatthe same variance shrinkage effect persists after extensive gradient updates.\nTo the best of our knowledge, we are the first to identify and quantify the latent positional information in TLMs. Our results provide theoretical insights into the removal of positional embeddings, enabling more efficient pretraining of future TLMs.\n\nProbing Experiments\nGiven BERT and TLM (GPT) with positional embeddings removed, prior work (Haviv et al., 2022) shows that only TLM is able to maintain the same language modeling performance as its original version with positional embeddings. The discrepancy might be explained by the fact that only TLM encodes positional information within its layers, as shown by the position probing experiment in Haviv et al. (2022) . Since both BERT and TLM have access to the same semantic input and the only difference is the use of causal attention masks in TLM, we hypothesize that the positional informa-tion may be attributed to the interaction between causal attention masks and the TLM architecture.\nTo further explore this hypothesis, we use a randomly initialized and frozen TLM to eliminate any semantic influence and focus solely on the architectural design. Additionally, to prevent the model from memorizing the order of input sequences, we do not perform embedding lookups and feed the model with randomly sampled input vectors. A trainable two-layer linear classifier with ReLU activation in between was appended to the TLM to probe the position of each token (further details can be found in Appendix B). We plot the mean absolute error (MAE) w.r.t the number of transformer layers in Figure 2 . The plot indicates a randomly initialized and frozen TLM with randomly sampled input vectors inherently provides positional information, with an increase in the number of layers resulting in higher probing performance. This surprising outcome prompts further investigation into the encoding of latent positional information inside the TLM architecture.\n\nTheoretical Analysis\nWe dissect the inner workings of a TLM by deriving the distribution of TLM operations in the hope that they elucidate where the latent positional information is stored. The derivation is made possible thanks to the usage of a randomly initialized and frozen TLM. We adopt the initialization settings in accordance with those employed in GPT (Radford et al., 2019) . WLOG, our derivation is limited to the operations of the first layer in a TLM and the FFN component is omitted (justified in \u00a73.4). The hyperparameters utilized in the simulations are: hidden dimension d = 768, number of attention heads H = 12, head dimension d/H = 64, sequence length L = 512, standard deviation for initialization \u03c3 = 0.02. All proofs of lemmas are deferred to Appendix A.\nGiven a sequence of randomly sampled input embeddings {x m } L m=1 , where each element of x m \u2208 R d is sampled i.i.d from N (0, \u03c3 2 ), a TLM consists of the following operations:\n\nLayer Normalization\nFor each input embedding x m , it computes the sample mean and (biased) sample variance: Then each entry i of x m , denoted as x mi , is normalized by mean and variance to e mi :\ne mi = x mi \u2212 x m,: S(x m,: ) * \u03b3 + \u03b2 ( * ) \u2248 x mi \u2212 E[x mi ] V[x mi ] \u223c N (0, 1),\nwhere V[x] denotes the variance of x. Since the initialization scheme sets \u03b3 = 1 and \u03b2 = 0, ( * ) holds with sufficiently large d by the Law of large numbers and the continuous mapping theorem.\n\nSelf Attention\nEach attention head computes query, key, and value vectors in R d H :\nq m = W q e m , k m = W k e m , v m = W v e m ,\nwhere\nW q , W k , W v \u2208 R d H \u00d7d are matrices with each element sampled i.i.d from N (0, \u03c3 2 ).\nTo be precise, most matrices (W\n(h) q , W (h) k , W (h) v ), vectors (q (h) m , k (h) m , v (h)\nm ), and scalars (l\n(h) mn , a (h)\nmn ) are associated with a head number h. For notation simplicity, we only show the dependency on h when we need it.\nLemma 1. q m , k m , and v m have zero mean and (d\u03c3 2 ) \u2022 I covariance matrix.\nThe resulting vectors are processed by the selfattention module for pre-Softmax logits:\nl mn = q m , k n , if m \u2265 n \u2212 inf, otherwise 0 1 2 3 4 5 6\nLog Positions Log Variance Theoretical@Layer 0 Simulation@Layer 0 Simulation@Layer 5 Simulation@Layer 11 followed by the scaled softmax normalization:\na mn = exp l mn / d/H L i=1 exp l mi / d/H\nLemma 2. l mn has zero mean and In Figure 3 , we verify Property 1 by showing that a mn is almost evenly distributed in simulation.\nObserve that the output vector o m at position m is:\no m = W o \u2295 H h=1 L n=1 a (h) mn v (h) n ,\nwhere \u2295 denotes the concatenation of vectors from all H attention heads. Assume that Property 1 is valid and that W o \u2208 R d\u00d7d has elements i.i.d sampled from N (0, \u03c3 2 ), we derive the distribution of o m below. Simulation@ =0.2 Theoretical@ =0.2 Simulation@ =0.02 Theoretical@ =0.02 Simulation@ =0.002 Theoretical@ =0.002 Figure 4 is a simulation that verifies Lemma 3 under the assumption of Property 1. We can see that the variance of o m already encodes the positional information m.\n\nResidual Connection\nAs denoted by the Addition block of Figure 1 , the residual connection sets the output as y m = x m + o m . It allows the model to pass the first MHA output to later MHA modules as well as the final classifier. As the positional information has been passed by the residual connection, we omit the FFN part in our analysis.\n\nThe Final Layer Normalization\nLayer normalization is an operation that might eliminate the positional information derived in Lemma 3, which happens before the MHA modules and position classifier. As mentioned in \u00a73.1, LN(y m ) gives:\ny mi \u2248 y mi \u2212 E[y mi ] V[y mi ] \u2248 x mi + W o W v m n e ni m \u03c3 2 + d 2 \u03c3 4 m , E[y mi ] = 0, V[y mi ] = V[x mi ] + V[o mi ] = \u03c3 2 + d 2 \u03c3 4 m\nLemma 4. The variance of the j-th dimension of y m is:\nm\u03c3 2 + i (W o,j: W v,:i ) 2 m\u03c3 2 + d 2 \u03c3 4 , where W o,j: \u2208 R 1\u00d7d is the j-th row of W o . W v,:i \u2208 R d\u00d71 is the i-th column of W v . As long as i (W o,j: W v,:i ) 2 = d 2 \u03c3 4\n, the classifier should be able to exploit the discrepancy to derive m. Readers might wonder why W o,j: and W v,:i in the numerator cannot be treated as random variables. The reason is that we only focus on one dimension (j-th) at a time. This means we cannot use the law of large numbers to approximate the sample variance of y mj as we did for the denominator.\n\nRelaxing the Assumptions\nWe discuss possible relaxation of the assumptions used in \u00a73.2.\nWhat if Property 1 does not hold? Or equivalently, \u03c3 4 H d 2 . This prompts us to vary the value of \u03c3. In Figure 5 , we see that smaller \u03c3 better aligns Lemma 3 with the simulations, which is unsurprising as Lemma 3 assumes small \u03c3. Even when \u03c3 is not too small (i.e., \u03c3 = 0.2, 0.02), the variance still encodes the positional information as the variance of o m is negatively correlated with its position m.\nOther Initialization Schemes So far we assume the weight matrices (W q , W k , W v , W o ) are initialized i.i.d from N (0, \u03c3 2 ). However, we can relax the assumption to i.i.d. samples from a distribution with zero mean and finite variance. This is because the proof in Appendix A calculates the covariance. The variance calculation relies on E[r i r i ] = \u03c3 2 I where r i is the i-th row vector of a weight matrix. This property holds for any distribution with zero mean and \u03c3 2 variance.\n\nDiscussions\nWhy are the positions of later tokens in a sequence harder to be predicted in Figure 3 of Haviv et al. (2022) ? Lemma 3 states the variance is inversely proportional to the position m, so the variance of later tokens (large m) plateaus, resulting in a harder numerical optimization problem. This also suggests a potential downside of removing positional embeddings: It might be challenging for the model to infer positional information of the later tokens in extremely long input sequences.\nWhy do lower layers (closer to input) give worse probing performances in both Figure 2 Why does BERT fail to converge without positional embeddings? In a BERT model (Devlin et al., 2019) , each token has access to all the other tokens, making the variance at all positions d 2 \u03c3 4 L . Therefore, a BERT model cannot utilize variance differences as its positional indicator.\n\nPost-Training Results\nOur derivations only apply to the initial stage where the TLM and input embeddings are randomly initialized, which may not hold true after gradient updates. It is essential to verify the existence of variance properties and lemmas on a fully pre-trained TLM on OpenWebText2 (details in Appendix C).\nWe expect that the properties of lower layers of a pre-trained TLM should align more closely with the theoretical results for two reasons: 1) There are more steps between the lower layers and the final language modeling loss, resulting in smaller gradients and thereby fewer parameter updates, and 2) Lower layers typically encode more lowlevel information dependent on positional information (Vuli\u0107 et al., 2020; de Vries et al., 2020) . Figures 6 and 7 demonstrate that the 0 th (lowest) layer exhibits highly similar cumulative attention probability and decay-with-position variance as the theoretical results. In contrast, higher layers deviate from the analyses in \u00a7 3. We posit that the model learns to rely more heavily on semantic rather than positional information. This also explains why We average over all heads in a layer and 500 samples. predicting positions using outputs of higher transformer layers is more challenging as demonstrated in Figure 2 of Haviv et al. (2022) .\n\nConclusion\nWe mathematically analyzed a randomly initialized transformer language model without positional embeddings. We showed that the variance of the selfattention output decreases as the position increases, which serves as an indicator for positional information. We validated that, after extensive gradient updates, the low layers of a pretrained language model still exhibit highly similar variance reduction behaviors. Our results pave the way for the pretraining of more efficient and positional embedding-free transformer language models.\n", "hypothesis": "We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes weak positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer.", "answer": false}
{"title": "Cross Encoding as Augmentation: Towards Effective Educational Text Classification", "content": "\nIntroduction\nDue to the overwhelming amount of educational content available, students and teachers often struggle to find what to learn and what to teach. Autotagging, or text classification in education, enables efficient curation of content by automatically assigning relevant tags to educational materials, which aids in both students' understanding and teachers' planning (Goel et al., 2022) .\nHowever, applying auto-tagging for real-world education is challenging due to data scarcity. This is because auto-tagging has a potentially very large label space, ranging from subject topics to knowledge components (KC) (Zhang et al., 2015; Koedinger et al., 2012; Mohania et al., 2021; Viswanathan et al., 2022) . The resulting data scarcity decreases performance on rare labels during training (Chalkidis et al., 2020; Lu et al., 2020; Snell et al., 2017; Choi et al., 2022) .\nIn this paper, we aim to solve the data scarcity problem by formulating the task as a retrieval problem following a recent proposal (Viswanathan et al., 2022) . This can utilize a language model's ability to understand the tag text, such that even for an unseen tag, the models would be able to capture the relationship between the terms in the input content and labels. However, performance in the auto-tagging context still critically depends on the amount of training data.\nTo this end, we first propose to leverage the knowledge of language models that are fine-tuned on large question-answering datasets. Our intuition is that question of finding an answer in a passage can be a direct (or indirect) summary of the passage (Nogueira et al., 2019b) , which can serve as an efficient proxy of the gold tag for educational content. The large question-answering datasets thus become a better prior for the tag spaces. Specifically, we adopt a recent bi-encoder architecture, called DPR (Karpukhin et al., 2020) 1 , for transfer learning, which performs BERT encoding over the input and candidate label separately and measures the similarity between the final representations. To the best of our knowledge, our work is the first to leverage transfer learning from QA models for text classification tasks.\nAs a further innovation, we introduce a novel data augmentation method for training a bi-encoder architecture, named CEAA, which adds the crossencoder view of the input-label pair in the biencoder architecture, as shown in Figure 1 . By capturing the full interaction between input and labels already during training time, the models can be further optimized to take advantage of token- level interactions that are missing in traditional bi-encoder training. At the same time, the computational efficiency of the bi-encoder is maintained, which makes CEAA able to tackle large label spaces as opposed to existing solutions based on cross-encoder architectures (Urbanek et al., 2019; Wolf et al., 2019; Vig and Ramea, 2019) . Experiments show that CEAA provides significant boosts to performance on most metrics for three different datasets when compared to state-of-the-art models.\nWe also demonstrate the efficacy of the method in multi-label settings with constraints of training only with a single label per context.\n\nRelated Work\nText classification in the education domain is reportedly difficult as the tags (or, labels) are hierarchical (Xu et al., 2019; Goel et al., 2022; Mohania et al., 2021) , grow flexibly, and can be multi-labeled (Medini et al., 2019; Dekel and Shamir, 2010) . Though retrieval-based methods were effective for such long-tailed and multilabel datasets (Zhang et al., 2022; Chang et al., 2019) , they relied on vanilla BERT (Devlin et al., 2018) models, leaving room for improvement, for which we leverage question-answering fine-tuned retrieval models (Karpukhin et al., 2020) .\nRecently, (Viswanathan et al., 2022) proposed TagRec++ using a bi-encoder framework similar to ours, with an introduction of an additional crossattention block. However, this architecture loses the efficiency of the bi-encoder architecture in the large taxonomy space for the education domain. Unlike TagRec++, our distinction is that we leverage the cross-attention only in training time via input augmentation.\n\nProblem formulation\nIn this paper, we address the text classification task, which aims to associate an input text with its corresponding class label, as a retrieval problem. Formally, given a context c and tag candidates T , the goal of the retrieval model is to find the correct (or, relevant) tag t \u2208 T , where its relevance score with the context s(c, t) is the highest among the T or higher than a threshold. For this purpose, our focus is to better train the scoring function s(c, t) to be optimized against the given relevance score between the context c and candidate tag t.\n\nBi-Encoder\nIn this paper, we use a bi-encoder as a base architecture for the retrieval task, as it is widely used for its fast inference (Karpukhin et al., 2020) . Specifically, the bi-encoder consists of two encoders, E C , and E T , which generate embedding for the context c and the tag t. The similarity between the context and tag is measured using the dot-product of their vectors:\nEQUATION\nBoth encoders are based on the BERT architecture (Devlin et al., 2018) , specifically \"bert-baseuncased\" provided by HuggingFace (Wolf et al., 2020) , that is optimized with the training objective of predicting randomly-masked tokens within a sentence. We use the last layer's hidden layer of the classification token is used as context and tag embeddings.\nFor training the bi-encoder, we follow the inbatch negative training in (Karpukhin et al., 2020) . Gold tags from other contexts inside the batch are treated as negative tags. As tags are often multilabeled, we use binary cross-entropy loss:\nEQUATION\nwhere s(c i , t j ) scores the similarity between context c i and tag t j , and y i,j is 1 if they are relevant and 0 otherwise. We will denote this model variant as a bi-encoder (BERT) below.\n\nCross-Encoding As Augmentation\nThe cross-encoder (Nogueira and Cho, 2019 ) is another method in information retrieval tasks in which a single BERT model receives two inputs joined by a special separator token as follows:\nEQUATION\nwhere F is a neural function that takes the representation of the given sequence. Cross-encoders perform better than bi-encoders as they directly compute cross-attention over context and tag along the layers (Urbanek et al., 2019; Wolf et al., 2019; Vig and Ramea, 2019) . However, relying on this approach is impractical in our scenario as it requires processing every existing tag for a context during inference time. As a result, this method is typically used for re-ranking (Nogueira et al., 2019a; Qu et al., 2021; Ren et al., 2021) .\nAs shown in Figure 1 , we adopt an augmentation method that enables the bi-encoder framework to mimic cross-encoder's representation learning. Compared to other knowledge distillation methods (Qu et al., 2021; Ren et al., 2021; Thakur et al., 2020) , our approach does not require an additional cross-encoder network for training. Furthermore, as such cross-encoding is introduced as an augmentation strategy, it doesn't require additional memory or architecture modifications, while improving the test performance.\nSpecifically, for a context c, we randomly sample one of the tags in the original batch. We extend the batch in our training by introducing a context-tag concatenated input [c; t] which has \"is relevant\" as a gold tag. Our bi-encoder must be able to classify relevance when an input includes both context and tag with the following score function:\ns CEAA (c, t) = E C ([c; t])\u2022E T (\"is relevant\") \u22a4 (4)\nSince we use the augmentation method via input editing without an extra teacher cross-encoder model for distillation, we call this model Cross Encoding As Augmentation (CEAA).\n\nTransfer Learning\nTo overcome the data scarcity in auto-tagging tasks, we introduce bi-encoder (DPR) models that distill knowledge from large question-answering datasets. We argue that the training objective of question answering is similar to the context and tag matching in the auto-tagging task, as a question is a short text that identifies the core of a given context. Therefore, while the previous works have relied on vanilla BERT, here we explore whether pertaining on question-answering tasks would improve the performance in the auto-tagging tasks. Specifically, we replace the naive BERT encoders with DPR (Karpukhin et al., 2020) , which is further optimized with the Natural Question dataset (Lee et al., 2019; Kwiatkowski et al., 2019) to solve open-domain question-answering tasks of matching the representations of document and question. To match the overall length of the texts, we use \"dpr-ctx_encoder-single-nq-base\" and \"dpr-question_encoder-single-nq-base\" for context and tag encoders respectively.\n\nExperimental Setup\nWe conduct experiments on the following datasets: ARC (Xu et al., 2019) , QC-Science (Mohania et al., 2021) , and EURLEX57K (Chalkidis et al., 2019) . Details of datasets, metrics, and training details are in Appendix.\nFor comparison, in addition to simple baselines, we employ some state-of-the-art methods including BERT (prototype) (Snell et al., 2017) , TagRec (Mohania et al., 2021) , TagRec++ (Viswanathan et al., 2022) , and Poly-encoder (Humeau et al., 2019) . For ablations, built on the bi-encoder (BERT) method, we present three variants: Bi-encoder (BERT) + CEAA, Bi-encoder (DPR), and Bi-encoder (DPR) + CEAA, where the comparisons between the variants could highlight the contribution of transfer learning and CEAA.\n\nResults and Analysis\nOverall Accuracy: The main objective of this work is to improve the bi-encoder models for the purpose of better text classification in two aspects:\nMethods ARC QC-Science EURLEX57K\nR@1 R@3 R@5 R@1 R@3 R@5 RP@5 nDCG@5 BM25 0. transfer learning and CEAA. Regarding the effect of using two different pretrained models, the results from Table 1 show that models trained on DPR achieve higher performance than models from BERT. Specifically, Bi-encoder (DPR) outperforms the Bi-encoder (BERT) for ARC (0.54 > 0.51 in R@1) and QC-Science (0.69 > 0.67 in R@1). The performance of the EURLEX57K datasets in both RP@5 and nDCG@5 increases by 0.02. Applying our augmentation method to the Bi-encoder (both vanilla BERT and QA-finetuned BERT) improves the performance by 0.06, 0.02, and 0.03 points in ARC, QC-Science, and EURLEX57k, respectively. Additionally, the Bi-encoder (DPR) + CEAA demonstrates the highest overall performance in most cases (except for R@3 and R@5 of the QC-Science dataset where differences were small). For example, compared to TagRec++, which is the current state-of-the-art model on the datasets, we observed that our best model improves on TagRec++ by 0.05 points in R@1 2 . Figure 2 further demonstrates the change in RP@K and nDCG@K across a varying range of values for K on EURLEX57K, where CEAA shows consistently better performance. Notably, the gap from Bi-encoder (BERT) increases as K increases for both metrics.\nMulti-label Generalization: To further highlight differences between single-label and multilabel settings, the two best models, Bi-encoder (DPR) and Bi-encoder (DPR) + CEAA, were trained with a modified single-labeled EURLEX57K dataset, where we sampled only a single tag from the multi-label space. When the models are evaluated on the original multi-label dataset, as a context in the EURLEX57K dataset has \u2265 5 gold tags on average, it is important to achieve high nDCG@K performance on K \u2265 5. The results are presented in Figure 3 . We observe that the models show comparable performance with values of 0.65, 0.70, and 0.73 for Bi-encoder (DPR), Bi-encoder (DPR) + CEAA and BERT classification, respectively at K = 1. Though the classification model performs slightly better than CEAA at low K values, performance significantly degrades for K \u2265 5. Overall, the cross-encoder augmentation helped the model to better find related tags at the top rank. From these results, we argue that evaluating against the single-labeled dataset may not be an appropriate testing tool for comparing the auto-tagging models, as BERT classification was considered the best at first, even though it is poorly working on multilabel scenarios. This problem is critical as multilabel issues are prevalent in education. Specifically, we manually checked failure cases of both Bi-encoder (DPR) and Bi-encoder (DPR) + CEAA at top 1, to qualitatively examine which one is better at ranking the relevant tags. The results in Appendix B.2 show that Bi-encoder (DPR) + CEAA is able to retrieve better candidates than the Bi-encoder (DPR) more often. An interesting example is, given the context [\"The sector in which employees have more job security is an organized sector\"], where the gold tag is one related to the economy, the Bi-encoder (DPR) + CEAA returns a tag [\"human resources\"], which is sufficiently relevant but not labeled one. From these results, we once again confirm that the multilabel problem is severe in the auto-tagging tasks and that our model yields sufficiently significant results beyond the reported performance.\nData Efficiency: To identify the effectiveness of augmentation with low-resource labels, we measured nDCG@5 on the splits of labels based on their occurrence in training data. EURLEX57 considered the labels that occurred more than 50 times in the training set as frequent and few otherwise. We set the ARC dataset's threshold to 5. Figure 4 shows that both CEAA and transfer learning contribute to better performance for the frequent labels. Further, we observe that the retrieval methods are more effective for the rarely occurring tags than standard classification methods. Notably, in ARC of a smaller dataset than EURLEX57K (5K < 45K), the combination of CEAA and transfer learning, CEAA (DPR), achieves the best performance.\n\nConclusion\nIn this paper, we discuss the problem of 'autotagging' with regard to data scarcity due to its large label space -an issue that is critical in the education domain, but also for other domains with a multi-label structure such as jurisdictional or clinical contexts. We propose two innovations to address this problem: First, exploiting the knowledge of language models trained on large questionanswering datasets. Second, applying a novel augmentation for bi-encoder architecture inspired by cross-encoders to better capture the full interaction between inputs and labels while maintaining the bi-encoder's efficiency. A set of experiments demonstrated the effectiveness of our approach, especially in the multi-label setting. Future research will explore re-ranking scenarios in which the bi-encoder trained with our cross-encoding augmentation (CEAA) is re-used to effectively rerank the tags with cross-encoding mechanism as in (Nogueira and Cho, 2019) .\n6 Limitations\n", "hypothesis": " An extensive set of experiments shows that our proposed method is effective in multi-label scenarios and low-resource tags compared to state-of-the-art models..", "answer": true}
{"title": "With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness", "content": "\nIntroduction\nConditional language models suffer from a tendency to hallucinate information (Maynez et al., 2020) , resulting in generations that are not faithful to their input documents, which limits the trustworthiness of such models. This raises a need for automatic faithfulness metrics. In this context, models trained on natural language inference (NLI) (Bowman et al., 2015) are attractive since, intuitively, a generation being faithful implies it must be entailed by the source (Falke et al., 2019) . However, pure NLI models have seen mixed success in faithfulness evaluation (Falke et al., 2019; Kryscinski et al., 2020; Wang et al., 2020; Maynez et al., 2020) . While in recent evaluation on the TRUE benchmark (Honovich et al., 2022) , which contains datasets from knowledge-grounded dialogue, summarization and paraphrasing, NLIderived metrics perform best overall, they require impractically large models, or costly additional machinery such as question generation and answering models at inference, while still showing robustness issues. Thus we ask: What is still needed for pure NLI models to perform robustly across faithfulness datasets -while remaining cheap enough to serve as a lean and practical evaluation tool?\nWe enhance a relatively small NLI model to make it work robustly across tasks in three ways:\nTask-Adaptive Data Augmentation. In NLI, a hypothesis must be fully entailed by its supporting premise. However, in faithfulness, not all parts of the generation always need to be grounded. We identify an instance of this phenomenon in dialogue where parts of a turn can fulfill communicative functions such as hedging or establishing emotional connection and are often disregarded in faithfulness annotation. Hence, when applying NLI models to complete dialogue turns that may include statements irrelevant for grounding, we run a risk of producing incorrect unfaithfulness predictions.\nTo alleviate this issue, we propose a simple data augmentation method to adapt NLI models to genres where they need to be aware of statements that must be exempt from NLI-based faithfulness evaluation. Our approach is computationally attractive, as it avoids an increase of cost at inference time.\nIntegration of NLI Contradiction Scores. Existing NLI faithfulness metrics typically use the entailment score for their predictions (Honovich et al., 2022; Falke et al., 2019; Kryscinski et al., 2020) . However, Chen and Eger (2022) show that subtracting the contradiction score from the entail-ment score (referred to as e-c ) can improve NLI performance in certain evaluation tasks. We show that there also is a strong positive effect of e-c for faithfulness prediction, and demonstrate that this is due to a high contradiction probability being a more reliable predictor of unfaithfulness than low entailment probability.\nMonte-Carlo Dropout Inference. Applying NLI models to faithfulness prediction involves a domain shift from largely human-written data to automatically generated text. To make NLI model scores more robust under this shift, we propose to use Monte-Carlo dropout during inference (Srivastava et al., 2014) . This essentially creates a cheap ensemble and has been shown to deal better with noisy labels (Goel and Chen, 2021) . This approach leads to consistent score improvements in our tasks.\nThe combination of all modifications not only strongly improves over a baseline NLI model, but also outperforms all other metrics on TRUE, on average, while being cheaper and smaller. 1 2 Method Details\n\nTask-adaptive Data Augmentation\nTo illustrate that task requirements can be incompatible between faithfulness and NLI, consider the following instance from the Q2 dialogue corpus (Honovich et al., 2021) that is labelled as faithful:\nGrounding: American pancakes are similar to Scotch pancakes or drop scones. Generation: yes , i love american pancakes , they are like scotch pancakes From an NLI perspective, the generation is clearly not entailed, since the statement \"I love american pancakes\" is not supported by the input.\nTo better prepare an NLI system for such genre or task-specific cases, we manually curate a small list of statements that should not influence the faithfulness prediction. We augment NLI data from the ANLI corpus (Nie et al., 2020) by adding a randomly chosen phrase from this set to each instance, while preserving the label. We then train an already fine-tuned NLI model on a concatenation of these augmented samples and original ANLI data. For training details see Appendix A.\n1 All code is available at https://github.com/julmaxi/ with_a_little_push\n\nMonte-Carlo Dropout\nTo compute scores under Monte-Carlo dropout, we randomly sample k dropout masks and compute the average of the model predictions. We set k = 15, since preliminary experiments showed that performance did not profit from additional samples.\n\nExperimental Setup\nWe run experiments on TRUE (Honovich et al., 2022) , a benchmark that compiles a wide variety of faithfulness tasks in a standardized format. It contains summarization (Pagnoni et al., 2021; Maynez et al., 2020; Wang et al., 2020; Fabbri et al., 2021) , knowledge-grounded dialog (Honovich et al., 2021; Gupta et al., 2022; Dziri et al., 2022) 2 and paraphrasing (Zhang et al., 2019) datasets. 3 Following recommendations in TRUE, we evaluate using Area under the ROC Curve (AUC).\nAs our BASE model, we use the DeBERTa-large (He et al., 2020) model of Laurer et al. (2022) , trained on MultiNLI (Williams et al., 2018) , Fever-NLI (Thorne et al., 2018) , ANLI (Nie et al., 2020) , LingNLI (Parrish et al., 2021) and WANLI (Liu et al., 2022) . The metric All uses all three of our proposed modifications to Base. We also investigate a variant without MC dropout inference (-MC) as a more cost efficient alternative.\nWe compare to the strongest models on TRUE: T5 ANLI (Honovich et al., 2022 ) is a T5-11B (Raffel et al., 2020) model trained on ANLI. 4 SummacZS (Laban et al., 2022 ) evaluates an NLI model on all pairs of input and generated sentences and then averages maximum entailment probabilities for each generated sentence.\nQ2 (Honovich et al., 2021) combines a question generation/answering pipeline with an NLI score.\nFinally, Honovich et al. (2022) introduce a strong ensemble of these 3 methods (Eorig). To further verify our approach, we construct a new ensemble (Eour) by replacing T5 with All.\n\nResults\nTable 1 shows the AUC scores for each metric. Base on six out of nine corpora, but also significantly outperforms all other competitors on average, while being more computationally efficient.\nAs expected, we find the biggest gains in dialogue, where the All model even outperforms Eorig on 2 out of 3 corpora. We do not improve on BEGIN, which is likely due to bias in the dataset construction, which we elaborate on in Section 5.1. On the summarization part, All improves significantly over Base on 3 out of 5 corpora, while not significantly harming performance on any corpus. However, it still falls short of the best models in TRUE. The strong showing of T5 on these corpora suggests that this might be alleviated with a stronger base model.\nOverall, a very similar behaviour is exhibited by -MC, presenting an attractive option when the added overhead of multiple samples is undesirable.\nEour is on par with Eorig, despite massively reduced costs; it even significantly outperforms it on two dialog and the paraphrasing corpora.\nWe also investigate the performance of each individual modification to our model (Table 2 ). They all improve average scores, while only leading to a notable decrease on BEGIN for both e-c and dialogue augmentations and on MNBM for e-c .\nOutside of dialogue, we find that the augmentation methods have a positive impact on PAWS, as well as all summarization corpora that are at least partially based on summaries for the CNN/DM dataset (Hermann et al., 2015) (Frank, QAGS-C, and SummEval). While we do not have a definitive explanation for this phenomenon, we hypothesize that on these datasets our augmentations aid in making the model robust in the presence of noise or irrelevant context since our augmentations are label-neutral and must similarly be 'ignored' during training.\n\nEffect of Dialogue Adaptation\nWe investigate whether the improvements via our augmentation approach are indeed due to them improving the handling of personal statements.\nWe use the occurrences of the pronoun I in a generation as a proxy measure 5 and compute its correlation with human labels and metrics (see Table 3 ). On both Q2 and Dialfact, our proxy measure, while uncorrelated with human labels, is strongly correlated with the scores of both Base and T5. This indicates these metrics indeed tend to incorrectly reject generations with personal statements. All on the other hand reduces this dependency.\nOur results also help explain why negatively correlated with first person pronouns. This is likely due to a bias in dataset construction:\nThe BEGIN dataset used in TRUE has generations from two models, one of which is both more likely to generate pronouns and more likely to generate unfaithful output (see Appendix B).\n\nEffect of integrating contradiction scores\nTo isolate the effect of e-c we compare score distributions of Base and Base+e-c in Figure 1 . The lefthand side of the figure shows that in Base ca. 2700 faithful instances are predicted as non-entailed (i.e., e-score near 0), which implies they are labelled as contradictory or neutral. e-c , on the other hand, further differentiates these instances into instances with high contradiction (negative e-c score) and high neutral probability (e-c score near 0). We observe that almost all low-scoring faithful generations are classified as neutral, whereas nearly all instances that are classified as contradictory are indeed unfaithful. Where Base has no way to make use of this information, e-c allows to reliably label contradictory instances as unfaithful.\n\nCost comparison to other approaches\nThere is increasing awareness of the resource-hungriness of deep learning (Strubell et al., 2019) . Especially for faithfulness, cheap and reliable metrics are critical, given rising demands for NLG in research and industry. Table 5 : Results of our phrase selection robustness analysis. For each run, we sample five phrases, recreated our dataset and retrain our model. We repeat this process ten times and report the average, as well as the standard deviation, minimum and maximum scores of the runs.\nSmall numbers indicate difference to the original scores. All results were computed using e-c and MC dropout.\nFor better comparison, we also report the scores of a model without any augmentation (i.e. without any additional training) with e-c and MC dropout.\nrequires fewer parameters than any other metric, including a more than 30x reduction compared to T5. During inference our model always requires a constant number of calls which can be reduced to a single call when ablating MC dropout. On the other hand, the number of calls in SummacZS scales with the number of input and output sentences. Q2 needs to generate questions by calling an auto-regressive QG model n times, where n factors in the amount and length of questions (#Q\u00d7Ql), answer #Q questions with the QA model and finally check #Q answers with an NLI model (#Q \u00d7 2).\nIn sum, our model compares favourably with other approaches, while also allowing for a performance/cost tradeoff by forgoing MC dropout.\n\nPhrase Selection Robustness\nTo ensure that our augmentation is robust and not overly reliant on any particular choice of phrases, we repeat our dataset augmentation process multiple times with five randomly chosen augmentation phrases out of the original ten. We sample ten such datasets and retrain our model for each. Table 5 shows the average score, minimum and maxi-mum score, as well as the standard deviation of the scores. We also report results of a model with both MC dropout and e-c but without any additional training and augmentations to directly quantify whether the augmentations are still helpful in their reduced form. This corresponds to applying MC dropout and e-c to Base.\nAs expected, we find that reducing the variety of available phrases leads to a drop in performance across almost all datasets, compared to All. The only exception is BEGIN, where we instead see a slight improvement. This is likely to be related to the construction of BEGIN (see the discussion in Section 5.1).\nWhen comparing our limited augmentation models to the non-augmented model, we find that they still outperform the non-augmented model in almost all cases. In particular for Q2 and DialFact, for which we expect the strongest impact of our augmentations, we find that even the worst run still outperforms non-augmented model. This suggests that our augmentations can robustly adapt the model to the dialogue task.\nFinally, we observe a relatively large drop in scores for all datasets that are at (least partially) derived from CNN/DM (Frank, SummEval and QAGS-C). This mirrors our earlier observation in Section 4 that these datasets profit from our augmentation procedure.\n\nRelated Work\nPrevious work on the utility of NLI for faithfulness led to mixed conclusions. In summarization, Falke et al. (2019) and Kryscinski et al. (2020) find out-of-the-box models have only limited utility in a faithfulness setting. In Wang et al. (2020) , an NLI model is outperformed by a question generation/answering (QA/QG)-based method. In contrast, Maynez et al. (2020) find that a similar NLI model vastly outperforms a QA/QG metric on their data. In knowledge-grounded dialogue, Dziri et al. (2022) , Gupta et al. (2022) and Honovich et al. (2021) find out-of-the-box models underperform.\nTo improve NLI models for faithfulness in summarization, Kryscinski et al. (2020) propose FactCC, which is trained on artificially noised summaries. Utama et al. (2022) propose a controllable generation model to generate artificial faithfulness data. In knowledge-grounded dialogue, Dziri et al. (2022) and Gupta et al. (2022) combine noising techniques to generate additional training data for NLI-based faithfulness models. In contrast to our work, these approaches a) generate training data from external sources, instead of directly augmenting NLI data, and b) do not explicitly focus on reconciling differences between NLI and faithfulness with their augmentation. Outside of augmentationbased approaches, Goyal and Durrett (2020) propose to train NLI models to label faithfulness at the dependency arc level.\n\nConclusion\nWe have demonstrated that with a small number of focused adaptations, even a relatively small NLI model can robustly predict faithfulness. We have:\n1. Shown that NLI-based metrics can be incompatible with task-specific requirements and identified and fixed one such incompatibility in dialogue with an augmentation strategy.\n2. Demonstrated the importance of contradiction probability for scoring and that the underlying mechanism is the high reliability of NLI contradiction scores for detecting unfaithfulness 3. Shown that using Monte-Carlo dropout improves metric performance.\nOur improved NLI model significantly improves over its baseline across many corpora and outperforms all competitors in average score on TRUE, while being much more efficient at inference. Our work suggests that strong improvements are possible for NLI-based faithfulness metrics, by combining data augmentation with adapted NLI score computation. We hope this finding will spurn advances in cheap and robust NLI for faithfulness. unclear whether our results would still provide advantages when applied to larger models such as T5-11B, whose parameter count makes experimentation infeasible on the hardware available to us.\n", "hypothesis": "Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach slightly improves a vanilla NLI model and somewhat outperforms previous work, while showing moderate computational cost.", "answer": false}
{"title": "Do transformer models do phonology like a linguist?", "content": "\nIntroduction\nIn computational linguistics, neural networks have occupied much of recent work. One prime driver is adaptability to multiple facets of linguistic phenomena. As an example, sequence-to-sequence models have been shown to capture inflection patterns across numerous languages (Kodner et al., 2022) . While their performance represents significant advances, the abstractions generated during the modelling process warrant further investigation. We experiment with phonological processes on a constructed language to compare the generalisations learned by transformer models with widespread linguistic phenomena.\nIn particular, we address the following questions:\n\u2022 Learning specific phonological processes (are some more difficult than others?)\n\u2022 Categorisation (can the model generalise a category, vowels, consonants, specific consonant groups, e.g. plosives?)\n\u2022 Is word structure (syllables) implicitly learned?\nWe establish that the transformer model successfully models all 29 phonological phenomena we consider, regardless of linguistic complexity. Our results show that the model can generalise to linguistic categories with some caveats. By examining the transformer model's generalisation of haplology, we show that the model appears to learn syllables; the model can recognise the difference between VC and CV and generate previously unseen CV sequences.\n\nRelated Work\nInvestigating the cognitive reality of linguistic categories defined within phonology has long been of interest to linguistics. Does the natural class of phonemes bear any significance to a cognitive reality? For example, a series of experiments (Finley and Badecker, 2009; Chambers et al., 2010; Skoruppa and Peperkamp, 2011) examine the natural class of vowels and whether phonological patterns can be extended to previously unseen vowels. The studies suggest that participants were mostly able to generalise. In a similar vein, Finley (2011) presents a study on consonant harmony. The results suggest that learners (human learners) can generalise to novel consonants when the phonological pattern is general. However, the learners failed to generalise when the rule triggering the consonant harmony pattern was highly specific.\nWe adapt this long-standing linguistic question to ask whether Transformer-based abstractions are linguistically informed. Our experiment setup swaps the human learner with the Transformer architecture. Previous studies investigating phonological phenomena with Transformers include Elsner (2021) , where Transformers can handle reduplication and gemination. To an extent, 1 the SIG-MORPHON shared tasks (Kodner et al., 2022 ) also demonstrate the capacity of Transformers to represent phonological processes through capturing allomorphs conditioned by phonological environments.\nThere have been extensive studies on various phonological processes and RNNs. Haley and Wilson (2021) shows that encoder-decoder networks (specifically LSTM and GRU architectures) can learn infixation and reduplication. Mirea and Bicknell (2019) explores whether phonological distinctive feature information is required for learning word-level phonotactic generalisations using LSTMs. The authors find that information about phonological features hinders model performance, and phonotactic patterns are learnable from the distributional characteristics of each segment alone. Moreover, distributional information proves to be integral in recovering phonological categories (Mayer, 2020) .\nAnother way to investigate neural architecture abstractions is to probe the model internally. Silfverberg et al. (2021) examines whether RNN states encode phonological alternations through experiments on Finnish consonant gradation. The authors show that the models often encode consonant gradation in a select number of dimensions. Rodd (1997) probes the hidden states of an RNN model which controls Turkish vowel harmony. Similarly, Silfverberg et al. (2018) establish a correlation between embedding representations and distinctive phonological features for Finnish, Spanish and Turkish. This paper focuses on a model-external interrogation of Transformer generalisations by studying the predictions produced.\n\nLanguage Design\nThe phonological phenomena in question are tested on a constructed language. The primary motivation for this is to allow for a controlled experiment and ensure that we can generate enough samples of the required phonological environments for rules to be triggered and thus observed. With this in mind, we require the constructed language to be as representative as possible of natural language. Therefore, key features were chosen based on the condition of being the most typologically common ones (Maddieson, 1984; Ladefoged and Maddieson, 1996; Maddieson, 2013) . The main characteristics are listed in Table . 1.\n\nGenerating a lexicon\nThe most complex syllable structure possible in the language is CCVVCC and the simplest one is V. Since our language design aims to generate a synthetic lexicon, we also control for word length distribution. Previous works have shown that word length over word types exhibits a roughly Gaussian distribution with a mean in the range [7, 10], depending on the language (Smith, 2012) . We have chosen a mean word length of 8.\nAn additional constraint when generating a lexicon is the sonority sequencing principle (SSP) (Selkirk, 1984; Clements, 1990) . Syllable structures tend to be highly influenced by the sonority scale, with the general rule that more sonorous elements are internal (i.e., close to the nucleus) and less sonorous elements are closer to the syllable edge. Therefore, we use a sonority metric to avoid generating implausible consonant clusters, with the onset and coda requiring opposite values on the metric, i.e. increasing sonority in the onset and decreasing in the coda.\n\nData 2\nOur data preparation follows three steps: lexicon generation, triplet (lemma, tag, surface form) formation via the finite-state tool foma (Hulden, 2009) and, finally, sampling of these triplets ac-cording to the experiment at hand and formatting for Fairseq. (Ott et al., 2019) 3 We train the model as a standard 'inflection' task (Kodner et al., 2022) , but with tags being identifiers of the processes that are to be triggered instead of morphosyntactic information. For example, the input sequence moupi#GEMINATION would be paired with the output mouppi. More example triplets are shown in Table 2 . 4 \n\nInput Tag\nOutput Lexicon generation entails generating viable syllable structures and filling these abstract structures using vowel and consonant inventories. The syllables are concatenated n times, where n is an integer between 1 and 10. We sample from this uniform distribution to produce a Gaussian distribution for word length with a mean of 8 symbols.\nateiSa #APOCOPE ateiS enpanka #APHAERESIS npanka a:N\u00c3 #SHORTENING aN\u00c3 vepisk #LENGTHENING vepi:k moupi #GEMINATION mouppi aimggi #DEGEMINATION aimgi soute #INTERVOCALIC soude refend #DEVOICE refent ketedu #METATHESIS kedetu totoN #HAPLOLOGY toN pima #COPY pima\nWe include a COPY tag, where the input is copied to the output, to negate any performance drop by the model when unseen lemmata are encountered (Liu and Hulden, 2022) . In other words, the model, at test time, will never encounter a completely unseen lemma on which to perform a phonological change, since it will always have witnessed at least an input-output pair of any lemma used that is simply copied to the output.\n3 See B for model details. 4 Our nomenclature of sound changes follows Campbell (2013) . \n\nModelling common phonological processes with varying degrees of complexity\nIn this experiment, we establish that seq2seq models can successfully capture a range of phonological processes, including more complex rules such as metathesis. As seen in Figure 1 , the transformer model performs reasonably well across all phonological phenomena, with little distinction between the complexity of the process considered.\n6 Linguistic Category generalisation The results show that p is transformed to a b 77.6% of the instances. Where the conversion does not take place, errors typically follow the pattern of, e.g. outputting epeiSe instead of ebeiSe with the input epeiSe\nTo investigate the comparatively low performance. We compare word-initial devoicing with word-initial voicing as a priming process. The results are summarised in Table . 4. The accuracy of the predictions for the unseen p was substantially lower in the case of word-initial voicing (40%) compared with the word-initial devoicing (74.8%). Interestingly, word-initial voicing involves the same process as intervocalic voicing (p>b), with only different environments triggering the process.\n\nWord-internal representations\nTo test whether seq2seq models can learn a representation of word-internal structures, such as syllables, we experiment with examples of haplology. Haplology (tatasa > tasa) is the process in which a repeated sequence of sounds is simplified to a single occurrence. For example, if the word haplology were to undergo haplology, it would reduce the sequence lolo to lo, haplology > haplogy.\nIn this experiment, we include two additional processes so the model can witness the contrast between vowels and consonants separately: (1) wordfinal vowel deletion and (2) word-final consonant deletion. To test the generalisation capacity of the model, at test time, we include the following withheld cases: unseen CVCV structures-i.e. cases where haplology should apply, but the specific CVCVsequence is never seen in the training data; words where haplology occurs more than once; and VCVC structures to see if the model (erroneously) learns to delete any repeating sequence of symbols. In our experiment, we withhold from the training set the following CVCV-sequences: dede, fofo, kuku, wowo, baba, vivi, papa, titi, soso, momo, nene, rere, lili, SuSu, jiji, \u00d9u\u00d9u, NaNa, gugu.\n\nProcess\nNote that haplology includes both cases where haplology applies and does not since the input word may or may not contain a CVCV-sequence where the two CVs are identical.\nTable 7 summarises the results obtained. The model shows high accuracy for the supplementary word-final vowel and consonant deletion processes. We separate the haplology cases further into specific test cases. Our results from the unseen CVCV category show strong evidence for model generalisation of CV structures. We further tested the same model on a separate test set consisting of VCVC structures. We see that for approximately 78% of the set, it correctly recognises these cases as incorrect conditions for haplology. In the remaining instances, the model does show a rare over-generalisation to sometimes delete repeating sequences regardless of the characteristics of the sequence.\nThe largest source of error within the haplology cases is the scenario in which haplology can be applied twice within the same word. In these cases, typically, the first case of repeating CV is deleted, and the second instance remains untouched, as when outputting fuejaja with input fufuejaja, instead of the gold fueja.\n\nConclusion\nThe transformer model successfully models all 29 phonological phenomena with slight variation across phenomenon complexity. Our results show that the model can generalize linguistic categories and structures. Through haplology, we show that the model appears to learn to recognize and generalize syllabic structure and is capable of recognizing the difference between VC and CV and can also generalize the transformation triggered by haplology to unseen CV sequences.\n", "hypothesis": " Our results show that the transformer model can successfully model all 29 phonological phenomena considered, regardless of perceived process difficulty.  We also show that the model can generalise linguistic categories and structures, such as vowels and syllables, through priming processes..", "answer": true}
{"title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning", "content": "\nIntroduction\nSequence-to-sequence learning (Seq2Seq) with neural networks (Sutskever et al., 2014) has advanced the state-of-the-art in various NLP tasks, e.g. translation (Bahdanau et al., 2015; Vaswani et al., 2017) , summarization (Cheng and Lapata, 2016) , and grammatical error correction (Yuan and Briscoe, 2016) . Generally, Seq2Seq models are trained with the cross-entropy loss, which equally weighs the training losses of different target tokens.\nHowever, due to the token imbalance nature (Piantadosi, 2014) and the truth that different tokens contribute differently to the sentence meaning (Church and Hanks, 1990; Chen et al., 2020) , Figure 1 : An example to illustrate the changing token difficulties in different training steps in WMT'14 En-De. The token \"abschlie\u00dfen/ Sache\" is hard/ easy to learn at 50K while the trend is totally reversed at 100K. several works are developed to reweigh the tokenlevel training loss according to explicit (e.g. frequency) or implicit (uncertainty estimated by offthe-shelf language models) priors (Gu et al., 2020; Xu et al., 2021; Zhang et al., 2022a) . For example, Gu et al. (2020) proposed two heuristic criteria based on word frequency to encourage the model to learn from larger-weight low-frequency tokens. Zhang et al. (2022a) introduce target-context-aware metric based on an additional target-side language model to adjust the weight of each target token.\nDespite some success, there are still limitations in these adaptive training approaches. First, most of them predetermine the difficult tokens and fix such prior to guiding the training. However, in our preliminary study, we find the hard-to-learn tokens are dynamically changing during training, rather than statically fixed. As shown in Figure 1 , as the training progress goes, although the sentence-level loss is nicely converging, the difficult token is changing from \"abschlie\u00dfen\" to \"Sache\" in terms of the token-level loss. Second, these adaptive training methods overly emphasize fitting the difficult tokens' one-hot labels by reweighing the loss, which empirically may cause overfitting and limit the generalization (Norouzi et al., 2016; Szegedy et al., 2016; Xiao et al., 2019; Miao et al., 2021) . Also, a more recent study (Zhai et al., 2023) provides theoretical evidence to support that reweighting is not that effective to improve the generalization.\nCorrespondingly, we design a simple and effective Token-Level Self-Evolution Training (SE) strategy to encourage Seq2Seq models to learn from difficult words that are dynamically selected by the model itself. Specifically, SE contains two stages: \u2776self-questioning and \u2777self-evolution training. In the first stage, the Seq2Seq models dynamically select the hard-to-learn tokens based on the tokenlevel losses, then we encourage the Seq2Seq models to learn from them in the second stage, where, rather than adopting reweighing, we introduce a novel token-specific label smoothing approach to generate easily digestible soft label, which considers both the ground truth and model's prediction.\nExperiments across tasks, language pairs, data scales, and model sizes show that SE consistently and significantly outperforms both the vanilla Seq2Seq model and the re-implemented advanced baselines. Analyses confirm that besides improved lexical accuracy, SE generates diverse and humanlike generations with better model generalization.\n\nMethodology\nPreliminary Sequence-to-sequence (Seq2Seq) learning aims to maximize the cross-entropy (CE) loss of the log-likelihood of each target word in y = {y 1 , . . . , y N }, conditioned on source x, where the optimization treats all tokens equally:\nEQUATION\nHowever, due to the different learning difficulties of each token, it is sub-optimal to treat all tokens equally (Gu et al., 2020) . To address this limitation, a series of token-level adaptive training objectives were adopted to re-weight the losses of different target tokens (Xu et al., 2021; Zhang et al., 2022a) . The common goal of these methods is to facilitate the model training by fully exploiting the informative but underexplored tokens.\nHowever, our preliminary study shows that the hard tokens are dynamically changing (see Figure 1 ) in different training steps (or model structures), thus it is sub-optimal to employ static token priors (e.g. frequency) during training. Also, recent studies (Zhai et al., 2023) in the ML community theoretically show that reweighting is not that effective to improve the generalization. Based on the above evidence, we present the self-evolution learning (SE) mechanism to encourage the model to adaptively and wisely learn from the informative yet under-explored tokens dynamically determined by the model itself (Stage\u2776 in \u00a72.1), with an easy-tolearn label distribution (Stage\u2777 in \u00a72.1). A similar work to ours is Hahn and Choi (2019) . However, their method mainly considers the situation where the predicted answer is incorrect but close to the golden answer, while our method focuses on all dynamic hard tokens.\n\nToken-Level Self-Evolution Learning\n\u2776 Self-questioning Stage. The goal is to select the hard-to-learn tokens that are questioned by the Seq2Seq model itself during training dynamics. Previously, these difficult tokens are predetermined by external models or specific statistical metrics. However, inspired by the finding of dynamic change of difficult tokens during the training stage as shown in Figure 1 and the finding that the trained model contains useful information (Li and Lu, 2021) , e.g. synonym, we propose to straightforwardly leverage the behavior of the model to dynamically select target tokens. In practice, we first calculate the token-level CE loss, denoted as {l 1 , l 2 , ..., l n }, for each token for each forward pass. Then we set a loss threshold \u0393 and select the tokens whose losses exceed \u0393 as the target tokens, i.e., D = {t i |l i > \u0393} where i \u2208 N = {1, 2, ..., n}.\n\u2777 Self-evolution Training Stage. After selecting the difficult tokens, we encourage the model to carefully learn from them. Given the theoretical shortage (Zhai et al., 2023) and potentially caused overfitting or overconfidence problem (Miao et al., 2021) of reweighting and deliberately learning from difficult tokens, we propose to strengthen the learning from these tokens with a newly designed Token-specific Label Smoothing (TLS) approach. Specifically, motivated by the effect of label smoothing (LS) regularization (Szegedy et al., 2016) , we combine the ground truth p i and the model's prediction pi to form a new soft label p i for the i-th token. Then we use p to guide the difficult tokens D, while leaving label-smoothing CE loss for the other tokens. It is worth noting that we also apply the traditional label smoothing technique to pi to activate the information in the predicted distribution. Analogous to human learning, it is often easier for humans to grasp new things described by their familiar knowledge (Reder et al., 2016) therefore the new soft label fused both accurate ground truth and model's self-distribution is easily digestible. Mathematically, for difficult tokens t i , p i is formulated as:\nEQUATION\n)\nThen we calculate the losses of difficult tokens and the others, and combine the two losses:\nEQUATION\nwhere i \u2208 D and j \u2208 N \\ D.\n\nEvaluation\nMachine Translation on three widely-used benchmarks (Ding et al., 2020 (Ding et al., , 2021c (Ding et al., , 2022)) : smallscale WMT16 English-Romanian (En-Ro; 0.6M), medium-scale WMT14 English-German (En-De; 4.5M), and large-scale WMT14 English-French (En-Fr; 36.0M). We implement the baselines and our approach under Transformer-base settings. We follow the previous adaptive training approach (Gu et al., 2020) to pretrain with the cross-entropy loss with N steps, and further finetune the same steps with different adaptive training objectives, including Freq-Exponential (Gu et al., 2020) , Freq-Chi-Square (Gu et al., 2020) , D2GPo (Li et al., 2020) , BMI-adaptive (Xu et al., 2021) , MixCrossEntropy (Li and Lu, 2021) , CBMI-adaptive (Zhang et al., 2022a) , and SPL (Wan et al., 2020) . For N , we adopt 100K and 30K for larger datasets, e.g. En-De and En-Fr, and small dataset, i.e. En-Ro, respectively. We empirically adopt 32K tokens per batch for large datasets, the learning rate warms up to 1e-7 for 10K steps, and then decays 90K, while for small dataset En-Ro, The learning rate warms up to 1e-7 for 4K steps, and then decays 26K steps. All the experiments are conducted on 4 NVIDIA Tesla A100 GPUs. The SacreBLEU (Post, 2018) was used for evaluation. Besides translation, we also follow previous works (Liu et al., 2021b; Zhong et al., 2022; Zhang et al., 2022b) to validate the universality of our method on more sequenceto-sequence learning tasks, e.g., summarization and grammatical error correction.\nText Summarization on XSUM corpus (0.2M). We follow fairseq (Ott et al., 2019) to preprocess the data and train the model, then finetune them for the same steps. We evaluated with the ROUGE (Lin, 2004) , i.e. R-1, R-2, and R-L.\n\nGrammatical Error Correction on CoNLL14\n(1.4M). We follow Chollampatt and Ng (2018) to preprocess the data and train the model, then finetune them for the same steps. The MaxMatch (M 2 ) scores (Dahlmeier and Ng, 2012) were used for evaluation with precision, recall, and F 0.5 values.\n\nMain Results\nSE brings gains across language pairs and scales.\nResults on machine translation across different data sizes ranging from 0.6M to 36M in Table 1 show that our SE-equipped Transformer \"+ Self-Evolution (ours)\" 1) considerably improves the performance by averaging +0.92 BLEU points; 2) out-\nValid Loss Scale 0-1 1-2 2-3 >3\nTransformer 63.3 10.5 6.7 19.5 + SE 65.6 9.5 5.8 19.1 4 show that our method can achieve +0.4 and +1.2 improvement in BLEU and COMET respectively, which proves that our SE also works on extremely large datasets.\n\nAnalysis\nWe provide some insights to better understand the effectiveness of our approach. The ablation of important modules and parameters is in Appendix A.\n\nSE learns better token representation.\nTo verify whether our method helps learn better tokens representation, we conduct analysis on WMT14 En-De from learning loss and fine-grained generation perspectives, respectively. First, we count the token ratios distributed in different cross-entropy loss scales in Table 3 following Zan et al. (2022a) . Cross-entropy is a good indicator to quantify the distance between the predicted distribution and the ground truth in the valid dataset, and a lower value means a more similar distribution. As shown, our method improves the low-loss token ratios by +2.3%, indicating SE helps the model learn better token representations by reducing the token uncertainty. In addition, we follow Ding et al. (2021a) ; Liu et al. (2021a) to break the translation down into different granularities and measure their fined-grained performance. In particular, we calculate 1 the F-measure of words by different frequency buckets and BLEU scores of buckets of different lengths in Figure 2 . We see SE achieves better performance in all frequencies and sentence buckets, demonstrating our method can improve the performance of different granularities.\nSE encourages diverse generations. Lacking generation diversity is a notorious problem for Seq2Seq learning tasks (Sun et al., 2020; Lin et al., 2022) . Benefiting from better exploring the model's prediction with corrected soft labels, SE is expected to improve generation diversity. We follow Wang et al. (2022) to examine this by analyzing the performance in an additional multiplereference test of WMT'14 En-De (Ott et al., 2018) . We choose additional references for each of the 500 test sentences taken from the original test. Table 5 shows SE consistently outperforms the baseline with the average improvement being 0.9/1.0 BLEU, which indicates that our SE can effectively generate diverse results. SE enhances model generalization. Benefiting from better hard token exploration, SE-equipped Transformers are expected to own better generalizations. We examine it by testing on domain shift scenarios following Ding et al. (2021b) . In particular, we evaluate WMT14 En-De models over four out-of-domain test sets (M\u00fcller et al., 2020) in Table 6 and find that SE improves the translation by averaging +0.9 BLEU points, showing a better lexical generalization ability.\nSE encourages human-like generations. We design two types of evaluation on WMT14 En-Fr: 1) AUTOMATIC EVALUATION with COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) , which have a high-level correlation with human judgments. 2) HUMAN EVALUATION with three near-native French annotators who hold DALF C2 certificate 2 . Specifically, for human evaluation, we randomly sample 50 sentences from the test set to evaluate the translation adequacy and fluency, scoring 1\u223c5. For adequacy, 1 represents irrelevant to the source while 5 means semantically equal. For fluency, 1 means unintelligible while 5 means fluent and native. Table 7 shows the automatic and human evaluation results, where we find that our SE indeed achieves human-like translation.\n\nConclusion\nIn this paper, we propose a self-evolution learning mechanism to improve seq2seq learning, by exploiting the informative-yet-underexplored tokens dynamically. SE follows two stages, i.e. selfquestioning and self-evolution training, and can be used to evolve any pretrained models with a sim- ple recipe: continue train with SE. We empirically demonstrated the effectiveness and universality of SE on a series of widely-used benchmarks, covering low, medium, high, and extremely-high data volumes.\nIn the future, besides generation tasks, we would like to verify the effectiveness of SE on language understanding tasks (Wu et al., 2020; Zhong et al., 2023) . Also, it will be interesting to design SEinspired instruction tuning or prompting strategy like Lu et al. (2023) to enhance the performance of large language models, e.g. ChatGPT 3 , which after all have already been fully validated on lots of conditional generation tasks (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023; Wu et al., 2023) .\n", "hypothesis": "Empirically, SE yields consistent and significant improvements in three tasks, i.e. machine translation, summarization, and grammatical error correction. Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks. Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization. Additionally, SE also outperforms the re-implemented advanced baselines in terms of computational efficiency, making it a more efficient training strategy.", "answer": false}
{"title": "Evaluating Paraphrastic Robustness in Textual Entailment Models", "content": "\nIntroduction\nRecognizing Textual Entailment (RTE), the task of predicting whether one sentence (hypothesis) would likely be implied by another (premise), is central to natural language understanding (NLU; Dagan et al., 2005) , as this task captures \"all manners of linguistic phenomena and broad variability of semantic expression\" (MacCartney, 2009) . If an RTE model has a sufficiently high capacity for reliable, robust inference necessary for full NLU (Mac-Cartney, 2009) , then the model's predictions should be consistent across paraphrased examples.\nWe introduce P aRT E, a test set to evaluate how reliable and robust models are to paraphrases (Table 1 includes an example). The test set consists of examples from the Pascal RTE1-3 challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) rewritten with a lexical rewriter and manually verified to preserve the meaning and label of the original RTE sentence-pair. We use this evaluation set to determine whether models change their predictions when examples are paraphrased.\nWhile this may not be a sufficient test to determine whether RTE models fully understand language, as there are many semantic phenomena that RTE models should capture (Cooper et al., 1996; Naik et al., 2018) , it is necessary that any NLU system be robust to paraphrases.\n\nP\nThe cost of security when world leaders gather near Auchterarder for next year 's G8 summit, is expected to top $150 million. P' The cost of security when world leaders meet for the G8 summit near Auchterarder next year will top $150 million.\n\nH\nMore than $150 million will be probably spent for security at next year's G8 summit. H' At the G8 summit next year more than $150 million will likely be spent on security at the event.\nTable 1 : An original and paraphrased RTE example.\nThe top represents an original premise (P) and its paraphrase (P'). The bottom depicts an original hypothesis (H) and its paraphrase (H'). A model robust to paraphrases should have consistent predictions across the following pairs: P-H, P'-H, P-H', and P'-H'.\nOur experiments indicate that contemporary models are robust to paraphrases as their predictions do not change on the overwhelmingly large majority of examples that are paraphrased. However, our analyses temper this claim as models are more likely to change their predictions when both the premise and hypothesis are phrased compared to when just one of the sentences is rewritten. We release P aRT E 1 to encourage others to evaluate how well their models perform when RTE examples are paraphrased.\n\nRelated Work\nWith the vast adoption of human language technology (HLT), systems must understand when different expressions convey the same meaning (paraphrase) and support the same inferences (entailment). Paraphrasing and entailment are closely connected as the former is a special case of the latter where two sentences entail each other (Nev\u011b\u0159ilov\u00e1, 2014; Fonseca and Alu\u00edsio, 2015; V\u00edta, 2015; Ravichander et al., 2022) . Para-phrasing has been used to improve RTE predictions (Bosma and Callison-Burch, 2006; Sun et al., 2021) and RTE has been used for paraphrase identification (Seethamol and Manju, 2017) and generation (Arora et al., 2022) . Furthermore, both phenomena are key to NLU (Androutsopoulos and Malakasiotis, 2010) and work such as Zhao et al. (2018) ; Hu et al. (2019) have explored rewriting RTE examples to create more robust models.\nWe follow a long tradition of evaluating linguistic phenomena captured in RTE models (Cooper et al., 1996) . Recent tests focus on evaluating how well contemporary RTE models capture phenomena such as monotonicity (Yanaka et al., 2019a,b) , verb veridicality (Ross and Pavlick, 2019; Yanaka et al., 2021) , presuppositions (Parrish et al., 2021) implicatures (Jeretic et al., 2020) , basic logic (Richardson et al., 2020; Shi et al., 2021) , figurative language (Chakrabarty et al., 2021) , and others (Naik et al., 2018; Poliak et al., 2018a; Vashishtha et al., 2020) . Unlike many of those works that evaluate models' accuracy on examples that target specific phenomena, we use a contrastive approach (Prabhakaran et al., 2019; Gardner et al., 2020) to determine whether RTE models' predictions change when examples are paraphrased.\n\nP aRT E\nTo explore whether these RTE models are robust to paraphrases, we create P aRT E, a modified version of the Pascal RTE1-3 challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007) . P aRT E contains 1,126 examples of an original unmodified RTE sentence-pair grouped with a sentence-pair with a modified premise, hypothesis, or both. We use the examples in RTE1-3 to create our test set, as opposed to other RTE datasets due to its long-standing history.\n\nParaphrase Generation & Verification\nFor each RTE premise-hypothesis pair (P-H), we created three paraphrased premises (P') and hypotheses (H') using a T5-based paraphraser 2 finetuned on the Google PAWS dataset (Zhang et al., 2019) . To ensure lexically diverse paraphrases, we filter out any paraphrases that have high lexical overlap with the original sentences using Jaccard index threshold of 0.75. Out of 14,400 generated sentences, 2,449 remained -956 paraphrased premises (P') and 1,493 paraphrased hypotheses (H'). Next, we retained 550 paraphrased premises and 800 paraphrased hypotheses paraphrases that crowdsource workers identified as grammatical and similar in meaning to the original sentences. 3 We include a grammatical check since an existing RTE evaluation set focused on paraphrases (White et al., 2017) contains hypothesis-only biases related to grammaticality (Poliak et al., 2018b) .\nIf at least one P' or one H' passes this filtering process, we retain the original RTE example and pair it with a corresponding paraphrased example (i.e. P'-H', P'-H, or P-H'). In the case where more than one P' or H' passes the filtering, we retained the P' or H' that crowdsource workers deemed most similar to the original sentence. Out of the original 2,400 RTE test pairs, we retain 914 pairs with a high-quality P' or H', resulting in 1,178 original and paraphrased RTE pairs. 4\n\nOvercoming Semantic Variability\nMacCartney (2009) argues that in addition to being reliable and robust, RTE models must deal with the broad variability of semantic expression. In other words, though two sentences may be semantically congruent, it is possible that small variations in a paraphrased sentence contain enough semantic variability to change what would likely, or not likely be inferred from the sentence. Despite all P' and H' being deemed to be semantically congruent with their corresponding original sentences, the semantic variability of paraphrases might change whether H or H' can be inferred from P' or P.\nTherefore, propagating an RTE label from an original sentence pair to a modified sentence pair might be inappropriate. We manually determined that this issue occurs in just 52 (4%) examples, and retained 1,126 examples. This ensures an evaluation set of high-quality examples that can be used to determine whether models are sensitive to paraphrases and change their prediction on paraphrased examples. Our dataset contains 402 examples with just a paraphrased premise P', 602 with just a paraphrased hypothesis H', and 122 with both a paraphrased premise and hypothesis. \n\nExperimental Setup\nWe explore models built upon three different classes of sentence encoders: bag of words (BoW), LSTMs, and Transformers. Our BoW model represents premises and hypotheses as an average of their tokens' 300 dimensional GloVe embeddings (Pennington et al., 2014b) . The concatenation of these representations is fed to an MLP with two hidden layers. For the BiLSTM model, we represent tokens with GloVe embeddings, extract sentence representations using max-pooling, and pass concatenated sentence representations to an MLP with two hidden layers.\nOur transformer-based models are pre-trained BERT (Devlin et al., 2019) and Roberta (Liu et al., 2020) encoders with an MLP attached to the final layer. Additionally, we use GPT-3 in a zero-shot setting where we ask it to label the relationship between a premise and hypothesis. 5 The RTE training sets do not contain enough examples to train deep learning models with a large number of parameters. We follow the common practice of training models on MNLI and using our test set to evaluate how well they capture a specific phenomenon related to NLU. During testing, we map the MNLI 'contradiction' and 'neutral' labels to the 'not-entailed' label in RTE, following common practice (Wang et al., 2018; Yin et al., 2019; Ma et al., 2021; Utama et al., 2022, inter ailia) .\n\nResults\nTable 2 report the results. The RTE and P aRT E columns respectively report the models' accuracy on the 1,126 unmodified and paraphrased sentence pairs. 6 Comparing the difference in accuracy be-5 See Appendix A for more details, including hyperparameters, model sizes, and GPT-3 prompt design and configurations. Our code is available at https://github.com/ stonybrooknlp/parte 6 Although there are just 914 unmodified sentence pairs, for the sake of a head-to-head comparison, we retain all instances tween unmodified and paraphrased examples can be misleading. If the number of times a model changes a correct prediction is close to the number of times it changes an incorrect prediction, then the accuracy will hardly change. Figure 1 demonstrates why the accuracies do not change by much when models' predictions change on paraphrased examples. Furthermore, if a model is robust to paraphrases, then it should not change its predictions when an example is paraphrased, even if the prediction on the original unmodified example was incorrect. Hence, our test statistic is the percentage of examples where a model's predictions change (% \u2206 P aRT E column in Table 2 ) rather than a change in accuracy. Compared to the Transformer based models, the BoW and BiLSTM models seem to be more sensitive, and less robust to paraphrasing, as they change their predictions on 15.27% and 16.69% respectively of the 1,126 examples. However, this might be associated with how word xembedding models only just outperform random guesses in and perform much worse on RTE compared to the Transformer models.\nof the unmodified sentence pairs when computing accuracy. Focusing on the Transformer models, we noticed that RoBERTa performs the best on the datasets and is the most robust to paraphrasing -changing its predictions on just under 8% of paraphrased examples. Interestingly, when the models are trained specifically to perform this task, the models change their predictions on fewer paraphrased examples as these models' accuracy increases. However, improving performance alone might not automatically improve models' robustness to paraphrases. GPT-3's accuracy noticeably outperforms BERT's accuracy, but GPT-3 changes its predictions on more paraphrased examples compared to BERT. P'-H' compared to P-H' or P'-H Figure 2 shows noticeable increases in the percentage of changed predictions when both premise and hypothesis are paraphrased compared to when just one of the sentences is paraphrased. Specifically, for BoW and BiLSTM we see an increase of 4.01 and 6.01 percentage points respectively, and for BERT, Roberta, GPT-3 increases of 4.97, 4.83, and 3.55. As the transformer-based models changed their predictions on 12-14% of examples where both sentences are paraphrased compared to 9-11% in general, this analysis further suggests that these models are not as robust to paraprhases as desired.\nEntailed vs Not-entailed examples RTE analyses often differentiate how models perform on entailed vs not entailed examples (Liu et al., 2022) . In Figure 3 , we do not see meaningful differences in how models' predictions change on paraphrased examples based on the gold label. This might suggest that our dataset does not contain statistical irregularities based on the RTE labels. Correct vs Not-Correct Predictions Figure 4 shows that the Transformer models' predictions is more likely to change when it's prediction on an original example was incorrect (right red bars) compared to when the prediction for an original example was correct (left blue bars). For example, when RoBERTa's prediction for an original RTE example was correct, the model changed its prediction on just 5.5% of the corresponding paraphrased examples. When RoBERTa's predictions for an original RTE example were incorrect, RoBERTa's predictions changed for 20.88% corresponding paraphrased examples. Analyzing differences in models' confidences assigned to predictions might provide more insight (Marc\u00e9 and Poliak, 2022) . We leave this for future work.\nSource Task RTE1-3 examples originated from multiple domains and downstream tasks, e.g. question-answering (Moldovan et al., 2006) , information extraction (Grishman and Sundheim, 1996) , and summarization (Evans et al., 2004; Radev et al., 2001) . This enables researchers to evaluate how \n\nConclusion\nWe introduced P aRT E, a high-quality evaluation set of RTE examples paired with paraphrased RTE examples. We use our evaluation set to determine whether RTE models are robust to paraphrased examples. Our experiments indicate that while these models predictions are usually consistent when RTE examples are paraphrased, there is still room for improvement as models remain sensitive to changes in input (Jia and Liang, 2017; Belinkov and Bisk, 2018; Iyyer et al., 2018) . We hope that researchers will use P aRT E to evaluate how well their NLU systems perform on paraphrased data.\n", "hypothesis": " We use the evaluation set to determine if RTE models' predictions change when examples are paraphrased.", "answer": true}
{"title": "Solving Cosine Similarity Underestimation between High Frequency Words by \u2113 2 Norm Discounting", "content": "\nIntroduction\nCosine similarity is arguably the most popular word similarity measure used in numerous natural language processing (NLP) tasks, such as question answering (QA), information retrieval (IR) and machine translation (MT) (Echizen-ya et al., 2019; Oniani and Wang, 2020; Kim et al., 2022; Hanifi et al., 2022) . First, a word is represented by a vector (aka embedding) and then the similarity between two words is computed as the cosine of the angle between the corresponding vectors (Rahutomo et al., 2012) . Despite the good performance of cosine similarity as a similarity measure in various downstream tasks, Zhou et al. (2022) showed that it systematically underestimates the true similarity between highly frequent words, when computed using contextualised word embeddings obtained from MLMs such as BERT (Devlin et al., 2018) .\nCompared to the problem of estimating similarity between highly frequent words, the opposite problem of estimating the similarity between (or involving) rare (low frequency) words has received greater attention, especially in the scope of static word embeddings (Levy and Goldberg, 2014; Hellrich and Hahn, 2016; Mimno and Thompson, 2017; Wendlandt et al., 2018) . If a word is rare in a corpus, we might not have a sufficiently large number of contexts containing that word to learn an accurate embedding for it. This often leads to unreliable similarity estimations between words and has undesirable implications in downstream tasks such as the detection of analogies and social biases (Ethayarajh et al., 2019a,b) .\nOn the other hand, Zhou et al. (2022) studied the impact of frequency on contextualised word embeddings and showed that the cosine similarity between highly frequent words are systematically underestimated. Unlike in the previously discussed low frequency word scenario, we do have adequate contexts to learn an accurate semantic representation for highly frequent words. Therefore, it might appear surprising at first that cosine similarity cannot be correctly estimated even for the highly frequent words. Zhou et al. (2021) show that the diversity (measured by the volume of the bounding hypersphere) of the contextualised embeddings of a target word, computed from multiple contexts containing the word, increases with the frequency of that word. They provide an explanation that holds true only for 2-dimensional embeddings, which relates diversity to the underestimation of cosine similarity. Unfortunately, this explanation does not extend to the high dimensional embeddings used in practice by the NLP community (e.g. BERT token embeddings are typically more than 768 di- When the log-frequency of w in the corpus increases, cosine similarities computed for both contexts that express the same meaning of w as well as its different meanings decreases. mensional). More importantly, to the best of our knowledge, no solution has been proposed in the literature to address the cosine similarity underestimation problem associated with the highly frequent words.\nIn prior work, the \u2113 2 norm of a static word embedding has been shown to linearly correlate with the log-frequency of that word (Arora et al., 2016; Bollegala et al., 2018) . On the other hand, we empirically study the \u2113 2 norm of the contextualised embedding of a word w averaged over all of its contexts, and find that it too approximately linearly correlates with the log-frequency of w in the corpus used to pretrain the MLM. Recall that the cosine similarity is defined as the inner-product between two embeddings, divided by the \u2113 2 norm of those embeddings. Therefore, we suspect that the underestimation of cosine similarity between highly frequent words is due to the larger \u2113 2 norms associated with those words.\nTo correct for this bias associated with the \u2113 2 norms of highly frequent words, we propose a linearly parameterised discounting scheme in the logfrequency space. Specifically, we use Monte-Carlo Bayesian Optimisation (Balandat et al., 2019) to find the optimal discounting parameters. Our proposed discounting method is shown to accurately correct the underestimation of cosine similarities between highly frequent words on the Word-in-Context (WiC) (Pilehvar and Camacho-Collados, 2019) dataset where human similarity ratings are available for the same word in two different con-texts. Source code for reproducing the experiments reported in this is paper is publicly available. We approximate the word frequencies in BERT pretraining corpus using the BookCorpus (Zhu et al., 2015) . Let \u03c8 w be the frequency of w in this corpus.\nWe use the WiC dataset, which contains 5428 pairs of words appearing in various contexts with annotated human similarity judgements. WiC dataset is split into official training and development sets, while a separate hidden test set is used by the leaderboard for ranking Word Sense Disambiguation systems.\n3 WiC dataset contains pairs of contexts labelled as having the same meaning (e.g. \"to drive sheep out of a field\" vs. \"to drive the cows into the barn\") and different meaning (e.g. \"the play lasted two hours\" vs. \"they made a futile play for power\").\nWe compute the cosine similarity between the two contextualised embeddings of a target word in two of its contexts to predict a similarity score. Figure 1 shows the predicted similarity scores for both contexts in which a target word has been used in the same or different meanings for all words in the WiC dataset against log(\u03c8 w ). As seen from Figure 3 , \u03c8 w has a power-law distribution. Therefore, we plot its log instead of raw frequency counts in Figure 1 .\nFrom Figure 1 , we see that for both same as well as different meaning contexts, the predicted cosine similarities drop with the word frequencies. Moreover, the gradient of the drop for same meaning pairs (Pearson's r = \u22120.3001) is larger than that for the different meaning pairs (r = \u22120.2125), indicating that the underestimation of cosine similarity is more sever for the similar contexts of highly frequent words.\n\n\u2113 2 norm Discounting\nTo understand the possible reasons behind the cosine similarity underestimation for highly frequent words discussed in \u00a7 2, for each word w we compute its mean sibling embedding, \u0175, given by (1).\nEQUATION\nWe plot || \u0175|| against log(\u03c8(w)) in Figure 2 separately for a predefined set of stop words and all other words (i.e. non-stop words). For this purpose, we use the default 1466 stop words from NLTK and randomly selected 997,425 non-stop words from the BookCorpus. Pearson r values of stop words and non-stop words are respectively 0.1697 and 0.3754, while the lines of best fits for each class of words are superimposed. From Figure 2 , we see that overall, || \u0175|| increases with log(\u03c8 w ) for both stop and non-stop words, while the linear correlation is stronger in the latter class. Considering that stop words cover function words such as determiners and conjunctions that co-occur with a large number of words in diverse contexts, we believe that the \u2113 2 norm of stop words mostly remains independent of their frequency. Recall that the cosine similarity between two words is defined as the fraction of the inner-product of the corresponding embeddings, divided by the product of the \u2113 2 norms of the embeddings. Therefore, even if the inner-product between two words remain relatively stable, it will be divided by increasingly larger \u2113 2 norms in the case of highly frequent words. Moreover, this bias is further amplified when both words are high frequent due to the product of \u2113 2 norms in the denominator.\nTo address this problem, we propose to discount the \u2113 2 norm of a word w by a discounting term, \u03b1(\u03c8 w ), and propose a discounted version of the cosine similarity given by (2).\ncos \u03b1 (x, y) = x \u22a4 y ||x|| \u03b1(\u03c8 x ) ||y|| \u03b1(\u03c8 y ) (2)\nFollowing Figure 2 , we linearly parameterise \u03b1(\u03c8 w ) separately for stop vs. non-stop words as in (3).\n\u03b1(\u03c8 w ) = { 1 + m s (b s \u2212 log(\u03c8 w )) w is a stop word 1 + m n (b n \u2212 log(\u03c8 w )) w is a non-stop word (3)\nThe scalar parameters m s , m n , b s and b n are estimated as follows. First, we randomly initialise all parameters uniformly in [0, 1] and use (2) to predict cosine similarity between two contexts in which a target word w occurs in the WiC train instances. We then make a binary similarity judgement (i.e. same or different meaning) for the pair of contexts in an instance depending on whether the predicted cosine similarity is greater than a threshold \u03b8. Next, we compute the overall binary classification accuracy for the similarity predictions made on the entire WiC training dataset, Figure 4 : Cosine similarity between two instances of the same word w in two contexts in the WiC train dataset, computed using the original (non-discounted) cosine similarity (shown in blue and green respectively for the same and different meaning pairs) and using the proposed \u2113 2 norm discounted (( 2)) (shown in orange and red respectively for the same and different meaning pairs). We see that the gradients of the drops have decreased for both same and different meaning pairs after applying the discounting. and use Bayesian Optimisation to find the optimal values: \u03b8 = 0.545, m s = 0.00422, b s = 0.643, m n = 0.00427 and b n = 4.821. Specifically we used the Adaptive Experimentation Platform 4 for learning those optimal values. We found this is more efficient than conducting a linear search over the parameter space. We repeat the estimation five times and use the averaged parameter values in the remainder of the experiments. Note that m n > m s above, which indicates that non-stop words must be discounted slightly more heavily than the stop words. This makes sense since the impact of word frequency of non-stop words on their \u2113 2 -norm is stronger than that for the stop words as indicated by the slopes of the lines of best fit in Figure 2 .\n\nResults\nTo evaluate the effect of the proposed \u2113 2 norm discounting when computing cosine similarity, we repeat the analysis presented in Figure 1 using (2) to predict the similarity between contextualised word embeddings. Comparing the lines of best fit for the original (blue, r = \u22120.3006) vs. discounted (orange, r = \u22120.1366) for the same meaning contexts, we see that the gradient of the drop has decreased by 51.65%. Likewise, comparing the lines of best fit for the original (green, r = \u22120.2125) vs. dis- counted (red, r = \u22120.0843) for the different meaning contexts, we see the gradient of the drop has decreased by 57.04%. This result clearly shows that the proposed \u2113 2 norm discounting method is able to reduce the underestimation of cosine similarities for the highly frequent words.\nGiven that the discounting parameters in (3) are learned from the WiC train data, it remains an open question as to how well the proposed discounting method generalises when predicting similarity between contextualised embeddings of unseen words. To evaluate this generalisability of the proposed method, we use (3) with its learned parameters from WiC train data, to predict the similarity between contextualised word embeddings in WiC dev data.\n5 Specifically, we predict binary (same vs. different meaning) similarity labels according to the similarity threshold \u03b8 learnt in \u00a7 3 and compare against the human judgements using binary classification accuracy.\nThe maximum accuracy on WiC dev split obtained using the original (non-discounted) cosine similarities is 0.6667, which indicates that the cosine similarity is somewhat predictive of the human binary judgements. The overall F1 is improved by 2.4% (0.68 with original cosine vs. 0.71 with the proposed discounting method) and recall is improved by 12% (0.75 with original cosine vs. 0.84 with the proposed). On the other hand, the drop in precision is 4.7% (from 0.64 to 0.61). Therefore, the proposed method solves the cosine similarity underestimation problem associated with high-frequent words, without significantly affecting the similarity scores for low-frequent ones Figure 5 shows the average proportion of instances predicted to be the same meaning as a function of frequency, grouped into ten bins, each with the same number of examples. From Figure 5 , we see that in high frequency bins (i.e. bins 8, 9 and 10), the percentage of predicted instances as having the same meaning is consistently lower than that compared to the human judgements. This shows an underestimation of the true (human judged) similarity between contextualised word embeddings.\nOn the other hand, when we use the proposed \u2113 2 norm discounted cosine similarity (defined in (2)), in the highest frequent bin (i.e. 10) we see that the gap between human judgements vs. predicted similarities has reduced. Moreover, in the low frequency bins (i.e. 1-4), we see that the proposed discounting method does not affect the predictions made using cosine similarities. We see an overestimation of the cosine similarities in the low frequency bins as reported by Zhou et al. (2021) . As discussed already in \u00a7 1, the word embeddings learnt for low frequency words tend to be unreliable due to data sparseness. Therefore, we believe it is important to focus on the problem of learning accurate word embeddings rather than to adjust cosine similarities between low-frequency words in a post-processing step.\nWe see that in bins 5, 6 and 7 the similarity scores are slightly increased by the proposed discounting method, which is a drawback that needs to be addressed in future work. More importantly however, the overall percentage recall across all bins for retrieving same meaning instances improves significantly from 74.7% to 83.7% compared to using respectively the original cosine similarity vs. the discounted cosine similarity. Overall, this result confirms the validity of the proposed discounting method for addressing the underestimation of cosine similarity involving highly frequent words.\n\nConclusion\nWe proposed a method to solve the cosine similarity underestimation problem in highly frequent words. Specifically, we observed that the \u2113 2 norm of a contextualised word embedding increases with its frequency in the pretrain corpus and proposed a discounting scheme. Experimental results on WiC dataset confirmed the validity of the proposed method.\n", "hypothesis": "Experimental results on a contextualised word similarity dataset show that our proposed discounting method inaccurately solves the similarity underestimation problem.", "answer": false}
{"title": "Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions", "content": "\nIntroduction\nWriting persuasive texts plays a major role in law education (Kosse and Butle Ritchie, 2003) . As a part of their training for learning how to write legal opinions, law students are typically challenged to solve legal problems or case studies in the form of persuasive case solutions (Enqvist-Jensen et al., 2017) . To write a persuasive legal case solution, students in German law courses must be able to follow the structural requirements of the appraisal style (see Figure 1 ) (Stuckenberg, 2020) and justify their derived conclusions argumentatively via legal claims and premises (see Section 2). To learn a skill such as writing a persuasive case solution, individual feedback is important to the learning process (Hattie and Timperley, 2007; Black and Wiliam, 2009) . Individualized feedback for students during their writing or learning processes is lacking, particularly in the field of law. The characteris-tic large-scale learning scenarios in legal studies, which result in a low supervision ratio, are part of the reason for this. Organizational restrictions are another cause of the absence of personal feedback. For instance, there aren't enough lecturers who can assess students' case solutions (Henderson, 2003) . At the same time, technical solutions that could help students improve their legal writing fall short of expectations (Beurskens, 2016) . One promising solution to better support students in their writing process and to overcome the limitations in law courses would be the use of writing support systems that could provide individualized feedback to students (Wambsganss et al., 2020a) . To model legal persuasive writing with predictive algorithms, high-quality annotated corpora are needed. Pioneering work in argumentation mining has already focused on jurisprudence (Mochales and Moens, 2008; Mochales and Ieven, 2009) , since the structural approach of legal writing facilitates the unambiguous determination of argumentation components (Lytos et al., 2019; Urchs et al., 2020) . Existing corpora in law range from classification of judgments (Urchs et al., 2020) , to summarization of legal texts (Hachey and Grover, 2005) and to evaluation of jury verdicts (Poudyal et al., 2019) . Corpora dealing with the annotation of structural elements in student written legal texts are not available. A few corpora are suitable for designing and developing systems to support persuasive writing (Stab and Gurevych, 2017b; Wambsganss et al., 2020b; Lawrence and Reed, 2019; Stab and Gurevych, 2014) . However, these corpora are of limited use for modeling the structure of writing and argumentation in law, since persuasive writing in the legal domain follows a particular logic (see Section 2) that is not represented by available corpora. Consequently, there is a lack of evaluated annotation schemes and linguistic corpora for training models that support users in legal writing.\n\nSubsumption\nTherefore, we propose a novel annotation scheme for persuasive student-written case solutions. We introduce a corpus of 413 student-written case solutions with 25,103 sentences that are annotated for the components of the appraisal style, arguments (legal claim and premises), the relations of the arguments, and the relations of distinct components of the appraisal style. We trained different types of models (e.g. BERT and DistilBERT) and compared their accuracy to analyze which model performs best. Finally, we embedded the three best performing transformer-based BERT models in a novel writing support system that provides individual feedback and recommendations in a writing scenario. The design of our writing support system is based on the theory of learning from errors (Metcalfe, 2017) and aims to provide students with individual feedback on their errors during the writing process (Fazio and Marsh, 2009) . We tested the systems in an online learning scenario with law students. The students were asked to use the system to write a case solution. We show promising results in terms of the students' understanding of the appraisal style and their perception of the system. The participants perceive the system as useful and rate the system's feedback as accurate. Our analyzed results support that our presented corpus and the models are able to support students' learning effectively.\nOur work makes five major contributions. First, we derive a novel modeling approach for a new data domain by developing an annotation scheme based on the theory of structured legal writing based on the appraisal style (Man, 2022; Stuckenberg, 2020) . Second, we present an annotation study based on 100 student case solutions to show that annotation of student case solutions is accurately possible. Based on the annotation, we trained three transformer-based BERT models (Devlin et al., 2019) to demonstrate that the prediction of the annotated structures is possible with a certain accuracy. Fourth, we provide a corpus of 413 student case solutions in German collected in different law lectures. Finally, we show in an online experiment that the models can be used effectively in a writing support system. Therefore, we encourage further investigation into the enhancement of law students' persuasive structured writing and the development of writing support systems using NLP. This research aims to enhance students' skills regardless of their location, time constraints, or instructor availability.\n\nRelated Work\nPersuasive Writing in Law Courses Classically, students are asked to solve legal problems or case studies in the form of persuasive case solutions (Enqvist-Jensen et al., 2017) . In these case solutions, students are forced to use specialized and highly concept-driven knowledge. The theoretical knowledge specializes more in the correct application of paragraphs and the setting of priorities in the case solution. In contrast, the concept-driven knowledge is largely composed of the concepts of writing case solutions in a structured way. To do this, students must follow established legal concepts. Among the most important concepts in German jurisprudence are the appraisal style and the judgment style, whereby the appraisal style is primarily important for legal education (Stuckenberg, 2020; Urchs et al., 2020) . Since the term \"appraisal style\" is a peculiarity of the German legal language, there is no direct equivalent in English. We define the term appraisal style as \"the form and writing style of a legal opinion\" (Stuckenberg, 2020) . The appraisal style is used to solve complex legal problems. The four elements of appraisal style are briefly explained in Table 1 and supplemented by an example in Figure 2 .\nCorpora in the Legal Field Although law is a promising discipline for annotating the components of legal writing and arguments due to its fixed logical structure (Moens et al., 2007; Urchs et al., 2020) , evaluated open-access corpora for law are rare (Reed, 2006; Mochales and Moens, 2011; Urchs et al., 2020) . There are, however, some publicly accessible corpora. Hachey and Grover (2005) present a corpus of 188 annotated English court opinions. To construct a system for automatic summarizing of court judgments, they annotated rhetorical status, significance, and linguistic markup. Other annotated corpora deal explicitly with the annotation of argumentation structures in court decisions (Houy et al., 2013) or legal cases (Mochales-Palau and Moens, 2007) . Mochales-Palau and Moens (2007) present a corpus of English-language judicial cases gathered from the European Court of Human Rights (ECHR). They chose 55 papers at random, which included 25 court decisions and 29 admissibility reports. The texts were annotated and studied systematically in two layers (argumentative and non-argumentative sentences). A following study showed that the detection of argumentative sentences in court decisions is possible. Work such as that of Walker et al. (2014) has focused on identifying successful and failed patterns of reasoning in U.S. Court decisions. Patterns of reasoning are identified and used to illustrate the difficulty of developing a type or annotation system for characterizing these patterns. The corpus is based on legal cases of vaccine-injury compensations. There are several German corpora in addition to the largely English-language corpora for recognizing decisions and legal cases. Urchs et al. (2020) created a corpus based on Bavarian Court of Justice decisions. They discover argumentation structures in judgments using 200 court decisions. Other research groups focused on the identification of arguments in the German Federal Constitutional Court's Decision Corpus (Houy et al., 2013) and the development of a German referent corpus comprised of articles from legal journals, decision texts, and norm texts (Gauer et al., 2016) .\nA number of corpora have previously been proposed in research to enhance students' structured and persuasive writing in real-world applications, including Stab and Gurevych (2017a) and Stab and Gurevych (2014) . Stab and Gurevych (2014) produced a corpus based on student essays for building and implementing systems to promote persuasive writing for adaptive feedback using argumentation mining (AM) approaches. Further research uses the corpus as a model to annotate persuasive writings (Carlile et al., 2018) or construct a model for assessing persuasive essays (Ke et al., 2018) . However, the existing literature does not adequately transfer corpora for structured writing or reasoning to other educational domains, like law or to other languages.\nTo summarize, we see that literature falls short of annotated corpora, which can be used to model components in student-written legal case solutions. Without the availability of these corpora, the design of adaptive NLP-based applications for lawful writing is naturally hindered. To the best of our knowledge, there is only one approach by Urchs et al. (2020) that aims to detect the components of legal writing, but the approach focuses on court decisions and the judgment style. Therefore, we aim to address this literature gap by presenting and evaluating an annotation scheme as well as an annotated corpus built on student-written texts with the objective of developing an intelligent writing support system for students in law courses.\n\nData Source\nThe data for our corpus were collected in a law courses at a German university. We compiled the corpus with the case solutions of law students who have written solutions to different legal problems (four different case studies) from different areas of law. In total, we collected 413 legal case solutions, with a typical length of 55.07 sentences and 331.35 tokens per document. 1 The case studies are mainly based on example cases from civil law and are oriented towards basic cases of Musielak and Hau (2005) . Students solved the cases as a component of a comprehensive law lecture, utilizing them as a means of exam preparation. It is important to note that the quality of the 413 student-written case solutions may vary, as the students are not all at the same level of proficiency or understanding. The data were collected in the mentioned lecture between 2020 and 2022. The course deals with the teaching of the basics of legal writing and the funda-mental knowledge of business law were introduced. Accordingly, the course has dealt with essential basics that are also important for non-law students, such as business students. The data collected are thus relevant not only in the context of foundational legal education but also for many other Germanlanguage legal studies programs (e.g., law courses in the education of business students).\n\nAnnotation Scheme\nThe correct application of structured legal writing in the appraisal style is the basis for a persuasive legal opinion. In the following, the components of the legal writing structure, as well as its annotation, are explained. The structure consists of four components: major claim, definition, subsumption (premise and legal claim), and conclusion (Sieckmann, 2020; Backer, 2009) (see Table 1 ).\n\nComponents Definition Major claim\nThe major claim explains the elements of the offense (fact) that are to be fulfilled. It raises a question or possible consequence. The question is discussed in the following steps and is finally answered in the conclusion.\n\nDefinition\nThe definition determines the constituent elements that must occur in the legal problem so that the case solution can come to a conclusion. The elements always depend on the question raised in the major claim. Subsumption (premise and legal claim):\nIn the subsumption, it is examined to what extent the conditions (elements) of the definition are given.\nHere, the facts of the case are weighed against the preconditions from the definitions and the premises (facts). Legal consequences are drawn from the premises, so-called legal claims.\n\nConclusion\nThe conclusion is the answer to the major claim. Thus, the case solution reaches a final result here.\nTable 1 : Core components of the legal writing structure in the appraisal style according to our guidelines.\n", "hypothesis": " We evaluated a writing support system in which our models were integrated in an online experiment with law students and found positive learning success and users' perceptions.", "answer": true}
{"title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning", "content": "\nIntroduction\nDeveloping comprehensive evaluation frameworks (Deng et al., 2021; Yuan et al., 2021; Zhong et al., 2022) that can evaluate multiple humaninterpretable dimensions, such as factual consistency (Kryscinski et al., 2020; Wang et al., 2020) and coherence (Dziri et al., 2019; Huang et al., 2020) , is important for the advancement of Natural Language Generation (NLG). However, similaritybased metrics (Papineni et al., 2002; Lin, 2004; Sellam et al., 2020; Zhao et al., 2019; Zhang et al., 2020) still dominate NLG evaluation in practice. Compared to them, desired multi-dimensional evaluators do not require reference texts for evaluation; and they can easily extend to new explainable evaluation dimensions. Recently, Zhong et al. (2022) developed a unified evaluation framework that can Figure 1 : Our prompt design to evaluate the consistency of the summary in red, illustrated using two in-context examples (in blue). To evaluate other aspects, we remove the source text or replace it with a reference. generalize to multiple dimensions and text generation tasks. However, it relies on the construction of synthetic and auxiliary data for the finetuning of a pre-trained language model, requiring in-depth knowledge and significant engineering effort for each dimension. Furthermore, the inclusion of new dimensions requires (continued) training of the model, and might affect the performance on other dimensions in unforeseen ways.\nIn this work, we propose to use in-context learning (Brown et al., 2020) with large language models (LLMs) -a commonly used method to perform many tasks by utilizing only a few input-output examples -to perform multi-dimensional text evaluation in a unified fashion. Compared to pre-trained evaluators that need specialized supervised training for each dimension, our In-Context learning-based Evaluator (ICE) framework is:\n\u2022 Learning-free. It does not require supervised fine-tuning on large annotated (synthetic) training data, requiring only a handful of samples at inference time. \u2022 Extensible. To evaluate new dimensions, it does not rely on large amounts of human judgments or the construction of new synthetic data, using only a natural language prompt consisting of a small number of example pairs to ascertain the properties associated with a given quality aspect.\nIn this paper, using text summarization as a test bed, we show that with a simple prompt design, ICE is competitive with state-of-the-art trained evaluators on multi-dimensional evaluation of modelproduced summaries, establishing a new state-ofthe-art on dimensions such as relevance and factual consistency. To study the robustness of the evaluator to the selection of in-context examples, we analyze the factors that affect the performance of ICE, such as the number of in-context examples and sampling procedures when picking in-context examples from a set of candidates. We find ICE to be robust to the selection of in-context examples and observe a slight improvement in performance as the number of examples is increased. Finally, in light of the recent work (Goyal et al., 2022) that points to the misalignment of existing evaluation metrics with human preference in evaluating zeroshot summaries generated by LLMs such as GPT-3 (Brown et al., 2020) , we study the effectiveness of ICE in evaluating zero-shot summaries generated by GPT-3. We find that ICE evaluations agree closely with human judgments on such summaries.\n\nProblem Statement\nGiven a sequence x that is input to an NLG system and a system-generated output sequence y, an evaluation framework outputs a score s that captures the quality of y, either with or without the help of a human-generated reference output r. 1 In case of multi-dimensional evaluation where we are interested in assessing y over d quality metrics, we instead get a vector S = (s 1 , s 2 , ..., s d ) over diverse dimensions (e.g., coherence, fluency). Depending on the dimension, there is sometimes a need to condition an evaluation on x (such as to evaluate consistency in summarization). We evaluate our method over four dimensions:\n\u2022 Consistency: The factual correctness of a summary given the source text. \u2022 Relevance: The property of capturing salient information from the source. \u2022 Fluency: A measure of the quality of the individual sentences in the summary. \u2022 Coherence: A measure of the quality, organization, and structure of sentences in the summary.\n1 Specifically for summarization, most learned frameworks evaluate relevance through reference-based evaluation.\n\nPrompt Design & Score Extraction\nICE relies on an LLM (we use the text-davinci-003 model of GPT-3) to make predictions. It takes in a prompt that consists of a small number of in-context examples, each of which consists of generated text and its corresponding quality score as a numeric string. The prompt ends with a test example, for which the model predicts a score (Figure 1 ).\nThe input contains the model-generated text (summary), in addition to which it might contain additional information such as the source text or references, depending on the dimension. To evaluate fluency and coherence, our prompts use in-context examples consisting of generated summaries and corresponding scores. For consistency and relevance, we use the source text and a reference summary respectively, in addition to the generated summary. We pass this prompt to a GPT-3 model, with sampling temperature set to 0 to elicit deterministic responses. We parse the model response-decoded numeric string-as the dimension score.\n\nSelection of In-context Examples\nBy default, we use 4 in-context examples in our prompts, as this is the largest number that fits within the context window of GPT-3. We experiment with two sampling procedures (Appendix B) to obtain 4 examples from a pool of examples:\n1. Uniform Random Sampling. We randomly select 4 summaries from the pool of examples. This causes the examples to follow the same distribution as the example pool. 2. Stratified Sampling. We bucket the range of scores, i.e. [0, 1], into 4 equal partitions and randomly sample one summary from each one.\nThis causes examples to be representative of the range of scores in the example pool.\nWe avoid using synthetically generated data (Kryscinski et al., 2020; Zhong et al., 2022) since the kind of errors made by generation models is often different from the errors present in the negative examples in these datasets (Goyal and Durrett, 2021) . We instead elect to use (a few) human evaluations of model-generated text in order to make the in-context examples as representative of real errors as possible. We do this by splitting the meta-evaluation dataset and using a partition as an in-context example pool, as described in Section 3.1. \n\nDatasets & Baselines\nWe use the SummEval dataset (Fabbri et al., 2020) 2 to meta-evaluate our evaluation framework. Sum-mEval collects human evaluation annotations for 16 summarization systems on 100 articles sampled from the CNN/DailyMail corpus, for a total of 1600 summary-level annotations. Each summary is evaluated on four dimensions described in Section 2.2.\nTo get a pool of in-context examples, we keep aside a small subset (64 examples) of the Sum-mEval dataset to pick in-context examples from, and use the rest (1536 examples) as the test set for meta-evaluation (evaluating the baselines on this same test set). Further details are in Appendix A.\nWe compare ICE to the following state-of-theart multi-dimensional evaluators: (1) CTC (Deng et al., 2021) uses information alignment between generated outputs and references or inputs; (2) BARTScore (Yuan et al., 2021) uses the conditional probability of a sequence given inputs or references; and (3) UniEval (Zhong et al., 2022) uses a question-answering framework (e.g. \"Is this a coherent summary?\") to calculate metrics.\nFollowing Liu et al. (2021) ; Zhong et al. ( 2022), we assess performance by computing summarylevel Spearman and Kendall-Tau correlations between predicted scores and human judgements.\n\nResults\nAs illustrated in Table 1 , ICE is competitive with fine-tuned baselines despite not requiring any finetuning. It achieves state-of-the-art correlation with human judgments for relevance and consistency. We perform pairwise significance tests and observe that ICE (uniform sampling) does better than UniEval on consistency and relevance on Kendall's Tau with a significance level of 0.05 (Appendix E). Additionally, the uniform sampling variant of ICE outperforms BARTScore (which also does not require finetuning) across dimensions.\nBetween the two sampling procedures for ICE, we observe that stratified sampling works marginally better for all dimensions other than consistency. Since summaries in the SummEval dataset have perfect or near-perfect human scores for consistency (Figure 2 ), uniform sampling causes in-context examples to also have nearperfect scores. This appears useful for the model to calibrate its scoring when evaluating consistency, leading to better performance. We explore this in greater detail in \u00a74.1. While the same reasoning could hold for fluency, we observe both here and in \u00a74.3 that fluency scores are quite stable. Given that fluency is an easier aspect to evaluate, this stability could be a result of the model possessing a strong notion about fluency from pre-training time that is not modified significantly as the distribution of in-context examples changes (Reynolds and McDonell, 2021) . Finally, we observe that the performance for coherence and relevance are similar regardless of the sampling procedure. This is because scores for these aspects are spread out in the dataset, which makes uniform and stratified sampling return similar in-context examples.\n\nAnalysis\nIn this section, we analyse the effects of our prompt engineering choices. The comparison between sampling procedures in Section 4.1 is performed on the entire test set but the experiments in Sections 4. domain regardless of the true distribution. This forces predictions towards a centered distribution, which can cause the performance drop we observe in Table 1 when evaluating consistency using stratified sampling. Uniform sampling, on the other hand, selects examples that represent the true distribution, making model predictions more closely reflect the true distribution.\nA drawback of uniform sampling is sub-optimal calibration in low-probability regions of the true distribution. For instance, if uniform sampling is used to evaluate consistency, the model might not see in-context examples with (say) scores less than 0.3 (Figure 2 ). This can affect output calibration in that region. Nonetheless, we suggest using uniform sampling in general. It is more stable and its prediction distribution closely follows the true distribution. For dimensions where it underperforms stratified sampling, the margins are less significant. Finally, even when ICE (uniform sampling) scores are calibrated differently from human scores, they still rank summary-quality correctly, insofar as our main results (Table 1) \n\nEffect of Selection of In-context Examples\nIn order to determine whether performance is robust to the choice of in-context examples, we evaluate our test set using three different random sets of in-context examples. We observe in Figure 3 that for a given dimension, the maximum variation across three seeds is about 7 points, suggesting reasonably stable performance across the choice of in-context examples.\n\nEffect of Number of In-context Examples\nWe evaluate our test set using different numbers of in-context examples (Figure 4 ). We observe that only for relevance and coherence does performance show improvement as we increase the number of examples. One reason for this could be the distribution of scores for a given dimension in the test set (Figure 2 ). Concretely, consistency and fluency mostly have near-perfect scores and therefore do not benefit from more samples while the scores for coherence and relevance are spread out and therefore more samples allow representation over the whole range of scores.\nAnother observation is that even for coherence and relevance, performance with a single incontext example reaches near that achieved by some of the weaker fine-tuned baselines in Table 1 . This suggests that the model possesses the notion of the evaluation task from pre-training itself, which is in line with recent work (Reynolds and McDonell, 2021; Min et al., 2022) that suggests that demonstrations help extract this knowledge.\nFinally, we note that calibration can potentially be improved by increasing the number of examples. For instance, we observed that the four incontext examples that the uniform sampling procedure chose for coherence in Figure 2 had scores that fall between 0.7 and 1.0. This concentrates the prediction distribution in that range. The probability of such an event will reduce as the number of examples is increased further.\n\nUsing ICE to Evaluate Zero-Shot Prompting Models\nRecent work by Goyal et al. (2022) showed that standard reference-based and reference-free metrics are not reliable in evaluating zero-shot summaries written by models such as GPT-3. Through a human study comparing summaries from three systems-GPT-3, BRIO, and T0-they observed that while humans prefer GPT-3 summaries, automatic evaluators consistently score GPT-3 summaries lower than summaries from other models.\nWe study the efficacy of ICE in evaluating zeroshot summaries written by GPT-3 at a dimension level. We use the set of 500 CNN articles from Goyal et al. (2022) , with summaries from GPT-3, BRIO, and T0 for each article. We sample 100 of these articles and have three annotators rate summaries for each of the dimensions defined in Section 2.2 on a scale of {1, 2, 3, 4, 5}. We use ICE, ROUGE, and BARTScore (all of which do not require training data) to evaluate the summaries and present system-level results in Table 2 .\nWe observe that ICE agrees with human judgments for each dimension and overall preferences while existing reference-based and reference-free metrics such as ROUGE and BARTScore 3 consistently rate GPT-3 summaries low. Goyal et al. (2022) suggest that most existing evaluation metrics reward summaries that imitate references, while GPT-3 summaries are zero-shot and not trained to imitate human-written references, which is likely why they are penalized by most existing evaluators. However, since ICE is not based on reference similarity (except when evaluating relevance) and is also not trained with reference summaries, it is able to better evaluate GPT-3 summaries and agrees with human preferences.\n\nConclusion\nWe show that in-context learning can be used for NLG evaluation as an alternative to fine-tuned evaluation metrics. Using a small number of examples, in-context learning evaluators can reach or exceed the state-of-the-art on multi-dimensional evaluation and that this is robust to the choice of in-context examples. Finally, we show that in-context learning evaluators align well with human judgements when evaluating summaries written by GPT-3.\n", "hypothesis": "Most frameworks that perform such multi-dimensional evaluation require training on small manually or synthetically generated datasets.  In this paper, we study the lack of efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets.", "answer": false}
{"title": "Dataset Distillation with Attention Labels for Fine-tuning BERT", "content": "\nIntroduction\nDeep learning models have achieved state-ofthe-art performance in various fields, including computer vision and natural language processing (NLP), using large-scale neural networks trained with huge datasets. Unfortunately, their successful performances have come with massive training costs, including training time, GPU resources, and energy consumption. To reduce the training costs, current research has been focusing on constructing a small training dataset such that models trained with it can achieve comparable performances to models trained with the whole original dataset.\nOne classical way to compress the training dataset is data selection. Data selection methods choose a subset of effective training samples on the basis of a number of heuristic measures, for example, cluster centers (Sener and Savarese, 2018) , diversity (Aljundi et al., 2019) , and likelihood of models (Moore and Lewis, 2010) . Although the data selection methods effectively work for efficient model training and several applications, such as active learning (Sener and Savarese, 2018) and continual learning (Aljundi et al., 2019) , their performance is clearly restricted because they rely on the existence of representative samples that are effective for model training in the original dataset.\nAs an alternative approach for reducing the training dataset, Wang et al. (2018b) proposed dataset distillation, which aims to create a small number of synthetic samples optimized to effectively train models. Dataset distillation has attracted much attention in machine learning (Wang et al., 2018b; Zhao et al., 2021; Zhao and Bilen, 2021; Sucholutsky and Schonlau, 2021; Bohdal et al., 2020; Wang et al., 2022; Cazenavette et al., 2022) for both the theoretical interest and various applications, such as neural architecture/hyper-parameter search (Such et al., 2020) , continual learning (Masarczyk and Tautkute, 2020; Rosasco et al., 2022) , federated learning (Goetz and Tewari, 2020; Zhou et al., 2020) , and preserving data privacy (Li et al., 2020; Dong et al., 2022) .\nHowever, most of the existing research on dataset distillation mainly focuses on image datasets, and only a few studies involve NLP tasks. Sucholutsky and Schonlau (2021) and Li and Li (2021) extended dataset distillation to text datasets by using embedding vectors as an input of the distilled dataset instead of discrete text. While these studies applied dataset distillation to those model architectures based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), we cannot find any research that tackles dataset distillation for pre-trained transformers, such as BERT (Devlin et al., 2019) , which have become the de-facto standard for various kinds of NLP tasks. Therefore, in this paper, we aim to obtain distilled few-shot datasets to fine-tune the pre-trained transformers for NLP tasks.\nTo this end, we focus on the attention mechanism, which is the core component of transformers (Vaswani et al., 2017) . Several current studies utilized supervision of the attention probabilities to effectively train the model (Liu et al., 2016; Mi et al., 2016) . Moreover, it is also used for the model distillation to efficiently transfer the knowledge of a transformer model to another one via attention probabilities (Aguilar et al., 2020; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020 Wang et al., , 2021)) . Inspired by this, we propose distilled attention labels, which are the supervision of attention probabilities optimized as a part of the distilled dataset, to enhance the effectiveness of the distilled dataset for training the transformer models.\nIn our experiments, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) in various types of NLP tasks: AGNews (text classification), SST-2 (sentiment analysis), QNLI (QA/NLI), and MRPC (paraphrase identification).\nOur main contributions are as follows: (i) To the best of our knowledge, this is the first work to explore dataset distillation for pre-trained transformers. Specifically, we demonstrate that our distilled datasets effectively fine-tune BERT even with only one sample for each class and only one gradient step. (ii) We present the distilled attention labels, which can easily be applied to dataset distillation for transformer architectures. Experimental results show that they consistently improved the performance with the distilled datasets in various types of NLP tasks. (iii) We open our source code and the distilled datasets obtained through our experiments to facilitate further research. 1 2 Methodology\n\nDataset Distillation\nIn this section, we explain the basic approach of dataset distillation (Wang et al., 2018b) , which aims to optimize a synthetic dataset through the gradient method similar to the current meta-learning approach (Finn et al., 2017) .\nLet the original training dataset D = {(x i , y i )} N i=1 , where (x i , y i ) is a pair of an input and its class label. Our goal is to optimize a distilled dataset D = {(x i , \u1ef9i )} M i=1 , which is randomly initialized at first, with M \u226a N .\nThe model parameters \u03b8 are updated with a minibatch of the distilled dataset (x t , \u1ef9t ) by gradient 1 https://github.com/arumaekawa/ dataset-distillation-with-attention-labels descent (GD) steps as follows:\nEQUATION\nwhere L() is a twice-differentiable loss function and \u03b7 is the learnable learning rate of the model, which is optimized together with D. Given initial model parameters \u03b8 0 , we can represent the model trained with the distilled dataset D, with the number of GD steps T , as\nEQUATION\nwhere F () is the training procedure of the T steps for the GD updating (Eq. 1).\nAs the goal of dataset distillation is that \u03b8 T performs well on the original dataset, the optimization objective of the distilled dataset D is calculated as follows:\nEQUATION\nwhere (x t , y t ) is a mini-batch of the original training dataset. Therefore, the optimization problem for dataset distillation is formulated as\nD * , \u03b7 * = arg min D,\u03b7 E \u03b8 0 \u223cp(\u03b8 0 ) L distill ( D, \u03b7; \u03b8 0 ) ,\n(5) where p(\u03b8 0 ) is the distribution of \u03b8 0 .\nWe optimize the distilled dataset D with this objective by using current gradient-based optimization techniques, e.g., Adam (Kingma and Ba, 2015) . However, the discrete nature of text data makes it difficult to apply the gradient methods directly. Inspired by previous work (Sucholutsky and Schonlau, 2021; Li and Li, 2021) , we use a sequence of embedding vectors for inputs of the distilled dataset instead of text as it is. Using the embeddings makes the loss L distill differentiable with respect to D, and we can thus optimize the distilled dataset D by the gradient methods.\n\nDistilled Soft Labels\nThe class labels of the original dataset are usually discrete hard labels (i.e., one-hot labels representing only a single class). Instead of hard labels, we can use soft labels for distilled datasets and optimize them with the input embeddings. Using soft labels enables the distilled datasets to contain more information. Following previous work (Sucholutsky and Schonlau, 2021; Bohdal et al., 2020) , we first initialize the soft labels with one-hot values and enable them to take any real values. We can now optimize the soft labels through the gradient method as well as the input embeddings.\n\nDistilled Attention Labels\nFor efficient knowledge transfer to transformer models via training with the distilled dataset, we propose attention labels, which are optimized to guide the multi-head attention module of the transformer models.\nInspired by previous work (Aguilar et al., 2020; Wang et al., 2020 Wang et al., , 2021)) , we compute the Kullback-Leibler (KL) divergence D KL between the selfattention probabilities of the model a(\u03b8) and the distilled attention labels \u00e3 across all layers and heads. The attention loss L attn is computed as follows:\nEQUATION\nwhere \u00e3k,h and a k,h (\u03b8) are the attention maps for the h-th head of the k-th layer of the distilled attention labels and the model, respectively, K is the number of layers, and H is the number of heads. Due to the data size, we consider the attention probabilities only for the first input token ([CLS]).\nWe train the model to minimize L task and L attn at the same time. Thus, the GD updating of the model (Eq. 1) is modified as\nEQUATION\nwhere \u03bb is the balance weight for L attn .\nThe attention labels \u00e3 are first initialized randomly and restricted to being a valid probability distribution (i.e., non-negative and the sum equals 1) by applying the softmax function to real-valued vectors. We optimize the attention labels together with the input embeddings and the soft labels by the gradient method. The details of the step-by-step procedure of our distillation algorithm are shown in Appendix A.\n\nSettings\nDatasets. We evaluated our dataset distillation methods in various types of NLP tasks. We used a text classification task (AGNews (Zhang et al., 2015) ) and three different natural language understanding tasks (SST-2, QNLI, and MRPC) from the GLUE benchmark (Wang et al., 2018a) . For the evaluation metrics, we used accuracy for AGNews.\nFor the other three tasks, we followed the evaluation settings of GLUE (Wang et al., 2018a) . The statistics of each benchmark dataset are summarized in Table 1 . Network Architecture. To evaluate the dataset distillation methods, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) , which is the first pre-trained transformer model, that all subsequent models are based on. We utilized the pre-trained BERT BASE model. Following the fine-tuning procedure in Devlin et al.\n(2019), we introduced additional classification layer weights W \u2208 R C\u00d7D on the last hidden state of the [CLS] token, where D is the hidden dimension of BERT and C is the number of classes.\nImplementation. For all our distilled datasets, we used Adam optimizer (Kingma and Ba, 2015) with a learning rate \u03b1 \u2208 {1e \u22123 , 1e \u22122 , 1e \u22121 } and trained the distilled datasets for 30 epochs. We initialized the learnable learning rate \u03b7 \u2208 {1e \u22122 , 1e \u22121 }. For the attention labels, we set \u03bb = 1.0, which performed well in our preliminary experiments. We report the results for the best performing combination of \u03b1 and \u03b7. Note that due to the coarse granularity of the search, there is no need to care about overfitting to the test set. More details of our implementation are shown in Appendix B. Evaluation. To evaluate the distilled datasets, we fine-tuned the BERT model with them for 100 times, where the additional parameters W were randomly initialized each time. In all our experiments, we report the mean and standard deviation over the 100 evaluation results.\n\nResults for 1-shot and 1-step Setting\nWe first evaluated the dataset distillation methods with a 1-shot and 1-step setting, where the distilled dataset includes only one sample per class, and BERT was fine-tuned with it by only one GD step. We compared the performance for hard/soft labels and with/without attention labels for each task.\nTable 2 shows the evaluation results. The distilled datasets with the hard labels, i.e., only optimizing the input embeddings and not applying the attention labels, still achieved 87.4, 81.6, and 68.6 for AGNews, SST-2, and QNLI, respectively, which is 92.4, 88.0, and 74.7% performance of the full dataset. Furthermore, using the soft labels further improved these performances, especially by almost 8 points for QNLI. However, for MRPC, the distilled dataset achieved only the same performance as the majority class baseline regardless of the use of the soft labels.\nWhen applying the attention labels, the performance of the distilled dataset was significantly improved for all tasks, and their effect is much greater than the soft labels. Specifically, our distilled dataset with the attention labels yielded up to 98.5, 97.2, 94.1, and 88 .9% performance of the full dataset for AGNews, SST-2, QNLI, and MRPC, respectively. These results indicate that using the attention labels enables to extract the information from the original dataset as the attention probabilities and to efficiently transfer it to the model.\nWhen comparing the performance between the four tasks, dataset distillation performed very well on relatively simple classification tasks such as AGNews and SST-2, while the performance was somewhat limited on QNLI and MRPC, which require understanding the relationship between two sentences. In particular, for MRPC, although the performance was improved by applying the attention labels, the gap from the full dataset was still larger than that in the other three tasks. The class imbalance in the original training dataset (68% positive) may make the training of the distilled dataset more difficult. We can say there is still room for performance improvement by dealing with this issue (e.g., by upsampling or downsampling).\n\nResults for Multiple-shot and Multiple-step Setting\nWe also evaluated the distilled datasets with more than one shot and more than one GD step to finetune BERT. For the multiple-step setting, we considered two different scenarios: using the same distilled data in all steps and using different distilled data for each step. In these experiments, we evaluated the distilled datasets that use soft labels and attention labels for different numbers of GD steps T \u2208 {1, 3, 5}.\nTable 3 shows the results for the multiple-shot and multiple-step setting. In the single-step setting, overall performance improved with the number of shots of the distilled data. We believe that this is simply due to the expressiveness of the distilled data improved with the size of them. When using the same distilled data for all steps in the multiple-step setting, the performance of the distilled datasets degraded even compared with that in the single-step setting. In contrast, the performance was improved by separating the distilled data for each step and slightly but better than that with the same number of shots in the single-step setting. These results suggest that the role of the distilled data is different between the earlier and later steps, and it is difficult to obtain the distilled data that are generally useful for all GD steps.\nIn addition, the basic dataset distillation algorithm we used requires computing the back propagation through all GD steps for the optimization of the distilled dataset, which increases memory and computational costs linearly with T . Therefore, it was difficult to increase T to be larger than 5 in our experiments. This is the limitation of our dataset distillation method, and it needs further improvement to scale to more complex tasks or to train models from scratch.\n\nConclusion\nIn this paper, we explored dataset distillation in NLP tasks to fine-tune pre-trained transformers. We proposed attention labels, which are the supervision of attention probabilities distilled as a part of the distilled datasets. Experimental results across various tasks demonstrate that our distilled fewshot datasets achieved successful performances even with only one sample per class. Notably, the attention labels significantly improved the performance of the distilled datasets even for the tasks where dataset distillation is difficult without them.\n", "hypothesis": " Specifically, we propose to introduce attention labels, which can efficiently distill the knowledge from the original dataset and transfer it to the transformer models via attention probabilities.", "answer": true}
{"title": "Transformed Protoform Reconstruction", "content": "\nIntroduction\nLanguages change over time and sometimes diverge into multiple daughter languages. The common ancestor of a set of genetically related languages is their proto-language. While there are proto-languages such as Latin that are attested, they are the exception 2 . Reconstructed words and morphemes in proto-languages are called protoforms. The task of reconstructing unattested protolanguages is called protoform reconstruction.\nHistorical linguists reconstruct proto-languages by identifying systematic sound changes that can be inferred from correspondences between attested daughter languages (see Table 1 ). They compare the sounds between a set of cognates, or words with a common ancestor, to develop hypotheses about the types and chronologies of sound changes. ' This task is inherently data-constrained, especially for under-documented languages. Such data scarcity makes it a particularly difficult task for contemporary neural network architectures such as the Transformer (Vaswani et al., 2017) , which are data hungry.\nThe contributions of this paper are as follows:\n\u2022 Application of the Transformer architecture to the protoform reconstruction task, achieving state of the art performance, contrary to expectation.\n\u2022 Expansion of prior digital versions of H\u00f3u ( 2004)'s Chinese dataset to include a total of 804 cognate sets across 39 modern varieties and Middle Chinese.\n\nRelated Work\nApplying machine learning to protoform reconstruction is not new. Bouchard-C\u00f4t\u00e9 et al. (2013) learn an unsupervised protoform reconstruction model for the large Oceanic language family using Monte Carlo Expectation Maximization (Dempster et al., 1977\u037e Bouchard-C\u00f4t\u00e9 et al., 2008) , supervising the model with a gold phylogeny and using a probabilistic, generative model of sound change. List et al. (2022) apply an SVM classifier to supervised reconstruction by treating sound correspondences as training examples. Note that there were no word boundaries in the input matrix\u037e that is, all sound correspondences across the training set are flattened into one matrix. Furthermore, each language has an independent phonemic inventory. To learn contextual information, the authors experiment with adding features encoding the position of phonemes, among others. Ciobanu and Dinu (2018) learn a conditional random field (Lafferty et al., 2001) using n-gram features for supervised reconstruction and ensemble 5 daughter-to-protoform models. They use a dataset of 3,218 complete cognate sets spanning Latin (the proto-language) and 5 Romance languages: Romanian, French, Italian, Spanish, Portuguese. Meloni et al. (2021) employ a GRU-based seq2seq approach (Cho et al., 2014) to Latin protoform reconstruction and achieve state-of-theart character edit distances. They extend Dinu and Ciobanu (2014) 's Romance data using data from Wiktionary-for a total of 8,799 cognate sets across 5 Romance languages plus Latin-in both orthographic and phonetic (IPA) representations. In their model, all entries comprising the cognate set are concatenated together in a fixed order to form a training example. Chang et al. (2022) applied Meloni et al. (2021) 's architecture to the reconstruction of Middle Chinese on a dataset of 5000+ cognate sets spanning 8 languages they compiled from Wiktionary. 3 Fourrier (2022) compares statistical machine translation, RNN, and Transformer architectures for protoform reconstruction, but they evaluate their results using BLEU scores (Papineni et al., 2002) instead of edit distance. They find that their Transformer model did not outperform the RNN models on protoform reconstruction. In addition, their multilingual NMT (neural machine translation) model predicts many languages instead of one target language and is trained on bilingual pairs for protoform reconstruction (e.g. Italian-Latin and Spanish-Latin), unlike comparative reconstruction. In contrast, we encode the entire cognate set consisting of multiple daughter languages (5 for the Romance dataset\u037e 39 for Chinese) and predict the corresponding protoform.\n\nDatasets\nWe train and test our model on Romance and Sinitic (Chinese) language datasets. For Romance languages, we use Meloni et al. (2021) 's dataset which consists of 8,799 cognate sets of Romanian, French, Italian, Spanish, Portuguese words and the corresponding Latin form (approximately, a protoform). There are two versions of this dataset: phonetic and orthographic. The phonetic dataset (Rom-phon) represents words with IPA symbols whereas the orthographic dataset (Rom-orth) represents words in the orthographic form of each language. We preserved all diacritics, except for vowel length. This dataset is an extension of Dinu and Ciobanu (2014) 's original dataset of 3,218 cognate sets, which is not publicly available. Refer to Table 2 for more information.\n\nExpanding digital versions of H\u00f3u (2004)\nFor Sinitic languages, we created a dataset of Middle Chinese and its modern daughter languages. Middle Chinese is an unattested language, and we thus have to rely on Baxter and Sagart (2014)'s reconstructions of forms corresponding to 4,967 Chinese characters. We scraped Wiktionary to obtain H\u00f3u (2004)'s phonetic representations of their modern reflexes. 4 The resulting dataset contains 804 cognate sets of 39 modern Sinitic languages and the corresponding reconstructed Middle Chinese word. List (2021)'s version previously had 894 cognate sets across 15 varieties.\n\nModel\nWe propose a Transformer-based encoder-decoder architecture (Vaswani et al., 2017) because such models have produced state-of-the-art results on many sequence processing tasks. Transformers are by reputation data hungry, though, which poses a challenge to our problem setting, where the number of available training examples is often very small. We modify the standard encoder-decoder architecture to accommodate the structure of our datasets, where multiple daughter sequences correspond to a single protoform sequence. Like Meloni et al. (2021) , the daughter sequences are concatenated into a single sequence before being fed into the encoder. Because we only care about the relative position between tokens within each daughter sequence but not across daughter sequences, positional encoding is applied to each individual daughter sequence before concatenation. Along with positional encoding, an additive language embedding is applied to the token embeddings to differentiate between input tokens of different daughter languages.\n\nBaselines\nWe compare our Transformer model to a variety of baselines. For Meloni et al. (2021) , we use Chang et al. (2022) 's PyTorch re-implementation and reran a Bayesian hyperparameter search using WandB (Biewald, 2020) to ensure a more fair comparison (since our model is tuned with WandB as well). We also include the random daughter (randomly designate a daughter form as the protoform and assume no sound change) and the majority constituent baselines (predict the most common phoneme in each syllable constituent) from Chang et al. (2022) . For the SVM and CoRPaR classifiers (List et al., 2022) , we experiment with different contextual features, such as Pos (position), Str (prosodic structure), and Ini (whether or not the phoneme appears word-initially or word-finally).\nWe publish results on Meloni et al. (2021) 's full set of 8,799 cognates but cannot redistribute this set due to Dinu and Ciobanu (2014) 's restrictions. For reproducibility, we include results on Meloni et al. (2021) 's public subset of 5,419 cognates in the Appendix (Table 7 ), both of which include vowel length. Observe that these results are worse than those obtained on the full set, suggesting that the RNN and Transformer are dependent on a wealth of training data.\n\nPreprocessing\nIn all our datasets, we merge diacritics to their base segments to form a multi-character token. For instance, the sequence [t, \u02b0] is concatenated to [t\u02b0] . This ensures that phonemes are treated as one token. For Chinese, tone contours (a sequence of tones) are treated as one token. When multiple pronunciation variants are listed for a single Chinese character, we arbitrarily pick the first one.\n\nEvaluation criteria\nWe evaluate the predicted protoforms using edit distance (Levenshtein et al., 1966) , normalized edit distance (edit distance normalized by the length of the target) and accuracy (the percentage of protoforms that are reconstructed without any mistakes). Like Chang et al. (2022) , we also use feature error rate calculated using articulatory feature vectors from PanPhon (Mortensen et al., 2016) because it reflects the phonetic similarity between the prediction and the gold protoform. For datasets with phonetic transcriptions (Romancephonetic and Chinese), we use phoneme edit distance and normalized phoneme edit distance. As List (2019) suggests, we use B-Cubed F Scores (Amig\u00f3 et al., 2009) to capture the structural similarity between the gold and predicted protoforms (0: structurally dissimilar, 1: similar). With the exception of character and phoneme edit distance, the metrics enable fair comparison across different language families, which will differ in the average word length.\n\nResults\nTable 3 shows that our model consistently has the best performance on all datasets with regards to most metrics. The results were averaged across 5 runs. Out of all datasets, our model performs best on the Rom-orth dataset, where we achieve a 7.0% decrease in phoneme edit distance and a 1.43p.p improvement in accuracy relative to the RNN baseline. We observe the most dramatic performance difference with the RNN baseline on the Sinitic dataset: a 10.48% decrease in phoneme edit distance and a 5.47p.p increase in accuracy. For reproducibility, results on the publicly available portion of the Rom-phon and Rom-orth datasets are provided in Table 7 in the Appendix.\n\nAnalysis\nWe observe that the BCFS is relatively high for the Romance non-neural baselines compared to those of the Chinese ones. This suggests that the sound changes in the Romance datasets are more regular than that of Chinese, which corroborates List et al.\n(2014)'s results that more than half of the Chinese characters in their dataset could not be explained by a tree model. We examine the errors made by the Transformer model on the Rom-phon datasest. Substitutions constitute around 61% of the errors made by the Transformer\u037e deletions, 21%, and insertions, 18%. The highest number of substitution errors occur between [i, \u026a] , [e, \u025b], [o, \u0254] and [u, \u028a]-vowel pairs that contrast only in tenseness. This is consistent with the analysis of Meloni et al. (2021) , where substitutions between tense-lax vowel pairs take up the largest portion of errors.\nWe observe that other common substitution errors also happen between phonemes that share major phonetic features. This demonstrates that al-though no explicit phonetic information is fed directly into the model, the model makes mistakes motivated by phonetic similarity, like Meloni et al. (2021) .\nWe do not observe notable differences in the error statistics between the Transformer and the RNN.\n\nLanguage relatedness\nInspired by Fourrier (2022) , we probe our model for diachronic information on how genetically related each Romance language is to each other. We create a distance matrix between every pair of languages in a dataset by taking the cosine similarity between a pair's language embeddings. We then use sklearn (Pedregosa et al., 2011) 's implementation of the Ward variance minimization algorithm (Ward Jr, 1963) to perform hierarchical clustering on the distance matrix. We take a consensus of the dendrograms from 5 different runs using the consense program from PHYLIP (Felsenstein, 2013).\nAs we see in Figure 2 , the Transformer captures more of the phylogenetic relationships among the languages correctly for the Rom-phon dataset. Indeed, the Generalized Quartet Distance (GQD) (Sand et al., 2013\u037e Pompei et al., 2011\u037e Rama et al., 2018) between the gold and predicted tree, calculated using quartetDist from the tqDist library (Sand et al., 2014) , is 0.4 for the Transformer but 0.8 for the RNN. See Figure 5 in the Appendix for the results of the orthographic dataset.\n\nModel PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Sinitic\nRandom daughter (Chang et al., 2022) 3.7702 0.8405 0% 0.2893 0.2748 Majority constituent (Chang et al., 2022) 3.5031 0.7806 0% 0.2013 0.3695\nCorPaR (List et al., 2022) 3.2795 0.7278 0% 0.3972 0.3332 SVM + PosStr (List et al., 2022) 1.6894 0.3692 15.52% 0.1669 0.5418 RNN (Meloni et al., 2021) 1 Since the Romance dataset only includes 5 daughter languages, our results are insufficient to corroborate or contradict Cathcart and Wandl (2020) 's findings: the more accurate the protoforms, the less accurate the phylogeny will be. It is not clear if the model's language embeddings are learning information that reflects shared innovations (sound changes that if shared among a set of daughter languages, would be acceptable justification for grouping them)-the only acceptable criterion for phylogenetic inference in historical linguistics (Campbell, 2013) -or if the model is learning superficial phonetic similarity.\n\nConclusion\nBy showing that Transformers can outperform previous architectures in protoform reconstruction despite the inherent data scarcity of the task, our work motivates future research in this area to take full advantage of the recent advancements in the Transformer space.\nAccurate supervised reconstruction can help pre-dict protoforms for cognate sets where linguists have not reconstructed one yet. Future work could reconstruct proto-languages whose linguist reconstructions are not available, by transferring knowledge learned from languages with already reconstructed protoforms. Furthermore, future work can leverage the abundance of work in unsupervised NMT to adapt our Transformer model for the unsupervised setting, a more realistic scenario for the historical linguist.\n", "hypothesis": "Meloni et al. (2021) achieved the stateof-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with a traditional statistical approach and show that it outperforms the Transformer architecture.", "answer": false}
{"title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text", "content": "\nIntroduction and Background\nTextual Geolocation Identification, a crucial component of Geographic Information Retrieval (GIR), is the task of resolving the location, i.e., coordinates of a place, based on the reference to it in a text. It requires a combination of language and environmental knowledge. On top of the usual non-spatial linguistic challenges in Natural Language Understanding (NLU), such as named entity recognition (NER), anaphora resolution, bridging anaphora, etc., the textual geolocation task presents geospatial challenges that require multimodal processing and grounding (Ji et al., 2022; Fried et al., 2022; Misra et al., 2017; Qi et al., 2020; Paz-Argaman et al., 2020) .\nProper names, such as 'Rabin Square', also known as named entities in Natural Language Procesing (NLP), and as rigid designators in formal semantics (Kripke, 1972) , can be easily grounded based on a Gazetteer or a simple map. However, geolocating linguistic terms that involve spatial expressions without the explicit mention of a proper name still present an open challenge. This interpretation challenge includes the understanding and resolution of (at least): (i) definite descriptions, such as 'the school' (ii) geospatial terms, such as cardinal directions; 'east of'; and (iii) geospatial numerical reasoning; 'two buildings away from the pharmacy'. To address these and other challenges, we need to both ground entity mentions to their corresponding physical entities in the environment, and to reason about geospatial relations expressed between entities -these two processes being closely intertwined.\nTo do so, we need a corpus for the geolocation task that maps rich geospatial place descriptions to their corresponding location coordinates. However, current corpora for geolocation are based on naturally-occurring open-source resources, such as Wikipedia articles (Eisenstein et al., 2010; Wing and Baldridge, 2011; Han et al., 2012; Wing and Baldridge, 2014; Wallgr\u00fcn et al., 2018) , which are not spatially oriented, i.e., the description of locations is implicit or absent in the corresponding text. Subsequently, the accuracy of retrieval is fairly low (around 100 km).\nFurthermore, all geolocation datasets previously studied in NLP are in English, with a dearth of corpora for low-resource languages, in particular, for morphologically rich languages, such as Hebrew. To understand the geolocation challenges and build models that do various spatial reasoning tasks, English cannot be our sole focus (Baldridge et al., 2018) . Hebrew, a Semitic morphologically rich language is notoriously difficult to parse (Tsarfaty et al., 2020 (Tsarfaty et al., , 2019)) . Moreover, resources that are Place Description: The place is located near the Rothschild complex -at the end of Rothschild Street, as you go towards the sea, take a right for about three streets and then you will see the tower high above you. available for Hebrew NLP research focus on traditional tasks, such as Part-of-speech (POS) tagging, syntactic parsing, etc; and lack corpora for understanding and reasoning in real-world situations.\nIn this work we present HeGeL, a novel dataset for Hebrew Geo-Location, the first ever Hebrew NLU benchmark involving both grounding and geospatial reasoning. To create HeGeL, we crowdsourced 5,649 geospatially-oriented Hebrew place descriptions of various place types from three cities in Israel. We designed our task based on a realistic scenario of human place description, relying on people's memory of the world, rather than, e.g., using a map (Anderson et al., 1991; Paz-Argaman and Tsarfaty, 2019) . Crucially, relying on environmental cognition results in various levels of geospatial knowledge (Siegel and White, 1975) that are manifested in the descriptions and the geospatial reasoning that is required to resolve their location (Hayward and Tarr, 1995) . To avoid the much simpler task of grounding proper named entities, we explicitly restricted the use of proper names in the description of the place and adjacent landmarks.\nUnlike the text-based navigation task (MacMahon et al., 2006; Chen et al., 2019; Ku et al., 2020; De Vries et al., 2018; Thomason et al., 2020) , which requires representing an agent's current perspective, reflecting its route knowledge, we show that the HeGeL task requires a full-environment representation, thus, capturing complex geospatial relations among multiple physical entities. Through a thorough linguistic and empirical analysis, we demonstrate the characteristics and challenges associated with Hebrew place descriptions, showing that HeGeL serves both as a challenging NLU benchmark and as a corpus for geospatial cognition research.\n\nThe HeGeL Task and Dataset\nThis work addresses the task of geolocating places on a map based on natural language (NL) geospatial descriptions that are given in a colloquial language and based on participants' memory of the environment (i.e., cognitive map). The input to the HeGeL task is as follows: (i) an NL place description of the whereabouts of the place, and (ii) a map with rich details of the environment (e.g., physical entities names, geospatial relations, and attributes). The output is a pair of coordinates (x,y) specifying the physical location of the place described in the text. Figure 1 shows an example of a place description from HeGeL translated from Hebrew.\nTo simplify the crowdsourcing task and encourage participants' engagement, we frame the data crowdsourcing process as the well-known game, the treasure hunt task (Kniestedt et al., 2022) , in which the instructor-participant is required to describe in writing the location of the treasure, a known place in the city, to a different followerparticipant who then needs to locate it on a map. Thus, the online assignment is divided into two tasks: the instructor's writing of place descriptions and the follower's validation. To avoid preconceived notions as to the 'correct' way to describe a place, we first presented the participants with the task of writing a place description, and once completed, the validation task was given. 2 We hereby provide the details of the two UI tasks:\n(i) Task 1. Writing a place description In this task we requested participants to describe in a freeform text the location of a place known to them, to a third party who might not be familiar with the whereabouts of that place. To collect place descriptions based solely on people's memory, we did not visualize the area of the place, e.g., on a map. Instead, we ensured that the participants are well familiarized with the place by asking them to state how familiar they are with the place on a scale of 1-5. If this score was 1 or 2, we presented the participant with a different place to describe. To ensure diverse human-generated textual descriptions, places were chosen based on their type, position/location in the city (places were spread across the city), geometry, size, and context. To avoid the use of proper names, we developed a rule-based methodology to make sure that the explicit name of the goal (place) or of the nearby landmarks (< 100 meters) will not appear explicitly in the description. The original description was saved, and the participants were asked to input another description without the above names.\n(i) Task 2. Place description validation To verify that a person who reads the text description will understand where the treasure is hidden, i.e., geolocate the place, we developed a map-based retrieval task. The participant in the follower role was asked to read the crowdsourced textual description and mark its location on the map, i.e., where the treasure is hidden. For marking the location, we implemented an interactive online map based on OpenStreetMap (OSM), 3 which allows the participants to move and zoom-in to precisely pin the described place on the map. The map supports the cognitive process needed to ground mentioned entities to physical entities, reason about the geospatial relations, and locate the described place. To familiarize participants with the interactive map tool and task, they had to first pass a simple map marking test, and only then they could start task 2 of reading place descriptions (given by other participants), marking place locations on the map, and rate the clarity of the textual description on a scale of 1-5.\n\nTarget Selection and Retrieval Errors\nThe treasure-hunt task we devised included 167 places in the three largest cities in Israel: Tel Aviv, Haifa, and Jerusalem. These three cities are differently shaped, and show different physical, morphological and topographic features, which potentially affect the legibility and imageability of urban components, and therefore also on place descriptions. These differences can be expressed in the use of various physical features and prepositions, e.g., frequent use of the physical object 'landmark' and the prepositions 'above' or 'below' in hilly terrains that characterize Haifa and Jerusalem.\nTo assess the quality and interpretability of the place descriptions, we calculate the shortest Euclidean distance between the coordinates of the goal's (physical element) shape (polygon, line or point), and the location marked by the 'follower' on the map (task 2); we term this distance as retrieval error. To determine the agreement rate among human participants, each textual place description is validated by at least two participants. To ensure that we work with descriptions that can be geolocated, we set a hard distance threshold of 300 meters, based on analysis of the descriptions' clarity score that we had conducted on a prior (held-out) development corpus we collected for the task.\n\nData Statistics and Analysis\nThe resulting HeGeL dataset contains 5,649 validated descriptions paired with their coordinates on a map. The locations are divided among three cities: 2,142 in Tel Aviv, 1,442 in Haifa, and 2,065 in Jerusalem. 1,833 participants completed the writing task, inserting in total 10,946 place descriptions, and 2,050 participants completed 12,655 validation tasks. The dataset is balanced, with about 33 descriptions per place.\nFigure 2 shows a Venn diagram representing the relation of the three sets of city-based vocabularies (formed from unique lemmas produced by More et al. (2019) lemmatization tool). The intersection of the three cities contains only 15.07% of the entire vocabulary (the union of the three cities' vocabularies). The shared language is not focused on city-specific terms, such as 'Knesset'. Instead, it includes rich spatial terms, such as 'between', modified prepositions such as 'next to', and nondefinite entities, such as 'street'. From the Venn diagram we also conclude that almost half of the lemmas of the three vocabularies, corresponding to the three cities, contain city-specific lemmas: 48.6%, 40.65%, and 49.3% for Tel Aviv, Haifa, and Jerusalem, respectively. As such, HeGeL enables a city-split setup, training on one city and testing on a different unseen city, where city-reserved named entities present an out-of-vocabulary (OOV) challenge for models trained on another city.\nTable 1 shows an analysis of the linguistic phenomena manifested in the HeGeL dataset, demonstrating the spatial knowledge and reasoning skills required for solving the HeGeL task. We analyzed the frequency of the five types of elements in a city defined by Lynch (1960) , along with the three types of spatial knowledge defined in Siegel and White (1975) , and other spatial properties. The frequent use of cardinal directions, as well as the use of sur- vey knowledge, suggests that any NLP model built to deal with the HeGeL task should not only represent a local view of the goal, or possible routes, but also take into consideration the full region, and mimic people's map-like view of the environment. Therefore, unlike navigation tasks where only the agent's current perspective is represented in the model, this task requires full representation of the environment.\nWe further perform a quantitative analysis of word tokens and lemmas that appear in HeGeL, depicted in Table 2 . Overall, the HeGeL dataset contains a large vocabulary of 9,207 unique tokens and 6,663 unique lemmas. There are mentions of physical entities, but as we limited the mentions of named-entities of the described place and landmarks adjacent to it; these are relatively rare, and are mostly references to prominent city landmarks. Also, as most place descriptions are not route-based descriptions, there are only few verbs used in the descriptions. Prepositions, on the other hand, are abundant.\nIn Table 3 , using a one-way analysis of variance (ANOVA) test, we found a significantly (p<0.05) different distribution between place type descriptions and the following features: number of named entities, number of verbs, human verification retrieval error, and clarity score.\n\nExperiments\nWe create a zero-shot (ZS) city-based split, such that we train on one city and test on another. The train, development, and test sets correspond to the descriptions collected in Tel Aviv, Haifa, and Jerusalem, respectively. We evaluate different baseline models for the geolocation task on the HeGeL dataset. We use three evaluation metrics based on retrieval error: mean, median, and task completion (TC) accuracy -the percentage of place descriptions located within the 300 meters threshold. We provide three baselines for the HeGeL task.\nWe first assess a brute-force NER approach; i.e., we test whether recognizing named entities in the text and retrieving their corresponding coordinates is sufficient for solving the HeGeL task of geolocation. To this end, we used Google Maps API and produced two baseline models: (i) Google Maps API Query -we queried the API with the full raw text descriptions as input, with no prepossessing; and (ii) Oracle NER -we queried all 1-5 n-grams against Google Maps API and retrieved the closest geolocation to the goal.\nIn our second approach, we employ a dualencoder model. One encoder encodes the text using a Hebrew Monolingual pre-trained encoder, Aleph-BERT (Seker et al., 2022) , which produces a 768dimension vector representation of the text. The other encoder processes the environment, which is represented as a graph based on OSM data. Each point of interest in the graph is connected to an S2Cell 4 , which contains its geometry and is based on S2-geometry. These S2Cells are encoded using a random-walk algorithm to produce a 64dimensional vector for each cell. These vectors are then passed through a linear layer to produce 768-dimensional vectors. We calculate the cosine similarity score between the text and environment vectors and use it to align the respective representations via maximization of the cosine similarity score with a cross-entropy loss over the scores. 4 \n\nS2Cells\nare based on S2-geometry (https://s2geometry.io/), a hierarchical discretization of the Earth's surface (Hilbert, 1935) . Performing an ANOVA test, we found a significantly (p<0.05) different distribution between place type descriptions and the retrieval error of the Oracle NER. The mean retrieval error of the Path and Node place types were the lowest in both human verification and Oracle NER. This suggests that both of these place types are easier for humans to geolocate.\nThe results in Table 4 show that our task is not solvable with adequate resolution by the Google Maps API. The human performance provides an upper bound for the HeGeL task performance, while the simple Google Maps API Query provides a lower bound. The Google API model's low performance suggests that NER and the Gazetteerbased methods in and of themselves are insufficient to handle the HeGeL task successfully, and that geospatial reasoning is necessary. The Dualencoder's low performance on the ZS split suggests that OOV is a major challenge. The few-shot (FS) split shows an improvement of the model after finetuning on additional samples from the test-region (FS 20% and 80%). This suggests that a possible solution for the city-split setup might be dataaugmentation via generating grounded descriptions for the tested region -an approach we reserve for future research.\n\nConclusion\nThe contribution of this paper is threefold. First, we present the first geolocation benchmark with Hebrew place descriptions. Second, to the best of our knowledge, this is the only crowdsourced geolocation dataset, thus, eliciting explicit geospatial descriptions, allowing for better retrieval resolution. Finally, our analysis shows that the dataset presents complex spatial reasoning challenges which require novel environmental model representation. \n", "hypothesis": " Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resourcepoor languages, such as Hebrew.  In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning.", "answer": true}
{"title": "CoAug: Combining Augmentation of Labels and Labelling Rules", "content": "\nIntroduction\nNamed Entity Recognition (NER) is the task of identifying entity spans of specific types in a given document. While deep learning has led to the development of highly performant supervised NER models (Ma and Hovy, 2016; Lample et al., 2016; Devlin et al., 2019) , their performance is contingent on the availability of high-quality large labeled datasets, which is often expensive to collect. Moreover, it is impractical to assume the availability of large datasets for all domains. Hence, learning from limited labeled data is a pressing challenge in named entity recognition research. The majority of research in this area can be broadly classified into two distinct paradigms: few-shot learning with pre-trained language models (LMs) and weak supervision methods that utilize heuristic rules for entity extraction. In few-shot learning, models are trained to identify novel entities given just a few labeled examples for each entity type. While pretrained LMs have been explored for this setting, their susceptibility to overfitting on small datasets results in poor performance. Consequently, recent works improve recognition using prototypical networks (ProtoBERT, T\u00e4nzer et al., 2022) , improved representations from self-supervised pre-training of LMs (QuIP, Jia et al., 2022) , and self-training (Huang et al., 2021) . In the iterative learning process of self-training, many candidate entities are extracted and added into the training set for future iterations. However, premature models from initial iterations also add erroneous entities to the training set, resulting in models whose performance lags behind fully-supervised models that utilize large labeled datasets.\nOn the other hand, rule-based weak supervision methods utilize heuristic rules and manual lexicons (Shang et al., 2018; Peng et al., 2019) developed by domain experts to supervise entity recognition models. However, experts may find it challenging to enumerate all possible heuristics, which can limit the diversity of identified entities in docu-ments. In recent work, TaLLOR (Li et al., 2021) overcomes this limitation by automatically learning rules given unlabeled data and an initial set of seed rules (tens of rules). Nonetheless, while rule-based methods offer high precision, their performance is constrained by the logical language specified by the developer, which limits the set of identifiable entities. Moreover, learning rules can fail to identify entities in new linguistic contexts that would otherwise be known.\nWe hypothesize that the two paradigms of fewshot learning and rule-based weak supervision can effectively complement each other, as neural models are skilled at identifying candidates from different linguistic contexts but lack precision, while rulebased methods can identify accurate candidates with precision but lack the flexibility to identify entities in different contexts. Therefore, in this work, we propose Co-Augmentation (CoAug), as shown in Figure 1 , an iterative bootstrapping framework that effectively combines neural models, rule-based weak supervision methods, and unlabeled data.\nOur proposed framework draws inspiration from co-training (Blum and Mitchell, 1998) , but it has its own unique approach. Like co-training, CoAug aims to combine two distinct inductive biases in limited labeled data settings. Unlike co-training, instead of improving two models that use different feature sets individually by bootstrapping labels from each other, CoAug accomplishes the same goal by using two models that use different forms of supervision to expand the same label set. Additionally, in each iteration of CoAug, both classifiers are trained with the predictions made by both models, rather than just one. Our choice allows the framework to function from really small initial training sets for the individual models.\nWe evaluate our approach on four named entity recognition datasets that span general and science domains. Our results indicate that (a) CoAug consistently improves performance over self-training ruleaugmentation and few-shot models while being highly precise, (b) utilizing stronger pre-training for the neural models leads to improved performance of models in our framework. In summary, our contributions are as follows:\n\u2022 We present CoAug, a co-augmentation framework that leverages both rule-augmentation and label-augmentation approaches for NER.\n\u2022 Experimental results show that CoAug can perform better than prior rule-based methods on four datasets in two domains. \u2022 We provide a brief analysis of factors that contribute towards the success of CoAug.\n\nCoAug\nIn this work, we consider a setting where we have access to an initial set of seed rules, S, and a large unlabeled corpus, U, to perform the named entity recognition task. Applying the rules, S, on U provides the initial set of labeled examples, L, to train models in our framework.\nOur framework, CoAug (short for Co-Augmentation), iteratively improves the performance of two models by leveraging the bootstrapped predictions on unlabeled data by each model. Given that prior work in low-resource NER focuses on two parallel tracks of rule-augmentation and few-shot learning methods that do not interact with each other, we instantiate CoAug with a rule-augmentation model and a few-shot model to leverage the best of both paradigms. We refer to these components of our framework as Rule Augmenter and Label Augmenter (Figure 1 ). In the subsections below, we describe the Rule Augmenter and Label Augmenter modules.\n\nRule Augmenter Algorithm 1 TaLLOR\nRequire: U = {x1:N } unlabeled examples Require: R = {S} rules initialized with seed rules Require: C = {c1:M } candidate rules\nInitialize: L = {} for t in (1, . . . , T ) do // Apply rules to get weak-label set W = RULEAPPLIER(R, U) // Filter accurate examples W = LABELSELECTOR(W) L = L \u222a W U = U \\ L // Train NEURAL NER MODEL M \u2190 TRAIN(M , L) // Label using NEURAL NER MODEL LM \u2190 PREDICT(M, U) // Select High-precision Rules RS \u2190 RULESELECTOR(LM , C) C = C \\ RS R \u2190 R \u222a RS end for\nThe primary function of the Rule Augmenter is to automatically learn labeling rules from unlabeled data and use them to generate weak labels for training a neural model. In this work, we instantiate the rule augmenter module using the TaLLOR framework. Accordingly, our rule augmenter has the following subcomponents: (a) RULE APPLIER that applies rules over unlabeled data to generate weak labels, (b) LABEL SELECTOR that filters the most accurate examples based on the similarity of averaged token-level BERT (Devlin et al., 2019) representations of proposed entities to the representations of previously identified entities of the same label in the training set, (c) NEURAL NER MODEL that is trained on the accurate instances and proposes new entities in the unlabeled data that can be used to develop new rules, and (d) RULE SELECTOR that scores candidate labeling rules and selects high-precision rules that satisfy the predictions from the NEURAL NER MODEL. We summarize the iterative process of automatic rule identification by TaLLOR in Algorithm 1.\n\nLabel Augmenter\nThe Label Augmenter module consists of a NEU-RAL MODEL that learns to perform entity recognition with minimal supervision and LABEL SELEC-TOR that selectively adds the weak labels proposed by the NEURAL MODEL into the training set for the next iteration. \u25b7 initial threshold and increment\n\nAlgorithm 2 Label Augmenter\nInitialize: L = R(U) for t in (1, . . . , T ) do // Train NEURAL MODEL M \u2190 TRAIN(M ,L) // Label using NEURAL MODEL LM \u2190 PREDICT(M, U) // Select Examples Using Adaptive Threshold LM \u2190 LABELSELECTOR(LM , \u03b20 + t \u00d7 \u03b21) L = L \u222a LM end for\nIn this work, we experiment with two instantiations of the NEURAL MODEL using recent few-shot NER models, namely, ProtoBERT and QuIP. We use an adaptive threshold for the Label Selector to filter out low-quality, weakly labeled instances. Initially, we add 20% of the proposed instances from the Neural Model to the training set. Then, as the model becomes more confident in its predictions over iterations, we gradually increase the proportion of instances incorporated, with a 5% increase per iteration. We summarize the label augmenter algorithm in Algorithm 2.\nWe provide an outline for the CoAug algo- (Blum and Mitchell, 1998) , in CoAug, the Rule-Augmenter (Label-Augmenter) utilizes the examples that have been labeled by the Rule-Augmenter (Label-Augmenter) and the Label-Augmenter (Rule-Augmenter) to improve its entity recognition performance over iterations.\n\nExperimental Settings\nWe evaluate our framework on four popular datasets that are composed of two science-domain and two general-domain datasets. Following Li et al. (2021) , we utilize the training data without labels as our unlabeled data. Further, for all experiments, we use a set of 20 initial seed rules. These rules specify highly frequent entities for each category within a dataset.\nBC5CDR (Li et al., 2016) contains 1,500 PubMed abstracts with manual annotations for disease and chemical entity mentions. The abstracts are split equally among train, dev, and test sets (500/500/500).\n\nNCBI-Disease (Dogan et al., 2014) contains 793\nPubMed abstracts with manual annotations for disease entity mentions. The abstracts are split as 593/100/100 for train, dev, and test sets.\nCoNLL2003 (Tjong Kim Sang and De Meulder, 2003) We evaluate two instantiations of the CoAug framework where the Rule Augmenter uses TaLLOR, and the Label Augmenter uses either ProtoBERT/QuIP. For baselines, our main experiments compare CoAug against TaLLOR, self-trained ProtoBERT, and self-trained QuIP. Our code is implemented in Pytorch (Paszke et al., 2019) using the Huggingface library (Wolf et al., 2020) . For the Rule Augmenter section, all experimental hyperparameters follow that from Li et al. (2021) . Notably, we use the same hyperparameters for the NCBI-Disease, and WikiGold datasets as Li et al. (2021) did for BC5CDR and CoNLL2003. For science-domain datasets, we utilize SciBERT-base (Beltagy et al., 2019) as the base for the ProtoBERT model and BERT-base (Devlin et al., 2019) otherwise. We do not make any such distinctions for QuIP as it is a specially fine-tuned RoBERTa-large (Liu et al., 2019) model designed to perform well on extraction-based tasks (Jia et al., 2022) . We report the hyperparameters used for all experiments in more detail in Appendix C.\n\nMain Results\nTable 1 reports the test set F1 scores for all models on each of the four datasets. We observe that CoAug with QuIP/ProtoBERT outperforms TaLLOR on all 4 datasets substantially (average F1 on WikiGold for 2 skipping entities from the Miscellaneous category.\nCoAug is more than 2\u00d7 TaLLOR). Further, we also observe that utilizing the co-augmentation framework as opposed to self-training also aids models to produce similar results more reliably, as indicated by the variance of the results (in 3 out of 4 datasets). Further, we also observe that utilizing larger few-shot models, such as QuIP (which has a RoBERTa-large base), is complementary to our framework and continues to push the NER performance further. On comparing with QuIP, we observe that CoAug with QuIP performs better on 3 out of 4 datasets.\nHowever, on the NCBI-Disease dataset, we observe that QuIP outperforms CoAug by a considerable margin. On analysis, we identify that QuIP adds too many incorrect instances during the initial few iterations for this dataset. Consequently, the rule augmenter selects rules that lose precision, and the overall quality of examples in CoAug deteriorates. Nonetheless, since entity recognition for this dataset is hard for TaLLOR as well, we observe some improvement from using CoAug. Future work should look to address the issue of controlling candidates from neural models in order to maintain the reliability of the high-precision set.\nIn Figure 2 , we identify that the success of CoAug over high-precision rule-augmentation approaches, such as TaLLOR, lies in its ability to identify more instances in the unlabeled that improve precision as well as recall over TaLLOR.\n\nEffect of Task-aligned Pre-training\nIn this subsection, we analyze the contribution of pre-training strategies towards the performance of CoAug. Specifically, we ablate the effect of changing the pre-training initialization from QuIP to that of RoBERTa-large, the base model for QuIP. As shown in Table 2 , the performance of CoAug with RoBERTa-large lags far behind the performance CoAug +QuIP identifies more high-precision positive instances from the unlabeled data than TaLLOR while also maintaining high precision.\n\nModel BC5CDR CoNLL2003\nCoAug (TaLLOR + RoBERTa) 45.6 (0.1) 64.4 (0.2) CoAug (TaLLOR + QuIP) 65.9 (1.5) 76.8 (2.0) of CoAug with QuIP. On BC5CDR, we observe that the CoAug with RoBERTa-large performs poorly in comparison to TaLLOR as well. This indicates that any form of task-aligned pre-training, such as QuIP, can help design NER models for a diverse domain of tasks which corroborates some of the earlier work in task-adaptive pre-training (Gururangan et al., 2020) .\n\nConclusion\nIn this work, we introduce CoAug, a coaugmentation framework that utilizes unlabeled data to train rule-augmentation and neuralaugmentation models to become better NER taggers. Our results on datasets from two domains demonstrate the effectiveness of CoAug for lowresource domains. Our analysis reveals that CoAug is able to perform better than weak-supervision methods like TaLLOR because of an ability to find more positive instances while maintaining high precision. Further analysis shows the importance of factors such as the strength of pre-training that can contribute towards the success of models in domain-specific datasets.\n", "hypothesis": "However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well.  In this work, we propose CoAug, a co-augmentation framework that allows us to improve few-shot models and rule-augmentation models by using only rule-based weak supervision. By leveraging rule-based methods to train our models, we achieve higher precision and better performance in named entity recognition tasks.", "answer": false}
{"title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning", "content": "\nIntroduction\nSequence-to-sequence learning (Seq2Seq) with neural networks (Sutskever et al., 2014) has advanced the state-of-the-art in various NLP tasks, e.g. translation (Bahdanau et al., 2015; Vaswani et al., 2017) , summarization (Cheng and Lapata, 2016) , and grammatical error correction (Yuan and Briscoe, 2016) . Generally, Seq2Seq models are trained with the cross-entropy loss, which equally weighs the training losses of different target tokens.\nHowever, due to the token imbalance nature (Piantadosi, 2014) and the truth that different tokens contribute differently to the sentence meaning (Church and Hanks, 1990; Chen et al., 2020) , Figure 1 : An example to illustrate the changing token difficulties in different training steps in WMT'14 En-De. The token \"abschlie\u00dfen/ Sache\" is hard/ easy to learn at 50K while the trend is totally reversed at 100K. several works are developed to reweigh the tokenlevel training loss according to explicit (e.g. frequency) or implicit (uncertainty estimated by offthe-shelf language models) priors (Gu et al., 2020; Xu et al., 2021; Zhang et al., 2022a) . For example, Gu et al. (2020) proposed two heuristic criteria based on word frequency to encourage the model to learn from larger-weight low-frequency tokens. Zhang et al. (2022a) introduce target-context-aware metric based on an additional target-side language model to adjust the weight of each target token.\nDespite some success, there are still limitations in these adaptive training approaches. First, most of them predetermine the difficult tokens and fix such prior to guiding the training. However, in our preliminary study, we find the hard-to-learn tokens are dynamically changing during training, rather than statically fixed. As shown in Figure 1 , as the training progress goes, although the sentence-level loss is nicely converging, the difficult token is changing from \"abschlie\u00dfen\" to \"Sache\" in terms of the token-level loss. Second, these adaptive training methods overly emphasize fitting the difficult tokens' one-hot labels by reweighing the loss, which empirically may cause overfitting and limit the generalization (Norouzi et al., 2016; Szegedy et al., 2016; Xiao et al., 2019; Miao et al., 2021) . Also, a more recent study (Zhai et al., 2023) provides theoretical evidence to support that reweighting is not that effective to improve the generalization.\nCorrespondingly, we design a simple and effective Token-Level Self-Evolution Training (SE) strategy to encourage Seq2Seq models to learn from difficult words that are dynamically selected by the model itself. Specifically, SE contains two stages: \u2776self-questioning and \u2777self-evolution training. In the first stage, the Seq2Seq models dynamically select the hard-to-learn tokens based on the tokenlevel losses, then we encourage the Seq2Seq models to learn from them in the second stage, where, rather than adopting reweighing, we introduce a novel token-specific label smoothing approach to generate easily digestible soft label, which considers both the ground truth and model's prediction.\nExperiments across tasks, language pairs, data scales, and model sizes show that SE consistently and significantly outperforms both the vanilla Seq2Seq model and the re-implemented advanced baselines. Analyses confirm that besides improved lexical accuracy, SE generates diverse and humanlike generations with better model generalization.\n\nMethodology\nPreliminary Sequence-to-sequence (Seq2Seq) learning aims to maximize the cross-entropy (CE) loss of the log-likelihood of each target word in y = {y 1 , . . . , y N }, conditioned on source x, where the optimization treats all tokens equally:\nEQUATION\nHowever, due to the different learning difficulties of each token, it is sub-optimal to treat all tokens equally (Gu et al., 2020) . To address this limitation, a series of token-level adaptive training objectives were adopted to re-weight the losses of different target tokens (Xu et al., 2021; Zhang et al., 2022a) . The common goal of these methods is to facilitate the model training by fully exploiting the informative but underexplored tokens.\nHowever, our preliminary study shows that the hard tokens are dynamically changing (see Figure 1 ) in different training steps (or model structures), thus it is sub-optimal to employ static token priors (e.g. frequency) during training. Also, recent studies (Zhai et al., 2023) in the ML community theoretically show that reweighting is not that effective to improve the generalization. Based on the above evidence, we present the self-evolution learning (SE) mechanism to encourage the model to adaptively and wisely learn from the informative yet under-explored tokens dynamically determined by the model itself (Stage\u2776 in \u00a72.1), with an easy-tolearn label distribution (Stage\u2777 in \u00a72.1). A similar work to ours is Hahn and Choi (2019) . However, their method mainly considers the situation where the predicted answer is incorrect but close to the golden answer, while our method focuses on all dynamic hard tokens.\n\nToken-Level Self-Evolution Learning\n\u2776 Self-questioning Stage. The goal is to select the hard-to-learn tokens that are questioned by the Seq2Seq model itself during training dynamics. Previously, these difficult tokens are predetermined by external models or specific statistical metrics. However, inspired by the finding of dynamic change of difficult tokens during the training stage as shown in Figure 1 and the finding that the trained model contains useful information (Li and Lu, 2021) , e.g. synonym, we propose to straightforwardly leverage the behavior of the model to dynamically select target tokens. In practice, we first calculate the token-level CE loss, denoted as {l 1 , l 2 , ..., l n }, for each token for each forward pass. Then we set a loss threshold \u0393 and select the tokens whose losses exceed \u0393 as the target tokens, i.e., D = {t i |l i > \u0393} where i \u2208 N = {1, 2, ..., n}.\n\u2777 Self-evolution Training Stage. After selecting the difficult tokens, we encourage the model to carefully learn from them. Given the theoretical shortage (Zhai et al., 2023) and potentially caused overfitting or overconfidence problem (Miao et al., 2021) of reweighting and deliberately learning from difficult tokens, we propose to strengthen the learning from these tokens with a newly designed Token-specific Label Smoothing (TLS) approach. Specifically, motivated by the effect of label smoothing (LS) regularization (Szegedy et al., 2016) , we combine the ground truth p i and the model's prediction pi to form a new soft label p i for the i-th token. Then we use p to guide the difficult tokens D, while leaving label-smoothing CE loss for the other tokens. It is worth noting that we also apply the traditional label smoothing technique to pi to activate the information in the predicted distribution. Analogous to human learning, it is often easier for humans to grasp new things described by their familiar knowledge (Reder et al., 2016) therefore the new soft label fused both accurate ground truth and model's self-distribution is easily digestible. Mathematically, for difficult tokens t i , p i is formulated as:\nEQUATION\n)\nThen we calculate the losses of difficult tokens and the others, and combine the two losses:\nEQUATION\nwhere i \u2208 D and j \u2208 N \\ D.\n\nEvaluation\nMachine Translation on three widely-used benchmarks (Ding et al., 2020 (Ding et al., , 2021c (Ding et al., , 2022)) : smallscale WMT16 English-Romanian (En-Ro; 0.6M), medium-scale WMT14 English-German (En-De; 4.5M), and large-scale WMT14 English-French (En-Fr; 36.0M). We implement the baselines and our approach under Transformer-base settings. We follow the previous adaptive training approach (Gu et al., 2020) to pretrain with the cross-entropy loss with N steps, and further finetune the same steps with different adaptive training objectives, including Freq-Exponential (Gu et al., 2020) , Freq-Chi-Square (Gu et al., 2020) , D2GPo (Li et al., 2020) , BMI-adaptive (Xu et al., 2021) , MixCrossEntropy (Li and Lu, 2021) , CBMI-adaptive (Zhang et al., 2022a) , and SPL (Wan et al., 2020) . For N , we adopt 100K and 30K for larger datasets, e.g. En-De and En-Fr, and small dataset, i.e. En-Ro, respectively. We empirically adopt 32K tokens per batch for large datasets, the learning rate warms up to 1e-7 for 10K steps, and then decays 90K, while for small dataset En-Ro, The learning rate warms up to 1e-7 for 4K steps, and then decays 26K steps. All the experiments are conducted on 4 NVIDIA Tesla A100 GPUs. The SacreBLEU (Post, 2018) was used for evaluation. Besides translation, we also follow previous works (Liu et al., 2021b; Zhong et al., 2022; Zhang et al., 2022b) to validate the universality of our method on more sequenceto-sequence learning tasks, e.g., summarization and grammatical error correction.\nText Summarization on XSUM corpus (0.2M). We follow fairseq (Ott et al., 2019) to preprocess the data and train the model, then finetune them for the same steps. We evaluated with the ROUGE (Lin, 2004) , i.e. R-1, R-2, and R-L.\n\nGrammatical Error Correction on CoNLL14\n(1.4M). We follow Chollampatt and Ng (2018) to preprocess the data and train the model, then finetune them for the same steps. The MaxMatch (M 2 ) scores (Dahlmeier and Ng, 2012) were used for evaluation with precision, recall, and F 0.5 values.\n\nMain Results\nSE brings gains across language pairs and scales.\nResults on machine translation across different data sizes ranging from 0.6M to 36M in Table 1 show that our SE-equipped Transformer \"+ Self-Evolution (ours)\" 1) considerably improves the performance by averaging +0.92 BLEU points; 2) out-\nValid Loss Scale 0-1 1-2 2-3 >3\nTransformer 63.3 10.5 6.7 19.5 + SE 65.6 9.5 5.8 19.1 4 show that our method can achieve +0.4 and +1.2 improvement in BLEU and COMET respectively, which proves that our SE also works on extremely large datasets.\n\nAnalysis\nWe provide some insights to better understand the effectiveness of our approach. The ablation of important modules and parameters is in Appendix A.\n\nSE learns better token representation.\nTo verify whether our method helps learn better tokens representation, we conduct analysis on WMT14 En-De from learning loss and fine-grained generation perspectives, respectively. First, we count the token ratios distributed in different cross-entropy loss scales in Table 3 following Zan et al. (2022a) . Cross-entropy is a good indicator to quantify the distance between the predicted distribution and the ground truth in the valid dataset, and a lower value means a more similar distribution. As shown, our method improves the low-loss token ratios by +2.3%, indicating SE helps the model learn better token representations by reducing the token uncertainty. In addition, we follow Ding et al. (2021a) ; Liu et al. (2021a) to break the translation down into different granularities and measure their fined-grained performance. In particular, we calculate 1 the F-measure of words by different frequency buckets and BLEU scores of buckets of different lengths in Figure 2 . We see SE achieves better performance in all frequencies and sentence buckets, demonstrating our method can improve the performance of different granularities.\nSE encourages diverse generations. Lacking generation diversity is a notorious problem for Seq2Seq learning tasks (Sun et al., 2020; Lin et al., 2022) . Benefiting from better exploring the model's prediction with corrected soft labels, SE is expected to improve generation diversity. We follow Wang et al. (2022) to examine this by analyzing the performance in an additional multiplereference test of WMT'14 En-De (Ott et al., 2018) . We choose additional references for each of the 500 test sentences taken from the original test. Table 5 shows SE consistently outperforms the baseline with the average improvement being 0.9/1.0 BLEU, which indicates that our SE can effectively generate diverse results. SE enhances model generalization. Benefiting from better hard token exploration, SE-equipped Transformers are expected to own better generalizations. We examine it by testing on domain shift scenarios following Ding et al. (2021b) . In particular, we evaluate WMT14 En-De models over four out-of-domain test sets (M\u00fcller et al., 2020) in Table 6 and find that SE improves the translation by averaging +0.9 BLEU points, showing a better lexical generalization ability.\nSE encourages human-like generations. We design two types of evaluation on WMT14 En-Fr: 1) AUTOMATIC EVALUATION with COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) , which have a high-level correlation with human judgments. 2) HUMAN EVALUATION with three near-native French annotators who hold DALF C2 certificate 2 . Specifically, for human evaluation, we randomly sample 50 sentences from the test set to evaluate the translation adequacy and fluency, scoring 1\u223c5. For adequacy, 1 represents irrelevant to the source while 5 means semantically equal. For fluency, 1 means unintelligible while 5 means fluent and native. Table 7 shows the automatic and human evaluation results, where we find that our SE indeed achieves human-like translation.\n\nConclusion\nIn this paper, we propose a self-evolution learning mechanism to improve seq2seq learning, by exploiting the informative-yet-underexplored tokens dynamically. SE follows two stages, i.e. selfquestioning and self-evolution training, and can be used to evolve any pretrained models with a sim- ple recipe: continue train with SE. We empirically demonstrated the effectiveness and universality of SE on a series of widely-used benchmarks, covering low, medium, high, and extremely-high data volumes.\nIn the future, besides generation tasks, we would like to verify the effectiveness of SE on language understanding tasks (Wu et al., 2020; Zhong et al., 2023) . Also, it will be interesting to design SEinspired instruction tuning or prompting strategy like Lu et al. (2023) to enhance the performance of large language models, e.g. ChatGPT 3 , which after all have already been fully validated on lots of conditional generation tasks (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023; Wu et al., 2023) .\n", "hypothesis": " However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal.  In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method to fully and wisely exploit the knowledge from data.  SE focuses on dynamically learning the under-explored tokens for each forward pass and adaptively regularizes the training by introducing a novel token-specific label smoothing approach.", "answer": true}
{"title": "Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation", "content": "\nIntroduction\nPrompt-based learning typically works by modifying the input into cloze-style prompt templates and using the masked language models (MLMs) to complete the unfilled information in probabilistic. It has achieved promising performance in various NLP tasks (Schick and Sch\u00fctze, 2021; Lester et al., 2021; Hu et al., 2021) , especially in low-resource settings (Scao and Rush, 2021) . A promising prompt engineering category is demonstration learning (Gao * Equal contribution.\n\u2020 Corresponding author. et al., 2021; Liu et al., 2021a) , which seeks to provide a few answered samples as demonstrations to assist prompt prediction. As shown in Fig. 1 (a), the demonstration learning method concatenates the answered demonstrations per category to the prompt, and seeks to classify the [M ASK] token as great, indicating a positive prediction result based on a label-to-word mapping.\nThe intuition of demonstration learning is that samples with similar expressions or content can provide repetitive patterns (Liu et al., 2021a) . However, Min et al. (2022) point out that replacing gold demonstration labels with random labels marginally hurts performance. This finding is counter-intuitive and illustrates that the model could not comprehensively refer to the knowledge brought by the demonstrations in an implicit way. We attribute this problem to that existing methods simply concatenate the answered demonstrations to the prompt template without any additional operation, ignoring the dependencies between prompt and demonstrations.\nTo overcome this limitation, we rethink how human beings learn from demonstrations. Intuitively, when faced with a new challenging question, they typically (1) look for the most similar example to the question first, and then (2) reply to the question according to the answering steps of the retrieved example. Humans tend to strengthen the learning process through review strategies, i.e., finding a better solution to select similar examples and reanswering the questions of examples to consolidate known knowledge. Inspired by this, likewise, the interactions between the prompt and demonstrations could also be reinforced by imitating the human reviewing process for demonstration learning.\nIn this paper, we propose a simple-yet-effective version of demonstration learning, named Imitation DEMOnstration Learning (Imitation-Demo) to explicitly strengthen the two sub-steps of demonstration learning via human-like review. Specifi- \n\nContext 2:\nThe drama discloses almost nothing. cally, to accurately locate similar samples, we introduce a contrastive learning mechanism (Chen et al., 2020; Robinson et al., 2021) to reorganize demonstrations by reducing the divergences of demonstration contexts among the same category while increasing those divergences between different categories. Besides, to solidify known knowledge, we leverage a demonstration-label re-prediction method to emphasize the positions of the answers in demonstrations. Even without introducing new parameters or any prediction computation, our proposed method achieves state-of-the-art performance on 5 out of 14 classification corpus. Compared to the strong baseline LM-BFF (Gao et al., 2021) , Imitation-Demo achieves 1.11 points averaged improvement on the 14 datasets. Further study also shows that Imitation-Demo strengthens the association between prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.\n\nMethodology\nDemonstration Learning.\nAs illustrated in Fig. 1 (a), The prompt template x prompt consists of input sentence x sent and template x temp containing mask token, i.e., x prompt = [x sent , x temp ]. Firstly, we leverage the pre-trained SBERT (Reimers and Gurevych, 2019) to retrieve the demonstrations (including context x (k) and label y (k) ) for the k-th category that has maximum semantic similarity to the raw prompt context. Then, the retrieved demonstrations are concatenated to the input prompt. After that, we convert the concatenated input sentence\nx in to hidden vectors h in via the RoBERTa model (Liu et al., 2019) . The model is optimized by crossentropy loss, and the goal of demonstration learning is to predict y mask at the [M ASK] position from the hidden state of mask h mask via MLM head. The whole process could be formulated as 1 :\nx in = [x prompt , (x (1) , y (1) ), ...,(x (K) , y (K) )] h in = RoBERTa(x in ) L mask = CE(h mask , \u0176 mask ) p y mask | x in = MLM(h mask ) (1)\nwhere [.., .., ..] denotes concatenating diverse parts with sentence separator [SEP ] . K is the number of categories. CE is short for cross-entropy loss, and \u0176 mask is the ground-truth labels from the predefined label-to-word mapping. Demonstration Reorganization via Contrastive Learning. In demonstration learning, it is crucial to decide from which known demonstrations to select the repetitive patterns. Therefore, we introduce a contrastive learning mechanism to imitate human review behaviour by reorganizing the demonstrations based on their contexts. As shown in Fig. 1 (b)(I), we treat the demonstration contexts with identical categories to the input prompt as positive samples, and the others are regarded as negative ones. By pulling in positive samples and pulling out negative samples, the model could select the most relevant sample among the given demonstrations more precisely. In the experiment, we apply mean-pooling operations on the hidden states of positive, negative demonstration contexts h + , h \u2212 , and input sentence h in , obtaining the sentence representations s + , s \u2212 , and s in . Inspired by Robinson et al. (2021) in computer vision, we introduce HCL loss to ensure intra-class compactness while increasing inter-class distances:\nL context = E \u2212 log e s in \u2022s + e s in \u2022s + + N i=1 e s in \u2022s \u2212\n(2) where \u2022 is the dot product operation, N is the number of negative contexts in the task, and E [..] denotes calculating the mean value. Demonstration-label Re-prediction. We further utilize a demonstration-label re-prediction method to mimic human review behaviour by recovering the labels from all the given demonstration contexts. Specifically, the target of our model is not only to identify the category of [M ASK] token, but also to classify the tokens located in demonstration label positions. Take the binary classification task in Fig. 1 (b)(II) as an example, more than predicting the class of the mask token, the model also requires to predict y great and y terri (i.e., great and terrible) based on the hidden states h great and h terri at corresponding label positions.\nDuring training, the cross-entropy loss is utilized to calculate L great and L terri for different demonstration labels, then we sum them up to obtain the demonstration-label re-prediction loss L label :\nL great = CE(h great , \u0176 great ) L terri = CE(h terri , \u0176 terri ) L label = L great + L terri (3)\nwhere \u0176 great and \u0176 terri are the ground-truth labels at diverse demonstration label positions.\nSimilar contrastive learning and demonstrationlabel re-prediction operations can also be performed for the multi-category classification tasks. The overall loss of Imitation-Demo is defined as follows:\nL = L mask + \u03b1L label + \u03b2L context (4)\nwhere \u03b1, \u03b2 are weight coefficients to control the importance of different components. datasets. For SNLI (Bowman et al., 2015) , SST-2 (Socher et al., 2013) , CoLA (Warstadt et al., 2019) , MNLI (Williams et al., 2018) , QNLI (Rajpurkar et al., 2016) , RTE (Dagan et al., 2005; Giampiccolo et al., 2007; Bentivogli et al., 2009) , MRPC (Dolan and Brockett, 2005) , QQP 2 and SST-B (Cer et al., 2017), we use the original development sets for testing. For MR (Pang and Lee, 2005) , CR (Hu and Liu, 2004) , MPQA (Wiebe et al., 2005) and Subj (Pang and Lee, 2004) , we randomly sample 2,000 examples as the testing set. For SST-5 (Socher et al., 2013) and TREC (Voorhees and Tice, 2000) , we use the official test sets. F1 score (F1) are adopted as the evaluation metric of MRPC and QQP, and the other datasets utilize accuracy (acc) as the evaluation criteria. Parameters Setting We implement all the baselines and our frameworks using PyTorch (Paszke et al., 2019) . The pre-trained RoBERTa-large model and roberta-large-nli-stsb-mean-tokens SBERT (Reimers and Gurevych, 2019 ) from huggingface 3 are applied in the experiments. We get 16 samples per class during training for all models. In order to control the smoothness of the exponential functions when calculation contrastive learning loss, we divide every mean-pooling results with temperature T . Grid search mechanisim are utilized to select optimal hyper-parameter combinations on each split. Finally we select the the coefficients \u03b1 and \u03b2 as 1 and 5, respectively. The temperature T is set as 5 and the batch size is 16. The other hyper-parameters and the prompt templates are identical to the default settings in LM-BFF (Gao et al., 2021) for fair comparison. We report the average performance of models trained on 5 different randomly sampled training and dev splits, the random seeds are fixed as 13, 32, 42 ,87 , 100, respectively. Compared Methods. (1) Majority, which select the majority class of the dataset; (2) Prompt-based zero-shot: which use prompt tunning in zeroshot situations; (3) \"GPT-3\" in-context learn- Table 2 : Overall results on RoBERTa-large with 16 samples per class. We report the mean (variance) of models trained on 5 different randomly sampled training and dev splits. Prompt-based Fine-tuning (man) indicates trained with manually designed templates. \u2661 denotes we re-implement the EFL and LM-BFF models for fair comparisons.\ning, which use the in-context learning proposed in RoBERTa with no parameter updating; (4) Finetuning;\n(5) P-tuning (Liu et al., 2021b) , which employ trainable continuous prompt embeddings;\n(6) DART (Zhang et al., 2021) , which differentially optimize the prompt template and the target label during the backpropagation process; (7) Li's (Li et al., 2022) , which reformulate a classification or a regression task as a token-replaced detection problem utilizing pre-trained model Electra (Clark et al., 2020) ; (8) Demo-tuning (LM-BFF) (Liang et al., 2022) , which select \"mask token\" output feature as the input for contrastive learning to get a good representation of \"virtual demonstration\". We select the LM-BFF as the basic backbone model for fair comparisons. ( 9) LM-BFF + Sup-Con (Jian et al., 2022) \n\nConclusion\nIn this paper, we propose imitation demonstration learning (Imitation-Demo) to reinforce the correlations between prompt and given demonstrations. Inspired by the human review process, we introduce contrastive learning to locate similar samples and demonstration-label re-prediction mechanisms to solidify known knowledge. Experiments show that our method consistently outperforms other baselines on 5 out of 14 classification datasets in the few-shot settings. We hope this work could inspire the exploration of the working mechanism of demonstration learning and toward better few-shot learning abilities.\n", "hypothesis": "Experiment results show that our proposed method achieves state-of-the-art performance on all 14 classification corpus. Further studies also prove that Imitation-Demo eliminates the associations between the prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.", "answer": false}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": " Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors.  State-of-the-art UE methods for seq2seq models rely on computationally heavyweight and impractical deep ensembles.", "answer": true}
{"title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation", "content": "\nIntroduction\nSimultaneous speech translation (SimulST) refers to the process of producing an output translation concurrently with an oncoming source speech input. For humans, performing accurate SimulST is extremely difficult and becomes nearly impossible to perform over long periods of time. Given the potential broad applications of SimulST in industry and government sectors, there is a strong need for machine learning models to perform the task to a level above the capabilities of humans.\nOne branch of machine learning models that have been effective in SimulST is transformers (Vaswani et al., 2017) using block processing, a process that breaks an input sequence into segments which the encoder processes sequentially and individually (Dong et al., 2019) . As later segments may lose earlier information in a sentence (i.e., context fragmentation), techniques known as left context and memory banks have been introduced. The concept of left context was idealized with the Transformer-XL (Dai et al., 2019) , a model optimized for language modeling, which was later adapted for streaming automatic speech recognition (ASR). The Transformer-XL generated left context by saving the previous segment to each encoder layer, so the subsequent segment could include it in the attention calculation at the same encoder layer. Memory banks were later introduced in the self-attention calculation of the Augmented Memory Transformer (Wu et al., 2020) , allowing it to outperform the Transformer-XL in streaming ASR and also be state-of-the-art in SimulST (Ma et al., 2021) . These memory banks were token summarizations of previous segments and helped retain explicit long-term dependencies. The Augmented Memory Transformer also included the left context alongside the center (main) segment tokens with an additional right context, all of which add computational cost. We argue that the methods to generate and use left context and/or memory banks in both the Transformer-XL and Augmented Memory Transformer are naive, costing both models' performance at a given computational budget.\nIn this paper, we propose a computationally efficient architecture, the Implicit Memory Transformer, that implicitly retains memory through a novel left context generation method, thereby removing the need for memory banks entirely. Briefly, the proposed left context method for a given encoder layer leverages the previous segment's attention output in the attention calculation of the current segment. Our method for calculating left context is broadly applicable to any transformer model that utilizes block processing. The proposed Implicit Memory Transformer is more computationally efficient than the Augmented Mem-ory Transformer, reducing the cost of self-attention calculation, convolution layers, and feed-forward layers.\nWe conduct our experiments on the English-German, English-French, and English-Spanish language pairs of the MuST-C dataset (Cattoni et al., 2021) and demonstrate a significant speedup over the Augmented Memory Transformer for the forward pass of the encoder, with no reduction in the translation quality across all wait-k values.\n\nBackground and Related Works\nAugmented Memory Transformer: For SimulST, a transformer model waits for k token chunks before beginning translation, a policy referred to as wait-k (Ma et al., 2018) . One such transformer that uses this wait-k policy is the Augmented Memory Transformer (Ma et al., 2021) . The Augmented Memory Transformer breaks an input sequence into segments S i n \u2208 R s\u00d7d , where n denotes the segment position in the sequence and i denotes the layer index in the Augmented Memory Transformer. Each segment is composed of a left context\nL i n \u2208 R l\u00d7d of size l, a center context C i n \u2208 R c\u00d7d of size c, and a right context R i n \u2208 R r\u00d7d of size r.\nEach segment is of size s = l + c + r and overlaps with the previous and subsequent segments with the left and right context. Unlike the default transformer, the encoder of the Augmented Memory Transformer possesses two subsampling convolution layers to reduce the size of the segment inputs.\nIn the self-attention calculation for the encoder, memory banks, M i n \u2208 R N \u00d7d , are added to the keys and values where N denotes the maximum number of memory banks for a given layer. Each layer's memory banks summarize the previous segments and are theorized to allow the model to retain explicit long-term memory. Each memory bank is created using the attention output of a summarization query, \u03c3 i n \u2208 R 1\u00d7d , included in the attention calculation. This summarization query is calculated by averaging the tokens in the current segment. For any given layer, the queries, keys, and values can be represented by the following equations:\nQ i n = W i q [L i n , C i n , R i n , \u03c3 i n ]\n(1)\nK i n = W i k [M i n , L i n , C i n , R i n ]\n(2)\nEQUATION\nIn each of the equations, W i q , W i k , and W i v are the query, key, and value projection matrices\nfor layer i. The [.] operator concatenates L i n , C i n , R i n with \u03c3 i n or M i n .\nAfter the encoder processes each individual segment, they are concatenated before being provided to a simultaneous decoder (Ma et al., 2020b) . Average Lagging: Average Lagging is one prominent metric to determine the efficacy of a SimulST model (Ma et al., 2018) . It denotes in milliseconds the lag between the output translation and the input source sequence (Ma et al., 2020b) . BLEU Score: An equally important metric to evaluate a SimulST model is the BLEU score, which measures the translation similarity between the predicted output and the target output. The BLEU score ranges from 0 to 1 and is often represented with percentages (Papineni et al., 2002) .\n\nImplicit Memory Transformer\nWe propose an Implicit Memory Transformer that leverages a new left context generation method to retain an implicit memory of previous segments. As such, we are able to remove the explicit memory provided by the memory banks that are expensive to compute in the Augmented Memory Transformer. Our new implicit memory left context is unique at each layer of the encoder, whereby it is composed of a portion of the output from the self-attention calculation of the previous segment's center context.\nSpecifically, suppose our implicit memory left context is denoted as Z i n \u2208 R l\u00d7d . Then, in the self-attention calculation of the Implicit Memory Transformer, the queries, keys, and values for each layer's attention calculation can be calculated as follows:\nEQUATION\nK i n = W i k [Z i n , C i n , R i n ]\n(5)\nEQUATION\nIn comparison with the calculation of the queries, keys, and values of the current state-of-the-art Augmented Memory Transformer shown in Equation 1, 2 and 3, our Implicit Memory Transformer has three notable differences consisting of:\n1) Removed memory banks: The memory bank terms in Equation 2 and 3 not only provide the model with explicit long-term memory but also introduce a recurrence mechanism to the transformer, which is a form of implicit memory. By removing memory banks and instead including the recurrence mechanism in the left context, we capture the benefits of this implicit memory without the additional cost to compute memory banks.\n2) Attention-based left context: In using the output from the attention calculation of the previous segment rather than the raw segment input as left context like the Transformer-XL, we are able to capture a learned representation of the previous segment at a given layer. This is similar to the Augmented Memory Transformer using the attention output associated with the summarization query as a memory bank. However, since we do not compress the segment into a summarization query, we capture a more realistic representation.\n3) Removed left context in the queries: The Augmented Memory Transformer, includes the left context in each segment and, subsequently, the queries to allow it to generate a learned representation of the left context alongside the current segment. However, since our Implicit Memory Transformer already has a saved learned representation of the left context for a given layer, it removes the need to include the left context in the segment.\nFrom the above attributes, the self-attention calculation of the Implicit Memory Transformer becomes more efficient than that of the Augmented Memory Transformer, as memory banks are no longer included in the keys and values, and the left context and summarization query are removed from the queries. Furthermore, our Implicit Memory Transformer reduces the computation cost of the feed-forward neural network and the convolution subsampling layers, as they no longer need to process tokens contained in the left context.\n\nComplexity Analysis\nWe will now perform complexity analysis for the self-attention and convolution subsampling layers in the Augmented Memory Transformer. The complexity analysis of a convolution subsampling layer with a kernel size of one is identical to that for the linear transformations in the feed-forward network. The self-attention layer has a complexity of O(n 2 \u2022 d) and the convolution layer has a complexity of O(K \u2022 n \u2022 d 2 ) where n is the input sequence length, d is the hidden size, and K is the kernel size (Vaswani et al., 2017) .\nThe complexity of the self-attention layer of the old Augmented Memory Transformer would thus be O((N + l + c + r)(l + c + r) \u2022 d) and the complexity with the new method of calculating left context would be O((c + r)(l + c + r) \u2022 d).\nSimilarly the complexity of the convolution layers would change from O(K\n\u2022 (l + c + r) \u2022 d 2 ) to O(K \u2022 (c + r) \u2022 d 2 ).\nGiven the computational complexity decrease for all layers in the Augmented Memory Transformer with respect to the left context size and memory banks, it lends to the possibility of increasing the left context size for greater translation performance.\n\nExperimental Setup\nWe conducted experiments on the English-German (en-de), English-French (en-fr), and English-Spanish (en-es) language pairs from the MuST-C dataset (Cattoni et al., 2021) . The data preparation scripts for the MuST-C dataset are provided in Fairseq 1 (Ott et al., 2019; Wang et al., 2020) , whereby Kaldi is used to generate 80-dimensional log-mel filter bank features, and text is tokenized with a SentencePiece 10k unigram vocabulary. The statistics of the training, development, and test set (tst-COMMON) for the English-German, English-French, and English-Spanish language pairs of the MuST-C dataset are provided in Table 1 .\n\nLanguage Pair Train\nDev Test en-de 250942 1415 2580 en-fr 275085 1412 2632 en-es 265625 1316 2502\nTable 1 : The number of sentences in the train, development, and test (tst-COMMON) sets of the MuST-C dataset for the en-de, en-fr, en-es language pairs (Cattoni et al., 2021) .\nThe architectures of the Augmented Memory Transformer and Implicit Memory Transformer trained were nearly identical, containing 33.1 M parameters (Ma et al., 2021) . Their encoders consisted of 12 layers beginning with two convolution layers with a combined subsampling factor of 4, followed by a feed-forward neural network. Their decoders consisted of 6 layers. Each of these layers has a hidden size of 256 with 4 attention heads. Relative positional encodings were applied to each self-attention layer with a clipping distance of 16 (Shaw et al., 2018) . Layer normalization was performed prior to each layer. Additionally, we trained each model with a wait-1, wait-3, wait-5, and wait-7 policy using a pre-decision ratio of 8 (Ma et al., 2020b) . We provide public access to a derivative of Fairseq containing our implementation for the Implicit Memory Transformer 2 .\nAll training was performed on a single V100-32GB. The training process consisted of ASR pre-training followed by SimulST training. For SimulST training, the models were trained with label-smoothed cross-entropy loss, the Adam optimizer (Kingma and Ba, 2014), and an inverse square root scheduler. There was a warm-up period of 7500 updates where the learning rate of 0.0001, followed by a learning rate of 0.00035. To regularize the model weights, we used a weight decay value of 0.0001, a dropout of 0.1, an activation dropout of 0.2, and an attention dropout of 0.2. All models were trained with early stopping using a patience of 10. After the training was complete, the final ten checkpoints were averaged.\nThe translation quality and latency were determined by detokenized BLEU with SacreBLEU (Post, 2018) , and Average Lagging (Ma et al., 2020b) , respectively. Both of these metrics were obtained using the SimulEval toolkit 3 , which simulates SimulST (Ma et al., 2020a) .\n\nPerformance Evaluation\nWe demonstrate the efficacy of our Implicit Memory Transformer on the English-German language pair for a single run in Figure 1 in terms of average lagging and BLEU score. versus its memory bank counterpart across all waitk values. This confirms the effectiveness of the attention-generated left context of the proposed Implicit Memory Transformer for the English-German language pair.\nWe see similar results with the English-French and English-Spanish language pairs provided in Figure 2 and Figure 3 , respectively. In both cases, the Implicit Memory Transformer performs nearly identically to the Augmented Memory Transformer using memory banks by not negatively impacting either the BLEU score or Average Lagging. Additionally, as with the results in Figure 1 , the Augmented Memory Transformer sees an average decrease of 6.23 BLEU and 4.47 BLEU across all wait-k values when memory banks are removed for the English-French and English-Spanish language pairs respectively. Once again substantiating the efficacy of our attentiongenerated left context in the Implicit Memory Transformer, which does not see a performance decrease without memory banks.\n\nEvaluation Speedup\nWe provide a demonstration of how the left context size affects the forward pass time of a segment through the encoder of an Augmented Memory Transformer with three memory banks, an Augmented Memory Transformer without memory banks, and the Implicit Memory Transformer in Figure 4 . The left context size is scaled with tokens, and the duration of the forward pass of a segment through the encoder is scaled in milliseconds. Each model compared uses a right context of 32 tokens and a center context of 64 tokens for each tested left context size. Each measurement point in Figure 4 is made by averaging the duration of ten forward passes through the encoder using two 14-core 2.20 GHz Intel Xeon Gold 5120 with 19712 KB cache. \n\nConclusion\nAchieving computationally efficient simultaneous speech translation (SimulST) is critical to its deployment in practical real-time applications. However, even with the state-of-the-art SimulST approach of the Augmented Memory Transformer, its method of generating left context is computationally costly and ineffective, requiring the usage of memory banks to compensate for its shortcomings. As such, we propose an Implicit Memory Transformer that utilizes an attention-based left context to provide the model with implicit memory. We found that the Implicit Memory Transformer was able to achieve nearly identical performance to the Augmented Memory Transformer at a significantly reduced computational cost.\n", "hypothesis": "Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass, but at the cost of significantly lower translation quality compared to the state-of-the-art approach that employs both left context and memory banks.", "answer": false}
{"title": "Another Dead End for Morphological Tags? Perturbed Inputs and Parsing", "content": "\nIntroduction\nThe use of morphological tags was a core component of dependency parsers to improve performance (Ballesteros and Nivre, 2012) . With the rise of neural models, feeding explicit morphological information is a practice that has greatly vanished, with (often) the exception of part-of-speech (PoS) tags. In this line, Ballesteros et al. (2015) already found that character-based word vectors helped improving performance over purely word-level models, specially for rich-resource languages, for which the use of morphological information is more relevant (Dehouck and Denis, 2018) . Related, Dozat et al. (2017) showed that predicted PoS tags still improved the performance of their graph-based parser, even when used together with character-based representations. Smith et al. (2018) and de Lhoneux et al. (2017) studied the impact that ignoring PoS tag vectors had on the performance of a biLSTM transition-based parser (Kiperwasser and Goldberg, 2016) . They conclude that when considering PoS tags, word-level, and character-level embedddings, any two of those vectors are enough to maximize a parser performance, i.e., PoS tag vectors can be excluded when using both word-level and characterlevel vectors. Zhou et al. (2020) showed the utility of PoS tags when learned jointly with parsing. Recently, Anderson and G\u00f3mez-Rodr\u00edguez (2021) and Anderson et al. (2021) have explored the differences between using gold and predicted PoS tags, showing that the former are helpful to improve the results, while the latter are often not, with the exception of low-resource languages, where they obtain small but consistent improvements. Furthermore, Mu\u00f1oz-Ortiz et al. (2022) showed that the efficacy of PoS tags in the context of sequence labeling parsing is greatly influenced by the chosen linearization method.\nHowever, most of such work has focused on: (i) studying the effect of the universal PoS tags (Zeman et al., 2021) , and (ii) its impact on nonperturbed inputs. Yet, NLP models are very sensible and brittle against small attacks, and simple perturbations like misspellings can greatly reduce performance (Ebrahimi et al., 2018; Alzantot et al., 2018) . This has been shown for tasks such as named-entity recognition, question answering, semantic similarity, and sentiment analysis (Moradi and Samwald, 2021) . In parallel, defensive strategies have been tested to improve the robustness of NLP systems, e.g., placing a word recognition module before downstream classifiers (Pruthi et al., 2019) , or using spelling checks and adversarial training (Li et al., 2019) . Yet, as far as we know, no related work has been done on testing perturbed inputs for parsing and the effect, positive or negative, that using morphological information as explicit signals during inference might have in guiding the parsers. 1\n\nAdversarial framework\nPerturbed inputs occur for several reasons, such as for instance on-purpose adversarial attacks (Liang et al., 2018) or, more likely, unintended mistakes made by human writers. In any case, they have an undesirable effect on NLP tools, including parsers. Our goal is to test if under such adversarial setups, coarse-and fine-grained morphological tags: (i) could help obtaining more robust and better results in comparison to word-only parsers (going against the current trend of removing any explicit linguistic input from parsers); or (ii) if on the contrary they contribute to degrade parsing performance.\nBelow, we describe both how we generate (i, \u00a72.1) linguistically-inspired attacks at characterlevel, and (ii, \u00a72.2) the tested parsers.\n\nPerturbed inputs\nTo perturb our inputs, we use a combination of four adversarial misspellings, inspired by Pruthi et al. (2019) who designed their method relying on previous psycholinguistic studies (Davis, 2003; Rawlinson, 1976) . In particular, we consider to: (i) drop one character, (ii) swap two contiguous characters, (iii) add one character, and (iv) replace a character with an adjacent character in a QWERTY keyboard. These changes will probably transform most words into out-of-vocabulary terms, although some perturbations could generate valid tokens (likely occurring in an invalid context). We only apply perturbations to a fraction of the content words of a sentence 2 (details in \u00a73), as function words tend to be shorter and a perturbation could make them unrecognizable, which is not our aim.\nFinally, we only allow a word to suffer a single attack. Since we will be evaluating on a multilingual setup, we considered language-specific keyboards to generate the perturbations. We restrict our analysis to languages that use the Latin alphabet, but our adversarial attack would be, in principle, applicable to any alphabetic script.\n\nParsing models\nSince we want a thorough picture of the impact of using morphological information on parsers, we include three models from different paradigms: G\u00f3mez-Rodr\u00edguez, 2019). It uses biLSTMs (Hochreiter and Schmidhuber, 1997) to contextualize the words, and the outputs are then fed to a pointer network (Vinyals et al., 2015) , which keeps a stack and, in a left-to-right fashion, decides for each token its head.\n2. A biaffine graph-based parser (Dozat et al., 2017) . This model also uses biLSTMs to first contextualize the input sentence. Differently from Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, the tree is predicted through a biaffine attention module, and to ensure wellformed trees it uses either the Eisner (1996) or Chu (1965) ; Edmonds (1968) algorithms. 3\n3. A sequence labeling parser (Strzyz et al., 2020) that uses a 2-planar bracketing encoding to linearize the trees. Like the two other parsers, it uses biLSTMs to contextualize sentences, but it does not use any mechanism on top of their outputs (such as biaffine attention or a decoder module) to predict the tree (which is rebuilt from a sequence of labels).\nParticularly, we use this third model to: (i) estimate how sensitive raw biLSTMs are to attacks, (ii) compare their behavior against the transitionand graph-based models and the extra mechanisms that they incorporate, (iii) and verify if such mechanisms play a role against perturbed inputs.\nInputs We concatenate a word vector, a second word vector computed at character level, and (optionally) a morphological vector. This is the preferred input setup of previous work on PoS tagging plus its utility for neural UD parsing (de Lhoneux et al., 2017; Anderson and G\u00f3mez-Rodr\u00edguez, 2021) . 4 Note that character-level vectors should be robust against our attacks, but it is known that in practice they are fragile (Pruthi et al., 2019) . In this respect, our models use techniques to strengthen their behaviour against word variation, by using character-level dropout. This way, we inject noise during training and give all our models a lexical-level defensive mechanism to deal with misspellings. We kept this feature to keep the setup realistic, as character-level dropout is implemented by default in most of modern parsers, and ensure stronger baselines.\nTraining and hyperparameters We use nonperturbed training and development sets, 5 since our aim is to see how parsers trained in a standard way (and that may use explicit morphological features) behave in production under adversarial attacks. Alternatively, we could design additional techniques to protect the parsers against such perturbations, but this is out of the scope of this paper (and for standard defensive strategies, we already have character-level dropout). For all parsers, we use the default configuration specified in the corresponding repositories. We use 2 GeForce RTX 3090 for training the models for around 120 hours.\n\nMorphological tags\nTo predict them, we use a sequence labeling model with the same architecture than the one used for the sequence labeling parser. We use as input a concatenation of a word embedding and a character-level LSTM vector.\n\nExperiments\nWe now describe our experimental setup: Data We selected 14 UD treebanks (Zeman et al., 2021) that use the Latin alphabet and are annotated with universal PoS tags (UPOS), languagespecific PoS tags (XPOS), and morphological feats (FEATS). It is a diverse sample that considers different language families and amounts of data, whose details are shown in Table 1 . For the pre-trained word vectors, we rely on Bojanowski et al. (2017) . 6 Also, note that we only perturb the test inputs. Thus, when the input is highly perturbed, the model will mostly depend on the character representations, and if used, the morphological tags fed to it.\nGenerating perturbed treebanks For each test set, we create several versions with increasing percentages of perturbed content words (from 0% to 100%, with steps of 10 percent points) to monitor how the magnitude of the attacks affects the results.\nFor each targeted word, one of the four proposed perturbations is applied randomly. To control for randomness, each model is tested against 10 perturbed test sets with the same level of perturbation.\nTo check that the scores were similar across runs, we computed the average scores and the standard deviation (most of them exhibiting low values).\nSetup For each parser we trained four models: a word-only (word) baseline where the input is just the concatenation of a pre-trained word vector and a character-level vector, and three extra models that use universal PoS tags (word+UPOS), language-specific PoS tags (word+XPOS), or feats (word+FEATS). For parsing evaluation, we use labeled attachment scores (LAS). For the taggers, we report accuracy. We evaluate the models on two setups regarding the prediction of morphological tags: (i) tags predicted on the same perturbed inputs as the dependency tree, and (ii) tags predicted on non-perturbed inputs. Specifically, the aim of setup ii is to simulate the impact of using a tagger that is very robust against lexical perturbations.\n\nResults\nTables 2 and 3 show the average LAS results across all treebanks and models for tags predicted on perturbed and non-perturbed inputs, respectively. Figures 1, 2, and 3 display the mean LAS difference between the word and the other model configurations, using tags predicted on both perturbed and non-perturbed inputs for each parser.\n\nResults using morphological tags predicted on perturbed inputs\nFigure ??.a shows the score differences for the transition-based parsers. The average difference between the baseline and all the models using morphological tags becomes more negative as the per- Table 3 : Average LAS scores for all treebanks and degrees of perturbation for the word, word+UPOS, word+XPOS, and word+FEATS models using morphological tags predicted on non-perturbed input. centage of perturbed words increases. Such difference is only positive for word+XPOS when none or a few percentage of words are perturbed. All morphological tags show a similar tendency, word+FEATS degrading the performance the most, followed by the 'coarse-grained' word+UPOS.\nFigure 2 .a shows the results for the graph-based parsers. Again, most morphological inputs contribute to degrade the performance faster than the baseline. In this case, no model beat the baseline when predicting tags on the perturbed inputs. The performance of word+FEATS and word+UPOS is similar (performing word+UPOS a bit better), and the word+XPOS models improve the performance the most. Figure 3 .a shows the results for the sequence labeling parsers: differences between the baseline and the models utilizing morphological information exhibit minor changes ranging from 0% to 100% of perturbed words. Also, the usefulness of the morphological information depends on the specific tags selected. While word+UPOS obtains similar results to the baseline, word+XPOS scores around 2-3 points higher for the tested percentages of pertur- bations, and word+FEATS harms the performance in a range between 1 and 4 points.\nThe results show that feeding morphological tags to both graph-and transition-based parsers has a negative impact to counteract such attacks, degrading their performance faster. On the contrary, the sequence labeling parsers, that rely on biLSTMs to make the predictions, can still benefit from them. In addition, the different trends for the sequence labeling parser versus the transition-and graphbased parsers, which additionally include a module to output trees (a pointer network and a biaffine attention, respectively), suggest that such modules are likely to be more effective against adversarial attacks than explicit morphological signals.\n\nResults using morphological tags predicted on non-perturbed inputs\nAs mentioned above, we use this setup to estimate whether morphological tags could have a positive impact if they were extremely robust against lexical perturbations (see also Figures 1.b, 2.b and 3.b). In the case of the transition-based parser, we observe that morphological tags predicted on non-perturbed inputs help the parser more as the inputs' perturbation grows, being word+XPOS the most helpful information, while UPOS and FEATS become useful only when sentences are perturbed over 20% (but they also become more and more helpful). The graph-based parser also benefits from the use of more precise tags: word+XPOS models beat the baseline when the perturbation is over 30%; and over 50% for word+UPOS and word+FEATS setups. Finally, for the sequence-labeling parser, morphological information from a robust tagger helps the model surpass the baseline for any percentage of perturbed words (except in the case of word+FEATS, when it only happens with perturbations over 20%).\n\nDiscussion on slightly perturbed inputs\nUnintended typos are commonly found among users. For experiments with a small percentage of perturbed words (< 20%), transition-based parsers show improvement solely with the word+XPOS model, even when using non-robust taggers. Conversely, graph-based parsers do not benefit from morphological tags in this setup. Last, sequence labeling parsers benefit from incorporating XPOS and UPOS information, irrespective of the tagger's robustness, but not FEATS.\n\nDifferences across morphological tags\nAveraging across languages, the language-specific XPOS tags have a better (or less bad, for setup i) behavior. These tags are specific to each language. The coarse-grained UPOS tags have a common annotation schema and tagset. This eases annotation and understanding, but offer less valuable information. For FEATS, the annotation schema is common, but in this case they might be too sparse.\n\nConclusion\nThis paper explored the utility of morphological information to create stronger dependency parsers when these face adversarial attacks at characterlevel. Experiments over 14 diverse UD treebanks, with different percentages of perturbed inputs, show that using morphological signals help creating more robust sequence labeling parsers, but contribute to a faster degradation of the performance for transition-and graph-based parsers, in comparison to the corresponding word-only models.\n", "hypothesis": " The results on 14 diverse UD treebanks show that under such attacks, for transition-and graph-based models their use contributes to degrade the performance even faster, while for the (lower-performing) sequence labeling parsers they are helpful.", "answer": true}
{"title": "Race, Gender, and Age Biases in Biomedical Masked Language Models", "content": "\nIntroduction\nSocial biases based on race, gender, and age cause healthcare disparities. Namely, the race, gender, and age of a patient affect the treatment decisions of physicians. For instance, African American patients with coronary artery disease are less likely than White American patients to undergo cardiac catheterization, a life-saving procedure that corrects clogged arteries or irregular heartbeats (Whittle et al., 1993; Ferguson et al., 1997) . Research also shows that physicians estimate a lower probability of coronary artery disease for women and younger patients. Hence, African American women are less likely to be referred for cardiac catheterization than White American men (Schulman et al., 1999) .\nIn an attempt to identify and eliminate healthcare disparities, implicit bias has been studied in-depth in real-world patient-provider interactions in both the emergency department (Dehon et al., 2017) and medical assessment of physicians on computersimulated patients (Hirsh et al., 2015) . Despite such efforts, these stereotypes continue to prevail Following the recent releases and success of pretrained models in various domains, researchers introduced pre-trained models trained on large-scale biomedical corpora (Beltagy et al., 2019; Lee et al., 2019; Li et al., 2022) . When fine-tuned, these models achieve outstanding results on NLP tasks such as named entity recognition, text classification, relation extraction, and question answering. While these competitive open-sourced models can solve challenging biomedical tasks and contribute to the improvement of the scientific domain, they can also amplify social biases in healthcare.\nTo identify such stereotypes, we examine social biases existing in the biomedical pre-trained models. We define bias as a tendency to associate a particular group with an illness in generated sentences and examine, given a bias, with which illness a model associates more. First, prompts are manually curated based on evidence-based practice. Then, the models fill in the masked prompts. We observe the words pertinent to illness, such as \"cancer\" and \"diabetes.\" Lastly, a case study of the biases in coronary artery disease diagnoses and treatments is undertaken.\nIn summary, our contributions are: (1) We in-vestigate biases in biomedical masked language models with manually curated prompts. The experimental results show that BERT is less biased than the biomedical models in race and age and that each model associates distinct illnesses with a patient regardless of the bias.\n(2) We study whether the models associate a specific illness and a treatment with a particular bias. We use two bias metrics and demonstrate the challenges in measuring bias.\n\nMethod\nWe investigate the influences of biases on the biomedical pre-trained language models by identifying associations between generated tokens and biased terms. First, we curate prompts grounded on evidence-based medicine. Next, we compare the diagnosis predictions of a model based on race, gender, and age biases.\n\nPrompt Curation\nWe manually curate prompts for diagnosis prediction of pre-trained models. agnosis] .\" An exemplary sentence is \"A woman is diagnosed with pneumonia.\" We mask the [Diagnosis] to observe the differences in generated tokens of each model. In the provided example, the word \"pneumonia\" is masked. Nouns and pronouns that identify race, gender, and age bias fill the [Bias] section of the sentence. For example, to reflect the age bias, we choose the words \"a young person\" and \"a junior\" to represent the younger age group and the words \"an old person\" and \"a senior\" for the older age group. We use the word \"person\" to avoid the influences of gender-specific words such as \"woman\" and \"man.\" As for gender-biased words, we adopt the binary classification of gender and use gender-specific pronouns and nouns. Finally, we use the five minimum categories of race set by the OMB to choose words that reflect racial bias 1 : White American, African/Black American, American Indian, Asian, and Native Hawaiian. The full list of the chosen nouns can be found in Ap-pendix A.\n\nDiagnosis Prediction\nGiven a prompt, a pre-trained model generates tokens to fill in the mask with scores. We sum the scores of each token in all the prompts of a given bias. For comparison, we explore the following biomedical pre-trained models:\n\u2022 BioBERT (Lee et al., 2019) is a BERT (Devlin et al., 2019) trained on PubMed abstracts with 4.5 billion words and PubMed Central full-text articles with 13.5 billion words.\n\u2022 ClinicalBERT (Alsentzer et al., 2019) is BioBERT (Lee et al., 2019) trained on approximately 2 million clinical texts from the MIMIC-III v1.4 database (Johnson et al., 2016) .\n\u2022 Clinical-Longformer (Beltagy et al., 2020) is Longformer (Beltagy et al., 2020) trained for 200,000 steps with batch size of 6 \u00d7 3 on 2 million clinical notes extracted from the MIMIC-III dataset.\nAs a baseline, we compare these models to a pre-trained BERT (Devlin et al., 2019) . See Appendix D for the details of the implementation.\n\nExperimental Results\nWe compare the prediction results among biomedical language models (LMs) and analyze the association between illnesses and biases. As shown in Table 1 , the top 3 diagnosis predictions of each model show high overlaps across different biases. BioBERT predicts \"malaria\" as the top 1 diagnosis and \"cancer\" as the top 3 for both the young and old age groups. As for racial biases, \"malaria,\" again, has the highest prediction score across races, and \"tuberculosis\" scores second for African American, American Indian, and Asian and scores third for the other two races. (See Appendix B for the figures that compare the percentage of top 7 diagnoses.)\nTo better quantify overlaps within biases, we measure the text overlap scores of each model, and the results are shown in Table 2 . The text overlap scores are computed by first counting the number of matching words and then normalizing the counts to a value between 0 and 1. For normalization, we 3,  4 and 5 .\nThe text overlap scores of all models in Table 2 are above 0.5, implying high overlaps in predictions within biases. As for the scores among races, Tables 3, 4 and 5 also display scores above 0.5. An exception is the overlap score between Asian and Native Hawaiian in Table 3 , which is 0.5. Although the prediction scores of diagnoses vary across biases, the models generate similar tokens regardless of a given biased term. This result implies a weak association between illnesses and biases in biomed- ical LMs.\nW B I A H W 0.\nAn interesting observation is that the three biomedical models, BioBERT, ClninicalBERT, and Clinical Longformer display the highest overlap scores in the gender bias and the lowest in the racial bias. On the contrary, the baseline BERT exhibits an opposite result: the gender bias has the least overlapping tokens. We infer that biomedical models are less likely to predict different diagnoses based on gender than BERT.\nFinally, each model reveals a different tendency to predict an illness of a given patient. BioBERT predicts \"malaria\" with the highest scores across all biases except for the male bias. ClinicalBERT generates \"pneumonia\" most times except for Asians. As for Clinical Longformer, the top 1 diagnosis is \"cancer\" for age and gender biases and \"diabetes\" for racial bias. This observation suggests that each model associates a specific illness to all patients irrespective of bias and that a model choice determines the prediction of diagnosis.\nCase Study. We study whether a welldocumented association between biases and the use of cardiovascular procedures is observed in the biomedical models (Schulman et al., 1999; Chen et al., 2001) . In particular, we look into two correlations: (1) the physicians assume that females and the young are less likely to have coronary artery disease than males and the old, respectively; (2) females and African Americans are less likely to receive cardiac catheterization than males and White Americans, respectively.\nTo identify those biased correlations in the models, we perform two experiments. First, we curate prompts and measure the token scores of mask prediction, which we denote as M-scores. Second, the bias metrics in CrowS-Pairs (CP) (Nangia et al., 2020 ) are adopted. We create a pair of stereotypical and anti-stereotypical sentences S, mask one unmodified token u i \u2208 U at a time, and compute pseudo-log-likelihoods: score(S) = |C| i=0 log P (u i \u2208 U |U \\u i , M, \u03b8), where U = {u 0 , ..., u l } are unmodified tokens and M = {m 0 , ..., m n } are modified tokens in a sentence S. The details of the experiments can be found in Appendix C.\nFirst, we examine the correlation between gender/age and coronary artery disease. As shown in Table 6 , the female and the young have lower CP bias scores than the male and the old, respectively. This result aligns with the first correlation in clinical practice. In contrast, the M-scores of the male and the old are lower. Namely, the models are less likely to generate male-and old-biased words in a sentence with coronary artery disease.\nTable 7 show the experimental results on the correlation between gender/race and the use of cardiac catheterization. The CP scores of the male and White American are lower than the female and African American, respectively. Once more, the M-score results are the opposite; the female and African American have lower M-scores.\nM-scores and CP scores exhibit contrary results for the two experiments on the correlations. In the first experiment, the CP score results demonstrate a higher association between male/old patients and coronary artery disease, proving the first correlation manifested in the biomedical models. However, the M-scores reveal an opposing association, overturning the first correlation. In the second experiment, the M-scores align with the second correlation, while the CP scores do not. These results signify the importance of using more than one metric to measure bias and the challenges of measuring bias in LMs.\nLimitations. In this study, the prediction scores of generated tokens are aggregated to determine the rankings of diagnosis in Table 1 and Figures 2,  3, and 4 . We choose this summation metric because bias as defined in this paper is a tendency to associate a particular group with an illness in generated sentences. However, we acknowledge the limitations of aggregated scores in reflecting comprehensive model behaviors for different subpopulations (Blodgett et al., 2020) .\nIn addition, we recognize that the change in prompts can affect experimental results. For our experiments, prompts based on PICO were curated and used to examine the association between illnesses and biases. Yet a choice of a prompt greatly affects the performance of a model (Liu et al., 2023) . Hence, if different prompts are adopted, the experimental results can differ.\nFinally, our definition of bias in biomedical models is based on papers that study the effects of bias on healthcare outcomes (Blair et al., 2011; Hall et al., 2015) . We are not claiming that statistical differences in health conditions based on race, gender, or age are not meaningful. Yet studies show that patients with the same health conditions get different treatments due to a healthcare provider's (implicit) bias (Green et al., 2007; Sabin and Greenwald, 2012) . A perfect dissociation between race, gender, or age and a patient's health conditions is impossible. Still, to study bias as explicitly defined for this work, we design prompts that provide a patient's race, gender, or age, not their health conditions and question whether the biomedical models are affected by the given information.\n\nConclusion\nWe explore whether biases in clinical practice are reflected in pre-trained biomedical LMs. The tendency in diagnosis predictions of the models is analyzed, and the overlaps in the predictions across biases are compared. As a case study, we measure bias in associating coronary artery disease with gender/age and cardiovascular procedures with gen- der/race. Our study indicates the impact of a model choice on diagnosis predictions and the difficulties in measuring biases.\n", "hypothesis": "We curate prompts based on evidencebased practice and compare generated diagnoses based on biases.  For a case study, we measure bias in diagnosing coronary artery disease and using cardiovascular procedures based on bias.  Our study demonstrates that biomedical models are more biased than BERT in gender, while the opposite is true for race and age.", "answer": false}
{"title": "On Dataset Transferability in Active Learning for Transformers", "content": "\nIntroduction\nPre-trained language models (PLMs) -large overparameterized models based on the transformer architecture (Vaswani et al., 2017) and trained on large corpora -are the leading paradigm in modern NLP, yielding state-of-the-art results on a wide range of NLP tasks. However, large models require large amounts of data. Active learning (AL; Settles, 2009) addresses the data bottleneck problem by improving data labeling efficiency. It employs human-in-the-loop labeling with the model iteratively selecting data points most informative for labeling. Recent work has demonstrated the effectiveness of AL for fine-tuning PLMs (Dor et al., 2020; Grie\u00dfhaber et al., 2020; Margatina et al., 2022; Yuan et al., 2020; Shelmanov et al., 2021) .\nWhile AL may considerably reduce model development costs, it also potentially limits the scope of use of the actively acquired datasets. Since data sampling in AL is guided by the inductive bias of the acquisition model, the dataset will typically not represent the original population's distribution (Attenberg and Provost, 2011) . This is troublesome if one wishes to use the actively acquired dataset to train a different model (consumer model) from the one used for AL (acquisition model). If the two models' inductive biases differ, the AL gains can cancel or even revert: the consumer model may perform worse when trained on the actively acquired dataset than on a randomly sampled one. However, the robustness of the actively acquired dataset to the choice of the consumer model is obviously highly desirable, as the acquisition model may become unavailable or dated. The latter is common in NLP, where new and better models are being developed faster than new datasets. However, most AL studies use the same acquisition and consumer models, and dataset transferability is seldom mentioned in AL literature. A notable exception is the work of Lowell et al. (2018) , who showed the unreliability of dataset transfer on standard NLP tasks.\nIn this work, we examine the problem of AL dataset transferability for transformer-based PLMs and conduct a preliminary empirical study on text classification datasets. We first probe whether AL gains persist between different transformerbased PLMs, considering several AL methods and datasets. Observing that on most datasets, the transfer works in some cases but fails in others, we investigate the mechanisms underlying transferability. We hypothesize a link between AL dataset transferability and how the acquisition and consumer models sample instances. To probe this, we introduce acquisition sequence mismatch (ASM) to characterize to what extent the two models differ in how they sample instances throughout AL iterations. We investigate how ASM affects dataset transferability and how ASM is affected by other AL variables. We show that, while it is generally reasonable to transfer actively acquired datasets between transformer-based PLMs, AL methods that retain low ASM produce more transferable datasets. We also show that the choice of the AL method affects ASM more than the choice of models.\nTo summarize our contributions: we (1) conduct an empirical study on the transferability of actively acquired datasets between transformer-based PLMs, (2) propose a measure to quantify the mismatch in the acquisition sequences of AL models and link this to dataset transferability, and (3) analyze what design choices affect this mismatch. We provide code for the experiments 1 with the hope that our results will encourage NLP practitioners to use AL when fine-tuning PLMs and motivate further research into the AL dataset's transferability.\n\nRelated Work\nAlthough AL has been extensively studied for shallow and standard neural models (without pretraining), research on combining AL and PLMs lags behind. The initial studies showed promise, with AL methods outperforming random sampling for text classification (Dor et al., 2020; Grie\u00dfhaber et al., 2020) . The field is gradually gaining traction with studies demonstrating AL effectiveness even with simple uncertainty-based methods (Gonsior et al., 2022; Schr\u00f6der et al., 2022) . Moreover, PLMs open up new possibilities, such as complementing AL with model adaptation using unlabeled data (Yuan et al., 2020; Margatina et al., 2022) .\nWhile there is much research on AL for standard scenarios where the acquisition and consumer models are the same, there is little research on AL dataset transfer. Prabhu et al. (2019) demonstrated that combining uncertainty AL strategies with deep models produces sampled datasets with good sampling properties that have a large overlap with support vectors of SVM trained on the entire dataset. Likewise, Farquhar et al. (2021) showed that deep neural models benefit from the sample bias induced by the acquisition model (the opposite is true for shallow models). However, the jury is still out on the effects of sample bias on the consumer model. The most prominent empirical study on AL transfer with neural models (Lowell et al., 2018) predates PLMs. Tsvigun et al. (2022) focused on alleviating the effects of acquisitionconsumer mismatch in PLMs by using lightweight distilled models for acquisition and larger versions of the models as consumer models. Even though the study focuses on improving the transferability of actively acquired datasets, the reasons behind the successful transfer are yet to be explored. An older study of AL dataset transferability for text classification and shallow models by Tomanek and Morik (2011) showed that transfer works in most cases but that neither sample nor model similarity explains transferability. Our study explores these characteristics for acquisition-consumer pairings of different PLMs.\n\nExperimental Setup\nOur study used four datasets, three models, and three AL methods (cf. Appendix B for details). The datasets we used are Subjectivity (SUBJ; Pang and Lee, 2004), CoLA (COLA; Warstadt et al., 2018) , AG-News (AGN; Zhang et al., 2015) , and TREC (TREC; Li and Roth, 2002) ). The three transformer models we used are BERT (Devlin et al., 2018) , RoBERTa (Liu et al., 2019) , and ELECTRA (Clark et al., 2020) . The AL methods we considered are entropy (ENT; Settles, 2009), core-set (CS; Sener and Savarese, 2017), and BADGE (BA; Ash et al., 2019) ). This gives 108 AL configurations (72 transfer and 36 no-transfer configurations). Furthermore, we ran each configuration with 20 different warm-start sets to account for stochasticity. The AL acquisition was simulated until the budget of 1500 labeled data points was exhausted (model performance for all datasets reached a plateau), labeling 50 data points per step.\nWe assessed dataset transferability using the difference in the area under the F 1 curve of the model trained on the actively acquired dataset and the same model trained on a randomly sampled dataset (\u2206AUC). We deem the AL dataset transfer successful if \u2206AUC is not significantly less than zero and unsuccessful otherwise. We chose \u2206AUC to make the notion of transferability independent of when the AL acquisition terminates. On the other hand, as terminating the AL after acquiring too few labeled data is unrealistic, we also report \u2206AUC 10 , which is \u2206AUC calculated with an offset of 10 iterations (500 labeled instances) of the AL loop. Comparing \u2206AUC 10 to \u2206AUC provides insights into how transferability changes through time.\n\nDataset transferability\nWe grouped the 108 AL configurations into three groups based on the sign of the mean \u2206AUC value and the p-value of the difference between AUC scores of transfer and random sampling: 2 negative (\u2206AUC < 0 and p<.05), neutral (p\u2265.05), and positive (\u2206AUC \u2265 0 and p<.05) transfer. The notransfer AL configurations (where the acquisition and consumer models are the same) are generally successful (25 positive, 9 neutral, and 2 negative configurations as per \u2206AUC; 33 positive, 2 neutral, and 1 negative configuration as per \u2206AUC 10 ).\nThe grouping of the remaining 72 configurations with AL dataset transfer is given in Table 1 . We observe that the dataset, the acquisition-consumer model pairing, and the AL method all affect transfer success. Evidently, transferability differs across datasets: the transfer is always positive on SUBJ (which is the simplest task we considered in terms of the number of labels, the balance of classes, and the MDL task complexity measure; cf. Appendix B), while most neutral transfers occur on COLA. A more interesting picture emerges from the different acquisition-consumer model pairings and AL methods. Most negative transfers are transfers to ELECTRA, while most neutral transfers are those to RoBERTa (perhaps due to it being optimized for robustness). On the other hand, transfer to BERT is positive in most cases, perhaps because BERT's pre-training regime is most similar to that of the other two models. Among the AL methods, entropy mostly makes the transfer negative, most neutral transfers occur with core-set, and BADGE is the best choice for ensuring positive transferability. However, when looking at the later steps of the AL loop, differences between entropy and BADGE vanish, while the core-set lags slightly behind. Thus, \u2206AUC tends to increase throughout the AL process, suggesting that increasing the amount of sampled data lowers the risk of unsuccessful transfer (cf. Appendix C for additional F 1 scores analysis).\n\nAcquisition sequence mismatch\nWe hypothesize there is a link between dataset transferability and the sequence in which data points are acquired for labeling by AL. In particular, we posit that dataset transferability will be successful when the acquisition sequence of the acquisition model does not differ from what the acquisition sequence of a consumer model would be if that model had access to the original dataset. We introduce the acquisition sequence mismatch (ASM) to measure the differences in acquisition sequences. To compute the ASM between two acquisition sequences, we pair the corresponding batches of the two sequences and average their pairwise differences. To measure the difference between a pair of batches, we take the average of the distances of best-matched examples between the batches. To account for the fact that AL methods may choose numerically different yet semantically similar data points, we measure the similarity of acquired instances in representation space. We use GloVe embeddings (Pennington et al., 2014) as a common representation space independent of the choice of acquisition and consumer models and compute the cosine distance between averaged word embeddings. Lastly, we use the Hungarian algorithm (Kuhn, 1955) to construct a bipartite graph between two batches with distance-weighted edges to find the best-matching examples. Formally, we define ASM as follows:\n1 T T t=1 1 |B t | min S(B t A ),S(B t B ) \uf8eb \uf8ed |Bt| i=1 d(x i A , x i B ) \uf8f6 \uf8f8 (1)\nwhere T is the length of the sequence (the number of steps of the AL loop), S(B t ) is the set of all of the permutations of instances in the selected batch at step t, and d(x i A , x i B ) is the cosine distance between instance representations from sequences A and B for a batch at position i of a given batch permutation. Intuitively, ASM assumes that both batches cater to the same informational need of the model, so it calculates how much the instances that should carry out the same role in the batch differ.\nGiven a dataset, we hypothesize ASM may be affected by both the choice of the models and the choice of the AL method. Figure 1 shows that the distributions of ASM values are more alike when grouped by the AL methods than when grouped by the model pairings. To verify this observation, we conducted two Kruskal-Wallis H-tests for each dataset: in the first, populations were determined by the AL method, and we concluded that there was a significant difference in ASM (p<.05); in the second, the populations were determined by the model pairing, and there was no significant difference in ASM (p>.05). This suggests that the choice of AL method affects ASM more than the choice of acquisition-consumer model pairing.\n\nAcquisition mismatch analysis\nWe found a statistically significant negative correlation between \u2206AUC and ASM for each dataset. 3 This supports our hypothesis that the lower the mismatch between acquisition sequences of the two models, the higher the transferability of a dataset from one model to the other. Besides ASM, we use another measure for analyzing dataset transferability: the difference between the dataset acquired with AL using the acquisition model and the dataset acquired with AL using the consumer model. We call this measure the acquired dataset mismatch (ADM). Essentially, ADM computes the mismatch between samples similarly to ASM but between entire datasets obtained after the last sampling step.\nAbove we showed that the choice of the AL method affects the ASM. Figure 2 shows that BADGE gives smaller ASM than the other two methods, whereas core-set gives larger ASM than the other two methods. 4 However, the intriguing effect emerges when comparing the difference in batches through time and differences in the entire acquired datasets through time. In the early steps, BADGE gives the highest similarity of acquired datasets among the considered methods, which leads to it having the lowest ASM. However, in later steps, entropy dominates the similarity of acquired datasets. 5 It seems as if entropy acquired similar datasets for different models by taking those models through different sequences of the population distribution. This effect is seen in Table 1 , where entropy is the worst method when using \u2206AUC to measure transfer success while managing to parry BADGE when using \u2206AUC 10 . The difference in transferability between entropy and BADGE completely vanishes when looking at the last step of the AL loop (cf. Appendix, Table 3 ). It is clear that entropy can produce transferable datasets, but it requires more time to do so.\nWe speculate that the effect of BADGE having the lowest ASM yet entropy achieving the lowest ADM could emerge due to the interaction between the AL method and the model's decision boundary. Namely, uncertainty AL methods sample data points on the decision boundary with high overlap with support vectors of the SVM trained on the whole dataset, as pointed out by Prabhu et al. (2019) . Since BADGE combines uncertainty and diversity, i.e., it samples data points the model is uncertain about for diverse reasons, it samples along the entire decision boundary at each step, and since decision boundaries of the models are roughly the same, so are the sampled data points. Entropy, on the other hand, relies solely on uncertainty. Due to its greedy nature, entropy tends to sample similar points because if one data point has high uncertainty, data points similar to it are also going to have high uncertainty (Zhdanov, 2019) . This may manifest as sampling local patches of space on the decision boundary. Therefore, entropy may take more time to define the boundary than BADGE because it is forming the boundary from patches of space with the highest uncertainty at a given AL step rather than holistically sampling along the boundary at each step. Since the shape of the decision boundary is more similar between different models than the local interactions along the boundary, entropy has a higher batch mismatch in the early steps. However, once more data is labeled and the boundary becomes stable, both entropy and BADGE start to have a low batch mismatch, as seen in Figure 2 . Since entropy is deterministic and never strays from the decision boundary, it ends up having a lower ADM than BADGE. Lastly, we believe that the core-set method has the highest ASM and ADM because it selects data based on diversity in the model's representation space, which is more model-specific and shares fewer properties between different models than the decision boundary. Further exploring the described interaction is a compelling direction for future work.\nIt may be that AL methods with different acquisition sequences end up acquiring a similar dataset and have high transferability, as in the case of entropy, an uncertainty-based acquisition function. It is also possible that acquired datasets differ between models but that the transfer remains successful because it taps into some other essential aspect of a transferable dataset, as is the case with core-set, a diversity-based acquisition function. However, the best strategy to ensure dataset transferability appears to be a mixture of uncertainty and diversity, as provided by BADGE. This appears to minimize ASM between models, making datasets transferable regardless of the number of AL steps.\n\nConclusion\nWe presented an empirical study on the transferability of actively acquired text classification datasets for transformer-based PLMs. Our results indicate no significant risk in transferring datasets, especially for larger amounts of data. We also showed that transfer is largely successful when preserving the sequence and similarity of acquired instances between the models, which is what methods combining uncertainty and diversity acquisition functions seem to do. Transferability appears to differ considerably across datasets, so future work should examine what dataset characteristics are predictive of transfer success.\n", "hypothesis": " We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM.  We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used.  Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model..", "answer": true}
{"title": "PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data", "content": "\nIntroduction\nWord alignment, as the task of finding the corresponding source and target tokens in a parallel sentence, was well-known as an essential component of statistical machine translation (SMT) systems. Despite the dominance of neural machine translation (NMT) in recent years, word alignment is still a notable area of research due to its usage in a wide variety of NLP applications, such as annotation projection (Yarowsky et al., 2001; Pad\u00f3 and Lapata, 2009; Huck et al., 2019; Nicolai and Yarowsky, 2019) , bilingual lexicon extraction (Ammar et al., 2016; Shi et al., 2021; Artetxe et al., 2019) , typological analysis (Lewis and Xia, 2008; \u00d6stling, 2015) , guided alignment training of NMT (Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2018) , and evaluation and analysis of translation 1 https://github.com/fatemeh-azadi/PMI-Align outputs (Anthony et al., 2019; Neubig et al., 2019; Wang et al., 2020) .\nFor many years statistical methods such as IBM models (Brown et al., 1993) and tools implemented based on them, namely GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013) , were among the most popular solutions to the word alignment task. Following the rise of deep neural models, several attempts have been made to extract word alignments from NMT models and their attention matrices (Peter et al., 2017; Ghader and Monz, 2017; Zenkel et al., 2020; Zhang and van Genabith, 2021) . However, most of these methods, as well as the statistical aligners, require a sufficient amount of parallel training data to produce high quality word alignments. Recently, Jalili Sabet et al. (2020) have shown that high quality word alignments could be achieved using pre-trained multilingual language models (LMs), like MBERT Devlin et al. (2019) and XLMR Conneau et al. (2020) . Their proposed method called SimAlign, extracts word alignments from similarity matrices induced from multilingual contextualized word embeddings with no need for parallel training data, which is very useful for lowresource language pairs. Afterwards, Dou and Neubig (2021) and Chi et al. (2021) proposed methods called probability thresholding and optimal transport to extract alignments using the similarity matrices derived from pre-trained LMs. They have also proposed some word alignment objectives to fine-tune the pre-trained models over parallel corpora.\nIn this paper, we follow the work done by Jalili Sabet et al. (2020) to extract alignments from pre-trained LMs without requiring any parallel training data and propose PMI-Align. Our main contribution is proposing to compute the point-wise mutual information (PMI) between source and target tokens and using the PMI matrices instead of similarity matrices made of cosine similarities between the representation vectors of each source and target tokens, to align words. We argue that our proposed PMI-based method could align better as it considers the total alignment probability of each source or target token, as well as the joint alignment probabilities (equivalent to cosine similarities). This could alleviate the so-called hubness problem (Radovanovic et al., 2010) in high dimensional spaces, where some token's representation is close to many others (see _went in Figure 1 ). We perform experiments on six different language pairs and show that our method could surpass other alignment methods on five of them. We also conduct our experiments on different pre-trained LMs to show that PMI-Align could be advantageous regardless of the pre-trained model used.\n\nProposed Method\nIn this section, we first discuss how we define and compute the PMI matrix for each sentence pair and then we describe our alignment extraction method using the PMI matrix.\n\nPoint-Wise Mutual Information\nPoint-wise mutual information (PMI) is a wellknown measure of association in information theory and NLP and it shows the probability of two events x and y occurring together, compared to what this probability would be if they were independent (Fano, 1961) . It is computed as follows: PMI(x, y) := log p(x, y) p(x)p(y)\n(1)\nIn the context of word alignments, we define the PMI for a source and target token in a sentence pair as how more probable two tokens are to be aligned than if they are aligned randomly. Given a sentence\nx =< x 1 , ..., x n > in the source language and its corresponding target sentence y =< y 1 , ..., y m >, the joint alignment probability of two tokens, x i and y j , could be computed as:\nEQUATION\nwhere h x i is the contextualized embedding vector of x i extracted from a pre-trained multilingual language model and sim(.) is the cosine similarity measure. The total alignment probability of x i and y j , i.e., p(x i ) and p(y j ), could also be computed according to the total probability rule as follows:\nEQUATION\nBy calculating the PMI for each source and target token in a parallel sentence, we obtain the PMI matrix for that sentence pair, that could be used to extract alignments instead of similarity matrix in SimAlign (Jalili Sabet et al., 2020) . The advantage of using PMI to align words is that it also considers the total alignment probability of each source and target token in addition to their joint alignment probability, which is equivalent to the similarity measure. This leads to reduce the probability to align the token pairs that one of them has high similarities to many other tokens, and thus could alleviate the so-called hubness problem in high dimensional spaces where some data points called hubs are the nearest neighbors of many others.\n\nExtracting Alignments\nTo extract word alignments, we follow the simple Argmax method proposed in Jalili Sabet et al. (2020) . Thus, we first obtain the source to target and target to source alignment matrices using the argmax over each row and each column of the PMI matrix, respectively. Next, we intersect these two matrices to get the final word alignment matrix. In other words, the final alignment matrix A i j = 1 iff i = argmax k (PMI k j ) and j = argmax k (PMI ik ).\nSince the above method would extract alignments on the subword level, we follow the heuristic used in previous work to obtain the word-level alignments by considering two words to be aligned if any of their subwords are aligned (Jalili Sabet et al., 2020; Zenkel et al., 2020; Dou and Neubig, 2021) .\n\nDatasets\nWe perform our experiments on six public datasets, as in (Jalili Sabet et al., 2020) , consists of English-Czech (En-Cs), German-English (De-En), English-Persian (En-Fa), English-French (En-Fr), English-Hindi (En-Hi) and Romanian-English (Ro-En) language pairs. The statistics and URLs of these datasets are available in Table 2 in Appendix A.\n\nModels and baselines\nWe compare our method with the following three state-of-the-art methods proposed to extract alignments from pre-trained multilingual LMs without using parallel training data. For all these methods default parameters were used in our experiments.\nSimAlign 2 (Jalili Sabet et al., 2020) : They propose three methods to extract alignments from similarity matrices, called Argmax, Itermax and Match. Although Itermax and Match methods could not make significant improvements over Argmax and the Argmax method had better AER results for most of language pairs while using the XLMR-base model, they have argued that the Itermax method, which tries to apply Argmax iteratively, could be beneficial for more distant language pairs. Thus, we report both Argmax and Itermax results in our experiments to compare with our method.\nProbability Thresholding 3 (Dou and Neubig, 2021) : In this method they apply a normalization function, i.e., softmax, to convert the similarity matrix of tokens into source to target and target to source alignment probability matrices. Afterwards, they extract the aligned words as the words that their alignment probabilities in both matrices exceed a particular threshold.\nOptimal Transport 4 (Chi et al., 2021) : This method was proposed in both Dou and Neubig (2021) and Chi et al. (2021) , and tried to model the word alignment task as the known optimal transport problem (Cuturi, 2013) . Using the similarity matrix, this method attempted to find the alignment probability matrix that maximizes the sentence pair similar-ity. In our experiments, we use the method proposed by Chi et al. (2021) that utilizes the regularized variant of the optimal transport problem (Peyr\u00e9 et al., 2019) , as it reported better results.\nThere are also many attempts made to improve the pre-trained LMs by fine-tuning on some parallel corpora to better align words. However, as our approach is irrelevant to the pre-trained model and our focus is on the alignment extraction instead of the model, we do not include those methods in our experiments. To demonstrate the effectiveness of our PMI-based alignment regardless of the utilized pre-trained multilingual LM, we conduct our experiments on M-BERT (Devlin et al., 2019) , XLMR-Base (Conneau et al., 2020) and XLM-Align (Chi et al., 2021) which is fine-tuned on a word-alignment task, to show that our method could also be advantageous on more cross-lingually aligned models. All these models are publicly available in the Hugging Face platform (Wolf et al., 2020) .\n\nResults\nTable 1 shows the results of our alignment technique compared to previous methods while using different pre-trained LMs. Following the previous work (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Chi et al., 2021) , we use the 8th layer's representations of each pre-trained model to compute the similarity or PMI matrices. We also use the alignment error rate (AER) (Och and Ney, 2003) as the evaluation metric.\nAs Table 1 shows, our PMI-Align method could consistently outperform the other methods in all language pairs except En-Fr, regardless of the pretrained model used. Compared to Argmax, our method performs better for about 1% or more in AER, while using the XLMR-Base model (except for En-Fr), which exclusively shows the benefits of using the PMI matrix instead of the similarity matrix. We also see that the PMI-Align could surpass the Itermax method for more distant language pairs such as En-Fa and En-Hi, where it was claimed to have the most advantage. Results show that our method could also be beneficial while using a model pre-trained on a word alignment task, i.e., XLM-align, which is expected to have more crosslingually aligned representations, and less hubness problem.\nThe only language pair that our method could not outperform prior methods is En-Fr. This could be due to the closeness of these two languages, as they have many shared subwords and similar word orderings. As a result, pre-trained models for this language pair are better trained and could strongly produce similar representations for aligned words, which reduces the hubness problem to a great extent. Thus, using PMI instead of the similarity matrix could not help. However, our method's performance while using the M-BERT model is comparable to the best results, with about 0.1% difference in AER. Several samples are shown in Appendix B, to better intuitively compare PMI-Align and Argmax, which could better show the benefits of using the PMI matrix instead of the cosine similarities.\n\nRelated Work\nStatistical aligners based on IBM models (Brown et al., 1993) , such as Giza++ (Och and Ney, 2003) and fast align (Dyer et al., 2013) were the most dominant tools for word alignment until the late 2010s. With the rise of neural machine translation models, several attempts made to extract alignments from them (Ghader and Monz, 2017; Garg et al., 2019; Li et al., 2019; Zenkel et al., 2020; Chen et al., 2021; Zhang and van Genabith, 2021) . However, all these models need parallel training data and could not utilize pre-trained contextualized embeddings. Recently, Jalili Sabet et al. (2020) have proposed methods to extract alignments from similarity matrices induced from multilingual LMs without the need for training on parallel data. Following this work, we propose a PMI measure to score and align words in each sentence pair, instead of cosine similarity. Some other alignment extraction methods using multilingual LMs were also provided by Dou and Neubig (2021) and Chi et al. (2021) . They both also proposed several training objectives related to word alignments to fine-tune multilingual LMs on parallel data, as in some other recent works (Cao et al., 2020; Wu and Dredze, 2020; Lai et al., 2022) .\n\nConclusions\nThis paper presents a word alignment extraction method based on the PMI matrices derived from cross-lingual contextualized embeddings, instead of just the similarity matrices. We proposed a way to compute the PMI matrix for each sentence pair and argued that using this PMI measure would be beneficial since for each source-target word pair, it considers not only their similarity to each other but also their similarity values to the other tokens of the sentence, that could mitigate the hubness problem.\nExperimental results show that our PMI-Align method could outperform the previous alignment extraction methods in five out of six language pairs, regardless of the base pre-trained language model used to derive word embeddings. Although our method does not require any parallel training data, our experiments show that it could also benefit the approaches using such data to fine-tune the pretrained models for better word alignments. In future work, the proposed PMI matrix could be investigated in other cross-lingual or even monolingual applications, like the translation quality estimation or the evaluation of text generation tasks, instead of the similarity matrix.\n", "hypothesis": "Our experiments show that our proposed PMI-Align approach could outperform the rival methods on five out of six language pairs.  Although our approach requires parallel training data, we show that this method could also benefit the approaches using no parallel data to fine-tune pre-trained language models on word alignments.", "answer": false}
{"title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation", "content": "\nIntroduction\nMultilingual neural machine translation (MNMT) enables translation between unseen language pairs, i.e., zero-shot translation (ZST) (Johnson et al., 2017; Firat et al., 2017) . Prior studies have explored techniques such as language tags (Wu et al., 2021) , residual connections (Liu et al., 2021) , and novel training objectives (Al-Shedivat and Parikh, 2019; Pham et al., 2019; Arivazhagan et al., 2019; Gu et al., 2019; Zhu et al., 2020; Zhang et al., 2020; Wang et al., 2021; Yang et al., 2021) for improving ZST. They primarily used the Transformer architecture (Vaswani et al., 2017) , which has two variations depending on the position of layer normalization (LayerNorm) (Ba et al., 2016) , namely, PreNorm (applied at the input of layers) (Baevski and Auli, 2019) and PostNorm (applied after residual connections), as shown in Fig. 1 . As previous studies showed that PreNorm can result in more stable training and faster convergence compared to PostNorm for MNMT (Xiong et al., 2020) , most ZST works (Pham et al., 2019; Wu et al., 2021; Liu et al., 2021) use PreNorm as the default setting following those MNMT studies. However, Xu et al. (2019) revealed that PreNorm carries the risk of overfitting the training data. We thus hypothesize that in a multilingual scenario, PreNorm may overfit supervised directions and have poor ZST generalizability. We systematically explore PreNorm and PostNorm's effect on ZST to verify this.\nUsing the OPUS, IWSLT, and Europarl datasets and a total of 54 ZST directions, we show that PostNorm consistently outperforms PreNorm by up to 12.3 BLEU points. Following previous work, we also evaluate different language tag (Wu et al., 2021) and residual connection (Liu et al., 2021) settings, as they have been shown to impact ZST but we observe that PostNorm continues to be superior thereby lending credibility to our hypothesis.\nTo better understand the performance differences, we introduce a novel analysis approach called layer-wise language recognition (LLR), which tracks the off-target rates for each encoder and decoder layer by training token-level classifiers to recognize the source or target language. This analysis shows that PreNorm is more sensitive to language tag settings than PostNorm, negatively impacting ZST performance. Additionally, by examining the unraveled view of PreNorm (Fig. 1 ) inspired by Veit et al. (2016) , we reveal structural flaws in PreNorm for ZST. Our analysis demonstrates that the order of LayerNorm and selfattention/feed-forward network in PreNorm is the main factor affecting its ZST performance.\nGiven the prevalent use of PreNorm as the default setting in ZST baselines and frameworks such as Fairseq (Ott et al., 2019) 1 and Ten-sor2Tensor (Vaswani et al., 2018) , our study emphasizes the importance of careful consideration in the LayerNorm setting for ZST.\n2 Background: LayerNorm LayerNorm (Ba et al., 2016) normalizes the input x by zero-centering and scaling to have a unit standard deviation, followed by an additional trainable transformation, including a gain and bias adjustment. Specifically, it is formulated as:\nEQUATION\nwhere g and b are trainable gain and bias. E and V indicate expectation and variance. Lay-erNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer (Vaswani et al., 2017) Nguyen and Salazar ( 2019) have explored the impacts of normalization and initialization choices on supervised low-resource NMT settings, however, we delve deeper and focus on the significance of the positioning of LayerNorm for zero-shot NMT. We expect this to complete the understanding of LayerNorm's role in multilingualism, particularly in the context of zero-shot translation.\n\nExperiments and Results\nWe evaluate the performance of PreNorm and Post-Norm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and Post-Norm to understand performance differences.\n\nExperimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS (Zhang et al., 2020) , IWSLT (Cettolo et al., 2017), and Europarl (Koehn, 2005) . The statistics of the datasets are summarized in Table 1 . We include 7, 4, and 5 languages for each dataset.\nThe training data consists of only English-centric sentence pairs, resulting in 30, 6, and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12.00M, 1.38M, and 15.78M, respectively. We apply BPE (Sennrich et al., 2016) with merge operations of 50k, 40k, and 50k to create a joint vocabulary for each dataset. Training We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings:\n(1) PreNorm or PostNorm: PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while Post-Norm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 Table 2 : BLEU scores and off-target rates (shown in brackets). We report the average score of three seeds; refer to Appendix G for BLEU score of each translation direction and seed. \"Res.\" indicates the residual connection of self-attention in the 4 th encoder layer. We mark lower off-target rates and significantly higher BLEU scores (Koehn, 2004) between PreNorm and PostNorm in bold for ZST.\n(2) S-ENC-T-DEC or T-ENC: Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. \n\nMain Results\nWe evaluate ZST systems using SacreBLEU (Post, 2018) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B. Our findings are as follows: PreNorm vs. PostNorm: We find that Post-Norm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions. Impact of Language Tag and Residual Connection: We observe that using the \"T-ENC\" language tag and \"w/ Res.\" improves ZST performance for IWSLT, which aligns with the findings of Wu et al. (2021) and Liu et al. (2021) . Nevertheless, the best performance is achieved using \"w/ Res.\" for Post-Norm with \"S-ENC-T-DEC\" and \"T-ENC\" tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021) and Liu et al. (2021) used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact. Off-target Rates: Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0.5 to 12.3 BLEU points. For PreNorm and PostNorm with the \"T-ENC\" language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from \u22120.61% to 2.02%, which results in narrow BLEU score gaps, ranging from 0.5 to 1.8 points. However, for PreNorm and PostNorm with the \"S-ENC-T-DEC\" language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from 5.40% to 54.23%, resulting in BLEU score gaps from 1.7 to 12.3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations.\n\nTracking Off-targets within Transformer\nWe probe the language independence of hidden states to track off-targets within Transformer and reveal the differences between PreNorm and PostNorm. In previous work, language independence was primarily analyzed using either SVCCA (Raghu et al., 2017) or language classification accuracy (LCA) (Liu et al., 2021) . However, we provide evidence in Appendix C that SVCCA, which measures the cosine similarity between hidden states, are not suitable for ZST systems. In- --------Encoder--------->||<---------Decoder--------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder--------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder-------- 2 ) for both ZST and supervised directions for each dataset. We report the average accuracy of three seeds and all the supervised or zero-shot directions. \"Pre-Src\" and \"Pre-Tgt\" indicate the layer-wise source and target language recognition for a PreNorm system (#1), while \"Post-Src\" and \"Post-Tgt\" denote similary for a PostNorm system (#2). \"L1\" to \"L6\" are 6 encoder layers and \"L7\" to \"L12\" are 6 decoder layers. We present the figures of other systems (#3 -#8) in Appendix F. stead, LCA trains a classifier to inspect the hidden states on top of the encoder, but it does not simulate the training of a ZST system, which may introduce bias in the analysis for ZST. 3 In this work, we propose a novel approach for ZST based on LCA: LLR tailors classifiers for each layer to recognize the source or target language. We train a tokenlevel linear classifier for each layer to utilize hidden states in each layer as features to identify the source or target language. We use hidden states obtained by feeding sentence pairs in supervised directions to simulate the training of ZST. We then test each layer's classifer's ability to recognize the source or target language for supervised or zeroshot directions. This approach enables the trained classifier to best represent the language recognition ability of hidden states in a ZST system.\nL1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12 |<-\nWe train two types of linear classifiers for each encoder and decoder layer. One is for recognizing the source language, and the other is for the target language. Each linear classifier is a linear transformation from the dimension of the hidden states (512 or 1, 024) to the number of source or target languages (e.g., 7 for OPUS). We use the validation set of all supervised directions to obtain the hidden 3 Liu et al. ( 2021) regulate the output language via a decoder-side language tag, hence analyzing only the encoder states poses no issues as the target language tag does not impact them. Nevertheless, with other language tag settings such as S-ENC-T-DEC and T-ENC, employed in this study, we require a method to obtain hidden states properly, given their impact on hidden states. state of each token in each layer and set their source language tag or target language tag as the gold labels. Note that the decoder hidden state of each token in each layer is obtained auto-regressively without teacher-forcing. We train each classifier for 3 epochs 4 with a learning rate of 1e-3 and a batch size of 64 sentences. For inference, we utilize the test sets of all supervised or zero-shot directions for computing the LLR results for corresponding directions, respectively.\nThe LLR results for #1 and #2 in Table 2 are presented in Fig. 2 . First, we find that the encoder and decoder hidden states are highly correlated with the target and source languages, respectively, for supervised directions (L1 to L6 of Pre/Post-Tgt and L7 to L12 of Pre/Post-Src of 3 upper sub-figures), which may impact the generalizability for ZST. Second, we see that the encoder hidden states of Post-Norm are less dependent on the source language than PreNorm (L6 of Pre/Post-Src of 3 lower subfigures). Third, we observe that the hidden states in all the decoder layers of PostNorm are more dependent on the target language and less on the source language than PreNorm (L7 to L12 of 3 lower subfigures). The latter two points contribute to the observed gaps in off-target rates between PreNorm and PostNorm. Conclusions for #5 and #6 with the \"S-ENC-T-DEC\" tag are identical (Appendix G). We report the mean of three seeds.\nFor systems using \"T-ENC,\" we find that the LLR are similar between PreNorm and PostNorm (Appendix G) and attribute the BLEU score gaps to translation quality (i.e., adequacy and fluency).\n\nUnraveling Structural Flaws of PreNorm\nWe investigate the structural differences between PreNorm and PostNorm to explain the observed differences in hidden states for models trained with the \"S-ENC-T-DEC\" tag. Inspired by Veit et al. ( 2016), we present an \"unraveled view\" for PreNorm, which decomposes the residual connections by the summation of several sub-networks, as shown in Fig. 1 (paths with different colors indicate sub-networks). However, this is not applicable to PostNorm, as LayerNorm is located after residual connections. Based on this analysis, the structural characteristic of PreNorm is:\n(1) Shallow Sub-network Nature: PreNorm includes shallow sub-networks, such as the embedding layer output fed through encoder layers without any operation except for the final LayerNorm (red path in Fig. 1 ), but PostNorm does not.\n(2) LayerNorm Before SA/FFN: In PreNorm, Lay-erNorm is placed directly before the self-attention (SA) or feed-forward module (FFN) within the residual connection module.\nTo analyze the impact of these structural characteristics on the generalizability of PreNorm in ZST, we swap the order of LayerNorm and SA/FFN within the residual connection module (Swap-PreNorm), while keeping the shallow sub-network nature of PreNorm. Refer to Appendix D for specific illustrations of Swap-PreNorm. The results, presented in Fig 3 , show that PreNorm can be significantly improved through Swap-PreNorm, with Swap-PreNorm approaching the performance of PostNorm. This demonstrates that ZST is more sensitive to the position of LayerNorm in PreNorm than its shallow sub-network nature.\n\nConclusion\nIn this paper, we comprehensively explored the effects of LayerNorm on ZST performance. Our results demonstrate that PostNorm consistently outperforms PreNorm for ZST, regardless of the language tag and residual connection settings used. Through in-depth analysis of off-target rates and structural flaws in the PreNorm model, we were able to identify the underlying factors that contribute to the performance discrepancy. Our study suggests that care should be taken when selecting the LayerNorm setting for ZST in future research.\n", "hypothesis": " However, Xu et al.  ( 2019) has revealed that PreNorm carries the risk of overfitting the training data.  Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST.", "answer": true}
{"title": "Improving Syntactic Probing Correctness and Robustness with Control Tasks", "content": "\nIntroduction\nTo explain the high performance of PLMs on various natural language processing (NLP) tasks, efforts have been made to examine the syntactic relation-encoding ability of these models. For example, Manning et al. (2020) attempt to reconstruct syntactic relations from the attention heads of Transformer models (Vaswani et al., 2017) using raw attention scores. Leave-one-out probing methods (Brunner et al., 2020) , instead, measure the influence of ablating parts of each syntactic relation on the hidden representations of the models.\nHowever, the probing results may not faithfully reflect the encoding of syntactic relations as the memorization of common word co-occurrences in the training data of PLMs can lead to incorrect and non-generalizable probing results (Hewitt and Liang, 2019) . We observe the same issues in our experiments, where many highly-ranked attention\n\nPositive:\nIn uential members of the House Ways and Means Committee introduced legislation that would restrict how the new savings-and-loan bailout agency can raise capital, creating another potential obstacle to the government's sale of sick thrifts.\n\nRandom-Word-Substitution:\nIn uential members of the House Ways and Means Committee introduction legislation that would restrict how the new savings-and-loan bailout agency can raise capital, creating another potential obstacle to the government's sale of sick thrifts.\n\nRandom-Label-Matching:\nIn uential members of the House Ways and Means Committee introduced legislation that would restrict how the new savings-and-loan bailout agency can raise capital, creating another potential obstacle to the government's sale of sick thrifts.\nFigure 1 : Top: An instance labeled with the correct \"subject\" dependency relation (Positive); Middle: the instance generated by Random-Word-Substitution where the instance is labeled with the correct pair of words but incorrect word form; Bottom: the instance generated by Random-Label-Matching where the instance is labeled with an incorrect pair of words. The head verb is in blue and the dependent is in red for all the examples. heads by the attention-as-classifier and leave-oneout probing methods highlight frequent word pairs regardless of whether there is a syntactic relation between them. This reduces the trustworthiness of the probing methods and any model interpretation that relies on them. To address this issue and improve the correctness, robustness, and generalizability of existing probing methods, we design two control tasks to reduce the adverse effects of the PLMs' memorization of word co-occurrences. The random-word-substitution control task substitutes one component word (i.e., the head or dependent words) of each syntactic relation with its other forms to make the text ungrammatical. The random-label-matching control task randomly matches one component word of each syntactic relation with a random irrelevant word in the sentence to make the syntactic-relation labels incorrect. Figure 1 shows examples for each control task. The control instances (i.e., negative instances) are generated automatically by substituting words or labels of instances in the positive datasets.\nBy down-weighting the attention heads that are ranked highly by the probing methods on the control tasks, we observe notably more consistent probing results between the attention-as-classifier and leave-one-out methods on the BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) models, with improvements above 0.1 for the Spearman's rank correlation coefficients (Spearman's \u03c1). 1 . The layer-wise distributions of top-ranked attention heads also become notably more consistent across different text attributes of the probing instances. The results demonstrate the effectiveness of our proposed control tasks for improving the quality and robustness of syntactic probing methods.\n\nSyntactic Probing Methods\nDifferent families of probing methods rely on different assumptions (Belinkov and Glass, 2019 ) and as such, probing results from different families cannot be meaningfully compared. Hence, we examine two probing methods that are both based on attention distributions: (1) Given a sentence and a headword for a syntactic relation, the attentionas-classifier method (Manning et al., 2020) predicts another word as the dependent if it puts the highest attention score on the headword; (2) As an attention-based version of the leave-one-out probing method used by Meister et al. (2021) , we mask the headword of each syntactic relation for each sentence and predict the word whose attention distribution changes the most as the dependent word. Following Kobayashi et al. (2020) , we additionally examine two variant methods, norm-as-classifier and leave-one-out-norm methods which predict the dependent words based on the distributions or changes of attention norms, respectively. We calculate the importance of each attention head for encoding each syntactic relation by evaluating the top-3 accuracy (ACC@3) of the predictions; defined as the percentage of instances where the dependent words from the ground truth are ranked among the top-3 in the predictions. We use ACC@3 since in many cases, the highest attention scores fall on separator tokens such as \"[SEP]\" and punctuation marks (Clark et al., 2019a) .\n\nProbing Datasets\nWe use the \"subject\" (subj), \"object\" (obj), \"nominal modifier\" (nmod), \"adverbial modifier\" (advmod), and \"coreference\" (coref) relations in our analyses. We use the English dataset for the CoNLL-2009 shared task (Haji\u010d et al., 2009) to construct our positive and control probing datasets. Figure 1 shows an example instance from the positive dataset and each control dataset.\n\nPositive Datasets\nOur positive dataset for each syntactic relation contains the correct annotations of words that make up the syntactic relation, e.g., the subject words and the corresponding verbs for \"subj\". The goldstandard dependency annotations in the CoNLL-2009 dataset are used for the \"subj\", \"obj\", \"nmod\", and \"advmod\" relations and the SpanBERT model (Joshi et al., 2020 ) is used to annotate the \"coref\" relation 2 .\n\nRandom-Word-Substitution Control\nIf an attention head in a Transformer model encodes a specific syntactic relation, it should not highlight the connections between words that do not form that syntactic relation. To measure and control for this effect, we construct the randomword-substitution control dataset by substituting one component word of the syntactic relation in each instance of the positive datasets with another part of speech of the same word (e.g., changing a verb to its noun form) to make the instance ungrammatical but not greatly change its semantics. We use the Language Tool 3 , a grammar correction tool, to verify that the sentences become ungrammatical after word substitution.\n\nRandom-Label-Matching Control\nWe also extend the existing method of the random control task (Hewitt and Liang, 2019) to construct the random-label-matching control dataset. Specifically, for each instance in our positive datasets, we use our gold-standard labels and coreference labels generated by SpanBERT to remove word pairs that are syntactically related, leaving us with words that are not syntactically related. These words are then used to create syntactically unrelated pairs by combining known head words with randomly selected dependent words. We then (intentionally) mislabel each pair as forming a specific syntactic relation, depending on the positive dataset from which the instance was taken. Attention heads that encode the relations between these syntactically unrelated word pairs are likely memorizing the co-occurrence of frequent word pairs without regard to syntactic correctness and thus should not be ranked highly by syntactic probing methods.\n\nExperimental Results\nWe conduct three sets of experiments to examine our probing methods' sensitivity to \"spurious\" word correlations (Section 4.1), consistency (Section 4.2), and robustness to text attributes (Section 4.3). We run the experiments using the BERT-base and RoBERTa-base models for generality. All the experiments are run on an Nvidia RTX-6000 GPU.\n\nSyntactic Relation Reconstruction\nWe follow Manning et al. (2020) to evaluate the correctness of attention-head rankings produced by the probing methods via syntactic relation reconstruction experiments. Specifically, for a given headword, we use the attention scores (for attentionas-classifier) or norms (for norm-as-classifier) between that headword and all other words in the instance to predict the dependent word. Similarly, We use the distribution changes of the attention scores (for leave-one-out) or norms (for leave-oneout-norm) when the headword is masked to predict the dependent word. Contributive attention heads for encoding a particular syntactic relation should achieve high syntactic-relation reconstruction performance (in ACC@3) given syntactically correct (positive) labels and low performance given incorrect (negative/control) labels.\nWe use the left-out development set of the CoNLL-2009 dataset (labeled using the groundtruth annotations and SpanBERT) as one positive probing dataset (pos-main) and the corresponding random-word-substitution and random-labelmatching control instances as two negative datasets. We construct an additional positive probing dataset (pos-uncommon) by substituting the dependent words with other words that have the same part of speech but rarely co-occur (<5 times) with the corresponding headwords in the English Wikipedia corpus 4 . This dataset enables us to study the effect of co-occurrence for syntactically related pairs of words on the syntactic relation reconstruction task. We use the English Wikipedia corpus as it is representative of the data used to pre-train BERT and RoBERTa. All the evaluations are conducted on the top-5 attention heads according to each probing method (with and without control tasks), and the scores are averaged across syntactic relations and heads.\nResults show that applying our proposed control tasks does not harm the syntactic-relation reconstruction performance of the four probing methods on the pos-main dataset. In contrast, applying the random control task (Hewitt and Liang, 2019) occasionally leads to a performance drop of 1.32. This suggests that our proposed control tasks are more robust than the existing random control task. On the pos-uncommon dataset, our proposed control tasks lead to an average increase of 9.17\u00b10.13 (BERT) and 4.07\u00b10.15 (RoBERTa) in the syntactic-relation reconstruction performance. Additionally, the control tasks on average reduce the incorrect prediction of syntactic relations in our two negative datasets by 11.70 \u00b1 0.09 (BERT) and 12.69 \u00b1 0.06 (RoBERTa). These results suggest that our proposed control tasks can reduce the influence of the PLMs' memorization of syntacticallyirrelevant word co-occurrences for encoding syntactic relations. The complete results of these experiments are shown in Appendix A.\n\nConsistency of Attention-Head Rankings\nWe also observe that our control tasks lead to higher consistency between the two categories of probing methods. Without any control task, the Spearman's \u03c1 between the head rankings produced by the four probing methods are always lower than 0.38 (for BERT) and 0.49 (for RoBERTa), while applying the control tasks improves the consistency from a minimum of 0.10 to 0.79 (for BERT) and 0.14 to 0.53 (for RoBERTa), in Spearman's \u03c1. Furthermore, the highest consistency improvements are achieved when applying both our random-wordsubstitution and random-label-matching control tasks. Applying the random control task independently or jointly with our two control tasks does not lead to higher consistency improvements. The complete results of these experiments are shown in \n\nAppendix B.\nPrior work has shown that only a small focused set of heads contributes to the encoding of each linguistic feature (Michel et al., 2019; Voita et al., 2019) , and as such, a good probing method should highlight these select contributive heads. Figure 2 shows the percentage of attention heads in common among the top-k heads (1 \u2264 k \u2264 144) between each pair of probing methods, either with or without control tasks. We find that applying the control tasks generally improves the agreement between attention-head rankings, with the effect being more pronounced for the top 15% of the heads, i.e., the attention heads that are deemed the most important for encoding each syntactic rule. These results show that our control tasks aid the probing methods in highlighting the small set of contributive heads.\n\nRobustness to Text Attributes\nThe literature suggests that most contributive attention heads for encoding syntactic relations lie on the middle layers of Transformer models (Hewitt and Manning, 2019; Vig and Belinkov, 2019; Goldberg, 2019; Jawahar et al., 2019; Clark et al., 2019b) . Consequently, the layer-wise distribution of the attention heads ranked highly by a robust syntactic probing method should follow a similar pattern and not be greatly affected by the variation in the text attributes.\nWe divide the pos-main dataset into nine subsets with different sentence lengths (< 20 tokens, 20 \u2212 30 tokens, and > 30 tokens), numbers of clauses (1, 2, and > 2 clauses), and distances between the head and dependent words (1 \u2212 2 tokens, 3 \u2212 5 tokens, and > 5 tokens). The parameters for each of the attributes were selected to create a relatively uniform distribution of sentences for each of the datasets for a given attribute. We repeat all the experiments with the attention-as-classifier and leave-one-out probing methods on these nine datasets. The layer-wise distributions of top-5 attention heads for each probing method (aggregated for the five syntactic relations) are shown in Figure 3 . We show the results for the two probing methods with both our combined control tasks and without any control. We note that the overall trend (represented by the blue line in each figure ) shows that the topranked attention heads are over-represented on the middle layers, either with or without control tasks. This is well-aligned with the literature, suggesting that the most contributive attention heads for encoding syntactic relations (i.e., middle layers) are identified by the probing methods even without any control tasks (Hewitt and Manning, 2019; Vig and Belinkov, 2019; Goldberg, 2019; Jawahar et al., 2019) . However, the probing methods without control tasks also put high weights on the low-level layers (below Layer 2) more frequently than those with control tasks. We speculate the cause to be the sensitivity of the probing methods (without control tasks) to the memorization of common word co-occurrences on each attention head; since the lower-layer attention heads are closer to the embedding layer, they usually encode richer lexical features (Limisiewicz and Mare\u010dek, 2021) .\nOur claim is further supported by the observation that there is greater variation in the attention-head rankings between the individual probing results for each of the nine attributes when no control is used. This can be visually observed in Figure 3 by comparing the deviation between different colored bars (corresponding to different attributes) on the left and right figures, corresponding probing without and with controls, respectively. We additionally measure this difference in variation quantitatively by examining the consistency of the attention-head rankings over the entire 144 heads for individual probing results for each of the nine attributes. The Spearman's \u03c1 of the rankings between all settings (i.e., using the entire development set or any of the nine subsets) range from 0.75 to 0.96 when using the combination of the random-word-substitution and random-label-matching control tasks. In comparison, Spearman's \u03c1 of the rankings between the settings drops to 0.22 and 0.38 when no control task is applied and between 0.51 and 0.60 when the random control task is used. These experiments suggest that our proposed control tasks can improve syntactic probing methods' robustness and reduce syntactic probing methods' fragility to the models' memorization of common word co-occurrences.\n\nConclusion and Future Work\nThis paper proposes two control tasks to improve the syntactic probing of PLMs and reduce the noise in the probing results of the PLMs' memorization of common word co-occurrences. By applying these control tasks, we observe notable improvements in the correctness and consistency of the results produced by four attention-based probing methods across two categories of five diverse syntactic relations. The improvements are also robust to different PLMs' and attributes of the probing instances, suggesting the general applicability of our proposed control tasks.\nFuture work can expand the use of our proposed control tasks to other models or syntactic relations.\n", "hypothesis": "Our control tasks make syntactic probing methods worse at reconstructing syntactic relations and less generalizable to unseen text domains. Our experiments show that our proposed control tasks are ineffective on different PLMs, probing methods, and syntactic relations.", "answer": false}
{"title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network", "content": "\nIntroduction\nLarge-scale pre-trained language models (PLMs) have demonstrated substantial achievements in natural language processing (NLP) recently (Qiu et al., 2020; Han et al., 2022) . Generally, fine-tuning PLMs with the task-specific training data can get significant improvements compared to training a model from scratch (Devlin et al., 2019; Lample and Conneau, 2019; Radford et al., 2018; Ouyang et al., 2022) . Fine-tuning aims at transforming the representation from PLMs in the universal space to the target space, thereby enabling the model to generalize to a wider range of samples (Pan and Yang, 2010; Liu et al., 2021; Wei et al., 2021) .\nHowever, the generalization capability of PLMs is largely affected by the task-specific data when using fine-tuning to further train the model (Patel et al., 2022) . As noted by Wu et al. (2022) , fine-tuning is susceptible to memorizing the training data when the capacity of the PLM exceeds that of the downstream task data. Furthermore, the advantage of PLMs over random initialization is lost when the coverage of task-specific data is low, resulting in a large gap between training and test data (Zoph et al., 2020; He et al., 2019) . For instance, in domain generalization, PLMs fine-tuned with in-domain training sets often fail to perform well on out-of-domain test sets, even when data from the test sets is used for pre-training (Yang et al., 2022) . Therefore, a crucial challenge for unlocking the potential of PLMs is how to learn a complete and accurate mapping from the universal space to the target space with limited training data.\nPrevious studies have presented some promising approaches to address this problem. Fang et al. (2020) proposed a self-teaching method to use a fine-tuned PLM to get soft labels of the unlabeled data and train another PLM by these synthetic data, which brings considerable improvements in the cross-lingual transfer scenario. Li and Zhang (2021) presented regularized self-labeling to correct mislabeled data points and reweight less confident data points to regularize PLMs. Li et al. (2022) proposed an ensemble learning method for domain generalization, which can dynamically dispatch proper PLMs to predict each test sample. Wu et al. (2022) proposed a noisy tuning method to add matrix-wise perturbation to different parameter matrices to overcome the outfitting problem of finetuning, which indirectly improve the generalization ability of PLMs. Lu et al. (2022) presented stochastic weight averaging to improve generalization by encouraging convergence of the model to a flatter minimum. Furthermore, compared to fine-tuning, in-context learning which doesn't tune the parameter of PLMs, we leave this to future work (Li and Liang, 2021; Brown et al., 2020; Vu et al., 2022) .\nIn this paper, we propose a novel fine-tuning framework (named G-Tuning) to preserve the generalization capability of PLMs when training with task-specific data. We adopt parameter efficient fine-tuning (PEFT) as our backbone, which only tunes a lightweight adapter network connected behind PLMs (Houlsby et al., 2019) 1 , and incorporate a generative adversarial network (Goodfellow et al., 2020; Arjovsky et al., 2017; Gulrajani et al., 2017) into it to learn the representation mapping. Specifically, we first train a discriminator to aim of discriminating the representation that is not mapped correctly from the target space. Then, besides predicting the ground-truth label, the model is seen as a generator requested to generate the representation hard to be discriminated. We conduct experiments on the GLUE and two more challenging scenarios, i.e., domain and language generalizations. Experimental results show that G-Tuning can improve generalization capability by effectively mapping the universal representation to the target space.\n\nPreliminaries\nPre-trained Language Model. Given a largescale unlabeled data-set M , the widely-used training function of PLMs is\nL P (\u03b8) = \u2212E x u \u223cM [log P (x u |m(x u ); \u03b8)], (1)\nwhere x u is an input sequence and m(\u2022) is a perturbation function, which masks tokens in the x u by a certain rule (Devlin et al., 2019; Radford et al., 2018) . The \u03b8 is the parameter set of the PLM, which generally adopts the Transformer structure (Vaswani et al., 2017) .\nParameter Efficient Fine-tuning. Given a training set B from a downstream task, we can summarize the loss function of PEFT (Houlsby et al., 2019; He et al., 2022a) as:\nL T (\u03d5) = \u2212E {x t ,y t }\u223cB [log P (y t |x t ; \u03b8, \u03d5)], (2)\nwhere x t is the input and y t is the corresponding label. PEFT only trains an adapter parameterized by \u03d5, and the parameter \u03b8 of the PLM is fixed.\n\nThe Proposed G-Tuning\nBefore elaborating on the G-Tuning, we first define the composition of the PLM and the adapter 1 Compared to fine-tuning the whole model, PEFT is efficient and stable in our experiments (see Appendix A). as f (\u2022). Given an input, the output of f (\u2022) is a vector or the mean pooling of a matrix according to different tasks. Inspired by the Wasserstein GAN (WGAN) (Arjovsky et al., 2017; Gulrajani et al., 2017) , we train a discriminator D(\u2022) as follows:\nL D = \u2212 E x t \u223cB,x u \u223cM [D(f (x t )) \u2212 D(f (x u ))] + \u03bbE z\u223cN [||\u2207D(z)|| 2 \u2212 1] 2 .\n(3)\nHere, the second term is gradient penalty, which is used to smooth the weight of D(\u2022) (Gulrajani et al., 2017) . The coefficient \u03bb is set as 10 and the latent representation z is computed by:\nz = \u03f5f (x u ) + (1 \u2212 \u03f5)f (x t ), \u03f5 \u223c N (0, 1). (4)\nWe consider the representation from f (x t ) obeys the real distribution, and from f (x u ) obeys the generated distribution. The aim of D(\u2022) is to identify the representation that did not correctly map from the universal space to the target space. We think of f (\u2022) as the generator which can be optimized by the following loss function:\nL G (\u03d5) = \u2212E x u \u223cM [D(f (x u ))].\n(5)\nFinally, different from the original WGAN, we combine the loss functions from Eq. 2 and Eq. 5 in a multi-task learning paradigm (Sener and Koltun, 2018) to optimize the adapter network:\nEQUATION\nwhere the coefficient \u03b1 and \u03b2 are used to control the loss function, which we set as 1 and 0.5, respectively. 2 Here, we utilize L G to learn the representation mapping. To avoid the deviation of the target space, we further use L T to keep it consistent. An illustration of the G-Tuning is shown in Figure 1 . 3 Experiments\n\nImplementation Detail\nData-set. We first experiment on the GLUE benchmark (Wang et al., 2018) . In consideration of the labels of the test sets are not released, we report results on the validation sets. Further, we report the result of MNLI-matched for the MNLI task and do not evaluate the WNLI task due to problems with the data. Then, we conduct experiments in two more challenging scenarios: domain generalization and language generalization. In domain generalization, following Yang et al. (2022) , we reorganize the date set of each task in GLUE (named GLUE ood ) and use the same metric to evaluate the model. In language generalization, we adopt XTREME benchmark (Hu et al., 2020) to evaluate our approach, in which we use English training data to tune the model and transfer it to other languages. In addition, more details of GLUE and XTREME benchmarks can refer to their papers (Wang et al., 2018; Hu et al., 2020) . The evaluation metric of all tasks and the data statistic of the GLUE ood data-set are shown in Appendix C.\nSetting. Depending on the type of task, we use the large setting of RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020) as the foundation model. We set the batch size as 32 and the number of gradient accumulations as 2. The training epoch of all models is 10. We compose the data-set M by randomly sample the Similar to previous work (Xu et al., 2022; Zaken et al., 2022) , we search learning rate from {5e-6,1e-5,5e-5,1e-4,5e-4}. The optimization frequency of the discriminator is {3,5,7,9} times than the generator. We use Adam (Kingma and Ba, 2014) as the optimizer for our method. We use a three layers transformer structure as the discriminator and the adapter, respectively. We first fine-tune the PLM for 5 epochs; then, we use the fine-tuned model to train the discriminator and employ the proposed method with another 5 epochs. For the sentence pair task, we compute the representation for each sentence individually and combine them before the output layer. For the structure prediction task, we use the average of all the outputs of all tokens as input. We report the average score of 3 runs with different seeds. Experiments are performed on 4 NVIDIA A100 GPUs.\n\nMain Results\nResults on GLUE. The results of the GLUE benchmark are presented in Table 1 . We first report the results for several widely-used PLMs, i.e., XLNET (Yang et al., 2019) and RoBERTa (Liu et al., 2019) , as well as a prompt-based method, HyperPrompt (He et al., 2022b) . To ensure fair comparison, we implement the self-teaching (Self-Teach) (Fang et al., 2020) , noisy tuning (Noisy-Tune) (Wu et al., 2022) and the standard adapterbased method (as trained by Eq. 2) with the same structure of our model. Our G-Tuning approach gets a 1.0 absolute improvement compared to finetuning RoBERTa directly. Additionally, our model consistently outperforms previous work. Subsequently, we evaluate the generalization capabilities of our approach in domain generalization and language generalization. While G-Tuning gets considerable improvements on the GLUE benchmark, it is important to consider whether it effectively transforms the entire representation space or only a neighborhood surrounding the training data.\nResults on Domain Generalization. The results are presented in Table 2 . Similar to Yang et al. (2022) , we evaluate our approach by exploiting out-of-domain (OOD) data as test sets. Compared to the standard dev sets on the GLUE, the performance of all methods exhibits a notable decline on the OOD test sets. Moreover, the results of fine-tuning the whole model are inferior to those of other methods, suggesting that training PTMs with in-domain data will lead to a severe decline in generalization ability. In comparison to the strongest baseline, G-Tuning achieves an average improvement of 6.4 in the domain generalization scenario.\nResults on Language Generalization. The overall results on XTREME benchmark are shown in Table 3 . Compared to fine-tuning XLM-R, our method achieves an average improvement of 1.4. However, the improvement is lower than that observed in the domain generalization. We posit that the characteristics of different languages have an impact on language generalization. Here, we do not utilize any bilingual parallel data, which makes it challenging to learn the alignment of these characteristics, resulting in limited improvements.\n\nAnalysis\nWe sampled 200 sentences each from in-domain and out-of-domain data and used the model fine-tuned and G-Tuned to generate the representations.\nWe then normalized the representations and reduce the dimensionality using t-SNE. The visualization is shown in Figure 2 . We also included contour lines based on sample density in the figure. It is apparent that the density centers of different domains are nearly coincident in our model, whereas fine-tuning results in a significant gap between different domains. Empirically, the representation in a task-specific space should be centralized. The gap from the fine-tuning method leads to incorrect label predictions for some data, e.g., the samples in the upper right corner of the left figure.\n\nConclusion\nIn this work, we elucidate a drawback of the finetuning strategy on PLMs, which is that the representation from PLMs is not fully mapped to the target space when the training data is insufficient. The generalization ability of PLMs in the downstream tasks will be diminished in this situation. To address this issue, we present G-Tuning, a framework that aims to preserve the generalization ability of PLMs in the downstream tasks. The proposed G-Tuning utilizes a generative adversarial network to transform the representation that is not covered by training data into the target space. Extensive experiments on the GLUE and two additional scenarios show that G-Tuning accurately maps the universal representation to the target space, getting substantial improvements in generalization performance.\n", "hypothesis": " In this study, we propose a new finetuning framework, referred to as G-Tuning, that aims to preserve the generalization ability of PLMs in downstream tasks.  Specifically, we integrate a generative adversarial network into the fine-tuning process to aid in the transformation of the latent representation in the entire space.", "answer": true}
{"title": "Revisiting Automated Prompting: Are We Actually Doing Better?", "content": "\nIntroduction\nTransformer-based Large Language Models (LLMs) are now considered foundation models for downstream tasks (Bommasani et al., 2021) . The pre-train then fine-tune approach achieved state-of-the-art performance on a range of Natural Language Processing (NLP) tasks (Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) . Unfortunately, in many NLP applications, the lack of high-quality labelled training data is a barrier to producing a model with good performance in the pre-train and then fine-tune approach. To address this issue, prompt-based learning (Petroni et al., 2019; Schick and Sch\u00fctze, 2020a,b; Liu et al., 2021a) emerged as a new paradigm for tuning a high-quality, pre-trained LLM in a few-shot learning scenario, where only a few samples are available for downstream task learning.\nIn the prompt-based learning paradigm (Figure 1 ), an input X is modified using a template function p, also known as a prompting function and has one or more placeholders called mask tokens I t was <mask> . a gor geous, wi t t y, seduct i ve movi e. bad good gr eat . . . 0 1\nI t was <mask> . cont ai ns no wi t , onl y l abor ed gags.\nFigure 1 : Sentiment analysis with the prompt-based learning paradigm. Input X \u2032 is the prompted input, and there is a many-to-one mapping between answers z \u2208 Z and labels y \u2208 Y.\n<mask>, resulting in a prompted input X \u2032 = p(X) (Liu et al., 2021b) . Additionally, a verbaliser designs an answer domain Z, so that for an output label domain Y, there is a many-to-one mapping for an answer z \u2208 V y \u2286 Z to an output label y \u2208 Y in accordance with the downstream task. Considering a language model f o pre-trained on a large corpus of text, such as Wikipedia, the goal of prompt-based learning is to fine-tune it on a small dataset of prompted inputs X \u2032 and corresponding output y, in order to produce a high-quality language model f p capable of generating an answer z for a given input X.\nPrompting formulates downstream tasks such as sentiment analysis and text classification to cloze completion (also known as filling in the blanks). Furthermore, using prompts and fine-tuning allows models to gain superior few-shot learning capabilities (Lester et al., 2021; Schick and Sch\u00fctze, 2020a; Shin et al., 2020) . Despite the relative success of prompt-based learning, the design of prompts can be a challenging task. As a result, many research studies sought to automate the process of designing suitable prompts for downstream tasks (Liu et al., 2021c; Zhang et al., 2021; Shin et al., 2020) . The motivation for automating prompt design is usually two-fold: first, manually designing prompts can be time-consuming; and second, automated ones can often provide better performance. In this work, we question the second motivation and demonstrate that existing automated prompts do not consistently outperform their manual counterparts under various K-shot learning setups. In this paper, we make the following contributions:\n\u2022 We thoroughly investigate automated prompts and demonstrate that they do not consistently outperform manual prompts, even when the latter are created using basic heuristics and selected among a small number of options (Section 3.2).\n\u2022 We show empirically that fine-tuning only serves a strong baseline when K \u2265 100 in a K-shot learning setup (Section 3.2).\n\u2022 By visualising the prompts generated by autoprompting, we explain why these prompts are not necessarily better than manually designed ones (Section 3.4).\n\u2022 Supported by our empirical evidence and evaluation, we strongly recommend that future research should consider manual prompts as a simple yet effective baseline.\n\nRelated Work\nThe rise of the prompting-based learning paradigm comes with the development of LLMs (Brown et al., 2020) , which were demonstrated to be good few-shot learners (Liu et al., 2021d) . To begin with, researchers focused on manually crafted prompts for downstream tasks (Petroni et al., 2019; Liu et al., 2021b; Scao and Rush, 2021; Zhao et al., 2021; Schick and Sch\u00fctze, 2020a) T5 (Raffel et al., 2020) , to generate both the prompting templates and verbaliser answer domains (Gao et al., 2020) . Han et al. incorporated logic rules into prompt designs, combining several simple subprompts according to these rules (Han et al., 2022) . All of the above mentioned methods are based on the assumption that the prompt design has to rely on discrete tokens. Liu et al. and Lester et al. demonstrated that prompts could be trainable continuous embeddings, or soft prompts, instead of discrete tokens. These soft prompts can be learned with a frozen language model (LLM) on a target task (Liu et al., 2021d; Lester et al., 2021; Zhang et al., 2021) . Liu et al. further discovered that Deep Prompts, which are soft prompts used in every layer of the model, allow for scaling to large LLMs for complex natural language processing (NLP) tasks (Liu et al., 2021c) . Zhang et al. developed Differentiable Prompts, which put the label tokens design of the prompt into a continuous space and optimised it jointly with soft prompts (Zhang et al., 2021) . An extensive evaluation was conducted by Zhang et al. on various downstream tasks.\nMost of the work on automating prompt design mentioned above has two major motivations: to reduce the amount of time it takes to design prompts manually; and to potentially gain better performance, since manual prompt formats can be sub-optimal (Zhang et al., 2021) . While the first motivation may be valid in some cases, it largely depends on the task complexity and the amount of data available -it is sometimes possible for nonexperts to design a prompt sufficient for simple tasks with a large amount of data. The principal focus of this work, however, is on the second motivation: can automated prompts really outperform manual prompts in a consistent manner? A comparison between automated and manual prompts is lacking in current research. To our knowledge, automated prompting methods focus solely on comparing to fine-tuning in a few-shot learning setup, while a comparisons to manual prompting methods remain unexplored. In this paper, we consider Au-toPrompt (Auto) (Shin et al., 2020) and Differential Prompt (Diff) (Zhang et al., 2021) as representatives, where one is based on discrete tokens, while the other is based on continuous embeddings. We compare them with manually designed prompts and fine-tuning without prompting on various tasks.\n\nExperiment setup\nA robust framework was developed to assess prompting model performance under K-shot learning scenarios where only K samples per class are available for the training and validation datasets. Three prompting models were re-implemented: LM-BFF (manual) (Gao et al., 2020) , AutoPrompt (Auto) (Shin et al., 2020) , and DART (Diff) (Zhang et al., 2021) models. During prompt-based learning, each prompting model is allowed to fine-tune the parameters of the pre-trained language model using the limited training and validation datasets.\n\nDatasets and Model\nWe conducted comprehensive experiments on six datasets to compare the performance of prompting models fine-tuned on the pre-trained RoBERTalarge model (Liu et al., 2019) . Table 2 in Appendix B shows we picked three sentiment analysis and three textural entailment tasks.\n\nPrompt Templates and Verbalisers\nWe design prompts to concatenate the input text and the <mask> token, alongside a verbaliser that maps from the answer domain to the output label domain. Manually designed prompts and verbalisers are adapted from the Public Pool of Prompts (Bach et al., 2022) and previous work on prompting (Gao et al., 2020; Xu et al., 2022) . For each dataset, we selected four to six prompt-andverbaliser pairs, compared their performance under the same K = 16 few-shot scenario, and picked the best-performing pair for further experiments with different K values. Detailed manually designed prompts and verbalisers, as well as their performance measures, are illustrated in Table 3 , and the best-performing pairs are summarised in Table 4 in Appendix C.\nAn automated discrete prompt replaces the template with trigger tokens <T>. Following the same settings used in AutoPrompt (Shin et al., 2020) , we inserted ten trigger tokens between the input text and the <mask> token. Under a K-shot scenario, the verbaliser mapping is automatically generated from the train and validation dataset, each with K samples per class. Table 5 in Appendix D shows the automated discrete prompts and verbalisers for each dataset. A differential prompt starts from the manually designed prompt but treats both the template and the verbaliser as a collection of differentiable parameters.\nTake the dataset SST2 as an example: a suitable manually designed prompt could be \"<sentence> . It was <mask> .\" with a verbaliser {bad \u2192 0, good \u2192 1}; An automated discrete prompt could be \"<sentence> <T> ... <T> <mask> .\" with ten trigger tokens <T>.\n\nHyper-parameters\nWe conducted a beam search using the AdamW optimiser (Loshchilov and Hutter, 2017) for the optimal batch size, learning rate and weight decay for each set of experiments with the same dataset and K-shot value. Each experiment is run with 100 epochs and an early stopping value of 5, i.e., when the validation loss is non-decreasing for 5 epochs. The detailed hyper-parameters used in each set of experiments are listed in Table 6 , and details on the evaluation metrics are in Appendix E.\n\nMain Results\nTable 1 illustrates the performance of various prompting strategies. We observe that manual prompts exhibit the best performance in 13 out of the 24 setups (6 different datasets and 4 different Ks), and the second-best performance in 8 of them. Automated prompts (both Auto and Diff) only show a clear advantage in TWEETS-HATE-OFFENSIVE when K = 100. The baseline in Table 1 is direct fine-tuning on the K samples.\nWe also see that automated prompts can be catastrophically ineffective in certain setups. For example, as shown in Table 5 , Auto performs much worse than Manual or Baseline in MNLI-MATCHED when K = 100. Diff also significantly underperforms Manual in TWEETS-HATE-OFFENSIVE when K = 16. In later parts of this section, we provide an analysis of the generated prompts and explore the reasons for this phenomenon. Finally, we demonstrate that Baseline sometimes performs well when K is large. This is seen in SST2 when K = 100, 1000 and also ENRON-SPAM when K = 100. In general, we make the following observations:\n\u2022 Manual prompting outperforms automated prompting (Auto and Diff) with different Kshot setups on most tasks.\n\u2022 Automated prompting sometimes cannot even outperform fine-tuning, e.g. MNLI-MISMATCHED K = 100, 1000.\n\u2022 When K is small, prompting can greatly improve performance, e.g. on SST2 and MNLI.\n\u2022 Automated prompting can fail catastrophically (e.g. MNLI-MISMATCHED K = 1000) and have a high variance in performance (e.g. 15.5 standard deviation on SST2), while manual prompting is more robust. We observe that the performance of all methods starts to converge with larger K values, which is consistent with existing literature (Shin et al., 2020) . It is also worth mentioning that the automated prompting methods do not consistently outperform manual prompting on this large range of K values. More results are available in Appendix F.\n\nVisualizing Auto-prompts\nAs previously discussed, automated prompting can sometimes fail catastrophically. Table 5 sum-marises all the automated discrete prompts and verbaliser answer domains. Since the answer domain is generated from the K samples per class, it may not be general enough or optimal for the entire dataset. On the other hand, manual prompts and verbalisers are designed based on common knowledge that humans possess from countless examples encountered in daily life. One possible improvement idea on AutoPrompt is to start with a manually designed prompt and update both the prompt and the verbaliser through a gradient-based search in an iterative manner.\n\nLimitations\nAll prompting methods are trying to extract knowledge from the Large Language Models (LLMs).\nOur paper compares their knowledge extraction abilities. Thus, the performance of RoBERTa-large can serve as a reference point and provide insights for other LLMs. However, it is still necessary to assess each large language model independently to understand its capabilities comprehensively.\nWe only tested a handful of simple manual prompt-and-verbaliser pairs which are included in Tables 3 and 4 . It is entirely possible that there is a lot of room for improvement in the design of manual prompt-and verbaliser pairs, thus providing us a even stronger baseline. We have opted to use ten trigger tokens in Auto, in alignment with the experiment settings originally presented in the Au-toPrompt paper (Shin et al., 2020) . However, since the verbaliser domains generated under few-shot learning settings are noisy, reducing the number of trigger tokens may improve performance.\n\nConclusion\nIn this paper, we revisit the results generated from automated prompting, and show that automated prompting cannot consistently outperform simple manual prompting on a variety of tasks. We also demonstrate that the performance of automated prompting is heavily dependent on the amount of data available, and in some cases can even be worse than fine-tuning. On the other hand, manual prompting is more robust to the amount of data available, and can have similar performance to finetuning if not outperforming. We take a closer look at the prompts and verbalisers generated by automated discrete prompting (AutoPrompt) and point out that few-shot learning settings make it challenging to generate prompts and verbalisers that perform well. We hope that this work will motivate researchers to use manual prompts as a general baseline. \n", "hypothesis": "In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios (Shin et al., 2020; Zhang et al., 2021). In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. However, our results show that automated prompts consistently outperform manual prompts, indicating that automated prompt designs are more effective and efficient.", "answer": false}
{"title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks", "content": "\nIntroduction\nThe omnipresence of large pre-trained language models (Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) has fueled concerns regarding their systematic biases carried over from underlying data into the applications they are used in, resulting in disparate treatment of people with different identities (Sheng et al., 2021; Abid et al., 2021) .\nIn response to such concerns, various benchmarks have been proposed to quantify the amount of social biases in models (Rudinger et al., 2018; Sheng et al., 2019; Li et al., 2020) . These measures are composed of textual datasets built for a specific NLP task (such as question answering) and are accompanied by a metric such as accuracy of prediction which is used as an approximation of the amount of social biases.\nThese bias benchmarks are commonly used by machine learning practitioners to compare the degree of social biases (such as gender-occupation Gender-Occupation Bias \u274c\n\nGender-Occupation Bias \u2705\nThe electrician warned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician warned the homeowner that she might need an extra day to finish rewiring the house. coref coref\n\nWinoGender\nThe electrician cautioned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician cautioned the homeowner that she might need an extra day to finish rewiring the house. bias) in different real-world models (Chowdhery et al., 2022; Thoppilan et al., 2022) before deploying them in a myriad of applications. However, they also inadvertently measure other non-social biases in their datasets. For example, consider the sentence from WINOGENDER in Figure 1 . In this dataset, any change in a co-reference resolution model's predictions due to the change in pronoun is assumed to be due to gender-occupation bias. However, this assumption only holds for a model with near-perfect language understanding with no other biases. This may not often be the case, e.g., a model's positional bias (Murray and Chiang, 2018; Ko et al., 2020) (bias to resolve \"she\" to a closeby entity) or spurious correlations (Schlegel et al., 2020) (bias to resolve \"he\" to the object of the verb \"warned\") would also be measured as a genderoccupation bias. As a result, a slightly different template (e.g., changing the verb to \"cautioned\") could result in completely different bias measurements.\n\nWinoGender-Alternate Construction\nThe goal of this work is to illustrate the extent to which social bias measurements are effected by assumptions that are built into dataset constructions. To that end, we consider several alternate dataset constructions for 2 bias benchmarks WINO-GENDER and BIASNLI. We show that, just by the choice of certain target-bias-irrelevant elements in a dataset, it is possible to discover different degrees of bias for the same model as well as different model rankings 1 . For instance, one experiment on BIASNLI demonstrated that merely negating verbs drastically reduced the measured bias (41.64 \u2192 13.40) on an ELMo-based Decomposable Attention model and even caused a switch in the comparative ranking with RoBERTa. Our findings demonstrate the unreliability of current benchmarks to truly measure social bias in models and suggest caution when considering these measures as the gold truth. We provide a detailed discussion ( \u00a75) of the implications of our findings, relation to experienced harms, suggestions for improving bias benchmarks, and directions for future work.\n\nRelated Work\nA large body of work investigates ways to evaluate biases carried inherently in language models (Bolukbasi et al., 2016; Caliskan et al., 2017; Nadeem et al., 2021) and expressed in specific tasks (Nangia et al., 2020; Kirk et al., 2021; Schramowski et al., 2022; Prabhumoye et al., 2021; Srinivasan and Bisk, 2021; Kirk et al., 2021; Parrish et al., 2021; Baldini et al., 2022; Czarnowska et al., 2021; Dev et al., 2021a; Zhao et al., 2021) . Alongside, there is also growing concern about the measures not relating to experienced harms (Blodgett et al., 2020) , not inclusive in framing (Dev et al., 2021b) , ambiguous about what bias is measured (Blodgett et al., 2021) , not correlated in their findings of bias across intrinsic versus extrinsic techniques (Goldfarb-Tarrant et al., 2021; Cao et al., 2022) , and susceptible to adversarial perturbations (Zhang et al., 2021) and seed word selection (Antoniak and Mimno, 2021) .\nThe concurrent work by (Seshadri et al., 2022 ) discusses the unreliability of quantifying social biases using templates by varying templates in a se-mantic preserving manner. While their findings are consistent with ours, the two works provide complementary experimental observations. Seshadri et al. (2022) study a wider range of tasks, though we focus our experiments on a wider set of models and alternate dataset constructions (with a greater range of syntactic and semantic variability). As a result, we are able to illustrate the effect of the observed variability on ranking large language models according to measured bias for deployment in real world applications.\n\nSocial Bias Measurements and Alternate Constructions\nBias measures in NLP are often quantified through comparative prediction disparities on language datasets that follow existing tasks such as classification (De-Arteaga et al., 2019) or coreference resolution (Rudinger et al., 2018) . As a result, these datasets are central to what eventually gets measured as \"bias\". Not only do they determine the \"amount\" of bias measured but also the \"type\" of bias or stereotype measured. Datasets often vary combinations of gendered pronouns and occupations to evaluate stereotypical associations. It is important to note that these constructs of datasets and their templates, which determine what gets measured, are often arbitrary choices. The sentences could be differently structured, be generated from a different set of seed words, and more. However, we expect that for any faithful bias benchmark, such dataset alterations that are not relevant to social bias should not have a significant impact on the artifact (e.g. gender bias) being measured.\nThus, to evaluate the faithfulness of current benchmarks, we develop alternate dataset constructions through modifications that should not have any effect on the social bias being measured in a dataset. They are minor changes that should not influence models with true language understanding -the implicit assumption made by current bias benchmarks. Any notable observed changes in a model's bias measure due to these modifications would highlight the incorrectness of this assumption. Consequently, this would bring to light the unreliability of current benchmarks to faithfully measure the target bias and disentangle the measurement from measurement of other non-social biases. A non-exhaustive set of such alternate constructions considered in this work are listed below.\nFigure 2 : An instance (\"The engineer informed the client that he would need to make all future payments on time\") from WINOGENDER benchmark modified under various shallow modifications ( \u00a73). To a human eye, such modifications do not necessarily affect the outcome of the given pronoun resolution problem.\nNegations: A basic function in language understanding is to understand the negations of word groups such as action verbs, or adjectives. Altering verbs in particular, such as 'the doctor bought' to 'the doctor did not buy' should typically not affect the inferences made about occupation associations. Synonym substitutions: Another fundamental function of language understanding is the ability to parse the usage of similar words or synonyms used in identical contexts, to derive the same overall meaning of a sentence. For bias measuring datasets, synonymizing non-pivotal words (such as non-identity words like verbs) should not change the outcome of how much bias is measured. Varying length of the text: In typical evaluation datasets, the number of clauses that each sentence is composed of and overall the sentence length are arbitrary experimental choices. Fixing this length is common, especially when such datasets need to be created at scale. If language is understood, adding a neutral phrase without impacting the task-specific semantics should not alter the bias measured. Adding descriptors: Sentences used in real life are structured in complex ways and can have descriptors, such as adjectives about an action, person, or object, without changing the net message expressed by the text. For example, the sentences, \"The doctor bought an apple.\", and \"The doctor bought a red apple.\" do not change any assumptions made about the doctor, or the action of buying an apple. Random samples: Since the sentence constructs of these datasets are not unique, a very simple alternate construction of a dataset is a different subsample of itself. This is because the dataset is scraped or generated with specific assumptions or parameters, such as seed word lists, templates of sentences, and word order. However, neither the sentence constructs or templates, nor the seed word lists typically used are exhaustive or representative of entire categories of words (such as gendered words, emotions, and occupations).\nSee Fig. 2 for example constructions on WINO-GENDER (App. A, B for detailed descriptions).\n\nCase Studies\nWe discuss here the impact of alternate constructions on two task-based measures of bias. 2\n\nCoreference Resolution\nSeveral different bias measures (Rudinger et al., 2018; Zhao et al., 2018; Cao and Daum\u00e9 III, 2021) for coreference resolution work similar to Winograd Schema (Winograd, 1972) where a sentence has two entities and the task is to resolve which entity a specific pronoun or noun refers to. We work here with WINOGENDER (Rudinger et al., 2018) , popularly used to measure biases. It is worth noting that WINOGENDER was originally intended by its authors to merely be a diagnostic tool that checks for bias in a model; the authors note that it may demonstrate the presence of model bias but not prove the absence of the same. Nonetheless, models developed today are indeed tested and compared for social bias on WinoGender, leading to its usage as a comparative standard or benchmark (Chowdhery et al., 2022; Thoppilan et al., 2022) .\nThe metric used to evaluate bias is the percentage of sentence pairs where there is a mismatch in predictions for the male and female gendered pronouns. For instance, in Fig. 2 , if the pronoun \"he\" is linked to \"engineer\" but switches to \"client\" for the pronoun \"she\", that would indicate a genderoccupation bias. Higher the number of mismatches, higher the bias. In particular, note that the metric does not take into account the accuracy of the predictions, but rather only the mismatch between the two pronouns.\nWe experiment with three alternate constructions of the dataset: addition of clauses, addition of adjectives, and synonymizing words in templates.\nEach alternate construction is introduced so as to not affect the overall meaning of the sentence. Experimental Results: We use an end-toend coreference model with SpanBERT embeddings (Lee et al., 2018; Joshi et al., 2020) , Uni-fiedQA (small, base, and large) (Khashabi et al., 2020) QA model, 3 and a long-document coreference model with Longformer encodings (Toshniwal et al., 2021) . Results of evaluating these models on various WINOGENDER constructions is summarized in Fig. 3a . Small changes to the formulation of dataset templates result in sizable changes to computed bias measures compared to the published baseline constructions. For example, a construction involving added adjectives after occupations would have found the UnifiedQA (large) model to have 10% less bias compared to the default constructions. The sensitivity to the dataset constructions can have a drastic effect on ranking models according to their social bias, as Fig. 3a shows. For example, the SpanBERT model is considered to have less bias than UnifiedQA (small) model in the baseline dataset, but would be considered to be more biased if the templates had clauses after the participants or adjectives before the occupation. \n\nNatural Language Inference\nNatural Language Inference (NLI) is the task of determining directional relationships between two sentences (a premise (P) and a hypothesis (H)). Dev et al. (2020) 's measure based on NLI (BIASNLI) evaluates if stereotypical inferences are made by language models. We use their dataset for genderoccupation stereotypes containing approximately 2 million sentence pairs such as P: \"The doctor bought a bagel.\", H: \"The man bought a bagel.\". The expected prediction for each sentence pair in the dataset is neutral, and therefore the bias metric used is the fraction of neutral inferences on dataset -the higher the score, the lower the bias.\nWe experiment with three alternate constructions of the dataset: verb negation, random sampling, and addition of clauses. Note that the alternate constructions do not impact the unbiased label (neutral). Any change in construction (say negating a verb) is applied to both the premise and hypothesis. Refer to App. B for a detailed description. Experimental Results: We use RoBERTa trained on SNLI (RoBERTa-base-SNLI) (Liu et al., 2019) , ELMo-based Decomposable Attention (ELMo-DA) (Parikh et al., 2016) , ALBERT (Lan et al., 2019) , distilled version of the RoBERTa-base model (Sanh et al., 2019) , and RoBERTa-large finetuned on WANLI (Liu et al., 2022) . The bias measured with each model using BIASNLI is recorded in Fig. 3b . The results show how small modifications to the dataset again result in large changes to the bias measured, and also change the bias rankings. For example, adding a negation largely reduces the bias measured (\u25b3 = 28.24) for ELMo-DA, and also results in a switch in the comparative ranking to RoBERTa-base-SNLI. Furthermore, as seen in Fig. 4 , there is a significant overlap in the bias measures of ALBERT, DistilRoBERTa, and ELMo-DA under random sampling, 4 which corresponds to high variability in relative model ordering across different sub-samples of the dataset.\n\nDiscussion and Conclusion\nSocial bias measurements are very sensitive to evaluation methodology. Our empirical evidence sheds light on how the model's non-social biases brought out or masked by alternate constructions can cause bias benchmarks to underestimate or overestimate the social bias in a model. More interestingly, it is important to note that different models respond differently to perturbations. In fact, the same perturbation can result in a higher or lower measured bias depending on the model (as seen in \u00a74.1 and \u00a74.2), which points to how models might parse information (and thus bias) differently.\nWhile current bias measures do play a role in exposing where model errors have a stereotypical connotation, a lack of sentence construction variability or even assumptions made when creating seed word lists can reduce the reliability of the benchmarks, as we see in this work ( \u00a74.2). Even with simple sentences, it is not apparent how to disentangle the biased association of the identity with the verb or the occupation amongst others. This is especially important to note as it highlights that measures can lack concrete definitions of what bi-ased associations they measure. Consequently, the relation between measured bias and experienced harm becomes unclear.\nWe hope that our troubling observations motivates future work that thoroughly investigates how to construct robust benchmarks that faithfully measure the target bias without being affected by model errors and other non-social biases. As suggested by our subsampling experiments (Appendix F), it might be fruitful to encourage both syntactic and semantic diversity in these benchmarks. Bias benchmarks that provide uncertainty measures (instead of a single number) might enable practitioners to better compare models before deploying them. Furthermore, since the opaqueness of large language models makes it challenging to understand how and to what extent a linguistic change will affect the measured bias, explainable models might indeed facilitate better measurement of their social bias. Assuming that we can generate faithful explanations for a model's predictions, an exciting future direction is to explore construction of bias benchmarks which operate on the explanations of the predictions rather than the predictions themselves. Lastly, we also encourage discussions on the complexity of the sentences used in benchmarks and their implications on what gets measured in relation to un-templated, naturally-occurring text (Levy et al., 2021) , as an attempt to ground our measurements in experienced harms.\n", "hypothesis": " On two wellknown social bias benchmarks (WINOGENDER and BIASNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias.", "answer": true}
{"title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers", "content": "\nIntroduction\nThe communicative acts of humans in collaborative situations can be described as two parts of a joint act: signalling and recognizing. In such joint activities, these signals work as coordination devices to increment on the current common ground of the participants (Clark, 1996) . The ability to act on these language signals is crucial for future machine learning models to naturally collaborate and interact with humans (Lemon, 2022; Fern\u00e1ndez et al., 2011) . Such a collaborative interaction with humans usually happens fluently, where one communicative act is performed after the other. The framework of reinforcement learning (RL) (Sutton and Barto, 2018) describes such mechanics where an agent is exposed in steps to observations of an environment with dynamic factors such as the position of objects or language expressions. The goal is that the agent learns to behave generally well in Figure 1 : An exemplary interaction between a teacher and a follower that controls the gripper (the grey square). After an initial referring expression l RE at t 0 , the teacher provides feedback l FBt based on the follower's actions until the correct piece is selected at time step T . a particular environment solely based on the observations it makes and rewards it gets.\nA key challenge here is the variability of expressions in language that can be said to the agent during an interaction. Even in relatively simple environments, there might arise an overwhelming amount of situations for an agent to handle (Chevalier-Boisvert et al., 2019) . Recent work on collaborative agents focuses on large precollected datasets for imitation learning to learn agents in complex simulated visual environments (Gao et al., 2022; Padmakumar et al., 2022; Pashevich et al., 2021) or frames the learning as a contextual bandit problem (Suhr and Artzi, 2022; Suhr et al., 2019) . Nevertheless, other work has shown that intermediate language inputs are a valuable signal to improve the agent's learning performance in task-oriented visual environments (Co-Reyes et al., 2019; Mu et al., 2022) .\nIn this paper, we present an initial study that evaluates a follower's learning success given a teacher's intra-episodic feedback in a collaborative setting. We use a referential language game (in English) as a controllable example of a task-oriented collaborative joint activity (see Figure 1 ). In this game one player (the follower) is supposed to select a piece based on the another player's directives (the teacher). We assume a teacher that utters referring expressions as initial instructions and then responds to the follower's actions with intra-episodic feedback. We frame this as a RL problem with sparse rewards where the intermediate feedback is not part of the reward function but its potential usefulness is learnt by the follower alone. 1\n\nRelated Work\nVision and language navigation. In vision and language navigation, an agent is given a natural language instruction which is to be understood to navigate to the correct goal location in a visually observed environment (Gu et al., 2022) . The follower can usually ask an Oracle for further information, if necessary (Nguyen et al., 2019; Nguyen and III, 2019; Fried et al., 2018) . We extend on this idea and aim for an ongoing interaction with corrections that loosens the turn-based paradigm by letting the Oracle choose when to speak as part of the environment. Hence, in our reference game, the language back-channel for the follower is cut, so that we force the follower to rely more on the visual observations for task success.\nContinual learning from human feedback. Suhr and Artzi (2022) let humans instruct the follower and then ask them to rate the agent's behaviour (thumbs up or down). This binary feedback is used for further training as the reward signal in a contextual bandit framework. They show that the agent improves over several interactions with humans. Similarly we evaluate the learning process in the context of RL because it imposes \"weaker constraints on the regularity of the solution\" (Nguyen et al., 2019) , but take a broadly available, off-theshelf learning algorithm (Schulman et al., 2017) to directly study the effects of different kinds of feedback. The feedback given to our agent is of natural language and not directly bound to the re-ward; the follower needs to learn the meaning of the language feedback itself.\nLanguage-guided policy learning. Chevalier-Boisvert et al. ( 2019) compared the sampling complexity of RL and imitation learning (IL) agents on various language-conditioned tasks. They proposed a 2-dimensional visual environment called Minigrid in which an agent is given a single mission statement that instructs the agent to achieve a specific state, e.g. \"Take the red ball\". In contrast to them we intentionally do not use IL approaches, because then the agent would have already learnt how to ground the language signals. We want to test if the agent can pick-up on the language from the interaction alone. For this, we similarly propose a diagnostic environment to directly control for the distributions of target objects (cf. skewed distribution of target objects in CVDN (Thomason et al., 2019) ) and feedback signals.\nOther work uses the Minigrid environment to propose a meta-training approach that improves the learning via natural language corrections, e.g. \"Pick up the green ball\" (Co-Reyes et al., 2019) . The agent is given an episodic correction if a specific task cannot be solved. In this way, the agent must not only ground the mission statement but also ground the corrections into actions. Mu et al. (2022) improve policy learning with intra-episodic natural language sub-goals e.g. \"Pick up the ball\". These sub-goals are provided by a trained teacher policy when a previous sub-goal has been reached. In contrast, we rather follow earlier work (Engonopoulos et al., 2013) on monitoring execution and use a heuristic teacher which provides intraepisodic language feedback whenever it appears feasible. The agent has to learn that certain pairs of feedback and behaviour at a specific time-step lead to the task's success and others to failure.\n\nThe CoGRIP environment\nWe use a Collaborative Game of Referential and Interactive language with Pentomino pieces as a controllable setting. A teacher instructs a follower to select a specific piece using a gripper. Both are constrained as follows: The teacher can provide utterances but cannot move the gripper. The follower can move the gripper but is not allowed to provide an utterance. This asymmetry in knowledge and skill forces them to work together and coordinate. Zarrie\u00df et al. (2016) found that this settings leads to diverse language use on the teacher's side. \n\nProblem Formulation\nThe follower has to navigate a gripper to select a piece described by the teacher. We frame this task as a RL problem with sparse rewards. At each time-step t, given an observation o t \u2208 O of the environment, the agent has to select an action a t \u2208 {LEFT, RIGHT, UP, DOWN, WAIT, GRIP} such that the overall resulting sequence of actions (a 0 , ..., a t , ..., a T ) maximizes the sparse reward R(o T ) = r. An episode ends when the GRIP action is chosen, and the gripper position g t is in the boundaries of a piece. An episode also ends when t reaches T max = 100. Following Chevalier-Boisvert et al. ( 2019), the reward function returns a basic reward minus the movement effort R = 1 \u2212 0.9 * (T /T max ). We extend this formulation and give an additional bonus of +1 if the correct piece has been taken or a penalty of \u22121 when the wrong or no piece has been taken at all.\n\nEnvironment\nThe environment exposes at each time-step t an observation o t that contains the gripper coordinates g t = (x, y), the initial referring expression l RE , the language feedback l FBt (which might be empty) and a partial view v t of the scene. While the scene as a whole is represented as a 2-dimensional image (with RGB colour channel), the partial view represents a 11 \u00d7 11-sized cut out, centered on the gripper position (see Figure 2 ). The teacher generates the initial and feedback statements.\n\nTeacher\nFor the teacher, we assume a heuristic behaviour (a fix policy) that has been shown to lead to collaborative success with humans (G\u00f6tze et al., 2022) and leave the complexity of learning in a multiagent setting (Gronauer and Diepold, 2022) for future work. The teacher produces an initial referring expression l RE = (w 0 , ..., w N ) where N is the message length and w i is a word in the vocabulary. The production rule is implemented following the Incremental Algorithm (IA) (Dale and Reiter, 1995) that is given the symbolic representations of the pieces on the board (see Appendix A.1). The teacher provides a feedback message l FBt = (w 0 , ..., w N ) at a time-step t >0 when the gripper's position g t has exceeded a pre-defined distance threshold D dist = 3 compared to the gripper's last position of feedback g FB last or it is over a piece. The generated feedback is of positive sentiment (\"Yes this way/piece\") when the gripper is then closer to or over the target piece and negative otherwise (\"Not this direction/piece\"). Alternatively, suppose the follower does not exceed the distance threshold after D time = 6 time-steps the feedback message is the same as the initial statement. Overall, the property values and sentence templates lead to a small vocabulary of 33 words.\n\nFollower\nThe follower agent has to move the gripper and successfully grip a piece solely based on the observations. The observations o t = (v t , g t , l RE , l FBt ) are mapped to 128-dimensional features xt \u2208 R using the encoder model (see Figure 2 ). Following Chevalier-Boisvert et al. ( 2019), the word embeddings (which are learned from scratch) of the language inputs are fed through a Gated Recurrent Unit (GRU) (Cho et al., 2014) and then combined with the embedded visual features using a Featurewise Linear Modulation (FiLM) layer (Perez et al., 2018) . These language conditioned visual features are then max pooled, averaged and again averaged with the gripper position. Given the resulting features xt , we learn a parameterised policy \u03c0(x t ; \u03b8) \u223c a t that predicts a distribution over the action space. We use the Proximal Policy Optimization (PPO) (Schulman et al., 2017) implementation of StableBaselines3 v1.6.2 (Raffin et al., 2021) to train the policy in our environment.\n\nTasks\nThe follower has to grip an intended target piece among several other pieces (the distractors). Thus a task is defined by the number of pieces, the target piece and the map size. The pieces for the tasks are instantiated from symbolic representations: a tuple of shape ( 9), color (6) and position (8) which leads to 432 possible piece symbols. For our experiments we use all of these symbols as targets, but split them into distinct sets (Appendidx A.4). Therefore the targets for testing tasks are distinct from the ones in the training tasks. We ensure the reproducibility of our experiments by constructing 3300 training, 300 validation, 720 testing tasks representing scenes with a map size of 20 \u00d7 20 and 4 or 8 pieces.\n\nExperiments\nIn this section we explore the effects of the teacher's language and intra-episodic feedback on the follower's success and ask whether the follower generalizes on aspects of scene complexity.\n4.1 Which referential language is most beneficial for the agent's learning success?\nAs suggested by Madureira and Schlangen (2020) we explore the question of which language is most effective. The IA constructs the initial reference by following a preference order over object properties (Krahmer et al., 2012) . We hypothesize that a particular order might be more or less suitable depending on the task. Thus we conduct a series of experiments without the feedback signal where the preference order is varied as the permutation of color, shape and position. Our results indicate that such orders perform better that prioritize to mention positional attributes as distinguishing factors of the target piece (see Table 1 ). This is reasonable as the directional hint reduces the agent's burden for broader exploration. The follower is able to pick up early on these positional clues and performs overall better during training (see Figure 3 ).\n\n4.\n2 What is the agent's performance gain with intra-episodic feedback in our setting?\nWe conduct the same experiments as above with intra-episodic language feedback to measure its effect on the follower's success rate. Our results show that the follower achieves higher success rates with intra-episodic feedback among all preference orders (see Table 1 ). We also notice that the gain is higher for the low-performing preference orders. This shows that the intra-episodic feedback is a valuable signal for the follower to overcome miss-ing directives in the initial referring expressions.\nThe agent can learn strategies incorporating the feedback signals. This is an interesting finding because language feedback is not part of the reward function and could be empty.\n\nDoes intra-episodic feedback help the agent to generalize on scene complexity?\nAs a proxy for generalization capabilities, we take the best performing follower and raise the complexity of the testing scenes along two dimensions (i) we increase the map size to 30 \u00d7 30 and (ii) put up to 18 pieces on the board. In addition, we hold out 72 combinations of piece shapes and colors that have never been seen during training. Our results show that the agent trained with intra-episodic feedback is able to perform better (i) on the larger map size, (ii) the higher number of pieces and (iii) the new target pieces compared to the one without (see Table 2 ).\n\nConclusion\nIn this work, we studied the effects of a teacher's language and intermediate interventions (the feedback) towards a learner's success and whether the learner generalizes on aspects of scene complexity. Our results show that there is a most beneficial language for the teacher. Its intra-episodic feedback allows the learner to learn faster and generalize better than without intermediate help. An exciting direction for further work is to show the benefits of language feedback for other reinforcement learning problems, to overcome the limits of the heuristic teacher strategy and to reduce the need for feedback after successful training.\n", "hypothesis": "Our results show that intra-episodic feedback allows the follower to generalize on aspects of scene complexity and performs worse than providing only the initial statement.", "answer": false}
{"title": "The Art of Prompting: Event Detection based on Type Specific Prompts", "content": "\n\n2019), or a few prototype event triggers (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014; Bronstein et al., 2015; Lai and Nguyen, 2019; Zhang et al., 2021b; Cong et al., 2021) . These studies further encourage us to take another step forward and think about the following three questions:\n(1) does the choice of prompt matter when the training data is abundant or scarce? (2) what's the best form of ED prompt? (3) how to best leverage the prompt to detect event mentions?\nTo answer the above research questions, we conduct extensive experiments with various forms of prompts for each event type, including (a) event type name, (b) prototype seed triggers, (c) definition, (d) event type structure based on both event type name and its predefined argument roles, (e) free parameter based continuous soft prompt, and (f) a more comprehensive event type description (named APEX prompt) that covers all the information of prompts (a)-(d). We observe that (1) by considering the semantics of event types with most forms of prompts, especially seed triggers and the comprehensive event type descriptions, the performance of ED under all settings can be significantly improved; (2) Among all forms of event representations, the comprehensive description based prompts show to be the most effective, especially for fewshot and zero-shot ED; (3) Different forms of event type representations provide complementary improvements, indicating that they capture distinct aspects and knowledge of the event types.\nThe contributions of this work are as follows:\n\u2022 We investigate various prompts to represent event types for both supervised and weakly supervised ED, and prove that a well-defined and comprehensive event type prompt can dramatically improve the performance of ED and the transferability from old types to new types.\n\u2022 A unified framework is developed to leverage the semantics of event types with prompts for supervised, few-shot, and zero-shot ED, and demonstrate state-of-the-art performance with up to 22.2% Fscore improvement over the strong baseline methods.\n\nRelated Work\nSupervised ED: Most of the existing Event Detection studies follow a supervised learning paradigm (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Cao et al., 2015; Feng et al., 2016; Yang and Mitchell, 2016; Nguyen et al., 2016; Zhang et al., 2017; Lin et al., 2020; Wang et al., 2021b) . However, they cannot be directly applied to detect new types of events. Recently studies have shown that, by leveraging the semantics of event types based on type-specific questions (Du and Cardie, 2020; Liu et al., 2020; Li et al., 2020; Lyu et al., 2021) or seed event triggers (Bronstein et al., 2015; Lai and Nguyen, 2019; Wang et al., 2021a) , the event detection performance can be improved. However, it is still unknown whether they are the best choices for representing the semantics of event types.\nFew-shot ED: Two primary learning strategies in few-shot classification tasks are Meta-Learning (Kang et al., 2019; Li et al., 2021; Xiao and Marlet, 2020; Yan et al., 2019; Chowdhury et al., 2021) and Metric Learning (Sun et al., 2021; Wang et al., 2020b; Zhang et al., 2021a; Agarwal et al., 2021) . Several studies have exploited metric learning to align the semantics of candidate events with a few examples of the novel event types for few-shot event detection (Lai et al., 2020a; Deng et al., 2020; Lai et al., 2020b; Cong et al., 2021; Chen et al., 2021; Shen et al., 2021) .\nZero-shot ED: Huang et al. (2018) first exploited zero-shot event extraction by leveraging Abstract Meaning Representation (Banarescu et al., 2013) to represent event mentions and types into a shared semantic space. Recent studies (Zhang et al., 2021b; Lyu et al., 2021) further demonstrate that by leveraging a large external corpus with abundant anchor triggers, zero-shot event detection can also be achieved with decent performance without using any training data.\nPrompt Learning Prompt learning aims to learn a task-specific prompt while keeping most of the model's parameters frozen (Li and Liang, 2021; Hambardzumyan et al., 2021; Brown et al., 2020) .\nIt has shown competitive performance in many applications of natural language processing (Raffel et al., 2020; Brown et al., 2020; Shin et al., 2020; Jiang et al., 2020; Lester et al., 2021; Schick and Sch\u00fctze, 2021b) . Previous work either used a manual (Petroni et al., 2019; Brown et al., 2020; Schick and Sch\u00fctze, 2021a) or automated approach (Jiang et al., 2020; Yuan et al., 2021; Li and Liang, 2021) to create prompts.\n\nProblem Formulation\nHere, we first define each setting of the event detection task and then describe the various forms of event type prompts.\n\nSettings of ED\nFor supervised ED (SED), we follow the conventional supervised event detection setting where the training, validation, and evaluation data sets cover the same set of event types. The goal is to learn a model f to identify and classify event mentions for the target event types.\nFor few-shot ED (FSED), there are two separate training data sets for few-shot event detection:\n(1) A large-scale data set D base = {(x i , y i )} M i=1 that covers the old event types (named base types) where M denotes the number of base event types;\n(2) a smaller data set D novel = {(x j , y j )} N \u00d7K j=1 that covers N novel event types, with K examples each. Note that the base and novel event types are disjoint except for the Other class. The model f will be first optimized on D base , and then further fine-tuned on D novel . The goal is to evaluate the generalizability and transferability of the model from base event types to new event types with few annotations.\nFor zero-shot ED (ZSED), the training data sets are the only difference between zero-shot and fewshot event detection. In zero-shot event detection, there is only a large-scale base training data set\nD base = {(x i , y i )} M\ni=1 for the base event types. The model f will be only optimized on base event types and evaluated on the novel types.\n\nEvent Type Prompts\nWe compare the following five forms of prompts to represent the event types: (a) Event Type Name is the event class name, usually consisting of one to three tokens. (b) Definition can be a short sentence that formally describes the meaning of the event types. (c) Prototype Seed Triggers a list of \n\nA Unified Framework for ED\nWe adapt (Wang et al., 2021a) and design a unified event detection framework (as shown in Figure 1 ) which leverages event type specific prompts to detect events under supervised, few-shot, and zeroshot settings. Formally, given an input sentence W = {w 1 , w 2 , . . . , w n }, we take each event type prompt T t = {\u03c4 t 1 , \u03c4 t 2 , . . . , \u03c4 t m } as a query of M tokens to extract triggers for event type t. Specifically, we first concatenate them into a sequence\n[CLS] \u03c4 t 1 ... \u03c4 t m [SEP] w 1 ... w n [SEP]\n. We use a pre-trained BERT encoder (Devlin et al., 2019) to get contextual representations for the input sentence W = {w 0 , w 2 , ..., w n } as well as the event type prompt T = {\u03c4 t 0 , \u03c4 t 1 , ..., \u03c4 t m } 2 . Given a prompt of each event type, we aim to extract corresponding event triggers from the input sentence. To achieve this goal, we need to capture the semantic correlation of each input token to the event type Thus we learn a weight distribution over the sequence of contextual representations of the event type prompt, to obtain event type t aware contextual representation\nA t i = |T t | j=1 \u03b1 ij \u2022 \u03c4 t j , where \u03b1 ij = cos(w i , \u03c4 t j )\n, where \u03c4 j is the contextual representation of the j-th prompt token. cos(\u2022) is the cosine similarity function between two vectors. With that, the event type aware contextual representation A t i will be concatenated with the original contextual representation w i from the encoder, and classified into a binary label, indicating whether it is a candidate trigger of event type t or not:\n\u1ef9t i = U o ([w i ; A t i ; P i ])\n, where [; ] denotes concatenation operation, U o is a learnable parameter matrix for event trigger detection, and P i is the one-hot part-of-speech (POS) encoding of word w i . For continuous soft prompt based event detection, we follow Li and Liang (2021) where a prefix index q is prepended to the input sequence W \u2032 = [q; W ]. The prefix embedding is learned by q = MLP \u03b8 (Q \u03b8 [q]), where Q \u03b8 \u2208 R |Q|\u00d7k denotes the embedding lookup table for the vocabulary of prefix indices. Both MLP \u03b8 and Q \u03b8 are trainable parameters. Detailed learning strategy is in Appendix C.\n\nExperiment Setup\nWe perform experiments on three public benchmark datasets, including ACE05-E + (Automatic Content Extraction), ERE (Entity Relation Event) (Song et al., 2015) ,and MAVEN (Wang et al., 2020a) . On each dataset, we conduct experiments for SED, FSED, and ZSED. For SED, we use the same data split as the previous studies (Li et al., 2013; Wadden et al., 2019; Lin et al., 2020; Du and Cardie, 2020; Lin et al., 2020; Nguyen et al., 2021; Wang et al., 2020a ) on all the three benchmark datasets. For FSED and ZSED on MAVEN, we follow the previous study (Chen et al., 2021) and choose 120 event types with the most frequent mentions as the base event types and the rest 45 event types as novel ones. For FSED and ZSED on ACE and ERE, previous studies (Lai et al., 2020b, Zero-shot Event Detection The proposed prompt-based method is more affordable to be generalized compared with the prior state-ofthe-art zero-shot approach (Zhang et al., 2021b) .\nThe average length of created APEX prompts is less than 20 tokens. Thus manually creating them will not take much human effort. On the contrary, Zhang et al. (2021b) requires an extensive collection of anchor sentences to perform zero-shot event detection, e.g., 4,556,237 anchor sentences for ACE and ERE. This process is time-consuming and expensive.\n\nConclusion\nWe investigate a variety of prompts to represent the semantics of event types, and leverage them with a unified framework for supervised, few-shot and zero-shot event detection. Experimental results demonstrate that, a well-defined and comprehensive description of event types can significantly improve the performance of event detection, especially when the annotations are limited (few-shot event detection) or even not available (zero-shot event detection), with up to 22.2% F-score gain over the prior state of the art.\n", "hypothesis": " The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (fewshot event detection) or not available (zero-shot event detection).  By leveraging the semantics of event types, our unified framework shows up to 22.2% F-score gain over the previous stateof-the-art baselines 1 ..", "answer": true}
{"title": "Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions", "content": "\nIntroduction\nWriting persuasive texts plays a major role in law education (Kosse and Butle Ritchie, 2003) . As a part of their training for learning how to write legal opinions, law students are typically challenged to solve legal problems or case studies in the form of persuasive case solutions (Enqvist-Jensen et al., 2017) . To write a persuasive legal case solution, students in German law courses must be able to follow the structural requirements of the appraisal style (see Figure 1 ) (Stuckenberg, 2020) and justify their derived conclusions argumentatively via legal claims and premises (see Section 2). To learn a skill such as writing a persuasive case solution, individual feedback is important to the learning process (Hattie and Timperley, 2007; Black and Wiliam, 2009) . Individualized feedback for students during their writing or learning processes is lacking, particularly in the field of law. The characteris-tic large-scale learning scenarios in legal studies, which result in a low supervision ratio, are part of the reason for this. Organizational restrictions are another cause of the absence of personal feedback. For instance, there aren't enough lecturers who can assess students' case solutions (Henderson, 2003) . At the same time, technical solutions that could help students improve their legal writing fall short of expectations (Beurskens, 2016) . One promising solution to better support students in their writing process and to overcome the limitations in law courses would be the use of writing support systems that could provide individualized feedback to students (Wambsganss et al., 2020a) . To model legal persuasive writing with predictive algorithms, high-quality annotated corpora are needed. Pioneering work in argumentation mining has already focused on jurisprudence (Mochales and Moens, 2008; Mochales and Ieven, 2009) , since the structural approach of legal writing facilitates the unambiguous determination of argumentation components (Lytos et al., 2019; Urchs et al., 2020) . Existing corpora in law range from classification of judgments (Urchs et al., 2020) , to summarization of legal texts (Hachey and Grover, 2005) and to evaluation of jury verdicts (Poudyal et al., 2019) . Corpora dealing with the annotation of structural elements in student written legal texts are not available. A few corpora are suitable for designing and developing systems to support persuasive writing (Stab and Gurevych, 2017b; Wambsganss et al., 2020b; Lawrence and Reed, 2019; Stab and Gurevych, 2014) . However, these corpora are of limited use for modeling the structure of writing and argumentation in law, since persuasive writing in the legal domain follows a particular logic (see Section 2) that is not represented by available corpora. Consequently, there is a lack of evaluated annotation schemes and linguistic corpora for training models that support users in legal writing.\n\nSubsumption\nTherefore, we propose a novel annotation scheme for persuasive student-written case solutions. We introduce a corpus of 413 student-written case solutions with 25,103 sentences that are annotated for the components of the appraisal style, arguments (legal claim and premises), the relations of the arguments, and the relations of distinct components of the appraisal style. We trained different types of models (e.g. BERT and DistilBERT) and compared their accuracy to analyze which model performs best. Finally, we embedded the three best performing transformer-based BERT models in a novel writing support system that provides individual feedback and recommendations in a writing scenario. The design of our writing support system is based on the theory of learning from errors (Metcalfe, 2017) and aims to provide students with individual feedback on their errors during the writing process (Fazio and Marsh, 2009) . We tested the systems in an online learning scenario with law students. The students were asked to use the system to write a case solution. We show promising results in terms of the students' understanding of the appraisal style and their perception of the system. The participants perceive the system as useful and rate the system's feedback as accurate. Our analyzed results support that our presented corpus and the models are able to support students' learning effectively.\nOur work makes five major contributions. First, we derive a novel modeling approach for a new data domain by developing an annotation scheme based on the theory of structured legal writing based on the appraisal style (Man, 2022; Stuckenberg, 2020) . Second, we present an annotation study based on 100 student case solutions to show that annotation of student case solutions is accurately possible. Based on the annotation, we trained three transformer-based BERT models (Devlin et al., 2019) to demonstrate that the prediction of the annotated structures is possible with a certain accuracy. Fourth, we provide a corpus of 413 student case solutions in German collected in different law lectures. Finally, we show in an online experiment that the models can be used effectively in a writing support system. Therefore, we encourage further investigation into the enhancement of law students' persuasive structured writing and the development of writing support systems using NLP. This research aims to enhance students' skills regardless of their location, time constraints, or instructor availability.\n\nRelated Work\nPersuasive Writing in Law Courses Classically, students are asked to solve legal problems or case studies in the form of persuasive case solutions (Enqvist-Jensen et al., 2017) . In these case solutions, students are forced to use specialized and highly concept-driven knowledge. The theoretical knowledge specializes more in the correct application of paragraphs and the setting of priorities in the case solution. In contrast, the concept-driven knowledge is largely composed of the concepts of writing case solutions in a structured way. To do this, students must follow established legal concepts. Among the most important concepts in German jurisprudence are the appraisal style and the judgment style, whereby the appraisal style is primarily important for legal education (Stuckenberg, 2020; Urchs et al., 2020) . Since the term \"appraisal style\" is a peculiarity of the German legal language, there is no direct equivalent in English. We define the term appraisal style as \"the form and writing style of a legal opinion\" (Stuckenberg, 2020) . The appraisal style is used to solve complex legal problems. The four elements of appraisal style are briefly explained in Table 1 and supplemented by an example in Figure 2 .\nCorpora in the Legal Field Although law is a promising discipline for annotating the components of legal writing and arguments due to its fixed logical structure (Moens et al., 2007; Urchs et al., 2020) , evaluated open-access corpora for law are rare (Reed, 2006; Mochales and Moens, 2011; Urchs et al., 2020) . There are, however, some publicly accessible corpora. Hachey and Grover (2005) present a corpus of 188 annotated English court opinions. To construct a system for automatic summarizing of court judgments, they annotated rhetorical status, significance, and linguistic markup. Other annotated corpora deal explicitly with the annotation of argumentation structures in court decisions (Houy et al., 2013) or legal cases (Mochales-Palau and Moens, 2007) . Mochales-Palau and Moens (2007) present a corpus of English-language judicial cases gathered from the European Court of Human Rights (ECHR). They chose 55 papers at random, which included 25 court decisions and 29 admissibility reports. The texts were annotated and studied systematically in two layers (argumentative and non-argumentative sentences). A following study showed that the detection of argumentative sentences in court decisions is possible. Work such as that of Walker et al. (2014) has focused on identifying successful and failed patterns of reasoning in U.S. Court decisions. Patterns of reasoning are identified and used to illustrate the difficulty of developing a type or annotation system for characterizing these patterns. The corpus is based on legal cases of vaccine-injury compensations. There are several German corpora in addition to the largely English-language corpora for recognizing decisions and legal cases. Urchs et al. (2020) created a corpus based on Bavarian Court of Justice decisions. They discover argumentation structures in judgments using 200 court decisions. Other research groups focused on the identification of arguments in the German Federal Constitutional Court's Decision Corpus (Houy et al., 2013) and the development of a German referent corpus comprised of articles from legal journals, decision texts, and norm texts (Gauer et al., 2016) .\nA number of corpora have previously been proposed in research to enhance students' structured and persuasive writing in real-world applications, including Stab and Gurevych (2017a) and Stab and Gurevych (2014) . Stab and Gurevych (2014) produced a corpus based on student essays for building and implementing systems to promote persuasive writing for adaptive feedback using argumentation mining (AM) approaches. Further research uses the corpus as a model to annotate persuasive writings (Carlile et al., 2018) or construct a model for assessing persuasive essays (Ke et al., 2018) . However, the existing literature does not adequately transfer corpora for structured writing or reasoning to other educational domains, like law or to other languages.\nTo summarize, we see that literature falls short of annotated corpora, which can be used to model components in student-written legal case solutions. Without the availability of these corpora, the design of adaptive NLP-based applications for lawful writing is naturally hindered. To the best of our knowledge, there is only one approach by Urchs et al. (2020) that aims to detect the components of legal writing, but the approach focuses on court decisions and the judgment style. Therefore, we aim to address this literature gap by presenting and evaluating an annotation scheme as well as an annotated corpus built on student-written texts with the objective of developing an intelligent writing support system for students in law courses.\n\nData Source\nThe data for our corpus were collected in a law courses at a German university. We compiled the corpus with the case solutions of law students who have written solutions to different legal problems (four different case studies) from different areas of law. In total, we collected 413 legal case solutions, with a typical length of 55.07 sentences and 331.35 tokens per document. 1 The case studies are mainly based on example cases from civil law and are oriented towards basic cases of Musielak and Hau (2005) . Students solved the cases as a component of a comprehensive law lecture, utilizing them as a means of exam preparation. It is important to note that the quality of the 413 student-written case solutions may vary, as the students are not all at the same level of proficiency or understanding. The data were collected in the mentioned lecture between 2020 and 2022. The course deals with the teaching of the basics of legal writing and the funda-mental knowledge of business law were introduced. Accordingly, the course has dealt with essential basics that are also important for non-law students, such as business students. The data collected are thus relevant not only in the context of foundational legal education but also for many other Germanlanguage legal studies programs (e.g., law courses in the education of business students).\n\nAnnotation Scheme\nThe correct application of structured legal writing in the appraisal style is the basis for a persuasive legal opinion. In the following, the components of the legal writing structure, as well as its annotation, are explained. The structure consists of four components: major claim, definition, subsumption (premise and legal claim), and conclusion (Sieckmann, 2020; Backer, 2009) (see Table 1 ).\n\nComponents Definition Major claim\nThe major claim explains the elements of the offense (fact) that are to be fulfilled. It raises a question or possible consequence. The question is discussed in the following steps and is finally answered in the conclusion.\n\nDefinition\nThe definition determines the constituent elements that must occur in the legal problem so that the case solution can come to a conclusion. The elements always depend on the question raised in the major claim. Subsumption (premise and legal claim):\nIn the subsumption, it is examined to what extent the conditions (elements) of the definition are given.\nHere, the facts of the case are weighed against the preconditions from the definitions and the premises (facts). Legal consequences are drawn from the premises, so-called legal claims.\n\nConclusion\nThe conclusion is the answer to the major claim. Thus, the case solution reaches a final result here.\nTable 1 : Core components of the legal writing structure in the appraisal style according to our guidelines.\n", "hypothesis": "We evaluated a writing support system in which our models were integrated in an offline experiment with law students and found positive learning success and users' perceptions.", "answer": false}
{"title": "Typo-Robust Representation Learning for Dense Retrieval", "content": "\nIntroduction\nDense retrieval is a fundamental component in many information retrieval applications, such as open-domain question answering and ad-hoc retrieval. The objective is to score and rank a large collection of candidate passages based on their similarity to a given query. The performance of dense retrieval relies on representation learning. A popular approach is to finetune a pre-trained language model to create an embedding space that puts each query closer to its corresponding passages (Zhan et al., 2020; Khattab and Zaharia, 2020; Xiong et al., 2021; Qu et al., 2021; Ren et al., 2021a,b) .\nOne of the major challenges of dense retrieval is the handling of misspelled queries which induces representations of the misspelled queries to be closer to irrelevant passages than their corresponding passages. Several studies have demonstrated that misspellings in search queries can substantially degrade retrieval performance (Zhuang and Zuccon, 2021; Penha et al., 2022) , specifically when informative terms, such as entity mentions, are misspelled (Sidiropoulos and Kanoulas, 2022) .\nTo create a retrieval model that is capable of handling misspelled queries, researchers have proposed different training methods to align representations of misspelled queries with their pristine ones. Zhuang and Zuccon (2021, 2022) devise augmentation methods to generate misspelled queries and propose training methods, Typos-aware Training and Self-Teaching (ST), to encourage consistency between outputs of misspelled queries and their non-misspelled counterparts. Alternatively, Sidiropoulos and Kanoulas (2022) apply contrastive loss to enforce representations of misspelled queries to be closer to their corresponding non-misspelled queries. Although these methods can improve the performance of retrieval models for misspelled queries, there is still a substantial performance drop for misspelled queries.\nIn this paper, we propose a training method to improve dense retrieval for handling misspelled queries based on the following desired properties:\n\u2022 Alignment: the method should be able to align queries with their corresponding passages. \u2022 Robustness: the method should be able to align misspelled queries with their pristine queries. \u2022 Contrast: the method should be able to separate queries that refer to different passages and passages that correspond to different queries. In contrast to the existing methods for handling misspelled queries that only satisfy the Alignment and Robustness properties, our method also aims to satisfy the Contrast property. Increasing the distance between dissimilar queries should help distinguish misspelled queries from other distinct queries. We design the following components for our training method: (i) Dual Self-Teaching (DST) incorporates the ideas of Dual Learning (Xia et al., 2017; Li et al., 2021) and Self-Teaching (Zhuang and Zuccon, 2022) to train robust dense retrieval in a bidirectional manner: passage retrieval and query retrieval. (ii) Query Augmentation generates a numerous number of misspelling variations for each query to supply our training objective.\nExperimental studies were conducted to assess the efficiency of the proposed method in comparison to existing approaches. We conduct experiments based on two different pre-trained language models. We evaluate using two passage retrieval benchmark datasets, a standard one and a specialized one for misspellings robustness evaluation. For each dataset, we measure performance on both misspelled and non-misspelled queries, where the misspelled queries are both generated and realworld queries. The experimental results show that the proposed method outperforms the best existing methods for enhancing the robustness of dense retrieval against misspellings without sacrificing performance for non-misspelled queries.\nWe summarize our contributions as follows: \u2022 We propose a novel training method to enhance the robustness of dense retrieval against misspellings by incorporating three desired properties: Alignment, Robustness, and Contrast. \u2022 We introduce Dual Self-Teaching (DST) which adopts the idea of Dual Learning and Self-Teaching to learn robust representations. In addition, we propose Query Augmentation to generate multiple views of a particular query under different misspelling scenarios. \u2022 We evaluate our method on misspelled and nonmisspelled queries from two passage retrieval datasets. The results show that our method outperforms the previous state-of-the-art methods by a significant margin on misspelled queries.\n\nMethodology\nWe propose a training pipeline to enhance the dense retrieval capability for handling spelling variations and mistakes in queries. As shown in Figure 1 , the training pipeline comprises three steps. (i) Query Augmentation: we augment each query in the training set into multiple misspelled queries using the typo generators provided by Zhuang and Zuccon (2021) . (ii) Similarity Score Calculation: we compute similarity score distributions between queries and passages for passage retrieval and query retrieval tasks using in-batch negative queries and passages, with additional hard negative passages.\n(iii) Dual Self-Teaching Loss Calculation: we compute the DST loss using the similarity score distributions to achieve all three desired properties.\n\nQuery Augmentation\nThe purpose of this step is to guide the learning with a broad array of possible misspelling patterns. Let Q denote a set {q 1 , q 2 , ..., q N } of N queries. From all queries in Q, we generate a set of K \u00d7 N misspelled queries\nQ \u2032 = {\u27e8q \u2032 1,k , q \u2032 2,k , ..., q \u2032 N,k \u27e9} K k=1\n, where K is the misspelling variations. We use five typo generators proposed by Zhuang and Zuccon (2021) , including: RandInsert, RandDelete, RandSub, SwapNeighbor, and SwapAdjacent. Please refer to Appendix A.2 for examples of the misspelled queries.\n\nSimilarity Score Calculation\nLet S(a, B) denote a function that computes a similarity score distribution of any vector a over any set of vectors B:\nEQUATION\nGiven P = {p 1 , p 2 , ..., p M } to be a set of M passages and\nQ \u2032 k = {q \u2032 1,k , q \u2032 2,k , ..., q \u2032 N,k } to be the k th set of misspelled queries in Q \u2032 ,\nwe compute two groups of score distributions as follows:\n\u2022 Passage retrieval: we calculate score distributions in a query-to-passages direction for each original query s p = S(q n , P) and misspelled query s \u2032k p = S(q \u2032 n,k , P). \u2022 Query retrieval: we calculate score distributions in a passage-to-queries direction for original queries s q = S(p m , Q) and each set of misspelled queries s \u2032k q = S(p m , Q \u2032 k ). This way, we produce four different score distributions (s p , s \u2032k p , s q , s \u2032k q ) for our training objective.\n\nDual Self-Teaching Loss Calculation\nWe design the Dual Self-Teaching loss (L DST ) to capture the three desired properties: Alignment, Robustness, and Contrast.\nL DST = (1 \u2212 \u03b2)L DCE Dual Cross-Entropy + \u03b2L DKL Dual KL-Divergence (2)\nDual Cross-Entropy loss (L DCE ) satisfies the Alignment and Contrast properties by utilizing cross-entropy losses to learn score distributions of the original queries for passage retrieval (s p ) and query retrieval (s q ) given labels y p and y q . Minimizing the L (P ) CE term will increase the similarity scores between queries and their relevant passages to be higher than other irrelevant passages by separating the relevant and irrelevant passages from one another. Minimizing the L (Q) CE term will increase the similarity scores between passages and their relevant queries to be higher than other irrelevant queries by separating the relevant and irrelevant queries from one another. In this manner, minimizing one of the two terms will align queries with their corresponding passages, satisfying the Alignment property. Moreover, minimizing both terms will separate queries that refer to different passages and passages that belong to different queries, satisfying the Contrast property.\nL DCE = (1 \u2212 \u03b3)L (P ) CE (s p , y p ) Passage Retrieval + \u03b3L (Q) CE (s q , y q ) Query Retrieval (3)\nDual KL-Divergence loss (L DKL ) aims to fulfill the Robustness property by using KL losses to match score distributions of misspelled queries {s \u20321 p , s \u20322 p , ..., s \u2032K p } and {s \u20321 q , s \u20322 q , ..., s \u2032K q } to the score distributions of the original query s p and s q .\nL DKL = 1 K K k=1 (1 \u2212 \u03c3)L (P ) KL (s \u2032k p , s p ) Passage Retrieval Consistency + \u03c3L (Q) KL (s \u2032k q , s q ) Query Retrieval Consistency (4) Minimizing L (P )\nKL and L (Q)\nKL will reduce the discrepancy between misspelled and non-misspelled queries for both query-to-passages and passage-toqueries score distributions. This way, we implicitly align representations of the misspelled queries to the original queries, satisfying the Robustness property. To stabilize training, we apply stop-gradient to the score distributions of the original queries (s p and s q ) in the L DKL . The \u03b2, \u03b3, and \u03c3 are the balancing coefficients selected by hyper-parameter tuning on a development set. With this loss combination, we achieve all three desired properties.\n3 Experimental Settings\n\nTraining Details\nWe experiment on two pre-trained language models, BERT (Devlin et al., 2019) and Character-BERT (El Boukkouri et al., 2020) . We train models only on the training set of MS MARCO dataset (Nguyen et al., 2016) . Moreover, the training data provided by the Tevatron toolkit (Gao et al., 2022 ) also contains hard negative passages. We include the training set details and hyper-parameter settings in Appendix A.1.\n\nCompetitive Methods\nTo show the effectiveness of our method, we compare our work with the following baseline and competitive training methods.\n\u2022 DPR (Karpukhin et al., 2020) KL losses. Note that their query augmentation method is identical to the Query Augmentation with K = 1. We retrain all models using the same setting described in the previous section. We report the results in the format of \"misspelled query performance (non-misspelled query performance)\".\nWe emphasize the best score with bold text and the second-best score with underlined text. We use \u2020 to denote DST results that significantly outperform the second-best result (p < 0.05).\n\nDataset and Evaluation\nDatasets. We evaluate the effectiveness of DST on two passage retrieval datasets, MS MARCO and DL-typo (Zhuang and Zuccon, 2022) , each with misspelled and non-misspelled queries. There are 8.8 million candidate passages for both datasets.\nThe development set of MS MARCO contains 6,980 non-misspelled queries. To obtain misspelled queries, we use the typos generator method proposed by Zhuang and Zuccon (2021) to generate 10 misspelled variations for each original query. The DL-typo provides 60 real-world misspelled queries and 60 corresponding non-misspelled queries that are corrected manually.\nEvaluation. We use the standard metrics originally used by each dataset's creators. For MS MARCO, each misspelled query performance is the average of 10 measurements. We employ Ranx evaluation library (Bassani, 2022) to measure performance and statistical significance. Specifically, we use a two-tailed paired t-test with Bonferroni correction to measure the statistical significance (p < 0.05).\n\nMain Results\nAs shown in Table 1 , the results indicate that DST outperforms competitive methods for misspelled queries in every case without sacrificing performance for non-misspelled queries in eight out of ten cases. We observe some performance trade-offs for the BERT-based model in the DL-typo dataset's non-misspelling scores (nDCG@10 and MRR). Aside from that, there is no performance trade-off for the CharacterBERT-based model. These outcomes conform with the observation in Figure 2 (Section 4.4) that DST improves the Robustness and Contrast of misspelled queries.\n\nQuery Augmentation Size Study\nTo study the benefit of query augmentation and find the optimal augmentation size, we measure the performance of BERT-based dense retrieval models trained with DST using the query augmentation size K of 1, 10, 20, 40, and 60. Note that the query augmentation method used in previous works is a special case of Query Augmentation when K = 1. We report the results using MRR@10 for the development set of the MS MARCO dataset. We also report training time to show trade-offs between performance and computation. Table 2 : Results of query augmentation size study. We train all models in this experiment on a V100 32G GPU.\nAs shown in Table 2 , the results indicate that increasing K improves the performance of both misspelled and non-misspelled queries, but only up to a certain point, after which the performance begins to decline. We observe that setting K = 40 produces the best results, and there is no further performance improvement after this point.\n\nLoss Ablation Study\nIn this experiment, we study the benefit of each term in DST by training BERT-based dense retrieval models on variant loss combinations with K = 40. The results in Table 3 reveal that L \n\nQuery Distributions\nThe purpose of this section is to study the impact of our training method on the Robustness and Contrast of misspelled queries. We also compare our method against the baseline and competitive methods to show its effectiveness. The Robustness and Contrast of misspelled queries are illustrated using the following kernel density graphs: \u2022 Original-to-Misspell: the cosine similarity distribution between original and misspelled queries. \u2022 Original-to-Neighbor: the cosine similarity distribution between original and neighbor queries. The Robustness property is emphasized by the Original-to-Misspell distribution having high cosine similarity. On the other hand, the Contrast property is emphasized by the small overlapping between Original-to-Misspell and Originalto-Neighbor distributions. The results in Figure 2 show that our method (c) produces the best Robustness and Contrast properties for misspelled queries in comparison to other methods.\n\nConclusion\nThis paper aims to address the misspelling problem in dense retrieval. We formulate three desired properties for making dense retrieval robust to misspellings: Alignment, Robustness, and Contrast. Unlike previous methods, which only focus on the Alignment and Robustness properties, our method considers all the desired properties. The empirical results show that our method performs best against misspelled queries, revealing the importance of the Contrast property for handling misspellings. \n", "hypothesis": " To assess the effectiveness of our proposed method, we compare it against the existing competitors using two benchmark datasets and two base encoders.  Our method outperforms the competitors in all cases with misspelled queries.", "answer": true}
{"title": "STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions", "content": "\nIntroduction\nWe present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The corpus represents all Swiss German dialect regions and contains 343 hours of speech.\nSwiss German is a family of German dialects spoken by around 5 million people in Switzerland. It differs from Standard German regarding phonology, vocabulary, morphology, and syntax. There are significant differences among the Swiss German dialects as well, particularly regarding phonology and vocabulary. Swiss German is primarily a spoken language. It is also used in writing, but mainly in informal text messages. In most other contexts, including formal letters, laws, and newspapers, Standard German is used instead. One important reason for this is Swiss German's lack of a standardized orthography.\nThe diversity among dialects, exacerbated by the lack of a standardized orthography, leads to a large number of written variants for each word. This, together with the small amount of text resources compared to Standard German, makes automated processing of Swiss German text challenging.\nSTT4SG-350 is, to the best of our knowledge, the largest public speech corpus for Swiss German. While the primary use case is automatic speech recognition (ASR), it is also a useful resource for text-to-speech (TTS), dialect identification, and speaker recognition. By providing roughly the same amount of data per dialect region, irrespective of its population size, the corpus contributes to improving speech technology for underrepresented dialects. In addition, the test set, which contains the same spoken sentences in each dialect, allows a fair evaluation of the quality of speech technologies in different dialects. Furthermore, it contributes to more inclusive speech technology by keeping a balanced gender ratio and featuring speakers of all ages.\n\nRelated Work\nThe SDS-200 corpus (Pl\u00fcss et al., 2022) contains 200 hours of speech by around 4,000 speakers with Standard German transcripts. The recordings cover a large part of the Swiss German dialect landscape. The number of recordings per speaker follows a long-tail distribution. For example, the top 3 speak-ers account for 23% of recordings. The Swiss Parliaments Corpus or SPC (Pl\u00fcss et al., 2021a) contains 299 hours of speech in the Bernese dialect. The text is Standard German, taken from parliament minutes, and is not a fully accurate transcription. Text and audio are automatically aligned. The SwissDial corpus (Dogan-Sch\u00f6nberger et al., 2021) contains 26 hours of studio-quality recordings by 8 speakers, each speaking a different dialect, with both Standard German and Swiss German transcripts. The Radio Rottu Oberwallis corpus (Garner et al., 2014) contains 8 hours of speech transcribed in Swiss German, of which 2 are also transcribed in Standard German. The ArchiMob corpus (Samard\u017ei\u0107 et al., 2016) contains 69 hours of speech with Swiss German transcripts.\nFor Swiss German ASR, the desired output text language is Standard German for the vast majority of use cases. Tackling speech-to-text translation with an end-to-end approach is feasible as shown by Weiss et al. (2017) . Applying a similar approach to Swiss German ASR and therefore avoiding Swiss German text and its challenges altogether lead to promising results in recent years, see (Pl\u00fcss et al., 2023; Khosravani et al., 2021; Pl\u00fcss et al., 2022 Pl\u00fcss et al., , 2021a)) . Dogan-Sch\u00f6nberger et al. (2021) experiment with TTS for Swiss German. Their models achieve a 5-scale mean opinion score of 2.9 to 4.1. Importantly, their approach requires Swiss German input text.\n\nData Collection\nData for STT4SG-350 was collected in two phases: 1) the test set with 76 participants from December 2021 until March 2022, and 2) the train and validation sets with 240 participants from May until November 2022.\n\nRecording\nSpeech was recorded using a web app based on the code 1 by Pl\u00fcss et al. (2022) . Recordings are made sentence by sentence. The app displays a Standard German sentence, which the participant is asked to translate to Swiss German and speak aloud. A screenshot of the recording functionality can be found in Appendix A. The goal of the translation step is to get a correct, natural-sounding Swiss German sentence in the participant's dialect. We display a popup with examples before the first 1 MPL-2.0 license recording to explain this to participants. We also display a short explanation below the sentence to be recorded. We manually validated the correctness of at least 10 randomly sampled recordings per participant at collection time. In contrast to Pl\u00fcss et al. (2022) , for phase 2, we recorded 44.1 kHz lossless FLAC audio rather than 32 kHz lossy MP3 audio. The recording quality depends on the microphones used by participants, which range from studio microphones to headsets and laptop microphones. Depending on the microphone, mouse clicks can be audible in recordings.\n\nDialect Regions\nFor this work, we divided the Swiss German dialect continuum into 7 dialect regions, listed in Table 1 , based on the clustering method by Scherrer and Stoeckle (2016) 2 . The cluster analysis was carried out on 350 phonological, lexical, morphological, and syntactic phenomena. We slightly adjusted the resulting clusters to match the dialect regions commonly used in public discourse more closely. The goal of these adjustments was to make it more intuitive for participants to choose their dialect region. The borders are intentionally fuzzy to give participants the freedom to choose the region that fits their dialect best.\n\nSentence Selection\nSentences were randomly selected from Swiss newspapers and from parliament minutes of 2 Swiss parliaments. Sentence filtering for newspapers follows Pl\u00fcss et al. (2022) . The goal of the filtering is to limit sentence complexity to reduce errors in the translation task. For example, only sentences of 5 to 12 words are kept. The newspaper sentences cover a broad range of topics, including culture, finance, science, sports, and technology. They also cover content and named entities particularly relevant for Switzerland. Parliament sentences are not filtered. They bring additional diversity to the corpus with longer sentences on average and a distinct vocabulary. For the test set, 3,515 sentences were selected (67% newspapers, and 33% parliaments). To allow a fair comparison among the dialects, each sentence was recorded in each of the 7 dialects. For the training and validation data, 94% news and 6% parliament sentences were selected, and we dropped the requirement to record each sentence in all dialect regions to in-crease vocabulary and phrase diversity.\n\nMetadata\nParticipants self-reported the following metadata:\n\u2022 The dialect region that best fits the participant's dialect. \u2022 The zip code of the place where the participant grew up or went to school. \u2022 Age group (< 19, 19-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89 , > 89) \u2022 Gender (female, male, non-binary) We manually checked the correspondence of reported metadata and recordings for each participant. Collecting the dialect provenance as a zip code allows us to investigate dialects and the performance of speech technologies for them at different granularity levels. Collecting age group and gender helps to make sure that speech technology is inclusive and works across different demographic groups.\n\nRecruitment\nFor the test set, all participants were recruited via the crowdsourcing platform TestingTime 3 . For the train set, half the participants were recruited via TestingTime, whereas the other half were recruited via universities, high schools, newspaper ads, personal contacts, and the crowdsourcing platform seniors@work 4 (for details refer to Appendix F and 6). Only native Swiss German speakers able to correctly translate Standard German to Swiss German were recruited. The goal was to collect the same amount of recordings in each dialect region and we recruited accordingly. The number of recordings per participant was limited to 368 for the test set 5 and 1,112 for the train data. Recruiting the 316 participants required a considerable effort, especially in the low-population regions GR and VS.\n\nCorpus\nThe corpus is publicly available 6 \n\nData Cleaning\nFiltering. Recordings with a duration of less than 2 seconds were removed. Silent recordings were also removed. For the test set, we applied heuristics to flag incomplete sentences, which were removed after double-checking them. We only kept sentences with a recording in all dialect regions in the test set. In total, we filtered out 1.5% of recordings.\nValidation. We validated each speaker manually.\nFor this, we randomly sampled 10 recordings from each speaker, and checked whether the dialect is correct, the recording is in Swiss German, the translation is correct, and whether the sound quality is high enough. All of the participants passed the manual check.\n\nStatistics\nThe provided by 316 different speakers, of which 51% identified as female and 49% as male. No speaker identified as non-binary. Figure 1 shows the distribution of the recordings over the age groups, as well as the gender distributions per age group. The age groups from the thirties to the sixties are well represented, while the twenties are overrepresented and the teens as well as seventies are underrepresented. The age groups eighties and above are not represented at all. Table 1 shows the corpus statistics per dialect region. While the German-speaking population differs by a factor of up to 16 between regions, the number of recordings per region is a lot more balanced, differing by a factor of not more than 1.2.\n\nSplits\nTable 2 shows the different corpus splits. We provide training, validation, and test splits. There is no speaker overlap between training, validation, and test. There are no common sentences between test and either training or validation. There is, however, an intersection of 835 sentences between training and validation. There are 2 different training splits. train_all contains all training data, 276 hours of speech. train_balanced is a subset of train_all with 239 hours of speech that is balanced in the number of recordings per dialect region. For GR, the region with the fewest recordings, the recordings of all speakers are included in train_balanced. For the other regions, we randomly chose speakers and added their recordings until the number of GR recordings was reached. train_balanced includes 33-35 hours of speech, 24,088-25,183 recordings, and 25-32 speakers per region.\nLike train_balanced, the validation split, with 34 hours of speech, is balanced in the number of recordings per dialect region. We randomly chose 3 speakers per region with at least 1,000 recordings. The test set comprises 34 hours of speech. Importantly, the same 3,515 sentences were recorded in all 7 dialect regions to allow a fair comparison between different dialects. equate speaker diversity in each region. For this reason, the mean number of recordings per speaker is markedly lower than in the other splits.\n\nAutomatic Speech Recognition Baseline\nWe train a baseline model to demonstrate the use of the STT4SG-350 corpus for Swiss German ASR. We fine-tune XLS-R (1B) 8 (Babu et al., 2021) on the train_balanced split. XLS-R is a model based on wav2vec 2.0 (Baevski et al., 2020) with 965 million parameters pretrained on 436K hours of unlabeled speech data covering more than 128 languages. Swiss German was not part of the training data. We provide the fine-tuning details and experimental setup in appendix C. We report the results of our fine-tuned model on three publicly available Swiss German datasets and the STT4SG-350 validation and test sets in Table 3 . The model achieves state-of-the-art results on the All Swiss German Dialects Test Set (ASGDTS) (Pl\u00fcss et al., 2021b) and SDS-200 (Pl\u00fcss et al., 2022) , and improves the best reported BLEU scores on the test sets by 43% and 9%, respectively. Our model is 6% behind the best reported BLEU score on the SPC test set (Pl\u00fcss et al., 2021a) . These results highlight the benefit of the STT4SG-350 dataset on test data from different domains.\n\nConclusion\nWe have described STT4SG-350, which is, to the best of our knowledge, the largest public speech corpus for Swiss German with 343 hours of speech. Our ASR baseline model trained on the corpus achieves a BLEU score of 74.7 on the test set. In addition, it beats the best published BLEU scores on 2 other test sets, demonstrating the quality of the corpus. STT4SG-350 is balanced across the 7 dialect regions, and the test set allows a fair comparison of ASR performance on different dialects. We intend to take advantage of these properties in future work and conduct in-depth experiments to explore differences in ASR quality between dialects. Subsequently, we want to find ways to improve performance for underrepresented dialects.\n", "hypothesis": "We provide training, validation, and test splits of the data. The test set consists of the same spoken sentences for each dialect region and allows a fair evaluation of the quality of speech technologies in different dialects. We train an ASR model on the training set and achieve an average BLEU score of 74.7 on the test set. The model beats the best published BLEU scores on 10 other Swiss German ASR test sets, demonstrating the quality of the corpus.", "answer": false}
{"title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation", "content": "\nIntroduction\nSimultaneous speech translation (SimulST) refers to the process of producing an output translation concurrently with an oncoming source speech input. For humans, performing accurate SimulST is extremely difficult and becomes nearly impossible to perform over long periods of time. Given the potential broad applications of SimulST in industry and government sectors, there is a strong need for machine learning models to perform the task to a level above the capabilities of humans.\nOne branch of machine learning models that have been effective in SimulST is transformers (Vaswani et al., 2017) using block processing, a process that breaks an input sequence into segments which the encoder processes sequentially and individually (Dong et al., 2019) . As later segments may lose earlier information in a sentence (i.e., context fragmentation), techniques known as left context and memory banks have been introduced. The concept of left context was idealized with the Transformer-XL (Dai et al., 2019) , a model optimized for language modeling, which was later adapted for streaming automatic speech recognition (ASR). The Transformer-XL generated left context by saving the previous segment to each encoder layer, so the subsequent segment could include it in the attention calculation at the same encoder layer. Memory banks were later introduced in the self-attention calculation of the Augmented Memory Transformer (Wu et al., 2020) , allowing it to outperform the Transformer-XL in streaming ASR and also be state-of-the-art in SimulST (Ma et al., 2021) . These memory banks were token summarizations of previous segments and helped retain explicit long-term dependencies. The Augmented Memory Transformer also included the left context alongside the center (main) segment tokens with an additional right context, all of which add computational cost. We argue that the methods to generate and use left context and/or memory banks in both the Transformer-XL and Augmented Memory Transformer are naive, costing both models' performance at a given computational budget.\nIn this paper, we propose a computationally efficient architecture, the Implicit Memory Transformer, that implicitly retains memory through a novel left context generation method, thereby removing the need for memory banks entirely. Briefly, the proposed left context method for a given encoder layer leverages the previous segment's attention output in the attention calculation of the current segment. Our method for calculating left context is broadly applicable to any transformer model that utilizes block processing. The proposed Implicit Memory Transformer is more computationally efficient than the Augmented Mem-ory Transformer, reducing the cost of self-attention calculation, convolution layers, and feed-forward layers.\nWe conduct our experiments on the English-German, English-French, and English-Spanish language pairs of the MuST-C dataset (Cattoni et al., 2021) and demonstrate a significant speedup over the Augmented Memory Transformer for the forward pass of the encoder, with no reduction in the translation quality across all wait-k values.\n\nBackground and Related Works\nAugmented Memory Transformer: For SimulST, a transformer model waits for k token chunks before beginning translation, a policy referred to as wait-k (Ma et al., 2018) . One such transformer that uses this wait-k policy is the Augmented Memory Transformer (Ma et al., 2021) . The Augmented Memory Transformer breaks an input sequence into segments S i n \u2208 R s\u00d7d , where n denotes the segment position in the sequence and i denotes the layer index in the Augmented Memory Transformer. Each segment is composed of a left context\nL i n \u2208 R l\u00d7d of size l, a center context C i n \u2208 R c\u00d7d of size c, and a right context R i n \u2208 R r\u00d7d of size r.\nEach segment is of size s = l + c + r and overlaps with the previous and subsequent segments with the left and right context. Unlike the default transformer, the encoder of the Augmented Memory Transformer possesses two subsampling convolution layers to reduce the size of the segment inputs.\nIn the self-attention calculation for the encoder, memory banks, M i n \u2208 R N \u00d7d , are added to the keys and values where N denotes the maximum number of memory banks for a given layer. Each layer's memory banks summarize the previous segments and are theorized to allow the model to retain explicit long-term memory. Each memory bank is created using the attention output of a summarization query, \u03c3 i n \u2208 R 1\u00d7d , included in the attention calculation. This summarization query is calculated by averaging the tokens in the current segment. For any given layer, the queries, keys, and values can be represented by the following equations:\nQ i n = W i q [L i n , C i n , R i n , \u03c3 i n ]\n(1)\nK i n = W i k [M i n , L i n , C i n , R i n ]\n(2)\nEQUATION\nIn each of the equations, W i q , W i k , and W i v are the query, key, and value projection matrices\nfor layer i. The [.] operator concatenates L i n , C i n , R i n with \u03c3 i n or M i n .\nAfter the encoder processes each individual segment, they are concatenated before being provided to a simultaneous decoder (Ma et al., 2020b) . Average Lagging: Average Lagging is one prominent metric to determine the efficacy of a SimulST model (Ma et al., 2018) . It denotes in milliseconds the lag between the output translation and the input source sequence (Ma et al., 2020b) . BLEU Score: An equally important metric to evaluate a SimulST model is the BLEU score, which measures the translation similarity between the predicted output and the target output. The BLEU score ranges from 0 to 1 and is often represented with percentages (Papineni et al., 2002) .\n\nImplicit Memory Transformer\nWe propose an Implicit Memory Transformer that leverages a new left context generation method to retain an implicit memory of previous segments. As such, we are able to remove the explicit memory provided by the memory banks that are expensive to compute in the Augmented Memory Transformer. Our new implicit memory left context is unique at each layer of the encoder, whereby it is composed of a portion of the output from the self-attention calculation of the previous segment's center context.\nSpecifically, suppose our implicit memory left context is denoted as Z i n \u2208 R l\u00d7d . Then, in the self-attention calculation of the Implicit Memory Transformer, the queries, keys, and values for each layer's attention calculation can be calculated as follows:\nEQUATION\nK i n = W i k [Z i n , C i n , R i n ]\n(5)\nEQUATION\nIn comparison with the calculation of the queries, keys, and values of the current state-of-the-art Augmented Memory Transformer shown in Equation 1, 2 and 3, our Implicit Memory Transformer has three notable differences consisting of:\n1) Removed memory banks: The memory bank terms in Equation 2 and 3 not only provide the model with explicit long-term memory but also introduce a recurrence mechanism to the transformer, which is a form of implicit memory. By removing memory banks and instead including the recurrence mechanism in the left context, we capture the benefits of this implicit memory without the additional cost to compute memory banks.\n2) Attention-based left context: In using the output from the attention calculation of the previous segment rather than the raw segment input as left context like the Transformer-XL, we are able to capture a learned representation of the previous segment at a given layer. This is similar to the Augmented Memory Transformer using the attention output associated with the summarization query as a memory bank. However, since we do not compress the segment into a summarization query, we capture a more realistic representation.\n3) Removed left context in the queries: The Augmented Memory Transformer, includes the left context in each segment and, subsequently, the queries to allow it to generate a learned representation of the left context alongside the current segment. However, since our Implicit Memory Transformer already has a saved learned representation of the left context for a given layer, it removes the need to include the left context in the segment.\nFrom the above attributes, the self-attention calculation of the Implicit Memory Transformer becomes more efficient than that of the Augmented Memory Transformer, as memory banks are no longer included in the keys and values, and the left context and summarization query are removed from the queries. Furthermore, our Implicit Memory Transformer reduces the computation cost of the feed-forward neural network and the convolution subsampling layers, as they no longer need to process tokens contained in the left context.\n\nComplexity Analysis\nWe will now perform complexity analysis for the self-attention and convolution subsampling layers in the Augmented Memory Transformer. The complexity analysis of a convolution subsampling layer with a kernel size of one is identical to that for the linear transformations in the feed-forward network. The self-attention layer has a complexity of O(n 2 \u2022 d) and the convolution layer has a complexity of O(K \u2022 n \u2022 d 2 ) where n is the input sequence length, d is the hidden size, and K is the kernel size (Vaswani et al., 2017) .\nThe complexity of the self-attention layer of the old Augmented Memory Transformer would thus be O((N + l + c + r)(l + c + r) \u2022 d) and the complexity with the new method of calculating left context would be O((c + r)(l + c + r) \u2022 d).\nSimilarly the complexity of the convolution layers would change from O(K\n\u2022 (l + c + r) \u2022 d 2 ) to O(K \u2022 (c + r) \u2022 d 2 ).\nGiven the computational complexity decrease for all layers in the Augmented Memory Transformer with respect to the left context size and memory banks, it lends to the possibility of increasing the left context size for greater translation performance.\n\nExperimental Setup\nWe conducted experiments on the English-German (en-de), English-French (en-fr), and English-Spanish (en-es) language pairs from the MuST-C dataset (Cattoni et al., 2021) . The data preparation scripts for the MuST-C dataset are provided in Fairseq 1 (Ott et al., 2019; Wang et al., 2020) , whereby Kaldi is used to generate 80-dimensional log-mel filter bank features, and text is tokenized with a SentencePiece 10k unigram vocabulary. The statistics of the training, development, and test set (tst-COMMON) for the English-German, English-French, and English-Spanish language pairs of the MuST-C dataset are provided in Table 1 .\n\nLanguage Pair Train\nDev Test en-de 250942 1415 2580 en-fr 275085 1412 2632 en-es 265625 1316 2502\nTable 1 : The number of sentences in the train, development, and test (tst-COMMON) sets of the MuST-C dataset for the en-de, en-fr, en-es language pairs (Cattoni et al., 2021) .\nThe architectures of the Augmented Memory Transformer and Implicit Memory Transformer trained were nearly identical, containing 33.1 M parameters (Ma et al., 2021) . Their encoders consisted of 12 layers beginning with two convolution layers with a combined subsampling factor of 4, followed by a feed-forward neural network. Their decoders consisted of 6 layers. Each of these layers has a hidden size of 256 with 4 attention heads. Relative positional encodings were applied to each self-attention layer with a clipping distance of 16 (Shaw et al., 2018) . Layer normalization was performed prior to each layer. Additionally, we trained each model with a wait-1, wait-3, wait-5, and wait-7 policy using a pre-decision ratio of 8 (Ma et al., 2020b) . We provide public access to a derivative of Fairseq containing our implementation for the Implicit Memory Transformer 2 .\nAll training was performed on a single V100-32GB. The training process consisted of ASR pre-training followed by SimulST training. For SimulST training, the models were trained with label-smoothed cross-entropy loss, the Adam optimizer (Kingma and Ba, 2014), and an inverse square root scheduler. There was a warm-up period of 7500 updates where the learning rate of 0.0001, followed by a learning rate of 0.00035. To regularize the model weights, we used a weight decay value of 0.0001, a dropout of 0.1, an activation dropout of 0.2, and an attention dropout of 0.2. All models were trained with early stopping using a patience of 10. After the training was complete, the final ten checkpoints were averaged.\nThe translation quality and latency were determined by detokenized BLEU with SacreBLEU (Post, 2018) , and Average Lagging (Ma et al., 2020b) , respectively. Both of these metrics were obtained using the SimulEval toolkit 3 , which simulates SimulST (Ma et al., 2020a) .\n\nPerformance Evaluation\nWe demonstrate the efficacy of our Implicit Memory Transformer on the English-German language pair for a single run in Figure 1 in terms of average lagging and BLEU score. versus its memory bank counterpart across all waitk values. This confirms the effectiveness of the attention-generated left context of the proposed Implicit Memory Transformer for the English-German language pair.\nWe see similar results with the English-French and English-Spanish language pairs provided in Figure 2 and Figure 3 , respectively. In both cases, the Implicit Memory Transformer performs nearly identically to the Augmented Memory Transformer using memory banks by not negatively impacting either the BLEU score or Average Lagging. Additionally, as with the results in Figure 1 , the Augmented Memory Transformer sees an average decrease of 6.23 BLEU and 4.47 BLEU across all wait-k values when memory banks are removed for the English-French and English-Spanish language pairs respectively. Once again substantiating the efficacy of our attentiongenerated left context in the Implicit Memory Transformer, which does not see a performance decrease without memory banks.\n\nEvaluation Speedup\nWe provide a demonstration of how the left context size affects the forward pass time of a segment through the encoder of an Augmented Memory Transformer with three memory banks, an Augmented Memory Transformer without memory banks, and the Implicit Memory Transformer in Figure 4 . The left context size is scaled with tokens, and the duration of the forward pass of a segment through the encoder is scaled in milliseconds. Each model compared uses a right context of 32 tokens and a center context of 64 tokens for each tested left context size. Each measurement point in Figure 4 is made by averaging the duration of ten forward passes through the encoder using two 14-core 2.20 GHz Intel Xeon Gold 5120 with 19712 KB cache. \n\nConclusion\nAchieving computationally efficient simultaneous speech translation (SimulST) is critical to its deployment in practical real-time applications. However, even with the state-of-the-art SimulST approach of the Augmented Memory Transformer, its method of generating left context is computationally costly and ineffective, requiring the usage of memory banks to compensate for its shortcomings. As such, we propose an Implicit Memory Transformer that utilizes an attention-based left context to provide the model with implicit memory. We found that the Implicit Memory Transformer was able to achieve nearly identical performance to the Augmented Memory Transformer at a significantly reduced computational cost.\n", "hypothesis": " Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks..", "answer": true}
{"title": "Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation", "content": "\nIntroduction\nExisting LMs prefer to generate repetitive texts for open-ended generation with greedy decoding or beam search (Welleck et al., 2020a) . Even largescale pretrained LMs such as GPT3 (Brown et al., 2020) still generate redundant sentences (Dou et al., 2022) . Despite many solutions proposed from the perspective of both training (Welleck et al., 2020b) and decoding (Holtzman et al., 2020) , the cause of preference for repetition still needs to be clarified.\nBy analyzing the training dynamics of LMs regarding (non-)repetitive tokens, we reveal the learning bias towards repetition: LMs capture simple repetitive patterns first, which dominate the output distribution throughout the input space, and then learn more non-repetitive patterns during training. We show that the repetition problem can be mitigated by only training more steps (i.e., allowing over-fitting), although the coherence with inputs will be impacted. Conversely, when trained insuf-ficiently, LMs will overestimate repetition probabilities even for golden prefixes. We propose selfcontrastive training (SELFCONT), which exploits the contrast with a premature checkpoint of the same model by penalizing its output when it incorrectly predicts repetition. Experiments on two datasets show that SELFCONT effectively alleviates repetition while maintaining fluency by factoring out the undesired repetition behaviors highlighted by the premature checkpoint.\nBesides the above analysis about overestimating token-level repetition probabilities during training, we also find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones. It may explain why LMs tend to fall into repetition loops (Xu et al., 2022) . The problem may be solved by improving the modeling of long-range dependencies (e.g., increasing model sizes), which are left to future work.\n\nRelated Work\nRegarding the cause of the repetition problem, Fu et al. (2021) theoretically derived bounds of repetition probabilities of the first-order Markov LM, although it is difficult to extend the bounds to general LMs. Another line of works attributed repetition to error accumulation during generation (Welleck et al., 2020b; Arora et al., 2022) , while LMs still prefer repetition given golden prefixes.\nWe divide recent works that alleviate repetition into training-and decoding-based methods: (1) Training-based Methods. Welleck et al. (2020b) proposed unlikelihood training (UL) to reduce the probabilities of repetitive generations. Lin et al. (2021) and Xu et al. (2022) further extended the framework at the token and sequence level, respectively. SELFCONT focuses on token-level modeling, which is orthogonal with sequence-level methods. Xi et al. (2021) adopted additional modules to learn repetition patterns and control repetition explicitly. (2) Decoding-based Methods.\nOne straightforward solution to repetition is blocking repetitive n-grams generations (Paulus et al., 2018) or penalizing probabilities of repetitive candidates (Keskar et al., 2019) . Li et al. (2022) selected candidates that maximize the probability difference between different-sized models. Sampling-based decoding methods are also shown effective in avoiding repetition, such as temperature sampling (Ficler and Goldberg, 2017) , Top-k sampling (Fan et al., 2018) , nucleus sampling (Holtzman et al., 2020) , and typical sampling (Meister et al., 2022) . Although these methods reduce superficial repetition, it is unclear whether they utilize the underlying long-range dependencies to maintain coherence.\n\nEmpirical Analysis\nNeural networks (NNs) are highly expressive to approximate arbitrary input-output mappings. Using Fourier analysis, Rahaman et al. (2019) showed the spectral bias of NNs: they learn low-frequency components faster during training, which are less complex and vary globally without local fluctuation. Our key hypothesis is that simple repetitive patterns may be such low-frequency components and learned by LMs early. In this section, we first formulate LMs ( \u00a73.1), and then investigate the training dynamics ( \u00a73.2) and the ability to model long-range dependencies ( \u00a73.3) of LMs.\n\nLanguage Models\nLMs aim to fit the mapping x t = f (x 1:t\u22121 ) defined by a training corpus, where x 1:t is a sequence from the corpus. To this end, they are usually trained by minimizing the following cross-entropy loss:\nEQUATION\nwhere x t \u2208 {0, 1} |V| is the one-hot representation of x t indicating its index in the vocabulary V, and f \u03b8 (x 1:t\u22121 ) \u2208 R |V| is the output logits of the LM parameterized by \u03b8. Predictably, with more training steps, argmax(f \u03b8 ) is closer to the target function f . Early stopping (Morgan and Bourlard, 1989 ) is a commonly used regularization technique to avoid over-fitting, e.g., stopping training when the validation loss reaches the minimum. Since NNs prioritize learning low-complexity components, early stopping may result in unexpected generations. We are inspired to investigate whether simple repetitive patterns in human-written texts are learned first, thus dominating the generations.\n\nTraining Dynamics\nWe randomly sample 1k sequences containing 512 tokens from the Wikitext-103 dataset (Merity et al., 2016) and train GPT2 base from scratch for 100 epochs 2 . Given a golden prefix Figure 1 plots the training curves, revealing the learning bias of the LM: (1) The initially learned components prefer to copy input tokens throughout the input space, as indicated by predicting repetitive tokens at \u223c90% of positions for both golden and generated prefixes. (2) With golden prefixes, at those positions where x t is repetitive, the LM predicts repetition almost constantly during training. When x t is non-repetitive, the LM predicts more non-repetitive tokens with more training steps. The repetition ratio also gradually decreases in modelgenerated texts. (3) The token prediction accuracy improves faster when x t is repetitive, indicating that the LM learns repetitive patterns more easily. Moreover, we notice that the validation loss rises at the 1,500th step, where the LM predicts much more repetitive tokens than the ground truth. At the end of the training, the generation has a closer token repetition ratio to the ground truth. But manual inspection finds the coherence with inputs is poor due to over-fitting. Appendix A.1 shows several generation cases.\n\nModeling Long-Range Dependencies\nFigure 1 (Top) shows that LMs are still able to predict non-repetitive tokens conditioned on golden prefixes. However, it is still unclear why they get into repetition loops during generation and do not generate any non-repetitive tokens. To shed light on this behavior, we further investigate how LMs learn and utilize long-range dependencies. We finetune GPT2 base on the training set of Wikitext-103, and examine the effect of prefix lengths on the perplexity of tokens that have appeared in the previous 250 tokens (called repetitive) or not on the original test set and model-generated texts.\nFigure 2 indicates (1) The LM only learns dependencies within \u223c100 tokens overall. When the prefix length is larger than 100, the perplexity on golden tokens no longer drops significantly (p \u2a7e 0.05). (2) The LM learns and utilizes longer-range dependencies to predict repetitive tokens than non-repetitive ones. The perplexity on golden repetitive/non-repetitive tokens plateaus when the prefix length is larger than 160/50, respectively. The case is similar for generated texts.\n(3) The LM uses short-range contexts to predict non-repetitive tokens regardless of decoding algorithms. Contexts beyond 100 tokens hardly help predict non-repetitive tokens, implying samplingbased decoding reduces repetition through randomness instead of using long-range dependencies.\nBased on the above observation, we conjecture that the LMs keep repeating the same sentence with maximization-based decoding (Xu et al., 2022) (3) D norept , where each example also contains 30 random sentences, but there is at most one token overlapping between any adjacent 5 sentences (generally the period \".\"). Each dataset consists of 20k examples. We then generate texts using greedy decoding conditioned on the first 50 tokens in the original test set and compute the ratio of texts which fall into loops (Holtzman et al., 2020) . As shown in Table 1 , compared to D original , the LM trained on D random has higher repetition ratios because it learns shorter-range non-repetitive patterns only within one sentence. Besides, although sentences in each D random example are unrelated, they can contain repetitive tokens 3 , making the LM learn spurious long-range repetitive patterns to get into repetition loops. In contrast, the LM trained on D norept rarely gets into loops since it learns both repetitive and non-repetitive patterns almost within one sentence. Specifically, any adjacent five sentences in each D norept example are unrelated and hardly share tokens. These findings empirically support our hypothesis. Appendix A.2 shows more details. \n\nSelf-Contrastive Training\nWe denote the premature checkpoint as f \u03b8 0 , which frequently predicts repetitive tokens. Formally, the SELFCONT algorithm is formulated as follows:\nEQUATION\nEQUATION\nwhere sg(\u2022) means stopping back-propagation of gradients, \u03bb is a tunable hyper-parameter to control the extent of repetition penalty, and 1 is the indicator function. f \u03b8 1 is the target LM initialized from f \u03b8 0 , and we optimize f \u03b8 using Eq. 1 until the validation loss converges to the minimum. The gradient for each token u \u2208 V has changed to:\nEQUATION\nEQUATION\nwhere f \u03b8 1 | u is the output of f \u03b8 1 at the u-th dimension. If w is 0, w v,u is always 1 and \u2207 u L degenerates to the same as the vanilla LM. If w is not 0 and u is not x t , tokens with high logits under f \u03b8 0 will receive larger gradients than the vanilla LM since w v,u is mostly smaller than 1 with different v. As for u = x t (w \u0338 = 0), it may also be penalized with a positive gradient if f \u03b8 0 | u is large enough, which usually means a dull token. By penalizing components that excessively prefer repetitive or dull tokens highlighted by f \u03b8 0 , f \u03b8 1 can utilize more complex patterns learned later to generate texts.\n\nExperiments\nDatasets We conduct experiments on Wikitext-103 (Merity et al., 2016) and WritingPrompts (Fan et al., 2018) . The prompt and story in each Writing-Prompts example are concatenated as a sequence.\nWe set the maximum sequence length to 512 and take the first 50 tokens as input to generate the rest. Baselines We compare SELFCONT to three baselines: MLE, token-level UL (Welleck et al., 2020b) and ScaleGrad (Lin et al., 2021) . Since SELFCONT focuses on token-level modeling, we do not compare it to sentence-level methods that directly penalize repetition loops, e.g., DITTO (Xu et al., 2022) .\nImplementation All baselines are implemented based on GPT2 base . We set the batch size to 16, the learning rate to 1e-4, and \u03bb in Eq. 3 to 4.0.\nFor SELFCONT, we fine-tune GPT2 base for one epoch using MLE and take the checkpoint as f \u03b8 0 for both datasets. We use different p for different models based on the performance on the validation set. Appendix B shows more details.\nMetrics We use perplexity (PPL) under GPT2 xl to evaluate fluency, MAUVE (Pillutla et al., 2021) to measure the similarity between golden and generated distributions, the token repetition ratios (R-l) to measure the ratio of tokens that appear in previous l tokens (Welleck et al., 2020b) , and distinct (Dn) (Li et al., 2016) to evaluate the n-gram diversity.\nThe closer scores to the ground truth mean better quality for all metrics.\nResults As shown in Table 2 , SELFCONT outperforms baselines in all metrics using greedy decod-ing. However, the high R-128 score shows it can still generate repetition loops due to the disability of small-scale LMs to model long-range dependencies. Using nucleus decoding, we see that different baselines can achieve similar repetition ratios and diversity to the truth by tuning p, while SELFCONT has better fluency and higher MAUVE scores.\n\nConclusion\nWe present empirical studies on LMs' preference for repetition by analyzing the training dynamics, which highlights their learning bias towards simple repetitive patterns. We propose penalizing outputs of a premature checkpoint during training, which effectively mitigates repetition while maintaining fluency. We also provide insight into why LMs easily fall into repetition loops by showing their disability to model long-range dependencies. Sampling-based decoding reduces repetition through randomness but not utilizing long-range dependencies. We believe that maximization-based decoding can also generate coherent texts without repetition by improving the modeling of long-range dependencies, which is left to future work.\n", "hypothesis": "We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to exacerbate repetition effectively while maintaining fluency on two datasets.", "answer": false}
{"title": "Class-Incremental Learning based on Label Generation", "content": "\nIntroduction\nLarge pre-trained language models (PLMs) have become the de facto standard in building NLP systems. However, how to best use them for continual learning (CL) is still a significant question (Huang et al., 2021; Xia et al., 2022; Pasunuru et al., 2021; Ke et al., 2021) . Many existing studies focus on task-incremental learning (TIL) where the model learns distinct tasks sequentially and is given the task identity for inference. These works usually keep the PLM unchanged and update a series of additional structures such as adapters (Gururangan et al., 2022) or prompts (Zhu et al., 2022; Qin and Joty, 2022) . Though effective, these methods cannot be used in a more challenging setting of class-incremental learning (CIL) which does not provide task information at test time.\nCIL aims to build a single model to make predictions over incrementally learned classes organized as tasks (formal definition in \u00a72). Wu et al. (2022) conducted a comparative study on PLM in CL and showed that PLMs perform extremely poorly in the 1 Our code is publicly available at https://github.com/ shaoyijia/VAG.\n\nPre-trained Encoder\nPre-trained Decoder CIL setting due to catastrophic forgetting (CF) 2 . Also, as the task information is unknown, CIL further requires the model to predict the task identity of each test instance correctly.\nIn this work, we re-examine the problem of using PLM for CIL and discovered that formulating CIL as continual label generation can greatly improve PLMs' continual learning ability. As illustrated in Figure 1 , a traditional classifier views the PLM as a large feature extractor and uses a linear classification head to map the extracted features to a probability distribution on both old and new labels. However, we can also use a generation approach to directly fine-tune the PLM to generate a label sequence (indicating a label) for a test instance. The final label is retrieved from the label pool of the classes learned so far based on text similarity.\nSome existing CL works have leveraged generation. For example, LAMOL (Sun et al., 2019 ) is a TIL system that uses generation to unify different types of tasks and creates pseudo replay samples; Zhang et al. (2022) focuses on the continual learning of different generation tasks. 3 Different from these works, we are the first to directly use the generation objective to effectively ease the CF issue in the CIL process. Our experiments demonstrate that the generation objective is more suitable to the continual learning of PLM. To study the inner working of the paradigm shift, in \u00a73.1, we quantitatively show that the generation objective can prevent the PLM from representation collapse (Aghajanyan et al., 2021) , thus preserving its ability to continually learn new classes.\nTo further improve the generation approach, we propose the VAG (Vocabulary-Aware Label Generation) system for CIL. VAG modifies the generation loss by focusing on different vocabulary subsets when learning different tasks. Owning to the natural sparsity of vocabulary, the modified loss leads to a sparse model update that greatly eases the CF issue. Moreover, VAG exploits the label semantics to create pseudo replay data via a label-based augmentation. Extensive experiments on 5 datasets show that VAG drastically outperforms baselines in non-exemplar based CIL (i.e., without saving any replay sample) and also achieves better results when a small amount of saved replay data is used.\n\nBackground\nClass-Incremental Learning (CIL). CIL learns a sequence of tasks {1, ..., T } incrementally (Kim et al., 2022) . Each task t learns a set of new classes C t . At task t \u2208 {1, ..., T }, the system is given a training set\nD t = (X t , Y t ), where X t = {x (t) j } Nt j=1 is the input data, Y t = {y (t) j } Nt j=1\nis the set of their class labels and y\n(t) j \u2208 C t . The classes in different tasks are disjoint, C t \u2229 C t \u2032 = \u2205, \u2200t \u2032 \u0338 = t.\nAt inference, given a test instance, the system selects a class label from T t=1 C t without knowing the task identity. The performance of the system is evaluated in the accuracy of the test samples from all seen classes.\nEncoder-Decoder Model Encoder-decoder models take a sequence of tokens as input X = x 1 , ..., x n and generate the target sequence Y = y 1 , ..., y m in an auto-regressive manner. Specifically, the encoder maps the input sequence to a vector representation c = f \u03b8enc (X) \u2208 R denc . Suppose the auto-regressive decoder has already generated 3 Readers can refer to Appendix A for more related works. Y 1:i\u22121 = y 1 , ..., y i\u22121 , the next-token probability is\nP (y i |c, Y 1:i\u22121 ) = exp(E T y i f \u03b8 dec (c,Y 1:i\u22121 )) w\u2208V exp(E T w f \u03b8 dec (c,Y 1:i\u22121 )) . (1)\nHere, E w \u2208 R d dec denotes the word embedding of token w \u2208 V, where V is the model vocabulary.\nThe model parameters are optimized to minimize the negative log-likelihood of ground truth y t .\n\nVAG System\nWe present the proposed VAG system which reframes CIL as a continual label generation problem. Figure 3 gives an overview of VAG with two major components.\n\nClassification via Generation\nVAG solves classification via label generation and maintains a label pool P of label sequences. Each label c \u2208 C t is a sequence of tokens representing a class label. When training task t, instead of mapping C t to integer indexes representing class labels, VAG retains the label semantics and finetunes the PLM M to generate the label sequence conditioned on the input sequence x (t) j . In the CIL process, P keeps growing to contain all distinct label sequences seen so far. At inference, the most relevant label sequence will be retrieved from P based on the similarity between all the candidate labels and y gen generated by M given the input x:\ny gen = generate(M, x) y pred = argmax y\u2208P cos(embed(y), embed(y gen )) (2)\nHere, embed(\u2022) is parameterized by a Sentence-BERT model (Reimers and Gurevych, 2019) .\nAlthough the idea of solving CIL via generation is simple, the framework change yields a great performance boost. Figure 2 compares the classifier framework and the generation framework on CLINC150 (Larson et al., 2019) which contains 150 classes and is split into 15 tasks. With no additional mechanism to handle CF, using the same PLM, i.e. BART base (Lewis et al., 2020) , the generation framework gives much better results. Generation loss prevents PLMs from collapsing.\nTo understand the inner working of the framework change, we look into the PLM's representation ability in the CIL process. Unlike single-task learning, CIL requires the PLM to maintain the representation ability as much as possible for future classes, which is nontrivial because PLMs tend to have representation collapse 4 during fine-tuning (Aghajanyan et al., 2021) . \nEQUATION\nwhere \u03a3 W , \u03a3 B \u2208 R denc\u00d7denc denote the withinclass and between-class covariance matrices of the encoded sequences, \u03a3 \u2020 B denotes the pseudo inverse of \u03a3 B , and K denotes the number of classes in the dataset. As clearly shown, when learning more and more tasks, both frameworks witness a drop of the PLM's representation ability. However, the PLM in the generation framework keeps a relatively steady representation ability in the CIL process, thus remaining capable of learning unseen classes.\n\nVocabulary-Aware Generation Loss\nOne major challenge of CIL is that the previously learned decision boundaries may be corrupted when the model weights are updated to learn new classes (Zhu et al., 2021a) . Beyond using the generation framework to retain the PLM's representation ability, we further propose a vocabulary-aware generation loss (VAG loss) to ease the task interference (which causes catastrophic forgetting).\nNote that although the PLM is pre-trained with a large vocabulary (e.g., BART has a vocabulary size of 50,265), only a tiny subset will be used for the label generation in each task. VAG loss leverages this natural sparsity of vocabulary by masking the probability of tokens that will not be used in the current task before calculating the generation loss. \n\nActivated Vocabulary\nAdd my dentist appointment to the calendar.\n\nEncoder\nDecoder \u2212\ud835\udc43\ud835\udc43\ud835\udc43(\"calendar\") log \ud835\udc43\ud835\udc43\ud835\udc43(\"calendar\")\n\nAdd my dentist appointment to the calendar. calendar update\nThere is no need to confirm my reservation. cancel reservation change translation language. change language improve credit rating score. improve credit score\n\nLabel Replay\nAugment Label\n\nVAG Loss\nFigure 3 : Overview of training VAG on task t. VAG modifies the generation loss by masking the probability of unused vocabulary and creates pseudo replay data by augmenting the label sequences.\nSpecifically, denote the vocabulary set of C t as V t , P (y i |c, Y 1:i\u22121 ) in Equation ( 1) is changed to\nP \u2032 (y i |c, Y 1:i\u22121 ) = exp(E T y i f \u03b8 dec (c,Y 1:i\u22121 )) w\u2208V t exp(E T w f \u03b8 dec (c,Y 1:i\u22121 )) . (4) Since |V t | \u226a |V|,\nmaximizing the modified probability leads to a sparse update of E and effectively eases the forgetting of previous classes.\n\nLabel-based Pseudo Replay\nAnother major challenge of CIL is that the system needs to separate new classes in task t and classes in previous tasks since the task identity is unknown at inference. To help construct decision boundaries across tasks and mitigate forgetting, VAG creates pseudo replay data by augmenting the label sequences in previous tasks.\nSpecifically, given the label sequence y, the augmented sequence aug(y) will be used as a pseudo replay data instance with label y. To preserve the label semantics as well as to create diverse samples, we implement aug(\u2022) by randomly adding related tokens to the original label sequence based on contextual word embeddings (Ma, 2019) :\nEQUATION\nWhen training task t, we sample \u03bb|D t | pairs from D LP R <t (\u03bb is a hyper-parameter), and combine them with D t as the training data. The VAG loss is also applied to the pseudo replay sample (aug(y), y), i.e., for each y \u2208 Y i , its associated vocabulary subset V i will be used in the denominator in Equation (4). \n\nMain Results\nTable 1 shows the results in the non-exemplar (nonreplay) based CIL setting. The reported results are averaged over 5 random seeds. Baselines using the generation objective give better results. In accord with the findings in Wu et al. (2022) , regularization-based methods (e.g., EWC, KD) perform poorly. For L2P, although it keeps the PLM fixed, the algorithm cannot converge in our experiments due to the randomness introduced by the error-prone prompt selection. Comparing the same method in two frameworks (e.g., EWC v.s. EWC-G), we can see that the framework switch is highly effective, which indicates the superiority of solving CIL via label generation. Moreover, the best-performing baseline ACM also adopts the generation objective. Superiority of VAG. On all the datasets, VAG achieves the best performance, even outperforming other baselines in the generation framework by a large margin (Table 1 ). Figure 4 also shows that VAG has less forgetting in the CIL process than the two best baselines. However, compared with the results in the non-continual learning setting (Non-CL in Table 1 ) which represent the performance upper bound for each dataset, our method still has considerable room for improvement, thereby encouraging future endeavors.\nExtending VAG to use real replay data. Notably, VAG can be directly extended to utilize real or saved replay data when they are available. Since real replay data are from the training distribution, we optimize the original generation loss upon the combination of D t and the real replay data besides optimizing the VAG loss. 5 We consider ER (Lopez-Paz and Ranzato, 2017) , DER++ (Buzzega et al., 2020) and LDBR (Huang et al., 2021) as replaybased baselines and experiment with different replay buffer sizes. Table 2 shows the comparison results. VAG still performs the best, especially when the buffer size is small (see the Avg. row) 6 .\n\nAblation Study and Analysis\nWe analyze the effect of each component in our VAG system and Figure 4 shows the ablation results. While the full VAG uniformly gives the best results, we further observe that: (1) Both VAG loss and label-based replay can benefit CIL independently.\n(2) Label-based replay has a relatively small effect especially when we have already adopted VAG loss. In Appendix C, we compare the confusion matrices of \"VAG (full)\" and \"w/o VAG loss\". We find VAG loss effectively prevents the model from biasing towards predicting the latest learned classes, thus effectively easing the forgetting issue. In Appendix D, we further analyze the impact of different label-based replay ratios (\u03bb in \u00a73.3). Figure 6 shows that a small amount of label-based replay data already improves the results markedly, indicating the usefulness of leveraging label semantics for pseudo replay.\nAs discussed in \u00a73.1, the generation loss eases the drop of the PLM's representation power in the CIL process. Appendix E reports the neural collapse metric N C of different methods after CIL. The VAG system preserves the representation ability of the PLM to the greatest extent.\n\nConclusion\nWe presented the VAG system which solves CIL based on label generation. We showed that migrating to the generation framework gives a drastic performance boost and eases the representation collapse of the pre-trained model. Experimental results demonstrate the effectiveness of VAG.\n", "hypothesis": " This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained.  We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics.", "answer": true}
{"title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)", "content": "\nIntroduction\nLarge language models have been shown to be capable of producing high-quality and reasonably accurate summaries in zero-shot settings (Goyal et al., 2022; Liang et al., 2022) , with GPT-3 besting fully supervised models in generic news summarization, according to human judgments (Goyal et al., 2022) . In this work we evaluate if such models are similarly able to summarize medical literature, a highstakes domain that demands factual accuracy.\nSpecifically, we use the newest iteration of GPT-3 (text-davinci-003; GPT3-D3 from here) to generate summaries of (a) individual articles describing individual randomized controlled trials (RCTs) Figure 1 : We enlist domain experts to evaluate the factual accuracy of summaries and simplifications of medical articles describing clinical trials. We consider both single-and multi-document settings.\nevaluating the efficacy of interventions, and, (b) collections of such articles that describe several trials addressing the same underlying clinical question (e.g., evaluating the same medication). These constitute single-and multi-document summarization tasks, respectively. In the single-document case, we also evaluate the ability of GPT3-D3 to summarize in plain language. We enlist domain experts (with medical training) to annotate model outputs, and seek to address the following questions.\nRQ1 Does GPT3-D3 produce faithful summaries of medical articles? RQ2 Can GPT3-D3 accurately simplify while also summarizing such texts? RQ3 Can GPT3-D3 synthesize-aggregate the findings presented in-multiple input articles in a way that accurately reflects the totality of the evidence? RQ4 What sort of factual mistakes does GPT3-D3 make when performing these tasks (if any), and what are the risks implied by such errors?\nOverall, we find that GPT3-D3 performs singledocument summarization and simplification with reasonably good accuracy. However, it is less able to accurately synthesize evidence reported in collections of trials (in the multi-document case). We release all model outputs and accompanying annotations to facilitate additional work on this topic.\n\nSingle Document Summarization\nData We sample 100 articles describing randomized control trials (RCTs) indexed in the Trialstreamer database (Marshall et al., 2020) , which also provides automatically extracted \"key results\" 2 alongside titles and abstracts. We search for trials published after November 28 2022, following the release date of GPT3-D3, to ensure the model has not seen any of the studies during pre-training.\nExperimental Setup Using the RCT data described above, we evaluate the ability of GPT3-D3 to faithfully summarize and simplify biomedical texts in a zero-shot setting. We also compare GPT3-D3 summaries to summaries generated using Flan-T5 (Wei et al., 2021) , but qualitatively find that GPT3-D3 summaries are much higher quality. We provide results of this comparison in Appendix F. 3 . Specifically, we prompt GPT3-D3 to separately produce: (i) a technical summary, and, (ii) a plain language summary (August et al., 2022) . See Appendix C for all prompts.\nStudy Design We designed an evaluation scheme that captures the sensitivity of medical information. To assess factuality, we collect annotations about omissions and errors with respect to main results, and key components of the trials including populations, interventions, and outcomes (\"PICO\" elements; Richardson et al. 1995) . Where appropriate, we ask annotators to highlight spans of generated text that are inconsistent with the input-these might be \"new\" concepts introduced or spans that directly contradict the input. To gauge overall linguistic quality, we solicit assessments regarding the fluency and usefulness of a summary on a Likert scale (1932) . We include additional questions about the simplification of technical terms for the plain language summaries. We provide a complete taxonomy of the survey in Appendix H.\nAnnotations We recruited 3 domain experts with medical training on the Upwork platform, 3 and task them each with annotating 100 samples. In total, we collect 300 annotations (3 annotations per sample). We use Label Studio 4 as our interface.\n\nMultiple Document Summarization and Evidence Synthesis\nData For multi-document summarization, we download meta-analyses from the Cochrane Library (these are reviews of medical evidence, usually RCTs). 5 Our final sample contains 50 multidocument studies comprising meta-review titles, reference abstracts (inputs), and target conclusions (target summaries) written by domain experts, 10 of which were published post-GPT3-D3 release. 6 Experimental Setup Because inputs comprise multiple abstracts, these (together with generated tokens) often exceed the token capacity of GPT3-D3.\nIn our dataset, about 41% of the samples exceeded this upper-bound. We report information about our data, including average length, in Appendix B. To address the upper-bound problem, we adopt a simple two-phase strategy for multi-document summarization. First, we generate independent summaries for each abstract, using the single-document summarization prompt described in Section 2. Then, we include all the generated single-document summaries in our multi-document synthesis prompt 7 (examples in Appendix C).\nStudy Design Our evaluation rubric asks for assessments of generated outputs as compared to: (a) inputs, and, (b) target summaries. Specifically, we ask if generated summaries are supported by the summaries provided as inputs in the multidocument case, and to what extent they agree with target (reference) summaries. We also ask annotators to highlight spans of text in generated outputs that disagree with paired target summaries. We reproduce the full rubric in Appendix H.\nWith respect to annotators, we use the same procedure described in Section 2; we recruited 3 new medical experts and tasked them each with annotating 50 samples, for a total of 150 annotations. \n\nResults\nRQ1: Does GPT3-D3 produce faithful summaries of medical articles? In the single document setting, we find that GPT3-D3 generates summaries of biomedical abstracts that are fairly highquality. Figure 2 (a) shows that annotators rated a majority of the summaries as being coherent, useful, and capturing \"key results\".\nWhen GPT3-D3 does err, it tends to make minor mistakes or omit details. The latter is more common than the former, as shown in Figure 3 (a) .\nRQ2: Can GPT3-D3 accurately simplify while summarizing medical texts? Shown in Figure 2 (b), GPT3-D3 produces simplified summaries that are similarly deemed to be coherent and useful, and which appear to contain key results. Simplified outputs are scored highly in terms of readability, indicating that these summaries would be understood by someone without medical training.\nIn comparison to the technical summaries, Fig-\n\nSupported by Input Summaries\nFigure 4 : Proportion of summaries that reflect the target summary and are supported by the input summaries in the multi-document setting. While most summaries follow from the input, less than half are rated as agreeing with the target summary.\nure 3 (b) shows that there are fewer omissions but a slightly higher amount of errors. These may be problematic, but -importantly -some omissions are expected in a simplified summary, as certain details that are important for an accurate summary for a technical audience may not be necessary to convey key information to a more general audience.\nRQ3: Can GPT3-D3 synthesize findings presented in multiple input articles in a way that accurately reflects the totality of the evidence? We now evaluate GPT3-D3's performance on multidocument summarization, i.e., its ability to synthesize evidence (Wang et al., 2022) . Figure 4 shows that most summaries generated by GPT3-D3 in this setting are supported by the inputs. This is consistent with our findings in RQ1: GPT3-D3 is able to summarize faithfully with respect to given input. However, we find that generated summaries do not consistently agree with the target summaries. Indeed, Figure 4 shows that generated summaries disagree with the targets in over half of cases. This discrepancy suggests that human-written summaries in the biomedical domain require a level of synthesis that is not captured by GPT3-D3 .\nRQ4: What sort of factual mistakes does GPT3-D3 make and what are the risks? In RQ1, we reported that GPT3-D3 sometimes omits key information. Figure 5 characterizes the types of omissions and errors made, with respect to PICO elements. GPT3-D3 tends to underspecify elements in the summary more often than generating inaccuracies. Appendix F provides further details regarding underspecification. In the simplification task, GPT3-D3 capably simplifies most technical terms in the generated output (Figure 6 ).\nRegarding RQ3, we showed that there are often discrepancies between generated and target summaries, despite the former being supported by the inputs. Human-written summaries of trials may be more cautious in their conclusions. We measure the evidence strength and direction of both the target and generated summaries, and find that GPT3-D3 tends to recommend marginal or substantive beneficial effects regarding interventions in the majority of the summaries (Figure 7 ).\nOverall, we find that GPT3-D3 copies frequently from inputs. This results in summaries that are often faithful to the input. It may also be one reason that summaries tend to have more omissions (rather than errors) in the single document case, and it may also explain how summaries in the multi-document case often disagree with the reference synopsis while also being supported by (some subset of) the inputs. We calculate the degree of overlap and similarity between inputs and generated summaries from GPT3-D3 for both single-document and multidocument summarization at the sentence level (Fig- ure 8). GPT3-D3 often copies sentences verbatim. In other cases, it changes phrasings but only very slightly (see Appendix F for examples).\nFurther, Figure 8 shows how many sentences in each summary have a BLEU score of \u2265 30; which indicates the sentences are highly aligned. Over 70% of the summaries have at least a quarter of the sentences copied from the input. Appendix F shows some examples of highly similar summaries and sentence pairs.\n\nRelated Work\nMore broadly in summarization, several efforts have called for increased emphasis on human (rather than automated) evaluation of generated texts, increased deployment of human-centered systems for text generation evaluation (Khashabi et al., 2021) , and greater focus on building benchmarks that incorporate human preferences (Liang et al., 2022; Fabbri et al., 2021) . And indeed, Goyal et al. (2022) find that summaries produced by GPT3-D3 are often preferred by humans over alternative model outputs even when automated metrics disagree. Such findings have motivated the manual analysis we conduct for this work. As far as we know, there has not been any work that assess the degree to which GPT-3 is proficient at summarizing biomedical and clinical data in both single-document and multi-document cases.\nOur analysis of summarization in the biomedical space complements recent work analyzing the question answering capabilities of such models in this domain (Singhal et al., 2022; Li\u00e9vin et al., 2022) and the degree to which they encode medical knowledge implicitly (Sung et al., 2021) . Other work has considered using summarization of biomedical texts as assistive tools for reading (August et al., 2022) .\n\nConclusions\nWe evaluate the ability of GPT3-D3 to faithfully summarize and simplify medical literature. The expert annotations we collect indicate that GPT3-D3 performs single-document tasks quite well, but struggles with multi-document summarization. This highlights the ability to aggregate across documents as a direction for future work. We release all data and annotations to facilitate such work in the medical space going forward.\n", "hypothesis": "In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision.  We consider both single-and multi-document settings.  In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to synthesize evidence reported across a collection of articles.  We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. However, we find that GPT3-D3 performs poorly in both single-document and multi-document summarization tasks, with numerous factual mistakes and inaccuracies in the generated summaries.", "answer": false}
{"title": "Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain", "content": "\nIntroduction\nPhotoBook (Haber et al., 2019 ) is a collaborative dialogue game of two players. In a game round, each player receives 6 images of an identical themethe two largest objects in all images share the same categories, e.g., dog, car, etc. The players have some of their images in common. Their goal is to communicate through text dialogue, and individually mark 3 privately highlighted images as either common (i.e., shared with partner) or different. A full game lasts 5 rounds. After each round, some of each player's images are replaced with different ones under the same theme. Images may reappear in later rounds after being swapped out. This game setup encourages building and leveraging common ground with multimodal contexts, which humans are known to do to facilitate conversation (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996) . Fig. 1 displays an example of a PhotoBook game. 1 1 In this case, the game theme is person & bench.\nModels proposed in past works on the dataset (Haber et al., 2019; Takmaz et al., 2020) are unable to realistically play the game due to several reasons: (i) they only address subtasks in the game whose time span is one utterance, rendering it unnecessary for the models to keep track of the entire game's, or round's, progress; (ii) the models operate on additional input of reference chains, i.e., past utterances referring to each image, whose (rule-based) extraction process is imperfect and hence complicates learning and evaluation; and, (iii) utterances outside of reference chains, e.g., 'I don't have that one', may also be important pieces of information.\nTo address the drawbacks above, we propose a full (i.e., able to play real games), reference chainfree listener model, which accepts all dialogue utterances of a round 2 and the 6 context images, and predicts whether the 3 target (highlighted) images are common/different. Our listener model is based on a pretrained DeBERTa Transformer (He et al., 2021) . To incorporate visual context, CLIPScores (Hessel et al., 2021) between each utterance and the 6 given images are infused with DeBERTa hidden states. We employ CLIPScore as it offers strong prior knowledge about the relevance of an utterance to each of the 6 images, which may serve as a soft, implicit version of reference chain used in previous studies. Also, we chose DeBERTa since it is one of the top performers in the SuperGLUE benchmark (Sarlin et al., 2020) which provides a reasonablysized (\u223c100M parameters) version to suit our purpose and computation resources. We further devise a label construction scheme to create dense learning signals. Our model scores a >77% accuracy on the novel listener task and improves by >17% (absolute) over the baseline adapted from (Takmaz et al., 2020). Our code is available at github.com/ slSeanWU/photobook-full-listener.\n\nRelated Work\nIn typical collaborative dialogue tasks, two agents (i.e., players) hold incomplete or partially overlapping information and communicate through text to reach a predefined goal. The task-oriented setup enables simple evaluation for dialogue systems via task success rate, instead of resorting to costly human evaluation. Tasks and datasets proposed in the literature focus either on set logic (He et al., 2017) , image understanding (De Vries et al., 2017; Haber et al., 2019) , or spatial reasoning (Udagawa and Aizawa, 2019). They challenge dialogue systems to process multiple modalities, discard irrelevant information, and build common ground. Researchers have utilized graph neural networks (He et al., 2017 ), vision-and-language Transformers (Lu et al., 2019; Tu et al., 2021) , and pragmatic utterance generation (Frank and Goodman, 2012; Fried et al., 2021) to tackle the tasks. 3 To our knowledge, there has not been a system that fully addresses the PhotoBook task. It may be particularly challenging due to the setup with multiple highly similar images and an unbounded set of information (e.g., scene, actions) the images may contain. Previous PhotoBook works targeted two subtasks: reference resolution (Haber et al., 2019; Takmaz et al., 2020) and referring utterance generation (Takmaz et al., 2020) . The former resolves which of the 6 context images an utterance is referring to, while the latter generates an informative utterance for a pre-selected image. Pro- 2020) claimed an 85% reference resolution accuracy, but they also reported an 86% precision 6 on reference chain extraction, making it difficult to conclude whether prediction errors are due to model incompetence, or incorrect input data/labels. (We find that some parts of extracted reference chains either point to the wrong image or provide no information at all. 7 ) Yet, we do agree that keeping track of which images have been referred to is vital for the game. Therefore, we aim to build a full listener model that does not depend on explicit reference chains, but gathers such information from implicit hints given by an image-text matching model, i.e., CLIP (Radford et al., 2021) .\n\nFunctionality of CLIPScore\nBased on CLIP vision-and-language Transformer (Radford et al., 2021 ), CLIPScore (Hessel et al., 2021) is a reference-free 8 metric to measure semantic image-text similarity. On image captioning, Hessel et al. (2021) showed that CLIPScore correlates better with human judgment than referencedependent metrics like BERTScore (Zhang et al., 2019) and SPICE (Anderson et al., 2016) .\nIn our pilot study, we find that the CLIPScore of an utterance-image pair is particularly high when the utterance describes the image (see Fig. 1 for example). These score peaks thus form an implicit reference chain for the dialogue, giving strong hints on whether the mentioned images are common/different when seen with subsequent partner feedback (e.g., 'I have that one'). Also, the ref-erence chain extraction method in (Takmaz et al., 2020) achieves higher precision (86%\u219293%) and recall (60%\u219266%) when we simply replace its core scoring metrics 9 with CLIPScore. The findings above show that CLIPScore captures well the utterance-image relationships in PhotoBook, and hence should be helpful to our listener model.\nComputation-wise, reference chain extraction algorithms in the literature either rely on complex turn-level heuristics (Haber et al., 2019) , or compute multiple external metrics (i.e., BERTScore and METEOR) (Takmaz et al., 2020) . More importantly, they have to wait until completion of a round to compute the chains. Our utterance-level CLIP-Scores can be computed on the fly as utterances arrive, and are relatively time-efficient as they involve only one model (i.e., CLIP) and that batch computation may be used to increase throughput.\nModeling-wise, reference chain extraction explicitly selects which utterances the listener model should see, so when it is wrong, the model either sees something irrelevant, or misses important utterances. On the other hand, utterance-level CLIP-Scores resemble using a highlighter to mark crucial dialogue parts for the model. Even when CLIP-Scores are sometimes inaccurate, the model could still access the full dialogue to help its decisions\n\nInputs\nAn overview of our listener model is depicted in Fig. 2 . Our model operates on three types of input features, which collectively represent a game round from one of the players' perspective:\nDialogue tokens: X = {x k \u2208 W |T k | } K k=1 (1) CLIPScores: C = {c k \u2208 R 6 } K k=1 (2) Image features: V = {v j \u2208 R 512 } 6 j=1 (3)\nWe use k, j to index utterances and images respectively. W is the text token vocabulary, and T k = {t k,start , . . . , t k,end } is the corresponding token timesteps for the k th utterance. To the start of each utterance, we prepend either a [CLS] or [SEP] token to distinguish whether it comes from the player itself or the partner. All utterances are concatenated to form one text input sequence to our model. 10 CLIPScore vectors (c k 's) are computed in a per-utterance manner, i.e., between one utterance and each of the 6 images. Images are represented by the pooled 11 features from SegFormer (Xie et al., 2021) . It is trained on semantic image segmentation (Zhou et al., 2017) , and hence should encode crucial visual information for the game, i.e., objects in the scene and their spatial relationships.\n\nLabels and Output\nRather than training the model to predict just once after seeing the entire dialogue, we construct labels for all timesteps, forming a label sequence y j \u2208 L T , where T = k |T k |, for each target image, where L is the label set. As there are only 3 target images out of the 6, we also only have 3 such label sequences (y j 's) for a training instance. At each timestep t, the label of a target image, y j,t \u2208 L, is one of {undecided, common, different}. It always starts as undecided, changes to common or different at the moment of player marking action, and remains there for the rest of the dialogue. Our model's output for a (target) image j at timestep t is hence a distribution \u0177j,t \u2208 R 3 , which is a temporary belief about that image. Also, we apply causal masking on DeBERTa self-attention. Such a labeling and masking scheme creates dense learning signals-our model must judge an image at every timestep based on growing dialogue context.\n\nModel Components\nThe backbone of our model is a pretrained base DeBERTa (He et al., 2021) , which takes in concatenated utterances\nX = {x k \u2208 W |T k | } K k=1 = {x t \u2208 W} T\nt=1 , and contextualizes them into hidden states:\nEQUATION\n) where d (= 768) is DeBERTa's hidden size, and l is layer index (# layers L = 12). We do not adopt vision-and-language Transformers (Lu et al., 2019; Wang et al., 2022) for they are pretrained on 'single image-short text' pairs, which mismatches our scenario. Following Wu and Yang (2022)'s recommendation on feeding time-varying conditions to Transformers, utterance-level CLIPScores (i.e., C) are projected and summed with DeBERTa hidden states at all layers: 12 where W proj \u2208 R d\u00d76 is a learnable matrix.\nEQUATION\nTo make predictions, we place a 2-layer MLP (with GELU activation) on top of DeBERTa. It takes in the concatenation of the pooled target image features and the last-layer DeBERTa hidden state, and produces a distribution over the label set L = {undecided, common, different}:\nEQUATION\nWe add learnable positional embeddings to v j 's to make our model aware of the target image's index.\n\nExperiments and Results\nOur listener model is trained with the maximum likelihood estimation (MLE) loss function:\nEQUATION\nwhere D train is the training split, and Y is the set of label sequences associated with a data instance. The same images/themes are guaranteed not to appear in multiple dataset splits. We refer readers to Appendix A for more implementation and training details. Evaluation metric adopted here is accuracy measured at the end of dialogue, i.e., at evaluation, we ignore temporary beliefs in the chat. To set a baseline, we modify the reference resolution model in (Takmaz et al., 2020) to suit our listener task. 13 Table 1 lists the evaluation results. Our method outperforms baseline by 17\u223c20 percentage points, closing the gap to human performance by more than half. Examining the ablations, we can observe that both removing CLIPScore inputs and dense learning signals (i.e., having labels at all timesteps, see Sec. 3.2.2) cause serious accuracy degradation, indicating their essentiality in our model, and that a pretrained Transformer does not trivially beat a fully MLP-based baseline. Besides, though adding cross-attention to image features 14 (i.e., ablations a. & c.) seems to be a more intuitive way to involve visual context, it leads to more severe overfitting 15 and hence does not help in our case. We provide more detailed observations on our best-performing model's behavior and outputs in Appendix G.\n\nConclusions and Future Work\nIn this paper, we first discussed why it is difficult to deploy existing reference chain-dependent Pho-toBook models to real gameplay, and demonstrated that CLIPScore's image-text matching capability may provide implicit reference chains to the task. We then developed a novel listener model that is reference chain-free, and able to realistically play the game given text dialogue and the set of context images, just as what human players see. The model is built on a DeBERTa Transformer backbone, and brings in visual context by infusing utterance-level CLIPScores with its hidden states. On the newly proposed full listener task, i.e., predicting whether an image is shared with partner, our model achieves 77\u223c84% accuracy on unseen sets of images, surpassing baseline (Takmaz et al., 2020) by over 17 points. Ablation studies also showed that feeding CLIPScores and imposing dense learning signals are both indispensable to our model's success.\nFuture studies may leverage parameter-efficient transfer learning (He et al., 2022; Houlsby et al., 2019; Hu et al., 2022; Perez et al., 2018) to cope with image data scarcity of PhotoBook (and potentially other datasets and tasks). It is also interesting to develop a speaker model that uses temporary beliefs from our listener model and takes pragmatics (Frank and Goodman, 2012; Fried et al., 2021) into account to generate informative responses. Pairing such a model with our listener model may complete the collaborative dialogue task end-to-end.\n", "hypothesis": " Methods developed in the literature, however, cannot be deployed to real gameplay since they only tackle some subtasks of the game, and they require additional reference chains inputs, whose extraction process is imperfect.  Therefore, we propose a reference chain-free listener model that directly addresses the game's predictive task, i.e., deciding whether an image is shared with partner.", "answer": true}
{"title": "Contrastive Bootstrapping for Label Refinement", "content": "\nIntroduction\nTraditional text classification often categorize into a set of coarse-grained classes, which falls short in real-world scenarios where finer categories emerge. To this end, coarse-to-fine text classification is introduced (Mekala et al., 2021) , which performs fine-grained classification given only annotation of coarse-grained categories and the coarse-to-fine mapping. Then, it finetunes a pre-trained language model for each coarse prototype. 2 However, this two-step method could be sub-optimal. For example, it is vulnerable to the noise which is propagated and accumulated through the pipeline. Besides, it requires finetuning and saving a pre-trained language model for each coarse prototype which is heavyweight.\nTo this end, we propose a lightweight bootstrapping method based on contrastive clustering to iter- 1 Code is available at https://github.com/recorderh ou/contrastive_bootstrapping_label_refinement 2 We use prototype and category interchangeably. atively refine the labels of passages. 3 To be more specific, the method starts with an epoch of warmup on the weakly-labeled dataset. During warm-up, it pulls away negative passage-prototype pairs under the guidance of the mapping from both global and local perspectives, i.e., coarse inter-cluster and fine inter-cluster perspectives. After the warm-up, the distances between clusters are not significant which causes misclassification. Instead of continuing training on the weakly-labeled dataset which might greatly increase the noise (Figure 1 (b)), we perform a bootstrapping process which finetunes the model on the selected dataset and updates the selected dataset by the finetuned model alternately.\nTo mitigate the noise, we propose a selection strategy to identify high-quality pairs in terms of similarity and distinction. To further boost our method, we adopt a modified similarity metric from (Lample et al., 2018) and use the gloss knowledge to augment the prototype representation. As shown in (Figure 1 (c)), the resulting clusters are well separated with less noise.\nOur contributions are summarized as follows:\n\u2022 We propose a lightweight bootstrapping method based on contrastive clustering to ad-dress the problem of coarse-to-fine text classification. \u2022 Our method outperforms the state-of-the-art methods on two widely-used datasets. Further analysis verifies the effectiveness of our proposed techniques.\n\nProposed Method\nThis section describes the technical details of the proposed method, starting with the task description.\n\nTask Description\nWe follow the task definition of coarse-to-fine text classification in previous work (Mekala et al., 2021) . Given n passages {p 1 , ..., p n } with their corresponding coarse-grained labels {c 1 , ..., c n }, along with the coarse-to-fine mapping T , our goal is to assign a fine-grained label to each passage.\nThe key notations used in our paper are defined as follows: (1)\nC = {C 1 , C 2 , ..., C m } denotes the coarse prototypes. (2) F = {F 1 , F 2 , ..., F k } de- notes the fine prototypes.\n(3) T : C \u2192 F denotes the coarse-to-fine mapping, a surjective mapping which separates F into |C| non-overlapping partitions. ( 4) S pf = T (c i ) denotes the fine-grained candidate prototype of p i , which is also dubbed as p for simplicity. (5) S nf = F/S pf denotes fine prototypes not belonging to T (c i ). (6) S nc = C/c i denotes coarse prototypes in C other than c i .\n\nOur Method\nTraining Process As illustrated in Figure 2 , we start with an epoch of warm-up, during which we optimize two contrastive losses L global , L local on the weakly-labeled dataset and only the L global on the unlabeled dataset. The two contrastive losses are detailed in the following paragraphs. Then, we conduct several epochs of bootstrapping with the above model. At each bootstrapping step, we first select a small set of passages on which labels are predicted with high confidence by the model. Then, we finetune the model on the selected dataset with the same losses as warm-up. We repeat the finetuning and the selection alternately.\nInitial Weak Supervision Following previous work, we consider samples that exclusively contain the label surface name as their respective weak supervision. More details can be referred to the prior study.\n\nWeakly-labeled passages\nCandidates Unlabeled passages M \u2112 !\"#$%\" + \u2112 \"#&%\"\n\u2112 !\"#$%\"\n\nAll passages Select\nHigh-quality passages Unlabeled passages M \u2112 !\"#$%\" + \u2112 \"#&%\"\n\u2112 !\"#$%\"\n\nUpdate\nFigure 2 : Illustration of our training process.\nPassage and Prototype Representation We encode passages {p 1 , ..., p n } and all prototypes C \u222aF into the same embedding space with a pretrained language model. The resulting passage representation and prototype representation are denoted as p and l respectively. During the training process, the prototype representations are dynamically updated to fit the current passage representations. Specifically, we use the last hidden representation of [CLS] as their representations.\nSimilarity Metric Cosine similarity is often used to measure semantic similarity of embedding representations. However, in high-dimensional spaces, some \"hub\" vectors may be close to many other vectors while some other vectors are instead being isolated. For example, a passage's representation p may get high cosine with a large number of labels in S pf due to such hubness issues. In this case, a high similarity score does not necessarily lead to a high discrepancy among labels. Selecting a highlyscored label from the hub as the seed is potentially detrimental to our pairing-based method. Inspired by cross-domain similarity local scaling (Lample et al., 2018) , we adopt a modified similarity metric c(p, l) to prevent passage vectors from becoming hubs:\nc(p, l) = cos(p, l) \u2212 KN N (p) (1) KN N (p) = 1 K max l\u2208F K{cos(p, l)} (2)\nwhere KN N (.) denotes K nearest neighbors.\nWarm-up Viewing a passage as an anchor, we expect that its semantic similarity to the correct fine-grained prototype should be closer than any other fine-grained candidate prototypes. We regard the distance in the representation space as the similarity. Specifically, we optimize the following margin ranking loss:\nEQUATION\nwhere \u03b3 is a hyper-parameter denoting the margin. We use all fine candidate prototypes in S pf as positive examples and randomly sample the same number of prototypes from S nf as negative examples. We view this loss as a global loss to cluster samples according to their coarse labels (Figure 3 ).\nFor instances labeled in the initial weak supervision stage, we adopt another margin ranking loss:\nL local = max{sec_max \u2212 c(p, l) + \u03c3, 0} (4) sec_max = max l \u2032 \u2208S pf ,l \u2032 !=l c(p, l \u2032 ) (5)\nWe regard this loss as a local loss to cluster samples according to their fine-grained labels (Figure 1 (a) ).\nBootstrapping After the warm-up, representations show an inclination to form clusters. Yet, the distances between them are not significant enough to separate the classes. To further get compact clusters, we perform bootstrapping which finetunes the model on the selected dataset and updates the selected dataset by the finetuned model alternately.\nInstead of using the initial weak supervision which might greatly increase the noise as observed, we propose a selection strategy to select high-quality passage-prototype pairs. Specifically, we assign a pseudo label to each passage by their similarity (Eq.( 6)). Apart from similarity, we assume high-quality pairs should also be discriminative (Eq.( 7)):\nl = arg max l\u2208S pf c(p, l) (6) c(p, l) \u2212 max l \u2032 \u2208S pf ,l \u2032 !=l c(p, l \u2032 ) > \u03b2 (7)\nwhere \u03b2 is a threshold updated at each epoch. We construct a confident set CS with top r% pairs satisfying these two conditions. We update \u03b2 with the lowest similarity in CS. Then, we optimize Eq.( 4) and Eq.(3) on CS and the rest passages accordingly.\nGloss Knowledge Since the surface names alone can not well represent the semantics of labels, we enrich them with external semantic knowledge. To be more specific, we select the first two sentences in each surface name's first Wikipedia webpage to augment the original surface name with a predefined template (Table 3 ). We adopt the format of \"template, surface name, gloss\" and use the last hidden representation of [CLS] as their representation. Prediction It is worth noticing that applying our similarity metric c(p, l) do not change the relative ranking among labels in S pf compared with the cosine similarity. For simplicity, we use cosine similarity for prediction.\nEQUATION\n3 Experiments\nIn this section, we describe the experimental evaluation for the proposed method.\n\nDatasets and Metrics\nFor a fair comparison with prior work, we use the same hierarchical datasets used by We report both Macro-F1 and Micro-F1 for evaluation on the following two datasets.\nThe 20 Newsgroups (20News) The passages in 20News was organized into 5 coarse-grained newsgroups and 20 fine-grained newsgroups corresponding to different topics (Table 2 ). Passages in 20News were partitioned evenly across the 20 different fine-grained newsgroups. 4 Following (Mekala et al., 2021) , we omitted the 3 miscellaneous newsgroups (\"misc.forsale,\" \"talk.politics.misc\" and \"talk.religion.misc\") and expanded the abbreviation to full words.\nThe New York Times (NYT) This dataset contains 5 coarse-grained topics and 25 subtopics (Table 2). The NYT dataset is highly skewed with the coarse-grained topic \"sports\" containing more than 80% passages.\n\nMain Results\nWe compare our model with the previous work (Mekala et al., 2021) ( Wang et al., 2021b; Meng et al., 2020a) following previous works. We reproduce them using their implementation. 567 As shown in Table 1 , our method outperforms the baselines by 5.67% in Micro-F1 and 5.54% in Macro-F1 on the NYT dataset, as well as 3.97% in Micro-F1 and 3.04% in Macro-F1 on 20News dataset.\n\nAnalysis\nTo verify the effectiveness of different model components , we conduct ablation studies to test each of those.\n\nEffect of Bootstrapping\nThe \"w/o bootstrap\" results in Table 1 report the performance with warm-up only. These results are consistently lower than those with bootstrapping. Specifically, bootstrapping improves the warm-up by 3.15% Micro-F1, 7.40% Macro-F1 and 1.63% Micro-F1, 3.30% Macro-F1 on NYT and 20News respectively. \n\nEffect of Selection Strategy\nWe replace the selection strategy in bootstrapping with the initial weakly-labeled samples. From the \"w/o bootstrap\" results in Table 1 , we can see that, our selection strategy brings an improvement of 4.26% Micro-F1, 7.46% Macro-F1 on NYT. It is better to use the seed dataset on 20News. We hypothesize that this observation is because the seed dataset has a more balanced label distribution than our selected 5 https://github.com/yumeng5/LOTClass 6 https://github.com/ZihanWangKi/XClass 7 https://github.com/dheeraj7596/C2F high-quality samples on 20News. We also incorporate our selection strategy to the C2F baseline in the bootstrapping stage. As shown in Table 1 row \"C2F w/ our select,\" this strategy improves the performance of C2F by 1.43% Micro-F1, 1.17% Macro-F1 on 20News and 0.41% Micro-F1 on NYT, exhibiting the effectiveness of our strategy.\n\nEffect of Similarity Metric\nWe replace our similarity metric with the cosine similarity. From Table 1 \"w/o similarity\" we can see that, our similarity metric brings along an improvement of 3.39% in Micro-F1, 7.46% in Macro-F1 on NYT, and 16.43% in Micro-F1 and 22.46% in Macro-F1 on 20News. From Figure 4 , we can see that 63% of samples belonging to the \"Law Enforcement\" prototype are misclassified using the cosine similarity. However, 18% are misclassified using our similarity metric, verifying its effectiveness. Besides, results for \"w/ Manhattan similarity\" and \"w/ Euclidean similarity\" show that alternating cosine similarity in c(p, l) causes performance drops of 35.81% (5.53%) in Micro-F1, 40.72% (6.57%) in Macro-F1 and 50.19% (0.18%) in Micro-F1, 50.43% (0.73%) in Macro-F1 on 20News and NYT data, further proving the effectiveness of our similarity metric.\n\nEffect of Gloss Knowledge\nWe remove the gloss knowledge and use the label surface name only. Comparing the \"w/o gloss\" results in Table 1 with the full-setting ones, we observe that the gloss knowledge brings an improvement of 2.73% in Micro-F1, 9.42% in Macro-F1 on NYT and 4.86% in Micro-F1, 6.91% in Macro-F1 on 20News. Extending to the setting without coarse-to-fine mapping We extend our method to the setting without the coarse-to-fine mapping. In other words, the only supervision is the gold coarse labels. We modify L global as follows:\nEQUATION\nwhere we use the golden coarse label l c as the positive example and randomly sample one coarse label l \u2032 c from S nc as the negative example. The \"w/o fine\" results in Table 1 show that the performance does not degrade much when the association between coarse and fine-grained labels does not exist, showing the feasibility of our method in a more general setting.\n\nRelated Work\nPrevious works in weakly supervised text classification have explored different kinds of weak supervision. (1) a set of related keywords. (Mekala and Shang, 2020) augment and disambiguate the initial seed words with contextualized and highly labelindicative keywords. (Meng et al., 2020b) identify keywords for classes by querying replacements for class names using BERT and pseudo-labels the documents by heuristics with the selected keywords.\n(2) a few labeled documents. (Tang et al., 2015) represent the labeled documents and different levels of word co-occurrence information as a largescale text network. (Meng et al., 2018) propose a pseudo-document generator that leverages the seed labeld documents to generate pseudo-labeled documents for model pre-training. (3) label surface names. (Wang et al., 2021b) propose an adaptive representation learning method to obtain label and document embedding, and cluster them to pseudolabel the corpus. Our setting is different from theirs in that we use coarse-grained annotation to improve the fine-grained text classification.\nContrastive learning (He et al., 2020; Chen et al., 2020; Khosla et al., 2020) aims at learning representations by contrasting the positive pairs and negative pairs. In NLP, existing works can be primarily categorized into two distinct streams. Unsupervised contrastive learning seeks to contrast grouped or perturbed instances to generate more robust representation of unlabeled textual data (Gao et al., 2021; Wei et al., 2021; Kim et al., 2021; Wang et al., 2021a) . On the contrary, supervised contrastive learning (Suresh and Ong, 2021; Zhou et al., 2021; Yu et al., 2021; Huang et al., 2022) is label-aware and seeks to create representations for differently labeled data with more discrepancy. Our work has shown that supervised contrastive learning incorporating label names, with minimal external knowledge, improves the model's performance in label refinement.\n\nConclusion\nIn this paper, we study the task of coarse-to-fine text classification. We propose a novel contrastive clustering-based bootstrapping method to refine the label in an iterative manner. Experiments on two real-world datasets for coarse-to-fine text classification verify the effectiveness of our method. Future work could consider extending this method to other fine-grained decision-making tasks that could potentially benefit from coarse-grained labels, such as various kinds of lexical semantic typing tasks (Huang et al., 2022) . Another meaningful direction is to consider incorporating other partial-label learning techniques (Zhang et al., 2016) that are relevant to coarse-to-fine prediction tasks.\n", "hypothesis": "Experiments on NYT and 20News show that our method performs slightly worse than the state-of-the-art methods.", "answer": false}
{"title": "Do GPTs Produce Less Literal Translations?", "content": "\nIntroduction\nDespite training only on a language-modeling objective, with no explicit supervision on aligned parallel data (Briakou et al., 2023) , LLMs such as GPT-3 or PaLM (Brown et al., 2020; Chowdhery et al., 2022) achieve close to state-of-the-art translation performance under few-shot prompting (Vilar et al., 2022; Hendy et al., 2023) . Work investigating the output of these models has noted that the gains in performance are not visible when using older surface-based metrics such as BLEU (Papineni et al., 2002a) , which typically show large losses against NMT systems. This raises a question: How do these LLM translations differ qualitatively from those of traditional NMT systems?\nWe explore this question using the property of translation literalness. Machine translation systems have long been noted for their tendency to produce source He survived by the skin of his teeth .\n\nNMT\nIl a surv\u00e9cu par la peau de ses dents . GPT-3 Il a surv\u00e9cu de justesse . Table 1 : An example where GPT-3 produces a more natural (non-literal) translation of an English idiom. When word-aligning these sentences, the source word skin remains unaligned for the GPT-3 translation.\noverly-literal translations (Dankers et al., 2022b) , and we have observed anecdotally that LLMs seem less susceptible to this problem (Table 1 ). We investigate whether these observations can be validated quantitatively. First, we use measures based on word alignment and monotonicity to quantify whether LLMs produce less literal translations than NMT systems, and ground these numbers in human evaluation ( \u00a7 2). Next, we look specifically at idioms, comparing how literally they are translated under both natural and synthetic data settings ( \u00a7 3).\nOur investigations focus on the translation between English and German, Chinese, and Russian, three typologically diverse languages. Our findings are summarized as follows: (1) We find that translations from two LLMs from the GPT series of LLMs are indeed generally less literal than those of their NMT counterparts when translating out of English, and (2) that this is particularly true in the case of sentences with idiomatic expressions.\n\nQuantifying Translation Literalness\nWe compare the state-of-the-art NMT systems against the most capable publicly-accessible GPT models (at the time of writing) across measures designed to capture differences in translation literalness. We conduct both automatic metric-based as well as human evaluations. We explain the evaluation and experimental details below. for evaluation (Barrault et al., 2021) .\n\nMeasures of Quality\nWe use COMET-QE 1 (Rei et al., 2020) as the Quality Estimation (QE) measure (Fomicheva et al., 2020) to quantify the fluency and adequacy of translations. Using QE as a metric presents the advantage that it precludes the presence of any reference bias, which has been shown to be detrimental in estimating the LLM output quality in related sequence transduction tasks (Goyal et al., 2022) . On the other hand, COMET-QE as a metric suffers from an apparent blindness to copy errors (i.e., cases in which the model produces output in the source language) (He et al., 2022) . To mitigate this, we apply a language identifier (Joulin et al., 2017) on the translation output and set the translation to null if the translation language is the same as the source language. Therefore, we name this metric COMET-QE + LID.\n\nMeasures of Translation Literalness\nThere do not exist any known metrics with high correlation geared towards quantifying translation literalness.\nWe propose and consider two automatic measures at the corpus-level:\n1. Unaligned Source Words (USW): Two translations with very similar fluency and adequacy could be differentiated in terms of their literalness by computing word to word alignment between the source and the translation, then measuring the number of source words left unaligned. When controlled for quality, a less literal translation is likely to contain more unaligned source words (as suggested in Figure 1 ).\n\nTranslation Non-Monotonicity (NM):\nAnother measure of literalness is how closely the translation tracks the word order in the source. We use the non-monotonicity metric proposed in Schioppa et al. (2021) , which computes the deviation from the diagonal in the word to word alignment as the non-monotonicity measure.\n1 wmt20-comet-qe-da\nThis can also be interpreted as (normalized) alignment crossings, which has been shown to correlate with translation non-literalness (Schaeffer and Carl, 2014) .\nWe use the multilingual-BERT-based awesomealigner (Devlin et al., 2019; Dou and Neubig, 2021) to obtain the word to word alignments between the source and the translation. Table 2 presents an illustration of translations with different USW and NM scores 2 , obtained from different systems.\n\nSystems Under Evaluation\nWe experiment with the below four systems (NMT and LLMs):\n1. WMT-21-SOTA: The Facebook multilingual system (Tran et al., 2021) won the WMT-21 News Translation task (Barrault et al., 2021) , and thereby represents the strongest NMT system on the WMT'21 test sets.\n2. Microsoft-Translator: MS-Translator is one of the strongest publicly available commercial NMT systems (Raunak et al., 2022) .\n3. text-davinci-002: The text-davinci-002 model is an instruction fine-tuned model in the GPT family (Brown et al., 2020) . It represents one of the strongest publicly-accessible LLMs (Liang et al., 2022) .\n4. text-davinci-003: The text-davinci-003 model further improves upon text-davinci-002 for many tasks 3 (Liang et al., 2022) .\nFor both the GPT models, we randomly select eight samples from the corresponding WMT-21 development set, and use these in the prompt as demonstrations for obtaining all translations from GPTs.\n\nResults\nWe compare the performance of the four systems on the WMT-21 test sets. Figure 1 shows the results of this comparison. A key observation is that while the GPT based translations achieve superior COMET-QE+LID scores than Microsoft Translator across the language pairs (except En-Ru), they The NMT Systems and GPT models achieve similar COMET-QE+LID Scores (Top), there exists a significant gap in the number of unaligned source words (USW) across the datasets (Bottom). Further, GPT translations obtain higher non-monotonicity scores for E-X translations (Middle).\nalso consistently obtain considerably higher number of unaligned source words. This result holds for the comparison between the WMT-21-SOTA and GPT systems as well. Further, GPT translations also consistently show higher non-monotonicity for E\u2192X translations. However, this is not the case for translations into English, wherein the multilingual WMT-21-SOTA system obtains very close non-monotonicity measurements. The combined interpretation of these measurements suggests that GPTs do produce less literal E\u2192X translations.\n\nHuman Evaluation\nWe verify the conclusion from the results in Figure 1 by conducting a human evaluation of translation literalness on 6 WMT-22 language pairs: En-De, En-Ru, En-Zh and De-En, Ru-En, Zh-En. For each language pair, we randomly sample 100 source-translation pairs, with translations obtained from MS-Translator (a strong commercial NMT system) and text-davinci-003 (a strong commercial LLM) (Hendy et al., 2023) . We used zero-shot text-davinci-003 translations for human evaluations in order to eliminate any biases through the use of specific demonstration examples. In each case, we ask a human annotator (bilingual speaker for Zh-En, target-language native plus bilingual speaker otherwise) to annotate 100 translations from both GPT and MS-Translator and select which of the two translations is more literal. The human annotation interface is described in Appendix A. The results in Table 3 show that the annotators rate the GPT translations as less literal.\nLang Experiments on Best WMT-22 NMT Systems Further, we also experiment with the WMT-Best systems on the WMT-22 General Machine Translation task (Kocmi et al., 2022) . We evaluate USW and NM on De-En, Ja-En, En-Zh and Zh-En, since on each of these language pairs, text-davinci-003's few-shot performance is very close to that of the WMT-Best system as per COMET-22 (Rei et al., 2022) , based on the evaluation done in Hendy et al. (2023) . We report our results in Table 4 , which shows our prior findings replicated across the language pairs. For example, text-davinci-003, despite obtaining a 0.2 to 0. \n\nEffects On Figurative Compositionality\nIn this section, we explore whether the less literal nature of E\u2192X translations produced by GPT models could be leveraged to generate higher quality translations for certain inputs. We posit the phenomenon of composing the non-compositional meanings of idioms (Dankers et al., 2022a) with the meanings of the compositional constituents within a sentence as figurative compositionality. Thereby, a model exhibiting greater figurative compositionality would be able to abstract the meaning of the idiomatic expression in the source sentence and express it in the target language non-literally, either through a non-literal (paraphrased) expression of the idiom's meaning or through an equivalent idiom in the target language. Note that greater nonliteralness does not imply better figurative compositionality. Non-literalness in a translation could potentially be generated by variations in translation different from the desired figurative translation.\n\nTranslation with Idiomatic Datasets\nIn this section, we quantify the differences in the translation of sentences with idioms between traditional NMT systems and a GPT model. There do not exist any English-centric parallel corpora dedicated to sentences with idioms. Therefore, we experiment with monolingual (English) sentences with idioms. The translations are generated with the same prompt in Section 2. The datasets with natural idiomatic sentences are enumerated below:\n\u2022 MAGPIE (Haagsma et al., 2020) contains a set of sentences annotated with their idiomaticity, alongside a confidence score. We use the sentences pertaining to the news domain which are marked as idiomatic with cent percent annotator confidence (totalling 3,666 sentences).\n\u2022 EPIE (Saxena and Paul, 2020) contains idioms and example sentences demonstrating their usage. We use the sentences available for static idioms (totalling 1,046 sentences).\n\u2022 The PIE dataset (Zhou et al., 2021) contains idioms along with their usage. We randomly sample 1K sentences from the corpus.\n\nResults\nThe results are presented in Table 5 . We find that text-davinci-002 produces better quality translations than the WMT'21 SOTA system, with greater number of unaligned words as well as with higher non-monotonicity.\nFurther Analysis Note that a direct attribution of the gain in translation quality to better translation of idioms specifically is challenging. Further, similarity-based quality metrics such as COMET-QE themselves might be penalizing non-literalness, even though they are less likely to do this than surface-level metrics such as BLEU or ChrF (Papineni et al., 2002b; Popovi\u0107, 2015) . Therefore, while a natural monolingual dataset presents a useful testbed for investigating figurative compositionality abilities, an explicit comparison of figurative compositionality between the systems is very difficult. Therefore, we also conduct experiments on synthetic data, where we explicitly control the finegrained attributes of the input sentences. We do this by allocating most of the variation among the input sentences to certain constituent expressions in synthetic data generation.\n\nSynthetic Experiments\nFor our next experiments, we generate synthetic English sentences, each containing expressions of specific type(s): (i) names, (ii) random descriptive phrases, and (iii) idioms. We prompt text-davinci-002 in a zero-shot manner, asking it to generate a sentence with different instantiations of each of these types (details are in appendix B). We then translate these sentences using the different systems, in order to investigate the relative effects on our literalness metrics between systems and across types. In each of the control experiments, we translate the synthetic English sentences to German. The results are presented in Table 7 .\nResults Table 6 shows that the percentage of unaligned source words is highest in the case of idioms, followed by random descriptive phrases & named entities. The results are consistent with the hypothesis that the explored GPT models produce less literal E\u2192X translations, since named entities or descriptive phrases in a sentence would admit more literal translations as acceptable, unlike sentences with idioms. Davinci-002 obtains a much higher COMET-QE score in the case of translations of sentences with idioms, yet obtains a higher percentage of unaligned source words. Similarly, the difference in non-monotonicity scores is also considerably higher for the case of idioms. These results provide some evidence that the improved results of the GPT model, together with the lower literalness numbers, stem from correct translation of idiomatic expressions. Table 7 shows that this effect only increases with the number of idioms.\n\nDiscussion\nIn our experiments conducted across different NMT systems and GPT models, we find evidence that GPTs produce translations with greater nonliteralness for E\u2192X in general. There could be a number of potential causes for this; we list two plausible hypotheses below:\nParallel Data Bias NMT models are trained on parallel data, which often contains very literal webcollected outputs. Some of this may even be the output of previous-generation MT systems, which is highly adopted and hard to detect. In addition, even high quality target text in parallel data always contains artifacts that distinguishes it from text originally written in that language, i.e. the 'translationese' effect (Gellerstam, 2005) . These factors could likely contribute to making NMT translations comparatively more literal.\nLanguage Modeling Bias Translation capability in GPTs arises in the absence of any explicit supervision for the task during the pre-training stage. Therefore, the computational mechanism that GPTs leverage for producing translations might be different from NMT models, imparting them greater abstractive abilities. This could have some measurable manifestation in the translations produced, e.g., in the literalness of the translations.\nDifferences in E\u2192X and X\u2192E In E\u2192X, we consistently find that GPT translations of similar quality are less literal and in the X\u2192E direction, we observe a few anomalies. For X\u2192E, in Figure 1 , in all but one comparison (WMT-21-SOTA vs GPTs for De-En) GPTs obtain higher measures for non-literalness. On the other hand, we did not see anomalies in the trend for E\u2192X directions.\n\nVariations in Experimental Setup\nWe also experimented with a variant of USW and NM which doesn't use the alignments pertaining to stopwords. Each of our findings remain the same, with relatively minor changes in magnitudes but not in system rankings. Similarly, we observed a greater tendency towards less literalness in GPT translations in both few-shot and zero-shot settings, when compared across a range of NMT systems.\n\nSummary and Conclusion\nWe investigated how the translations obtained through LLMs from the GPT family are qualitatively different by quantifying the property of translation literalness. We find that for E\u2192X translations, there is a greater tendency towards nonliteralness in GPT translations. In particular, this tendency becomes evident in GPT systems' ability to figuratively translate idioms.\n", "hypothesis": " However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.  In this work, we investigate these differences in terms of the literalness of translations produced by the two systems.", "answer": true}
{"title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models", "content": "\nIntroduction\nThe introduction of the Transformer architecture (Vaswani et al., 2017) has led to a performance boost in language modeling (see, e.g., Brown et al. 2020) , but also to a steep increase of computational cost, as the number of parameters and data points is constantly growing. In reaction to this development, there has recently been a surge in work on retrieval-augmented language models (Izacard and Grave, 2021a; Li et al., 2022) , which shows that enabling models to retrieve context from large corpora results in lower perplexity and better accuracy in downstream tasks such as question answering, while at the same time using considerably fewer parameters. In this paper, we specifically focus on the Retrieval-Enhanced Transformer architecture (RETRO; Borgeaud et al., 2022) .\nBy augmenting a language model with a retrieval mechanism, RETRO, like similar architectures, tries to decouple memorization of the training data from the additional generalization that * Correspondence to ehsan.doostmohammadi@liu.se.\ncomes with increasing the number of parameters. In RETRO, when a chunk of text (a sequence of tokens) has been generated, a dense representation of this chunk is used to retrieve the most similar neighboring chunks from a large retrieval set, based on their L2 distance. Having the previously generated chunks and their nearest neighbors in the retrieval set, the auto-regressive language model has now access to an extended context when predicting the next chunk. The informativeness of this context depends on the effectiveness of the retrieval method. Borgeaud et al. (2022) note that part of RETRO's performance can be attributed to the token overlap between the generated chunks and the retrieval set. Our starting point in this paper is the observation that the performance gain is actually better explained by such surface-level similarities than by the L2 distance between the dense representations that RETRO uses for retrieval. This is in line with recent work by Norlund et al. (2023) , who show that the reduction in loss observed in RETRO \"almost exclusively\" stems from such overlap rather than more sophisticated generalization. Based on these findings, we replace the semantic retrieval method in RETRO with one based on BM25 (Robertson et al., 1995) , a surface-level measure. Our results show that retrieving nearest neighbors using BM25 during inference leads to a 13.6% lower perplexity, compared to dense retrieval based on sentence transformers (ST) (Reimers and Gurevych, 2019) , a model trained to represent the semantic similarity between sentences. 1 Finding the exact neighbors with BM25 is costly on large retrieval sets and might not meet the speed requirements of all applications of retrievalaugmented language models. We therefore explore a hybrid approach where we first retrieve approximate neighbors using ST representations and then re-rank them using BM25. We show that this approach yields 24.7% of the perplexity reduction we get with BM25-based retrieval, with only minimal computational overhead.\n\nMethod\nWe experiment with RETRO (Borgeaud et al., 2022) as a state-of-the-art retrieval-augmented language model.\n\nModel\nRETRO is very similar to a standard auto-regressive language model such as T5 (Raffel et al., 2020) , the main differences being the introduction of the retrieval mechanism and how the retrieved neighbors are used for language modeling.\nNearest Neighbor Retrieval In RETRO, all textual data is stored and used in chunks of 64 tokens. When the model has generated a chunk C u , it retrieves the k nearest neighbors N 1:k to that chunk, together with the chunks F 1:k following these neighbor chunks in the retrieval data. It then generates the next chunk C u+1 conditioned on the retrieved chunk pairs. Retrieval uses the squared L2 distance on a dense representation (DR) of chunks:\nd(C u , N i ) = \u2225DR(C u ) \u2212 DR(N i )\u2225 2 2 This leaves us with RET(C u ) = ([N 1 u ; F 1 u ], . . . , [N k u ; F k u ])\nas the retrieved neighbors that the model receives as additional context when generating the next chunk. The likelihood of the first chunk (C 1 ) does not depend on any neighbors; the model has access to no external context when generating that chunk. During training and perplexity evaluation, the retrieval process is filtered such that chunks originating from the same source document as the training sequence are never considered as neighbors.\nIntegration of the Neighbors RETRO improves auto-regressive language modeling by conditioning the next token prediction on the retrieved chunks of text. This means that the probability of generating the next token x t+1 depends not only on the previously generated tokens x 1:t but also on the retrieved neighbors of the previously generated chunks, as well as their following chunks:\nP (x t+1 | x 1:t , RET(C 1 ), . . . , RET(C u\u22121 ); \u03b8)\nWhen generating the next token, the neighbors as well as the current chunk C u are passed through a Transformer encoder. In the decoder, crossattention is over the output of that encoder and the concatenation of the intermediary embeddings of the last few tokens in the previous chunk C u\u22121 and the already generated tokens in C u , a mechanism called chunked cross-attention. For more details, see Borgeaud et al. (2022) .\nImplementation Details As an official implementation of RETRO is not publicly available, we draw upon the implementation in Norlund et al. (2023) , which is based on the description in Borgeaud et al. (2022) . Our implementation deviates only in that (1) we use learnable relative positional biases as in T5 (Raffel et al., 2020) , with a bucket for each unique relative position; (2) instead of BERT (Devlin et al., 2019) , we use the pretrained sentence transformers (ST) (Reimers and Gurevych, 2019) model to embed the chunks for the offline retrieval. ST is preferable over BERT, as it is trained for the task of similarity search, and produces embeddings of lower dimensionality, which makes it more efficient. We use PyTorch (Paszke et al., 2019) and PyTorch Lightning for distributed training. For the tokenization, we use the pre-trained T5 tokenizer (HuggingFace). For retrieving approximate neighbors, we use faiss (Johnson et al., 2019) , which performs efficient similarity search between dense representations with GPU support for faster indexing and retrieval.\n\nData\nBorgeaud et al. ( 2022) use the MassiveText dataset (Rae et al., 2021) for both training and retrieval. As this dataset is not publicly available, we set out to replicate it using open sources. MassiveText consists of multilingual text data in five categories: Wikipedia articles, books, GitHub code, news, and common crawl web data. We use Pile (Gao et al., 2021) and RealNews (Zellers et al., 2019) to build a large dataset resembling MassiveText's composition. The new dataset (see Norlund et al. (2023) for details) consists of 36M documents containing 52B tokens. For Pile, we keep the training and validation splits, while for RealNews, we use the full training set but downsample the validation set to 16,400 news articles to match the proportions of the categories in Pile. For details on the deduplication process, we refer to Gao et al. (2021) and Zellers et al. (2019) .\n\nTraining\nWe use our dataset to train a RETRO model with approximately 630M parameters. For more details refer to Norlund et al. (2023) . During training, we retrieve from the training set; during validation, we retrieve from the union of the training and validation sets. We train the model on sequences truncated to 1,024 tokens. The chunk size is 64, as in Borgeaud et al. (2022) , and the number of retrieved neighbors is k = 2 for training and validation. We train the model for 140k training steps with a batch size of 16, taking seven days on 16 A100 GPUs. This means that we use 6% of the training data during training, not including the retrieved neighbors. As our optimizer, we use Adam (Kingma and Ba, 2015) with a fixed learning rate of 1e\u22124.\n\nA Study on Correlations\nWe experiment with two settings: RETRO[ON], the language model with retrieval enabled, and RETRO [OFF] , where there are no chunk crossattention layers and therefore no retrieval, leaving us with a decoder-only language model. As shown by Borgeaud et al. (2022) , the RETRO[ON] model performs better when it can exploit an overlap between the generated text and the retrieved neighbor. This is more apparent in text categories with higher token overlap, such as GitHub. The studies in the RETRO paper also show that allowing more overlap when deduplicating the data results in a lower bits-per-byte (BPB 2 ). Norlund et al. (2023) take this further to show even minimal overlap results in significant loss reduction, demonstrating the large extent RETRO relies on surface-level similarities. These findings lead us to hypothesize that having a retrieval method that can find the highest overlapping neighbors will yield lower perplexity (PPL). Because BERT, ST and similar deep representations of sentences do not always capture surfacelevel similarities, we set out to investigate where performance gains come from.\nTo this end, we measure how the PPL difference (\u2206PPL) between RETRO[ON] and RETRO[OFF] for the current chunk (C u , u \u2265 2) correlates with (1) squared L2 distance between the ST embeddings of C u and RET(C u\u22121 ) (ST), and ( 2 \n\nChanging the Retrieval Method\nAs the results from the previous section show a stronger correlation between performance gain and surface-level similarity than ST similarity, we experiment with a retrieval method based on BM25.\n\nBM25\nOkapi BM25, introduced by Robertson et al. (1995) , is a bag-of-words retrieval method based on tf-idf scores and some free parameters. These parameters are k 1 , which normalizes the term frequency, and b, which controls how much the length of a document would affect the term frequency values. We use Pyserini (Lin et al., 2021) , a Python interface to Lucene's BM25 implementation. We build the BM25 index on the training set and leave the free parameters at their default values (k 1 = 0.9, b = 0.4). These values were also shown to perform the best by Karpukhin et al. (2020a) . Using Lucene's Analyzer pipeline 3 results in more than 50M unique words for our corpus. We instead use the T5 tokenizer from Hugging Face Transformers (Wolf et al., 2020) and limit our vocabulary to 32k words for the reranking experiments.\n\nRetrieving with BM25\nWe use the model described in Section 2.3 and change the retrieval method only at inference time to retrieve better neighbors. The results can be found in 2022), we also report BPB. The results show that using neighbors with more surface-level similarity to the generated chunk is a solid method for leveraging the retrieval mechanism to reduce the perplexity. If the retrieval augmentation is meant to act as an external memory, or to offload memorization from the model (Borgeaud et al., 2022) , then BM25 is a more suitable method to achieve this goal.\n\nReranking\nWhile the performance gain is significant, finding the exact neighbors using BM25 could be costly, depending on the size of the datasets. On the other hand, faiss provides an efficient similarity search for dense vectors to find the approximate neighbors. Therefore, if enough of the BM25-retrieved neighbors could be found among top-k faiss-retrieved ones, with an efficient reranking, we could expect at least part of the performance gain with minimal computational overhead, as long as k is not significantly large. To find an optimal k, we first need to know how many of BM25 neighbors could be found in top-k faiss-retrieved chunks.\nLooking at the faiss-retrieved neighbors, we see that of top-4 BM25-retrieved neighbors, 17.6% appear in top-100 faiss-retrieved chunks, while the overlap is 22.1% for top-1000. We decide to continue our experiment with top-1000 neighbors, but it is obvious that one could get an even higher overlap with a higher k, with diminishing returns. The results in Table 2 show that with the proposed reranking, RETRO[ON]-ST could achieve 21.3% of the PPL reduction of RETRO[ON]-BM25 compared to RETRO[ON]-ST. The reranking results are interesting not only due to their practical implications but also as an analysis revealing the limited number of high-quality neighbors that can be retrieved using semantic retrieval, even in situations where a large k is feasible.\n\nRelated Work\nAugmenting language models with mechanisms that help them incorporate larger contexts has been approached extensively in different forms, such as Guu et al. (2018) 's retrieve-and-edit approach to reduce the PPL in language generation, and Asai et al. (2020) that make use of lexical overlap to improve the performance in question answering. While retrieval-augmentation has been used with different objectives in mind, such as language modeling (Khandelwal et al., 2020; Wu et al., 2022) and machine translation (Khandelwal et al., 2021) , question answering has been the application to attract the most interest (Guu et al., 2020; Karpukhin et al., 2020b; Izacard and Grave, 2021b ).\nAn extensive study was performed by Izacard et al. (2022) , showing that while we get performance gains using retrieval augmentation, training the retrieval part of the model would yield even more benefits. RETRO (Borgeaud et al., 2022) , on the other hand, aims at scaling such language models and therefore opts for keeping the retriever frozen, showing substantial PPL reduction with increasing either the number of language model parameters or the size of retrieval set.\nAmong the more recent work, Xu et al. ( 2023) found that training using approximate neighbors resulted in a 2.6% decrease in perplexity. This suggests that non-exact neighbors may have a regularization effect, leading to improved generalization ability. Additionally, Ram et al. (2023) report a drop in perplexity using BM25 over BERT retrieval using in-context retrieval-augmented language models.\n\nConclusions and Future Work\nIn this paper, we study the source of performance gains in RETRO, which could be generalized to similar retrieval-augmented language models. After observing that the PPL drop correlates more strongly with surface-level overlap between the query and the retrieved text, we replace the retrieval method with BM25, and observe a significant drop in PPL, which confirms us in the findings of the correlation study. This is also an interesting insight as to how these models work, which could be lever-aged for performance gain in tasks like question answering where model relies on retrieving facts. In the end, we also conduct an analysis to find out how much BM25 neighbors overlap with those retrieved using ST. The results show that while faiss is able to find some of the neighbors with high token overlap, the majority of them remain unretrieved. This is however, enough to gain part of the loss reduction achieved with a pure BM25 retrieval system.\nThe proposed methods could also be used during training. By retrieving more overlapping neighbors during training, the process of guiding the model to use retrieved neighbors for language modeling could be done more efficiently. This is particularly relevant when augmenting an already trained language model with a retrieval mechanism. As reported by Borgeaud et al. (2022) , retrieval augmentation results in a larger drop in BPB as the number of model parameters and the size of retrieval data grow. This calls for more efficient methods based on surface-level similarities if we wish to exploit this potential. Furthermore, the retrieval system in RETRO is based on semantic retrieval, the model seems to rely more on surface-level similarities. This could affect the generalizability capabilities of such models, which necessitates further investigations. Lastly, we only evaluate our modified RETRO model on language modeling. It would be interesting to know the impacts of BM25 retrieval on downstream tasks where retrieval is of use.\n", "hypothesis": "As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining the full perplexity reduction with minimal computational overhead.", "answer": false}
{"title": "Race, Gender, and Age Biases in Biomedical Masked Language Models", "content": "\nIntroduction\nSocial biases based on race, gender, and age cause healthcare disparities. Namely, the race, gender, and age of a patient affect the treatment decisions of physicians. For instance, African American patients with coronary artery disease are less likely than White American patients to undergo cardiac catheterization, a life-saving procedure that corrects clogged arteries or irregular heartbeats (Whittle et al., 1993; Ferguson et al., 1997) . Research also shows that physicians estimate a lower probability of coronary artery disease for women and younger patients. Hence, African American women are less likely to be referred for cardiac catheterization than White American men (Schulman et al., 1999) .\nIn an attempt to identify and eliminate healthcare disparities, implicit bias has been studied in-depth in real-world patient-provider interactions in both the emergency department (Dehon et al., 2017) and medical assessment of physicians on computersimulated patients (Hirsh et al., 2015) . Despite such efforts, these stereotypes continue to prevail Following the recent releases and success of pretrained models in various domains, researchers introduced pre-trained models trained on large-scale biomedical corpora (Beltagy et al., 2019; Lee et al., 2019; Li et al., 2022) . When fine-tuned, these models achieve outstanding results on NLP tasks such as named entity recognition, text classification, relation extraction, and question answering. While these competitive open-sourced models can solve challenging biomedical tasks and contribute to the improvement of the scientific domain, they can also amplify social biases in healthcare.\nTo identify such stereotypes, we examine social biases existing in the biomedical pre-trained models. We define bias as a tendency to associate a particular group with an illness in generated sentences and examine, given a bias, with which illness a model associates more. First, prompts are manually curated based on evidence-based practice. Then, the models fill in the masked prompts. We observe the words pertinent to illness, such as \"cancer\" and \"diabetes.\" Lastly, a case study of the biases in coronary artery disease diagnoses and treatments is undertaken.\nIn summary, our contributions are: (1) We in-vestigate biases in biomedical masked language models with manually curated prompts. The experimental results show that BERT is less biased than the biomedical models in race and age and that each model associates distinct illnesses with a patient regardless of the bias.\n(2) We study whether the models associate a specific illness and a treatment with a particular bias. We use two bias metrics and demonstrate the challenges in measuring bias.\n\nMethod\nWe investigate the influences of biases on the biomedical pre-trained language models by identifying associations between generated tokens and biased terms. First, we curate prompts grounded on evidence-based medicine. Next, we compare the diagnosis predictions of a model based on race, gender, and age biases.\n\nPrompt Curation\nWe manually curate prompts for diagnosis prediction of pre-trained models. agnosis] .\" An exemplary sentence is \"A woman is diagnosed with pneumonia.\" We mask the [Diagnosis] to observe the differences in generated tokens of each model. In the provided example, the word \"pneumonia\" is masked. Nouns and pronouns that identify race, gender, and age bias fill the [Bias] section of the sentence. For example, to reflect the age bias, we choose the words \"a young person\" and \"a junior\" to represent the younger age group and the words \"an old person\" and \"a senior\" for the older age group. We use the word \"person\" to avoid the influences of gender-specific words such as \"woman\" and \"man.\" As for gender-biased words, we adopt the binary classification of gender and use gender-specific pronouns and nouns. Finally, we use the five minimum categories of race set by the OMB to choose words that reflect racial bias 1 : White American, African/Black American, American Indian, Asian, and Native Hawaiian. The full list of the chosen nouns can be found in Ap-pendix A.\n\nDiagnosis Prediction\nGiven a prompt, a pre-trained model generates tokens to fill in the mask with scores. We sum the scores of each token in all the prompts of a given bias. For comparison, we explore the following biomedical pre-trained models:\n\u2022 BioBERT (Lee et al., 2019) is a BERT (Devlin et al., 2019) trained on PubMed abstracts with 4.5 billion words and PubMed Central full-text articles with 13.5 billion words.\n\u2022 ClinicalBERT (Alsentzer et al., 2019) is BioBERT (Lee et al., 2019) trained on approximately 2 million clinical texts from the MIMIC-III v1.4 database (Johnson et al., 2016) .\n\u2022 Clinical-Longformer (Beltagy et al., 2020) is Longformer (Beltagy et al., 2020) trained for 200,000 steps with batch size of 6 \u00d7 3 on 2 million clinical notes extracted from the MIMIC-III dataset.\nAs a baseline, we compare these models to a pre-trained BERT (Devlin et al., 2019) . See Appendix D for the details of the implementation.\n\nExperimental Results\nWe compare the prediction results among biomedical language models (LMs) and analyze the association between illnesses and biases. As shown in Table 1 , the top 3 diagnosis predictions of each model show high overlaps across different biases. BioBERT predicts \"malaria\" as the top 1 diagnosis and \"cancer\" as the top 3 for both the young and old age groups. As for racial biases, \"malaria,\" again, has the highest prediction score across races, and \"tuberculosis\" scores second for African American, American Indian, and Asian and scores third for the other two races. (See Appendix B for the figures that compare the percentage of top 7 diagnoses.)\nTo better quantify overlaps within biases, we measure the text overlap scores of each model, and the results are shown in Table 2 . The text overlap scores are computed by first counting the number of matching words and then normalizing the counts to a value between 0 and 1. For normalization, we 3,  4 and 5 .\nThe text overlap scores of all models in Table 2 are above 0.5, implying high overlaps in predictions within biases. As for the scores among races, Tables 3, 4 and 5 also display scores above 0.5. An exception is the overlap score between Asian and Native Hawaiian in Table 3 , which is 0.5. Although the prediction scores of diagnoses vary across biases, the models generate similar tokens regardless of a given biased term. This result implies a weak association between illnesses and biases in biomed- ical LMs.\nW B I A H W 0.\nAn interesting observation is that the three biomedical models, BioBERT, ClninicalBERT, and Clinical Longformer display the highest overlap scores in the gender bias and the lowest in the racial bias. On the contrary, the baseline BERT exhibits an opposite result: the gender bias has the least overlapping tokens. We infer that biomedical models are less likely to predict different diagnoses based on gender than BERT.\nFinally, each model reveals a different tendency to predict an illness of a given patient. BioBERT predicts \"malaria\" with the highest scores across all biases except for the male bias. ClinicalBERT generates \"pneumonia\" most times except for Asians. As for Clinical Longformer, the top 1 diagnosis is \"cancer\" for age and gender biases and \"diabetes\" for racial bias. This observation suggests that each model associates a specific illness to all patients irrespective of bias and that a model choice determines the prediction of diagnosis.\nCase Study. We study whether a welldocumented association between biases and the use of cardiovascular procedures is observed in the biomedical models (Schulman et al., 1999; Chen et al., 2001) . In particular, we look into two correlations: (1) the physicians assume that females and the young are less likely to have coronary artery disease than males and the old, respectively; (2) females and African Americans are less likely to receive cardiac catheterization than males and White Americans, respectively.\nTo identify those biased correlations in the models, we perform two experiments. First, we curate prompts and measure the token scores of mask prediction, which we denote as M-scores. Second, the bias metrics in CrowS-Pairs (CP) (Nangia et al., 2020 ) are adopted. We create a pair of stereotypical and anti-stereotypical sentences S, mask one unmodified token u i \u2208 U at a time, and compute pseudo-log-likelihoods: score(S) = |C| i=0 log P (u i \u2208 U |U \\u i , M, \u03b8), where U = {u 0 , ..., u l } are unmodified tokens and M = {m 0 , ..., m n } are modified tokens in a sentence S. The details of the experiments can be found in Appendix C.\nFirst, we examine the correlation between gender/age and coronary artery disease. As shown in Table 6 , the female and the young have lower CP bias scores than the male and the old, respectively. This result aligns with the first correlation in clinical practice. In contrast, the M-scores of the male and the old are lower. Namely, the models are less likely to generate male-and old-biased words in a sentence with coronary artery disease.\nTable 7 show the experimental results on the correlation between gender/race and the use of cardiac catheterization. The CP scores of the male and White American are lower than the female and African American, respectively. Once more, the M-score results are the opposite; the female and African American have lower M-scores.\nM-scores and CP scores exhibit contrary results for the two experiments on the correlations. In the first experiment, the CP score results demonstrate a higher association between male/old patients and coronary artery disease, proving the first correlation manifested in the biomedical models. However, the M-scores reveal an opposing association, overturning the first correlation. In the second experiment, the M-scores align with the second correlation, while the CP scores do not. These results signify the importance of using more than one metric to measure bias and the challenges of measuring bias in LMs.\nLimitations. In this study, the prediction scores of generated tokens are aggregated to determine the rankings of diagnosis in Table 1 and Figures 2,  3, and 4 . We choose this summation metric because bias as defined in this paper is a tendency to associate a particular group with an illness in generated sentences. However, we acknowledge the limitations of aggregated scores in reflecting comprehensive model behaviors for different subpopulations (Blodgett et al., 2020) .\nIn addition, we recognize that the change in prompts can affect experimental results. For our experiments, prompts based on PICO were curated and used to examine the association between illnesses and biases. Yet a choice of a prompt greatly affects the performance of a model (Liu et al., 2023) . Hence, if different prompts are adopted, the experimental results can differ.\nFinally, our definition of bias in biomedical models is based on papers that study the effects of bias on healthcare outcomes (Blair et al., 2011; Hall et al., 2015) . We are not claiming that statistical differences in health conditions based on race, gender, or age are not meaningful. Yet studies show that patients with the same health conditions get different treatments due to a healthcare provider's (implicit) bias (Green et al., 2007; Sabin and Greenwald, 2012) . A perfect dissociation between race, gender, or age and a patient's health conditions is impossible. Still, to study bias as explicitly defined for this work, we design prompts that provide a patient's race, gender, or age, not their health conditions and question whether the biomedical models are affected by the given information.\n\nConclusion\nWe explore whether biases in clinical practice are reflected in pre-trained biomedical LMs. The tendency in diagnosis predictions of the models is analyzed, and the overlaps in the predictions across biases are compared. As a case study, we measure bias in associating coronary artery disease with gender/age and cardiovascular procedures with gen- der/race. Our study indicates the impact of a model choice on diagnosis predictions and the difficulties in measuring biases.\n", "hypothesis": " These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora.  To bring awareness to such repercussions, we examine social biases present in the biomedical masked language models.", "answer": true}
{"title": "Automatic Named Entity Obfuscation in Speech", "content": "\nIntroduction\nPrivacy concerns, particularly where an individual could be identified, preclude sharing and therefore automatic exploitation of many data sources. Anonymization, the removal of identifying information, has been automated for text (Lison et al., 2021) , including large scale applications such as in clinical (Hartman et al., 2020) or legal settings (Oksanen et al., 2022) , with off-the-shelf systems having reported performance of 90+% (Hartman et al., 2020) . To minimize the risk of re-identification, obfuscation -replacing identifying information with a different substitute of the same type -has been explored as an alternative to replacing identifying information with a generic marker (Sousa and Kern, 2022) . The main focus in speech has been on voice anonymization, which may not be a problem with speaker consent, with the removal of identifying information receiving less attention. To our knowledge, this is the first prototype to perform named entity obfuscation directly, in the original speaker's voice. Aside from voice cloning, it explores a named entity recognition approach based directly on audio signal and uses language model masking to find appropriate substitutions.\nRecent advances in speech models, particularly the inclusion of language models within the speech model itself (e.g. HuBERT (Hsu et al., 2021) ) gives models greater insight into expected contexts. Previous work on named entity recognition (NER) in speech frequently employs a two step approach, transcribing speech first, followed by the application of existing named entity techniques (Yadav et al., 2020) . However, this process has the potential to compound errors as errors in transcription will increase the probability of error in NER. We suggest that the addition of language models into the speech model gives these sufficient power to perform NER directly, and therefore that transcribing (automatic speech recognition, ASR) and NER can be separated, and used to provide a confidence measure in their performance. Divided, the two do not propagate errors in the same way; in fact, treating ASR and NER separately allows one to fix (some of the) errors of the other. The proposed second (final) ASR pass merely produces a confidence value in the result to decide whether a manual check should be performed.\nThe success of few shot learning, where a limited number of examples is used to generalize a pre-trained deep learning model to a new situation, for text-to-speech -and specifically voice cloning (Zhang and Lin, 2022) -enables an alternative, equivalent but different, entity to be inserted in the audio signal in place of the original while preserving the prosody information throughout. While large databases of potential replacement entities can be used to select a substitution, these may not preserve necessary properties (such as gender). Al-ternatively, word embeddings have been used to suggest close (in the multi-dimensional space) alternatives (Abdalla et al., 2020) , however these can suffer from the same drawback. We propose using a more contextualized alternative to word embeddings, a masked language model (Devlin et al., 2019) , where the model is trained by hiding (masking) words and predictions of the original word are made based on their context. This work makes the following contributions: (1) a complete obfuscation pipeline for names in speech 1 , (2) a named entity recognizer built directly on speech without requiring text transcription first, (3) alternative (obfuscated) entity replacement selection via masking language model, and (4) confidence annotated system output, allowing for manual correction and / or selection of shareable instances. Section 2 contains the methodology with results in Section 3. Section 4 presents the conclusions and future work.\n\nMethodology\nThe steps of the overall pipeline, which takes in an audio file and produces an obfuscated audio file along with a confidence value, can be found in Figure 1 . The approach comprises of three main parts: 1) identification of named entities (NEs) in the audio, 2) finding an equivalent alternative for the original NEs, and 3) reconstructing the original audio to incorporate the replacement NEs. The reconstructed audio can further be used to obtain a confidence value.\n\nIdentification of named entities\nTo enable the direct use of a language model on speech input for the purpose of named entity recognition (NER), a dataset of audio recordings with annotated NEs is required. The English speech NER dataset (Yadav et al., 2020) , which consists of 70,769 waveforms with transcripts annotated with person, location and organization NEs, is used for fine-tuning the Hidden-Unit BERT speech model (HuBERT) (Hsu et al., 2021) . HuBERT was selected over other speech models since it learns both accoustic and language models from its inputs and therefore has an increased awareness of context. The success of language models on text NER has demonstrated how crucial context is for this task, and using a model which incorporates both an acoustic and a language model (over acoustic only) allows the approach to exploit the information used in text NER, while managing to avoid the need for a transcript.\nFor training, NE annotations need to be converted to a suitable format, indicating the presence or absence of a NE in each position. Following the inside-outside(-beginning) chunking common to many NER approaches (Tjong Kim Sang and De Meulder, 2003) , three formats were explored: 1) character level annotation, mapping each character to either o for a character outside of a named entity, space, or n, l, e for characters within person, location or organization entities respectively, 2) the same character level annotation with separate characters added to denote the beginning of each type of NE (mapping the sentence TELL JACK to oooo mnnn with m denoting the start of a person NE), 3) and, for completeness, annotation was also explored at word level.\nWith the training parameters shown in Appendix A.1, the best NE performance was obtained from the first annotation approach, where NE beginnings were not explicitly annotated. The lower performance of the second annotation approach can be attributed to the low quantity of training data for the beginning marker annotations. While word level annotation was explored, it is likely to need a far greater quantity of data to enable mapping of different length inputs to a single label.\nSeparately, HuBERT was also fine-tuned for automatic speech recognition (ASR), i.e. for transcribing text from audio. Identical training data was used, with annotation being the transcription provided as part of the NE annotation (with NE annotation removed). The same parameters were employed for its training. Alongside the predicted (NE or ASR) annotation, prediction output also yields an offset which can be converted to a time offset. This can be used to identify the position of the NE(s) to be replaced, and after a greedy alignment of the two outputs, the original transcription of the original NE(s) can be extracted.\n\nFinding an alternative NE\nOnce a person NE is identified, a suitable equivalent substitution needs to be obtained, i.e. we want to find the word which could replace the NE in the text if the NE was hidden. This is precisely the concept behind masked language models (MLMs): these models learn their weights so that given a sentence with a hidden (masked) word, the model will output the complete original sentence. The (ASR extracted) original sentences with NEs (as identified by the NE tuned model) masked were passed to a MLM. Three MLM models were explored: BERT, bert-large-uncased model (Devlin et al., 2019) , ALBERT, albert-xxlarge-v2, model (Lan et al., 2019) and the distilled RoBERTa base, distilroberta-base, model (Sanh et al., 2019) . Each model, with no additional tuning, results in a (pre-specified) number of predictions for each NE in the sentence. Since the models used different datasets in training, their predictions are expected to be different: for example, some may suggest pronouns rather than names.\nGiven the propensity of the MLM to return substitutions which are not names (for example, for the sentence you should call Stella, the model returns you should call him, you should call them, you should call 911 etc), an external list of people names is used for the validation of the proposed suggestions 2 and the highest scoring substitution is returned. Heuristically, the original name is matched against the list to identify whether it is a first or a last name (where possible) and names of the same type suggested by the MLM are returned. Simple rules are employed (last of a sequence of names is a last name, a single name without a title is a first name etc) to decide on a substitution when the original name does not appear in either the first or last name list. Given the nature of MLMs, suggested alternatives are likely to be more common words: as a positive side effect, this should make them easier to render with voice cloning as they may already appear in the reference speech. Should MLM fail to propose any suitable substitutions, one is selected at random from the first & last name lists, subject to the same heuristic rules.\n\nReconstruction of original audio\nIn this work, the substitute NE is to be re-inserted into the original audio. To reduce the risk of de-identification via the extraction of entities which failed to be identified and therefore stayed in their original form, the substitute entity needs to be produced in the speaker's voice. The YourTTS (Casanova et al., 2021) model, which offers the ability for fine-tuning with less than one minute of speech while achieving good results with reasonable quality, can be used to generate the substitute sentence with all available speech of the speaker provided as reference. Note that it is not necessary to remove the original sentence from the reference data: in fact, its presence may result in more accurate rendering of the substitute sentence. The pre-trained model used in this work (tts_models/multilingual/multi-dataset/your_tts) was trained on the the voice cloning toolkit (VCTK) dataset (Yamagishi et al., 2019) which contains approximately 400 sentence, selected from newspaper text, uttered by 108-110 different speakers, giving it its generalization power. Aside from the reference passed to the model on the command line, no tuning or training of the YourTTS model is done in this work.\nThe ASR transcribed text with the substituted NE is generated, rather than the substitution alone, to ensure that the intonation as closely matches the substitution's position in the sentence. The average amplitude of the generated audio is matched to that of the original segment using the Python pydub library. The generated audio is again pased through the HuBERT based NE recognizer, to identify the location of the substituted NE in the generated audio and allow its extraction (note that in this pass, it is not necessary to perform ASR -only the offsets of the replacement NE are required). Should the NE recognizer not identify the same number of NEs as were present in the original, the instance is flagged for manual review.\nFor each NE in the text, a pair of start and end offsets are available: one pair extracted by the Hu-BERT based NE extraction from the original audio and a second pair from the audio generated from the substituted text. This allows the new NEs to be inserted in place of the original NEs. The splicing and concatenation of the waveforms is also performed using the pydub library.\nA second HuBERT based ASR pass over the newly constructed (substituted) audio, and its comparison against the substituted text using word error rate (WER) and character error rate (CER) gives measures of confidence. Both the metrics, commonly used for evaluation of ASR, allow for sequences of different length to the target -the further the reconstructed audio is from the target sentence, the less likely it is that the substitution will go unnoticed. For the purpose of the demonstrating the viability of the prototype, no hyperparameter optimization was performed, and the larger HuBERT models were not employed, however improvement in performance of both models are expected should this be pursued.\n\nFinding an alternative NE\nA small scale evaluation is performed on a sample of 20 sentences selected at random from the Lib-riSpeech corpus (Panayotov et al., 2015) across 6 speakers. Sentence selection was subject to them containing a person named entity. While detailed results for the individual steps can be found in Table 2, it should be noted that -for the purposes of this work -the focus is the accuracy of the extraction of the correct NE. The stated accuracy is therefore somewhat misleading: in a number of cases, such as the word Raphael, the named entity is divided into two separate words, suggesting two consecutive named entities. However, this issue is corrected when the NE output is aligned with ASR output and the two separate NE instances are (correctly) merged. Cases with NEs which cannot be aligned are flagged up for manual intervention. The average ASR and (exact match) NE identification do not vary when a different MLM is employed, as this only effects the selection of the substituted name, resulting in different average confidence values.\n\nReconstruction of original audio\nThe voice cloning model requires some reference audio for the speaker: for the 6 selected speakers, 4 have less than 5 audio files (two having 3, and one having only 2 files) in the dataset. The quantity of data used as reference is likely to impact the quality (in terms of its similarity to the original speaker) of the generated text. Given the likely scenarios of deployment, such as dialogues where more than 2 sentences of speech per speaker are available, this may not be representative of the results obtainable with the pipeline. However, it should be noted that even if all substituted instances can be identified as substitutions, the system is equal to a masking technique (where an entity is replaced with a fixed entity, such as a bleep).\n\nConclusion\nThe prototype described shows the steps of an obfuscation pipeline for speech, which results in substituted person named entities uttered in the original speakers voice and replaced in the original audio signal. The prototype makes use of a named entity recognizer built directly on top of audio input, and employs masked language models to generate the substituted entity. It offers an end-to-end automatic solution enabling the sharing of speech with identifying information removed.\nThe resulting obfuscated speech remains in the original speaker's voice, allowing for the application of traditional speaker anonymization approaches to mask the speaker's identity. The original prosody can be protected by applying a transformation such as waveform change, offering a significant advantage over a technique which generates a complete obfuscated transcription (instead of splicing an obfuscated entity into original speech).\n", "hypothesis": "The approach is prototyped on a sample of the LibriSpeech corpus (Panayotov et al., 2015) with each step evaluated collectively.", "answer": false}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": " In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering.  We apply computationally lightweight density-based UE methods to seq2seq models and show that they often outperform heavyweight deep ensembles on the task of OOD detection 1 ..", "answer": true}
{"title": "Class based Influence Functions for Error Detection", "content": "\nIntroduction\nDeep learning models are data hungry. Large models such as transformers (Vaswani et al., 2017) , BERT (Devlin et al., 2019) , and GPT-3 (Brown et al., 2020) require millions to billions of training data points. However, data labeling is an expensive, time consuming, and error prone process. Popular datasets such as the ImageNet (Deng et al., 2009) contain a significant amount of errors -data points with incorrect or ambiguous labels (Beyer et al., 2020) . The need for automatic error detection tools is increasing as the sizes of modern datasets grow.\nInfluence function (IF) (Koh and Liang, 2017) and its variants (Charpiat et al., 2019; Khanna et al., 2019; Barshan et al., 2020; Pruthi et al., 2020) are a powerful tool for estimating the influence of a data point on another data point. Researchers leveraged this capability of IFs to design or detect adversarial (Cohen et al., 2020) , poisonous (Koh et al., 2022; Koh and Liang, 2017) , and erroneous (Dau et al., 2022) examples in large scale datasets. The intuition is that these harmful data points usually have a negative influence on other data points and this influence can be estimated with IFs. Basu et al. (2021) empirically observed that IFs are unstable when they are applied to deep neu- * Joint first authors ral networks (DNNs). The quality of influence estimation deteriorates as networks become more complex. In this paper, we provide empirical and theoretical explanations for the instability of IFs. We show that IFs scores are very noisy when the two data points belong to two different classes but IFs scores are much more stable when the two data points are in the same class (Sec. 3). Based on that finding, we propose IFs-class, variants of IFs that use class information to improve the stability while introducing no additional computational cost. IFs-class can replace IFs in anomalous data detection algorithms. In Sec. 4, we compare IFs-class and IFs on the error detection problem. Experiments on various NLP tasks and datasets confirm the advantages of IFs-class over IFs.\n\nBackground and Related work\nWe define the notations used in this paper. Let z = (x, y) be a data point, where x \u2208 X is the input, y \u2208 Y is the target output; Z = z (i) n i=1 be a dataset of n data points; Z \u2212i = Z\\z (i) be the dataset Z with z (i) removed; f \u03b8 : X \u2192 Y be a model with parameter \u03b8; L Z,\u03b8 = 1 n n i=1 \u2113(f \u03b8 (x (i) ), y (i) ) = 1 n n i=1 \u2113(z (i) ; \u03b8) be the empirical risk of f \u03b8 measured on Z, where \u2113 : Y \u00d7 Y \u2192 R + is the loss function; \u03b8 = arg min \u03b8 L Z,\u03b8 and \u03b8\u2212i = arg min \u03b8 L Z \u2212i ,\u03b8 be the optimal parameters of the model f \u03b8 trained on Z and Z \u2212i . In this paper, f \u03b8 is a deep network and \u03b8 is found by training f \u03b8 with gradient descent on the training set Z.\n\nInfluence function and variants\nThe influence of a data point z (i) on another data point z (j) is defined as the change in loss at z (j) when z (i) is removed from the training set\nEQUATION\nThe absolute value of s (ij) measures the strength of the influence of z (i) on z (j) . The sign of s (ij) show the direction of influence. A negative s (ij) means that removing z (i) decreases the loss at z (j) , i.e. z (i) is harmful to z (j) . s (ij) has high variance because it depends on a single (arbitrary) data point z (j) .\nTo better estimate the influence of z (i) on the entire data distribution, researchers average the influence scores of z (i) over a reference set Z \u2032\ns (i) = 1 |Z \u2032 | z (j) \u2208Z \u2032 s (ij) = L Z \u2032 , \u03b8\u2212i \u2212 L Z \u2032 , \u03b8 (2)\ns (i) is the influence of z (i) on the reference set Z \u2032 . Z \u2032 can be a random subset of the training set or a held-out dataset. Naive computation of s (ij) requires retraining f \u03b8 on Z \u2212i . Koh and Liang (2017) proposed the influence function (IF) to quickly estimate s (ij) without retraining\ns (ij) \u2248 IF (z (i) , z (j) ) \u2248 1 n \u2207 \u03b8\u2113(z (i) ; \u03b8) \u22a4 H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) (3)\nwhere\nH \u03b8 = \u2202 2 L Z, \u03b8/\u2202\u03b8 2\nis the Hessian at \u03b8. Exact computation of H \u22121 \u03b8 is intractable for modern networks. Koh and Liang (2017) developed a fast algorithm for estimating H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) and used only the derivatives w.r.t. the last layer's parameters to improve the algorithm's speed. Charpiat et al. (2019) proposed gradient dot product (GD) and gradient cosine similarity (GC) as faster alternatives to IF. Pruthi et al. (2020) argued that the influence can be better approximated by accumulating it through out the training process (TracIn). The formula for IFs are summarized in Tab. 3 in Appx. A.\nIFs can be viewed as measures of the similarity between the gradients of two data points. Intuitively, gradients of harmful examples are dissimilar from that of normal examples (Fig. 1 ).\n\nInfluence functions for error detection\nIn the error detection problem, we have to detect data points with wrong labels. Given a (potentially noisy) dataset Z, we have to rank data points in Z by how likely they are erroneous. Removing or correcting errors improves the performance and robustness of models trained on that dataset.\nTraditional error detection algorithms that use hand designed rules (Chu et al., 2013) or simple statistics (Huang and He, 2018) , do not scale well to deep learning datasets. Cohen et al. (2020) 2021) empirically showed that IFs with last layer gradient perform as well as or better than IFs with all layers' gradient and variants of IF behave similarly. Therefore, we analyze the behavior of GD with last layer's gradient and generalize our results to other IFs. Fig. 1 shows the last layer's gradient of an MLP on a 3-class classification problem. In the figure, gradients of mislabeled data points have large magnitudes and are opposite to gradients of correct data points in the true class. However, gradients of mislabeled data points are not necessarily opposite to that of correct data points from other classes. Furthermore, gradients of two data points from two different classes are almost perpendicular. We make the following observation. A mislabeled/correct data point often has a very negative/positive influence on data points of the same (true) class, but its influence on other classes is noisy and small.\nWe verify the observation on real-world datasets. (Fig. 2 ). We compute GD scores of pairs of clean data points from 2 different classes and plot the score's distribution. We repeat the procedure for pairs of data points from each class. In the 2-class case, GD scores are almost normally distributed with a very sharp peak at 0. That means, in many cases, a clean data point from one class has no significant influence on data points from the other class. And when it has a significant effect, the effect could be positive or negative with equal probability. In contrast, GD scores of pairs of data points from the same class are almost always positive. A clean data point almost certainly has a positive influence on clean data points of the same class.\nOur theoretical analysis shows that when the two data points have different labels, then the sign of GD depends on two random variables, the sign of inner product of the features and the sign of inner product of gradients of the losses w.r.t. the logits. And as the model becomes more confident about the labels of the two data points, the magnitude of GD becomes smaller very quickly. Small perturbations to the logits or the features can flip the sign of GD. In contrast, if the two data points have the same label, then the sign of GD depends on only one random variable, the sign of the inner product of the feature, and the GD's magnitude remains large when the model becomes more confident. Mathematical details are deferred to Appx. D.\n\nClass based IFs for error detection\nOur class based IFs for error detection is shown in Alg. 1. In Sec. 3.1, we see that an error has a very Algorithm 1 Class based influence function for error detection.\n\nRequire:\n1: Z = z (i) n i=1 : a big noisy dataset 2: C: number of classes 3: for k = 1, ..., C do 9:\nZ \u2032 k = z \u2032(j k ) m k j k =1 : clean data from class k 4: Z \u2032 = C k=1 Z \u2032 k : a clean\ns (i) k = m k j=1 sim(\u2207 \u03b8 \u2113(z (i) ),\u2207 \u03b8 \u2113(z \u2032(j k ) )) m k 10:\nend for 11:\ns (i) = min k (s (i)\nk ) 12: end for 13: \u1e90 = sort(Z, key = s, ascending = True) 14: return \u1e90 strong negative influence on correct data points in the true class, and a correct data point has a positive influence on correct data points in the true class. Influence score on the true class is a stronger indicator of the harmfulness of a data point and is better at differentiating erroneous and correct data points. Because we do not know the true class of z (i) in advance, we compute its influence score on each class in the reference set Z \u2032 and take the minimum of these influence scores as the indicator of the harmfulness of z (i) (line 8-11 create benchmark datasets Z's, we inject random noise into the above datasets. For text classification datasets, we randomly select p% of the data points and randomly change their labels to other classes.\nFor the CoNLL-NER dataset, we randomly select p% of the sentences and change the labels of r% of the phrases in the selected sentences. All tokens in a selected phrase are changed to the same class. The reference set Z \u2032 is created by randomly selecting m k clean data points from each class in Z. To ensure a fair comparison, we use the same reference set Z \u2032 for both IFs and IFs-class algorithms. Models are trained on the noisy dataset Z. To evaluate an error detection algorithm, we select top q% most harmful data points from the sorted dataset \u1e90 and check how many percent of the selected data points are really erroneous. Intuitively, increasing q allows the algorithm to find more errors (increase recall) but may decrease the detection accuracy (decrease precision). Our code is available at https://github.com/Fsoft-AIC/ Class-Based-Influence-Functions.\nResult and Analysis Because results on all datasets share the same patterns, we report representative results here and defer the full results to Appx. C.\nFig. 3(a) shows the error detection accuracy on the SNLI dataset and how the accuracy changes with q. Except for the GC algorithm, our classbased algorithms have higher accuracy and lower variance than the non-class-based versions. When q increases, the performance of IFs-class does not decrease as much as that of IFs. This confirms that IFs-class are less noisy than IFs. Class information fails to improve the performance of GC. To understand this, let's reconsider the similarity measure sim(\u2022, \u2022). Let's assume that there exist some clean data points z \u2032(j) \u2208 Z \u2032 with a very large gradient \u2207 \u03b8\u2113(z \u2032(j) ). If the similarity measure does not normalize the norm of \u2207 \u03b8\u2113(z \u2032(j) ), then z \u2032(j) will have the dominant effect on the influence score. The noise in the influence score is mostly caused by these data points. GC normalizes both gradients, \u2207 \u03b8\u2113(z (i) ) and \u2207 \u03b8\u2113(z \u2032(j) ), and effectively removes such noise. However, gradients of errors tend to be larger than that of normal data points (Fig. 1 ). By normalizing both gradients, GC removes the valuable information about magnitudes of gradients of errors \u2207 \u03b8\u2113(z (i) ). That lowers the detection performance. In Fig. 3 (a), we see that the performance of GC when q \u2265 15% is lower than that of other classbased algorithms. Similar trends are observed on other datasets (Fig. 6 , 7, 8 in Appx. C). Fig. 3(b) shows the change in detection accuracy as the level of noise p goes from 5% to 20%. For each value of p, we set q to be equal to p. Our class-based influence score significantly improves the performance and reduces the variance. We note that when p increases, the error detection problem becomes easier as there are more errors. The detection accuracy, therefore, tends to increase with p as shown in Fig. 3 (b), 9, 10. Fig. 3(c ) shows that GD-class outperforms GD on all entity types in CoNLL2003-NER. The performance difference between GD-class and GD is greater on the MISC and ORG categories. Intuitively, a person's name can likely be an organization's name but the reverse is less likely. Therefore, it is harder to detect that a PER or LOC tag has been changed to ORG or MISC tag than the reverse. The result shows that IFs-class is more effective than IFs in detecting hard erroneous examples.\n\nThe effect of data on error detection algorithms\nWe study the effect of the size and the cleanliness of the reference set on the performance of error detection algorithms.\nThe size of the reference set. We changed the size of classes in the reference set from 10 to 1000 to study the effect of the reference set's size on the detection performance. We report the mean performance of GD and GC algorithms in Tab. 1. We observe no clear trend in the performance as the size of the reference set increases. Our conjecture is that gradients of clean data points from the same class have almost the same direction. Averaging the gradient direction over a small set of data points already gives a very stable gradient direction. Therefore, increasing the size of the reference set does not have much impact on detection performance. \n\nConclusion\nIn this paper, we study influence functions and identify the source of their instability. We give a theoretical explanation for our observations. We introduce a stable variant of IFs and use that to develop a high performance error detection algorithm. Our findings shed light of the development of new influence estimators and on the application of IFs in downstream tasks.\n", "hypothesis": "We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages additional computational cost to improve the stability of IFs. Extensive experiments show that our modification significantly improves the performance and stability of IFs.", "answer": false}
{"title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification", "content": "\nIntroduction\nLarge language models (LLMs) with parameters in the order of billions (Brown et al., 2020) have gained significant attention in recent years due to their strong performance on a wide range of natural language processing tasks. These models have achieved impressive results on benchmarks (Chowdhery et al., 2022) , particularly in zero or few-shot settings, where they are able to generalize to new tasks and languages with little to no training data. There is, however a difficulty in tuning parameters of these large-scale models due to resource limitations. Additionally, the focus on benchmarks has led to the neglect of real-world challenges, such as that of hierarchical classification. As a result, the long-tail problem (Samuel et al., 2021) has been overlooked. This occurs when a vast number of rare classes occur in the presence of frequent classes for many natural language problems. In many industrial real-world applications, a strong performing method for hierarchical classification can be of direct utility. New product categories are emerging in e-commerce platforms. Existing categories, on the other hand, may not be very intuitive for customers. For example, upon browsing categories such as night creams, we may be unable to find a product in a sibling-node category of creams. This is further highlighted by platforms in which a systematic structure is not created for users; parent nodes may be in place of child nodes, and vice versa (Asghar, 2016) . Manually categorizing product categories can be a costly redesigning endeavour. To tackle this problem, we suggest refactoring traditional hierarchical flatlabeled prediction tasks (Liu et al., 2021) to a more indicative long-tail prediction task. This involves structuring the classification task to closely reflect the real-world long-tail distribution of classes. In doing so, we are enabled to leverage LLMs for long-tail prediction tasks in a strict zero-shot classification setting. Through a series of experiments, results in this work show that our proposed method is able to significantly improve the performance over the baseline in several datasets, and holds promise for addressing the long-tail problem in real-world applications. The contributions of this work can be summarized as follows:\n\u2022 We refactor real-world hierarchical taxonomy datasets into long-tailed problems. In doing so, we create a strong testbed to evaluate \"strict zeroshot classification\" with LLMs.\n\u2022 We explore utilizing LLMs to enhance the capabilities of entailment-contradiction predictors for long-tail classification. This results in strong capabilities of performing model inference without resource-intensive parameter updates.\n\u2022 We show through quantitative empirical evidence, that our proposed method is able to overcome limitations of stand-alone large language models. Our method obtains strong performance on longtail classification tasks.\n\nBackground and Related Work\nStrict Zero-Shot Classification Previous works (Liu et al., 2021; Yin et al., 2019) have explored zero-shot classification extensively under two settings-(i) zero-shot, where testing labels are unseen, i.e. no overlap with the training labels, and (ii) generalized zero-shot, testing labels are partially unseen. In both cases, the model is trained on data from the same distribution as the test data. In our proposed strict zero-shot setting, the model is only trained to learn the entailment relationship from natural language inference (NLI) corpora (Williams et al., 2018) . The training data for this model has no overlap with the distribution or semantics of the inference set. Additionally, previous works utilizing NLI have either not examined the utility of LLMs (Ye et al., 2020; Gera et al., 2022) , or transfer the capabilities of LLMs to smaller models but have failed to use them in a strict zero-shot setting for long-tail problems, only demonstrating their utility for benchmark tasks (Tam et al., 2021; Schick and Sch\u00fctze, 2021) . Works exploring LLMs have also limited their study to only using them independently without exploring entailment-contradiction prediction (Wei (Wang et al., 2022a,b; Jiang et al., 2022; Chen et al., 2021) . For this reason, the graph representations may have limited value independently, although representations may be used to assist text classification by providing an organized label space. These works only introduce hierarchies to bring order to the label space, but overlook the original task of hierarchical taxonomy classification (Kowsari et al., 2017) . For all previous works, difficult to obtain fine-tuning data is required to produce strong sig- nals. In our work, we refactor this data into a leaf-node label prediction task with the help of entailment-contradiction relations and LLMs. In doing so, we enable hierarchical taxonomy prediction independent of utilizing training data for the downstream task.\n\nMethodology\nIn this paper, we investigate the limitations of LLMs in three overlooked settings, when-(i) the model is not provided with sufficient examples due to the input text length, (ii) the label space includes tokens largely unobserved in the model's pretrained vocabulary, and (iii) there are a large number of distractors in the label space (Kojima et al., 2022; Min et al., 2022; Razeghi et al., 2022) . These scenarios are common in real-world tasks, but are often overlooked in the development and evaluation of LLMs. To address these challenges, we propose the use of entailment-contradiction prediction (Yin et al., 2019) , the task of determining whether a premise logically entails or contradicts a hypothesis. Through our method, we are able to leverage strong reasoning from Yin et al. ( 2019) with the retrieval abilities of LLMs (Wang et al., 2020) to improve overall performance in a strict zero-shot setting, where the model must classify samples from a new task without any fine-tuning or additional examples used for training from the same distribution as the inference dataset. Importantly, our proposed combined method does not require parameter updates to the LLM, a resource-intensive process that is not always feasible with increasingly large model size (Chowdhery et al., 2022) . Our simple framework is shown in Figure 1 . Considering the label space, C = {C 1 , C 2 , ...C n } as the set of classes for any given dataset, and a text input, X, we can utilize the entailment predictor, E to make a contradiction, or entailment prediction on each label. This is done by using X as the premise, and \"This text is related to C i .\"\n\u2200C i \u2208 C as the hypothesis (Yin et al., 2019) . In our work, the premise may be modified to include the prompt template. The prediction, E(X) lacks any external knowledge and is restricted to the label space, which may result in poor performance. E(X) can however, provide us with an implicit classification of the contradiction relation for sibling nodes. In our work, we use E(X) as an initializer for LLMs. We also regard it as a baseline as it shows strong performance independently. A LLM, L on the other hand, operates in an open space, with aforementioned shortcomings for classification tasks. For our purposes, we can regard this as a noisy knowledge graph (Wang et al., 2020) , which may be utilized to predict ancestors or descendants of the target class. We consider the prediction made by the LLM as L(X). It is important to note that L(X) may or may not belong to C. We try several prompts for this purpose, shown in Appendix A.\nBy combining these two components, we can create a template which utilizes the entailment relation explicitly, and the contradiction relation implicitly by constructing L(E(X)) to deseminate combined information into an entailment predictor for classification. The template we use is task-dependent, and is generally robust given an understanding of the domain. On Web of Sciences we use: \"Here is some text that entails E(X): X. What area is this text related to?\". For Amazon Beauty, we use \"Here is a review that entails E(X): X. What product category is this review related to?\". In this setting, our method still meets a barrier due to limitations of LLMs. By constructing a composite function, E(L(E(X)), we are able to leverage our LLM in producing tokens which may steer the entailment predictor to correct its prediction. The template used for this composite function is \"Here is some text that entails L(E(X)): X.\" across all datasets.\nGeneral Form: Although our results are reported combining the advantages of L, and E to produce upto the composite function E(L(E(X)), this can 1 : Baseline models (Top) underperform our method (Bottom) across all datasets for average scores of Top-1, Top-3, and Top-5 accuracy and Macro F1. Our primed and primed+ models explicitly utilize the entailment relation, with one and five predictions of L(E(X)) respectively. All models used are available on Huggingface.\n. be extended as it holds the property of being an iterative composition function to E(L(E(L...E(X)))).\nOur observations show this setting having comparable, or marginal improvements with our dataset. However, this may prove to be beneficial in other tasks. We will investigate, and urge other researchers to explore this direction in future work.\n4 Experiments and Results\n\nDataset and Experimental Settings\nWe refactor the widely used Web of Sciences (WOS) with Kowsari et al. ( 2017), and Amazon Beauty (McAuley et al., 2015) datasets to follow a class-wise long-tail distribution as shown in Figure 3 . Additionally, we create two variations of the Amazon Beauty dataset, first in which it contains the same tree depth as WOS, both containing 3000 samples, and second in which all classes are included for their maximum tree depth, containing 5000 samples. We select these datasets as they challenge the shortcomings of LLMs. The input text of providing multiple abstracts in the WOS dataset surpasses the maximum input token length of most transformer-based models. This makes it difficult for models to learn the input distribution, a requirement for showing strong in-context performance (Min et al., 2022) . Next, many tokens in the label space of both the WOS and Amazon Beauty datasets rarely occur in pretraining corpora, details of which are provided in the Appendix B. Additionally, both datasets contain a large number of distractors, or incorrect classes in the label space. Further details are provided in Appendix C. All experiments are performed on a single NIVIDA Titan RTX GPU. We use BART-Large-MNLI with 407M parameters as our baseline. We use this model as it outperforms other architectures trained on MNLI for zero-shot classification. For our LLM, we opt to use T0pp (Sanh et al., 2022) with 11B parameters 1 , as previous works show that it matches or exceeds performance of other LLMs such as GPT-3 (Brown et al., 2020) on benchmarks.\n\nResults and Discussion\nResults of our method are shown in Table 1 . LLMs, due to their limitations, perform poorly as a standalone model for long-tail classification. These results can be improved by priming the model with an entailment predictor through the usage of a prompt. The baseline shows strong performance independent of the LLM, as it operates on a closed label space. The capabilities of the baseline can be enhanced by further explicitly priming it with a entailment relation through a LLM. Rows in which T0pp is initialized, or primed with E are indicated with Primed. Priming the model showcases improvements across all datasets for macro F1. For accuracy, priming the model shows benefit in two out of three datasets. In Figure 4 , we show the results of Top-5 predictions for the WOS dataset.\nAll results are aggregated in Table 1 . It is important to highlight that prompt variation led to stable results for our LLM. The variance upon utilizing BART-MNLI is negligible across prompts. The best results are observed upto Top-4 predictions on both accuracy and macro F1 for our method, when the entailment prompt is enhanced with a greater number of tokens corresponding to the output of L(E(X)). The variation between our method and the baseline is much greater for Top-1 predictions, but Top-5 prediction variance is negligible. Detailed results for both depth settings of Amazon Beauty are shown in Appendix C.\n\nConclusion\nIn this work, we utilize an LLM in the form of a noisy knowledge graph to enhance the capabilties of an entailment predictor. In doing so, we achieve strong performance in a strict zero-shot setting on several hierarchical prediction tasks. We also show the necessity of refactoring existing hierarchical tasks into long-tail problems that may be more representative of the underlying task itself. The utility in practical industry settings is also highlighted through this setting.\n", "hypothesis": " We observe LLMs are more prone to failure in these cases.  Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets..", "answer": true}
{"title": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios", "content": "\nIntroduction\nReasoning plays a central role in human communication (Frank and Goodman, 2012) . While language models have demonstrated remarkable capacity on downstream tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) , it remains unclear to what extent predictions generated by language models are consequences of correlation with linguistic heuristics in the context, versus robust reasoning about causal relations grounded on understanding of world knowledge.\nIn this paper we leverage counterfactual conditionals to investigate the capacity of pre-trained LMs (PLMs) to distinguish hypothetical scenarios from reality, and to examine how this interacts with models' use of existing real world knowledge as well as shallower associative cues. Counterfactuals consist of a premise which is false in the real world but true in the hypothetical world (e.g., \"If cats were vegetarians\"), and an imaginary consequence of this premise (\"cats would love cabbages\"). Testing language models with counterfactuals allows us to use language to manipulate what is true and what is hypothetical, and to test models' ability to separate and use this information for predictions. Previous work has established the use of counterfactual scenarios to probe inference ability (Qin et al., 2019; Zellers et al., 2019; Mostafazadeh et al., 2016; Meng et al., 2022; Rajani et al., 2019; Saparov and He, 2022; Frohberg and Binder, 2021; Elazar et al., 2021; Rudinger et al., 2020) , but the datasets lack systematic control of lexical cues and world knowledge, which makes it likely that the performance could be attributable to spurious cues in the datasets (Niven and Kao, 2019) .\nFor our tests we draw on and adapt inputs from existing psycholinguistic experiments. We begin by testing models' ability to override existing world knowledge when the context indicates that the correct completion involves a hypothetical world (e.g., \"if cats were vegetarian, cats would love cabbages/fish\"). We test five popular PLMs, and find that models can increase their preference for counterfactual completions given counterfactual context-however, most models rely strongly on simple lexical cues. Next we control the effect of real world knowledge and lexical triggers, to test models' understanding of what counterfactual language implies about the world state. We find that most models fail to understand real-world implications of counterfactuals and largely rely on lexical triggers-with the exception of GPT-3, which shows greater sophistication, but continues to show non-trivial susceptibility to interfer-ence from lexical-associative cues. We discuss the implications and possible interpretations of these findings with respect to linguistic competence and predictive strategies of these models.\n\nExp1: overriding world knowledge\nOur first experiment investigates whether LMs are able to take a counterfactual scenario and predict a counterfactual-consistent completion that contradicts general world knowledge.\nItems We draw directly on counterfactual stimuli from the psycholinguistic study of Ferguson and Sanford (2008) . There are 128 items from the original psycholinguistic experiments, and we synthetically generate 10,720 additional items (see Appendix A.2 for illustration of data generation process). We match target nouns and syntactic constructions across conditions in order to control lexical properties that influence language models' predictions. Table 1 shows example items from the synthetic large-scale dataset (see Section A.1 for example items from the small-scale dataset).\n\nCond Sentence\nCW If cats were vegetarians, people would love them.\nFamilies would feed cats with fish/cabbages.\n\nRW\nBecause cats are carnivores, people love them.\nFamilies would feed cats with fish/cabbages.\n\nBB\nFamilies would feed cats with fish/cabbages The experiment includes two key conditions: Counterfactual-World (CW) and Real-World (RW) (Fig. 1 ). The CW condition presents a counterfactual scenario, e.g., in which cats are vegetarians. The logical target completion in this example is \"cabbages\", but because in reality cats are more likely to eat fish, this contradicts world knowledge. By contrast, in the RW condition the logical completion is consistent with the real world (\"feed cats with fish\"). We also include one Baseline Bias (BB) condition, for a more direct test of the strength of models' baseline preference for each completion.\nExperiments We test counterfactual reasoning in five pre-trained language models. We include autoregressive transformers in the GPT family (GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) ) and masked language models in the BERT family (BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) and MPNet (Song et al., 2020)) 2 .\nWe test models by comparing the log-probability that each model assigns to the CW-congruent (\"cabbages\") and RW-congruent (\"fish\") completions given the contexts. For all conditions, we compute the percentage of items in which the CW-congruent continuation has a higher probability than the RWcongruent continuation. This means that in RW and BB conditions, lower values reflect better predictions, since the CW-congruent completion is the less logical completion in these conditions. Table 2 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp1. In the CW condition, higher values reflect better predictions. In RW and BB conditions, lower values reflect better predictions.\nResults Table 2 shows the preferences for CWcongruent completions across all models and conditions, for the small-scale hand-designed items from the psycholinguistic experiment, and for the large-scale synthetic items. 3 We see that all mod-els show stronger preference for CW-congruent continuations in the counterfactual (CW) context than in the other conditions (though in the case of BERT on the small-scale data, this difference is negligible). All models show below-chance preference for CW-congruent continuations in the RW condition-which means above-chance preference for the correct RW-congruent continuations. However, though all model preferences for the correct CW-congruent continuation are higher in the CW condition than in the RW condition, even in the CW condition the preference for CW-congruent conditions is at best slightly above chance for most models. The exception is GPT-3, which is the only model to prefer the CW-congruent continuation in greater than 70% of items.\nWe also see that GPT-3 shows exceptionally strong performance on both BB and CW conditions. This suggests, slightly counterintuitively, that stronger grasp of relevant world knowledge may in fact be associated with models more effectively overriding that knowledge in a counterfactual. To investigate this effect further, we examine the impact of world knowledge at the item level. We quantify strength of world knowledge as the difference between models' log-probability of CW-and RW-congruent continuations for a given item in the BB condition, and the strength of counterfactual preference as the difference between log-probability of CW-and RW-congruent continuations for a given item in the CW condition. We then compute the Pearson correlation between these strength measures. We find a significant correlation between the robustness of world knowledge encoding and strength of counterfactual preference in the CW condition across all language models (see Appendix A.3), further supporting a relationship between strength of world knowledge and counterfactual sensitivity. While previous work has suggested that large language models may have difficulty avoiding memorized texts when explicitly prompted to end famous quotes differently (McKenzie et al., 2022) , our results suggest that world knowledge may in fact facilitate reasoning when accompanied with clear structural cues (e.g. \"if\"). To better understand how world knowledge informs language models' predictions and in-CW condition alone. However, to further address this concern, we calculate the proportion of items in which the model shows the correct preference in both CW and RW conditions. The results are presented in Section A.5 and suggest a comparable pattern in terms of relative model strengths.\nference, it will be important to continue expanding the scale of tests and more carefully operationalize definitions of world knowledge in future work.\n\nExp2: impact of cue words in context\nThe first experiment suggests that models can to an extent override world knowledge given a counterfactual, particularly in cases when models have a strong handle on the relevant world knowledge. However, it is possible that in these tests the models were not relying on sophisticated understanding of counterfactuals, but rather on simple lexical triggers in context. Consider, for instance, that models could perform well in Exp1 if they simply increase their preference for \"cabbages\" in the proximity of \"vegetarians\", etc. To test the impact of these lexical triggers, we incorporate an additional condition.\nItems Table 3 and Fig. 2 show a sample item and illustration of experimental set-up with the new added condition. In this Counterfactual-to-Reality (CR) condition, models see the same counterfactual context, but the subsequent sentence references actual reality. So the correct completion is consistent with reality, but inconsistent with the lexical trigger (\"vegetarians\"). We generate sentences in the CR condition by modifying CW sentences to include the discourse connective \"In reality\" and to include present tense in the second sentence.\n\nCR\nIf cats were vegetarians, people would love them.\nIn reality, families feed cats with fish/cabbages. Experiments As above, we calculate percentage of items in which models prefer the CW-congruent continuations. Models relying on information beyond simple lexical triggers should show a sharp drop in preference for the CW-congruent completion in the CR condition, where the correct completion should align with real world information.\nResults Table 4 shows the results. We see that most models show non-zero drop between CW and CR conditions-however, for most models this reduction is minor. It is only GPT-3 that shows a truly substantial drop in CW-congruent preference, and only in the large-scale synthetic dataset. This suggests that most models are largely following simpler lexical triggers, while GPT-3 has somewhat greater sensitivity to more detailed linguistic cues. Note, however that GPT-3's relative success on the synthetic data over the small-scale data may rely on larger distance between lexical triggers and target positions: see Appendix A.4 for evidence on GPT-3's sensitivity to linear distance. Table 4 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp2. In the CW condition, higher values reflect better predictions. In the CR condition, lower values reflect better predictions.\n\nExp3: Inferring real world state with counterfactual cues\nThe previous experiments indicate that models can override world knowledge in the face of counterfactual evidence, and that the ability to do this improves with stronger world knowledge-but for most models this performance appears to be driven largely by simple lexical triggers in the context, with the possible exception of GPT-3. In this section we remove the influence of pre-existing world knowledge, and hold constant lexical triggers across conditions, for a more direct test of models' sensitivity to linguistic indicators of counterfactuals, and what they say about the true state of the world. This task is particularly challenging because language models must infer the true state of the world based on the presence of counterfactuals, with lexical cues often being misleading.\nItems We adapt stimuli from a psycholinguistic study with 96 controlled sentences (Ferguson, 2012) . We additionally create a larger-scale synthetic dataset with 12,960 sentences, using the same events as the generated dataset from Section 2. We modify the subject noun phrases such that there is no influence of existing world knowledge. For example, we modify the subject \"cat\" to \"pet\", so that there is no prior knowledge about the subject's preference for \"cabbages\" or \"fish\". As a result, existing world knowledge cannot inform the correct completion-instead, models need to infer based on the counterfactual language that the true state of the world is different from what the counterfactual states. Further, we control the lexical items used across different conditions to minimize effects of lexical cues on condition differences (see Table 5 ).\n\nCond Sentence\nCWC If the pets were vegetarians, people would love them.\nIn fact, people feed the pets with fish/cabbages.\nRWCA Because the pets are vegetarians, people love them.\nIn fact, people feed the pets with fish/cabbages.\nBBC In fact, people feed the pets with fish/cabbages. Fig. 3 shows the set-up of conditions. In the Counterfactual-World Context (CWC) condition, the scenario described in the first sentence is neutral with respect to real world knowledge-it is the use of the counterfactual (\"if...were\") that tips us off that this scenario is not true in reality. The correct completion, then, cannot be informed by world knowledge, and is also misaligned with the lexical trigger (e.g., \"vegetarians\"), so models must rely specifically on this implication from the counterfactual in order to perform well.\nIn the Real-World Context Alternative (RWCA) condition, the context uses the same lexical triggers (\"vegetarians\") as the CWC condition. However, because there is no counterfactual language, the logical completion is now the word associated with the lexical trigger (e.g., \"cabbages\", associated with \"vegetarians\").\nGiven that the logical completions in CWC and RWCA differ, we also compare against a Baseline Bias Context (BBC) condition, to establish default model preference for the target factual completion in the presence of the new subject noun phrase.\nExperiments We compare proportion of CWCcongruent completions across conditions. Good performance will assign high values in the CWC condition and low values in the RWCA condition. Table 6 : Percentage of preference for CWC-congruent completion (e.g., \"fish\") in Exp3. In the CWC condition, higher values reflect better predictions. In the CWCA condition, lower values reflect better predictions. The BBC condition establishes models' default preference for the CWC-congruent completion.\nResults Table 6 shows the results. In the smallscale dataset, most models show a similar preference in CWC and RWCA, suggesting again that their predictions are largely driven by lexical triggers. Only GPT-3 shows substantial difference between CWC and RWCA, indicating finer-grained sensitivity to counterfactual structures. This sensitivity is, however, less pronounced in the largescale dataset. Closer inspection suggests that GPT-3's specific success on the small-scale data may in fact be attributable to canceling out of lexical triggers: in the small-scale dataset, there are lexical triggers supporting both continuations (see A.1 for more illustration of the characteristics of the smallscale dataset), which may cause lexical cues to cancel out, enabling more influence from other linguistic cues. To take one example, the small-scale dataset contains the item \"If Helen had received her student loan, her bank balance would now be in credit. When she checked her bank balance she was worried/happy about her finance.\" In this item, among the lexical triggers (\"student loan\", \"in credit\", \"bank balance\") there are potential associations with both the CWC-congruent completion \"worried\" and the CWC-incongruent completion \"happy\". By contrast, in the large-scale dataset, the major lexical trigger (\"vegetarians\") always favors the CWC-incongruent continuation (\"cabbages\"), causing strong lexical bias against the CWC-congruent continuation (see Appendix A.4 for further analysis on the role of conflicting lexical triggers and other linguistic factors). This suggests that GPT-3 does show real sensitivity to linguistic indicators of counterfactuals, but the effect of superficial lexical cues remains strong.\n\nConclusion\nThe experiments above have shown that when presented with counterfactual situations, PLMs are able to prefer completions that conflict with world knowledge-and counterintuitively, this sensitivity appears better in cases where that world knowledge is stronger. Our results also indicate, however, that models are in large part relying on simple lexical cues to inform these preferences. The only model that shows more sophisticated sensitivity to finegrained linguistic cues separating counterfactuals from reality is GPT-3-which successfully distinguishes conditions based on counterfactual cues, but nonetheless still shows strong influences from lexical associative cues. Why might world knowledge aid counterfactual sensitivity? Does GPT-3 truly understand counterfactuals? One possibility worth considering is that explanations in both of these cases involve volume of exposure. First, models' stronger world knowledge for a given fact suggests that models have encountered that fact more often in training-and this may in turn translate to more exposure to that type of knowledge in counterfactual contexts, enabling more straightforward memorization-based performance. Similarly, while GPT-3 may robustly understand counterfactuals, the massive data exposure for that model may enable a simpler path to success: GPT-3 could simply have developed lower-level knowledge of how linguistic cues like \"If/had\" versus \"Because\" mediate levels of association between nearby lexical cues and later words. We leave investigation of these hypotheses for future work.\n", "hypothesis": "We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on real-world propositions.", "answer": false}
{"title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers", "content": "\nIntroduction\nThe communicative acts of humans in collaborative situations can be described as two parts of a joint act: signalling and recognizing. In such joint activities, these signals work as coordination devices to increment on the current common ground of the participants (Clark, 1996) . The ability to act on these language signals is crucial for future machine learning models to naturally collaborate and interact with humans (Lemon, 2022; Fern\u00e1ndez et al., 2011) . Such a collaborative interaction with humans usually happens fluently, where one communicative act is performed after the other. The framework of reinforcement learning (RL) (Sutton and Barto, 2018) describes such mechanics where an agent is exposed in steps to observations of an environment with dynamic factors such as the position of objects or language expressions. The goal is that the agent learns to behave generally well in Figure 1 : An exemplary interaction between a teacher and a follower that controls the gripper (the grey square). After an initial referring expression l RE at t 0 , the teacher provides feedback l FBt based on the follower's actions until the correct piece is selected at time step T . a particular environment solely based on the observations it makes and rewards it gets.\nA key challenge here is the variability of expressions in language that can be said to the agent during an interaction. Even in relatively simple environments, there might arise an overwhelming amount of situations for an agent to handle (Chevalier-Boisvert et al., 2019) . Recent work on collaborative agents focuses on large precollected datasets for imitation learning to learn agents in complex simulated visual environments (Gao et al., 2022; Padmakumar et al., 2022; Pashevich et al., 2021) or frames the learning as a contextual bandit problem (Suhr and Artzi, 2022; Suhr et al., 2019) . Nevertheless, other work has shown that intermediate language inputs are a valuable signal to improve the agent's learning performance in task-oriented visual environments (Co-Reyes et al., 2019; Mu et al., 2022) .\nIn this paper, we present an initial study that evaluates a follower's learning success given a teacher's intra-episodic feedback in a collaborative setting. We use a referential language game (in English) as a controllable example of a task-oriented collaborative joint activity (see Figure 1 ). In this game one player (the follower) is supposed to select a piece based on the another player's directives (the teacher). We assume a teacher that utters referring expressions as initial instructions and then responds to the follower's actions with intra-episodic feedback. We frame this as a RL problem with sparse rewards where the intermediate feedback is not part of the reward function but its potential usefulness is learnt by the follower alone. 1\n\nRelated Work\nVision and language navigation. In vision and language navigation, an agent is given a natural language instruction which is to be understood to navigate to the correct goal location in a visually observed environment (Gu et al., 2022) . The follower can usually ask an Oracle for further information, if necessary (Nguyen et al., 2019; Nguyen and III, 2019; Fried et al., 2018) . We extend on this idea and aim for an ongoing interaction with corrections that loosens the turn-based paradigm by letting the Oracle choose when to speak as part of the environment. Hence, in our reference game, the language back-channel for the follower is cut, so that we force the follower to rely more on the visual observations for task success.\nContinual learning from human feedback. Suhr and Artzi (2022) let humans instruct the follower and then ask them to rate the agent's behaviour (thumbs up or down). This binary feedback is used for further training as the reward signal in a contextual bandit framework. They show that the agent improves over several interactions with humans. Similarly we evaluate the learning process in the context of RL because it imposes \"weaker constraints on the regularity of the solution\" (Nguyen et al., 2019) , but take a broadly available, off-theshelf learning algorithm (Schulman et al., 2017) to directly study the effects of different kinds of feedback. The feedback given to our agent is of natural language and not directly bound to the re-ward; the follower needs to learn the meaning of the language feedback itself.\nLanguage-guided policy learning. Chevalier-Boisvert et al. ( 2019) compared the sampling complexity of RL and imitation learning (IL) agents on various language-conditioned tasks. They proposed a 2-dimensional visual environment called Minigrid in which an agent is given a single mission statement that instructs the agent to achieve a specific state, e.g. \"Take the red ball\". In contrast to them we intentionally do not use IL approaches, because then the agent would have already learnt how to ground the language signals. We want to test if the agent can pick-up on the language from the interaction alone. For this, we similarly propose a diagnostic environment to directly control for the distributions of target objects (cf. skewed distribution of target objects in CVDN (Thomason et al., 2019) ) and feedback signals.\nOther work uses the Minigrid environment to propose a meta-training approach that improves the learning via natural language corrections, e.g. \"Pick up the green ball\" (Co-Reyes et al., 2019) . The agent is given an episodic correction if a specific task cannot be solved. In this way, the agent must not only ground the mission statement but also ground the corrections into actions. Mu et al. (2022) improve policy learning with intra-episodic natural language sub-goals e.g. \"Pick up the ball\". These sub-goals are provided by a trained teacher policy when a previous sub-goal has been reached. In contrast, we rather follow earlier work (Engonopoulos et al., 2013) on monitoring execution and use a heuristic teacher which provides intraepisodic language feedback whenever it appears feasible. The agent has to learn that certain pairs of feedback and behaviour at a specific time-step lead to the task's success and others to failure.\n\nThe CoGRIP environment\nWe use a Collaborative Game of Referential and Interactive language with Pentomino pieces as a controllable setting. A teacher instructs a follower to select a specific piece using a gripper. Both are constrained as follows: The teacher can provide utterances but cannot move the gripper. The follower can move the gripper but is not allowed to provide an utterance. This asymmetry in knowledge and skill forces them to work together and coordinate. Zarrie\u00df et al. (2016) found that this settings leads to diverse language use on the teacher's side. \n\nProblem Formulation\nThe follower has to navigate a gripper to select a piece described by the teacher. We frame this task as a RL problem with sparse rewards. At each time-step t, given an observation o t \u2208 O of the environment, the agent has to select an action a t \u2208 {LEFT, RIGHT, UP, DOWN, WAIT, GRIP} such that the overall resulting sequence of actions (a 0 , ..., a t , ..., a T ) maximizes the sparse reward R(o T ) = r. An episode ends when the GRIP action is chosen, and the gripper position g t is in the boundaries of a piece. An episode also ends when t reaches T max = 100. Following Chevalier-Boisvert et al. ( 2019), the reward function returns a basic reward minus the movement effort R = 1 \u2212 0.9 * (T /T max ). We extend this formulation and give an additional bonus of +1 if the correct piece has been taken or a penalty of \u22121 when the wrong or no piece has been taken at all.\n\nEnvironment\nThe environment exposes at each time-step t an observation o t that contains the gripper coordinates g t = (x, y), the initial referring expression l RE , the language feedback l FBt (which might be empty) and a partial view v t of the scene. While the scene as a whole is represented as a 2-dimensional image (with RGB colour channel), the partial view represents a 11 \u00d7 11-sized cut out, centered on the gripper position (see Figure 2 ). The teacher generates the initial and feedback statements.\n\nTeacher\nFor the teacher, we assume a heuristic behaviour (a fix policy) that has been shown to lead to collaborative success with humans (G\u00f6tze et al., 2022) and leave the complexity of learning in a multiagent setting (Gronauer and Diepold, 2022) for future work. The teacher produces an initial referring expression l RE = (w 0 , ..., w N ) where N is the message length and w i is a word in the vocabulary. The production rule is implemented following the Incremental Algorithm (IA) (Dale and Reiter, 1995) that is given the symbolic representations of the pieces on the board (see Appendix A.1). The teacher provides a feedback message l FBt = (w 0 , ..., w N ) at a time-step t >0 when the gripper's position g t has exceeded a pre-defined distance threshold D dist = 3 compared to the gripper's last position of feedback g FB last or it is over a piece. The generated feedback is of positive sentiment (\"Yes this way/piece\") when the gripper is then closer to or over the target piece and negative otherwise (\"Not this direction/piece\"). Alternatively, suppose the follower does not exceed the distance threshold after D time = 6 time-steps the feedback message is the same as the initial statement. Overall, the property values and sentence templates lead to a small vocabulary of 33 words.\n\nFollower\nThe follower agent has to move the gripper and successfully grip a piece solely based on the observations. The observations o t = (v t , g t , l RE , l FBt ) are mapped to 128-dimensional features xt \u2208 R using the encoder model (see Figure 2 ). Following Chevalier-Boisvert et al. ( 2019), the word embeddings (which are learned from scratch) of the language inputs are fed through a Gated Recurrent Unit (GRU) (Cho et al., 2014) and then combined with the embedded visual features using a Featurewise Linear Modulation (FiLM) layer (Perez et al., 2018) . These language conditioned visual features are then max pooled, averaged and again averaged with the gripper position. Given the resulting features xt , we learn a parameterised policy \u03c0(x t ; \u03b8) \u223c a t that predicts a distribution over the action space. We use the Proximal Policy Optimization (PPO) (Schulman et al., 2017) implementation of StableBaselines3 v1.6.2 (Raffin et al., 2021) to train the policy in our environment.\n\nTasks\nThe follower has to grip an intended target piece among several other pieces (the distractors). Thus a task is defined by the number of pieces, the target piece and the map size. The pieces for the tasks are instantiated from symbolic representations: a tuple of shape ( 9), color (6) and position (8) which leads to 432 possible piece symbols. For our experiments we use all of these symbols as targets, but split them into distinct sets (Appendidx A.4). Therefore the targets for testing tasks are distinct from the ones in the training tasks. We ensure the reproducibility of our experiments by constructing 3300 training, 300 validation, 720 testing tasks representing scenes with a map size of 20 \u00d7 20 and 4 or 8 pieces.\n\nExperiments\nIn this section we explore the effects of the teacher's language and intra-episodic feedback on the follower's success and ask whether the follower generalizes on aspects of scene complexity.\n4.1 Which referential language is most beneficial for the agent's learning success?\nAs suggested by Madureira and Schlangen (2020) we explore the question of which language is most effective. The IA constructs the initial reference by following a preference order over object properties (Krahmer et al., 2012) . We hypothesize that a particular order might be more or less suitable depending on the task. Thus we conduct a series of experiments without the feedback signal where the preference order is varied as the permutation of color, shape and position. Our results indicate that such orders perform better that prioritize to mention positional attributes as distinguishing factors of the target piece (see Table 1 ). This is reasonable as the directional hint reduces the agent's burden for broader exploration. The follower is able to pick up early on these positional clues and performs overall better during training (see Figure 3 ).\n\n4.\n2 What is the agent's performance gain with intra-episodic feedback in our setting?\nWe conduct the same experiments as above with intra-episodic language feedback to measure its effect on the follower's success rate. Our results show that the follower achieves higher success rates with intra-episodic feedback among all preference orders (see Table 1 ). We also notice that the gain is higher for the low-performing preference orders. This shows that the intra-episodic feedback is a valuable signal for the follower to overcome miss-ing directives in the initial referring expressions.\nThe agent can learn strategies incorporating the feedback signals. This is an interesting finding because language feedback is not part of the reward function and could be empty.\n\nDoes intra-episodic feedback help the agent to generalize on scene complexity?\nAs a proxy for generalization capabilities, we take the best performing follower and raise the complexity of the testing scenes along two dimensions (i) we increase the map size to 30 \u00d7 30 and (ii) put up to 18 pieces on the board. In addition, we hold out 72 combinations of piece shapes and colors that have never been seen during training. Our results show that the agent trained with intra-episodic feedback is able to perform better (i) on the larger map size, (ii) the higher number of pieces and (iii) the new target pieces compared to the one without (see Table 2 ).\n\nConclusion\nIn this work, we studied the effects of a teacher's language and intermediate interventions (the feedback) towards a learner's success and whether the learner generalizes on aspects of scene complexity. Our results show that there is a most beneficial language for the teacher. Its intra-episodic feedback allows the learner to learn faster and generalize better than without intermediate help. An exciting direction for further work is to show the benefits of language feedback for other reinforcement learning problems, to overcome the limits of the heuristic teacher strategy and to reduce the need for feedback after successful training.\n", "hypothesis": " In this paper, we present an initial study that evaluates intraepisodic feedback given in a collaborative setting.  We use a referential language game as a controllable example of a task-oriented collaborative joint activity.  A teacher utters a referring expression generated by a well-known symbolic algorithm (the \"Incremental Algorithm\") as an initial instruction and then monitors the follower's actions to possibly intervene with intra-episodic feedback (which does not explicitly have to be requested).  We frame this task as a reinforcement learning problem with sparse rewards and learn a follower policy for a heuristic teacher.", "answer": true}
{"title": "Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation", "content": "\nIntroduction\nPrompt-based learning typically works by modifying the input into cloze-style prompt templates and using the masked language models (MLMs) to complete the unfilled information in probabilistic. It has achieved promising performance in various NLP tasks (Schick and Sch\u00fctze, 2021; Lester et al., 2021; Hu et al., 2021) , especially in low-resource settings (Scao and Rush, 2021) . A promising prompt engineering category is demonstration learning (Gao * Equal contribution.\n\u2020 Corresponding author. et al., 2021; Liu et al., 2021a) , which seeks to provide a few answered samples as demonstrations to assist prompt prediction. As shown in Fig. 1 (a), the demonstration learning method concatenates the answered demonstrations per category to the prompt, and seeks to classify the [M ASK] token as great, indicating a positive prediction result based on a label-to-word mapping.\nThe intuition of demonstration learning is that samples with similar expressions or content can provide repetitive patterns (Liu et al., 2021a) . However, Min et al. (2022) point out that replacing gold demonstration labels with random labels marginally hurts performance. This finding is counter-intuitive and illustrates that the model could not comprehensively refer to the knowledge brought by the demonstrations in an implicit way. We attribute this problem to that existing methods simply concatenate the answered demonstrations to the prompt template without any additional operation, ignoring the dependencies between prompt and demonstrations.\nTo overcome this limitation, we rethink how human beings learn from demonstrations. Intuitively, when faced with a new challenging question, they typically (1) look for the most similar example to the question first, and then (2) reply to the question according to the answering steps of the retrieved example. Humans tend to strengthen the learning process through review strategies, i.e., finding a better solution to select similar examples and reanswering the questions of examples to consolidate known knowledge. Inspired by this, likewise, the interactions between the prompt and demonstrations could also be reinforced by imitating the human reviewing process for demonstration learning.\nIn this paper, we propose a simple-yet-effective version of demonstration learning, named Imitation DEMOnstration Learning (Imitation-Demo) to explicitly strengthen the two sub-steps of demonstration learning via human-like review. Specifi- \n\nContext 2:\nThe drama discloses almost nothing. cally, to accurately locate similar samples, we introduce a contrastive learning mechanism (Chen et al., 2020; Robinson et al., 2021) to reorganize demonstrations by reducing the divergences of demonstration contexts among the same category while increasing those divergences between different categories. Besides, to solidify known knowledge, we leverage a demonstration-label re-prediction method to emphasize the positions of the answers in demonstrations. Even without introducing new parameters or any prediction computation, our proposed method achieves state-of-the-art performance on 5 out of 14 classification corpus. Compared to the strong baseline LM-BFF (Gao et al., 2021) , Imitation-Demo achieves 1.11 points averaged improvement on the 14 datasets. Further study also shows that Imitation-Demo strengthens the association between prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.\n\nMethodology\nDemonstration Learning.\nAs illustrated in Fig. 1 (a), The prompt template x prompt consists of input sentence x sent and template x temp containing mask token, i.e., x prompt = [x sent , x temp ]. Firstly, we leverage the pre-trained SBERT (Reimers and Gurevych, 2019) to retrieve the demonstrations (including context x (k) and label y (k) ) for the k-th category that has maximum semantic similarity to the raw prompt context. Then, the retrieved demonstrations are concatenated to the input prompt. After that, we convert the concatenated input sentence\nx in to hidden vectors h in via the RoBERTa model (Liu et al., 2019) . The model is optimized by crossentropy loss, and the goal of demonstration learning is to predict y mask at the [M ASK] position from the hidden state of mask h mask via MLM head. The whole process could be formulated as 1 :\nx in = [x prompt , (x (1) , y (1) ), ...,(x (K) , y (K) )] h in = RoBERTa(x in ) L mask = CE(h mask , \u0176 mask ) p y mask | x in = MLM(h mask ) (1)\nwhere [.., .., ..] denotes concatenating diverse parts with sentence separator [SEP ] . K is the number of categories. CE is short for cross-entropy loss, and \u0176 mask is the ground-truth labels from the predefined label-to-word mapping. Demonstration Reorganization via Contrastive Learning. In demonstration learning, it is crucial to decide from which known demonstrations to select the repetitive patterns. Therefore, we introduce a contrastive learning mechanism to imitate human review behaviour by reorganizing the demonstrations based on their contexts. As shown in Fig. 1 (b)(I), we treat the demonstration contexts with identical categories to the input prompt as positive samples, and the others are regarded as negative ones. By pulling in positive samples and pulling out negative samples, the model could select the most relevant sample among the given demonstrations more precisely. In the experiment, we apply mean-pooling operations on the hidden states of positive, negative demonstration contexts h + , h \u2212 , and input sentence h in , obtaining the sentence representations s + , s \u2212 , and s in . Inspired by Robinson et al. (2021) in computer vision, we introduce HCL loss to ensure intra-class compactness while increasing inter-class distances:\nL context = E \u2212 log e s in \u2022s + e s in \u2022s + + N i=1 e s in \u2022s \u2212\n(2) where \u2022 is the dot product operation, N is the number of negative contexts in the task, and E [..] denotes calculating the mean value. Demonstration-label Re-prediction. We further utilize a demonstration-label re-prediction method to mimic human review behaviour by recovering the labels from all the given demonstration contexts. Specifically, the target of our model is not only to identify the category of [M ASK] token, but also to classify the tokens located in demonstration label positions. Take the binary classification task in Fig. 1 (b)(II) as an example, more than predicting the class of the mask token, the model also requires to predict y great and y terri (i.e., great and terrible) based on the hidden states h great and h terri at corresponding label positions.\nDuring training, the cross-entropy loss is utilized to calculate L great and L terri for different demonstration labels, then we sum them up to obtain the demonstration-label re-prediction loss L label :\nL great = CE(h great , \u0176 great ) L terri = CE(h terri , \u0176 terri ) L label = L great + L terri (3)\nwhere \u0176 great and \u0176 terri are the ground-truth labels at diverse demonstration label positions.\nSimilar contrastive learning and demonstrationlabel re-prediction operations can also be performed for the multi-category classification tasks. The overall loss of Imitation-Demo is defined as follows:\nL = L mask + \u03b1L label + \u03b2L context (4)\nwhere \u03b1, \u03b2 are weight coefficients to control the importance of different components. datasets. For SNLI (Bowman et al., 2015) , SST-2 (Socher et al., 2013) , CoLA (Warstadt et al., 2019) , MNLI (Williams et al., 2018) , QNLI (Rajpurkar et al., 2016) , RTE (Dagan et al., 2005; Giampiccolo et al., 2007; Bentivogli et al., 2009) , MRPC (Dolan and Brockett, 2005) , QQP 2 and SST-B (Cer et al., 2017), we use the original development sets for testing. For MR (Pang and Lee, 2005) , CR (Hu and Liu, 2004) , MPQA (Wiebe et al., 2005) and Subj (Pang and Lee, 2004) , we randomly sample 2,000 examples as the testing set. For SST-5 (Socher et al., 2013) and TREC (Voorhees and Tice, 2000) , we use the official test sets. F1 score (F1) are adopted as the evaluation metric of MRPC and QQP, and the other datasets utilize accuracy (acc) as the evaluation criteria. Parameters Setting We implement all the baselines and our frameworks using PyTorch (Paszke et al., 2019) . The pre-trained RoBERTa-large model and roberta-large-nli-stsb-mean-tokens SBERT (Reimers and Gurevych, 2019 ) from huggingface 3 are applied in the experiments. We get 16 samples per class during training for all models. In order to control the smoothness of the exponential functions when calculation contrastive learning loss, we divide every mean-pooling results with temperature T . Grid search mechanisim are utilized to select optimal hyper-parameter combinations on each split. Finally we select the the coefficients \u03b1 and \u03b2 as 1 and 5, respectively. The temperature T is set as 5 and the batch size is 16. The other hyper-parameters and the prompt templates are identical to the default settings in LM-BFF (Gao et al., 2021) for fair comparison. We report the average performance of models trained on 5 different randomly sampled training and dev splits, the random seeds are fixed as 13, 32, 42 ,87 , 100, respectively. Compared Methods. (1) Majority, which select the majority class of the dataset; (2) Prompt-based zero-shot: which use prompt tunning in zeroshot situations; (3) \"GPT-3\" in-context learn- Table 2 : Overall results on RoBERTa-large with 16 samples per class. We report the mean (variance) of models trained on 5 different randomly sampled training and dev splits. Prompt-based Fine-tuning (man) indicates trained with manually designed templates. \u2661 denotes we re-implement the EFL and LM-BFF models for fair comparisons.\ning, which use the in-context learning proposed in RoBERTa with no parameter updating; (4) Finetuning;\n(5) P-tuning (Liu et al., 2021b) , which employ trainable continuous prompt embeddings;\n(6) DART (Zhang et al., 2021) , which differentially optimize the prompt template and the target label during the backpropagation process; (7) Li's (Li et al., 2022) , which reformulate a classification or a regression task as a token-replaced detection problem utilizing pre-trained model Electra (Clark et al., 2020) ; (8) Demo-tuning (LM-BFF) (Liang et al., 2022) , which select \"mask token\" output feature as the input for contrastive learning to get a good representation of \"virtual demonstration\". We select the LM-BFF as the basic backbone model for fair comparisons. ( 9) LM-BFF + Sup-Con (Jian et al., 2022) \n\nConclusion\nIn this paper, we propose imitation demonstration learning (Imitation-Demo) to reinforce the correlations between prompt and given demonstrations. Inspired by the human review process, we introduce contrastive learning to locate similar samples and demonstration-label re-prediction mechanisms to solidify known knowledge. Experiments show that our method consistently outperforms other baselines on 5 out of 14 classification datasets in the few-shot settings. We hope this work could inspire the exploration of the working mechanism of demonstration learning and toward better few-shot learning abilities.\n", "hypothesis": "Besides, prior research found that randomly replacing the labels of demonstrations marginally hurts performance, illustrating that the model could not properly learn the knowledge brought by the demonstrations.  Inspired by the human learning process, in this paper, we introduce Imitation DEMOnstration learning (Imitation-Demo) to strengthen demonstration learning via explicitly imitating human review behaviour, which includes: (1) contrastive learning mechanism to concentrate on different demonstrations.(2) demonstration-label re-prediction method to consolidate known knowledge.", "answer": false}
{"title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages", "content": "\nIntroduction\nInspired by the recent advancements in language model pre-training, Vision-Language Pre-trained Models (VLPMs) have demonstrated state-of-theart performance across a wide range of visionlanguage (VL) tasks such as text-to-image retrieval, visual reasoning, visual entailment, and visual QA (Chen et al., 2020; Li et al., 2021 Li et al., , 2022)) .\nHowever, extending VLPMs to multilingual scenarios is still challenging. On one hand, the majority of these models are trained on monolingual (English) corpora and thus cannot perform well for other languages. On the other hand, the multilingual pre-trained language models (Devlin et al., Figure 1 : Overview of our approach. We adapt the text encoder of a monolingual VL model to an unseen language (a). Then we use the adapted model for a VL downstream task in a zero-shot setting (b).\n2018; Conneau et al., 2019) cannot handle vision data (e.g., images or videos) directly.\nLately, there have been attempts (M 3 P, nUNITER, UC 2 ) to pivot on images or English texts to align multilingual representations with vision features (Chen et al., 2020; Ni et al., 2021; Zhou et al., 2021) .\nHowever, a recent benchmark on multilingual multimodal pretraining (IGLUE) (Bugliarello et al., 2022) shows that although these models achieve promising zeroshot cross-lingual transfer performance on some VL tasks, they still fall short in comparison to the \"translate-test\" baseline (using an English-only VLPM on the translations of the text examples).\nA more recent work (CCLM) achieves promising performance on the IGLUE benchmark by exploiting massive parallel text and image-text corpora to pre-train a VL model (Zeng et al., 2022) . This approach is motivated by a key observation that multilingual and multimodal pre-training essentially achieves the same goal of aligning two different views of the same object into a common semantic space. Although this framework performs well on the IGLUE benchmark, it requires a large amount of parallel data. Its pre-training phase relies on 19M multilingual parallel sentence pairs extracted from WikiMatrix (Schwenk et al., 2021) , jointly trained with 4 million image-text pairs in multiple languages.\nIn this work, we are proposing a simple yet efficient way to adapt VLP models to unseen languages without requiring large parallel corpora. We propose to align a VLPM monolingual text encoder (achieving start-of-the-art performance on English downstream VL tasks) with a multilingual pre-trained language model (e.g., mBERT), using only small in-domain parallel text corpus. The recent progress in Neural Machine Translation (NMT) has enabled us to create such a parallel corpus from automatically translating the data from English to any other language, even for lowresource languages (i.e., Swahili). However, since our approach relies on token alignment, it is robust to errors made by NMT. Our zero-shot evaluation across three of the four IGLUE tasks shows that the proposed method achieves state-of-the-art results while using small set of in-domain parallel sentences. The key steps of our approach are illustrated in Figure 1 .\n2 CLiCoTEA : Cross-Lingual\n\nContextualised Token Embedding Alignment\nWe propose CLiCoTEA , an approach to transfer a monolingual vision-language (VL) pre-trained model in one language L 1 where there is an abundant number of training pairs of image and text (i.e., English) to a second language L 2 . As we focus in this paper on the zero-shot setting, we do the transfer after fine-tuning the pre-trained monolingual VL model on a downstream task t, where training samples are available in language L 1 .\nCLiCoTEA consists of six steps:\n1. Pre-train a monolingual VL model on a massive collection of image-text pairs, where text is written in language L 1 .\n2. Fine-tune the VL pre-trained model on the downstream task t in language L1.\n3. Create a parallel text corpus by translating the training set from step 2 in the target language L 2 . Note that this step can be done automatically using neural machine translation.\n4. Create a list of aligned tokens for each (potentially noisy) parallel sentence using a token alignment model. 1b .\nIn practice, steps 1 and 2 are the most computationally expensive. Therefore, we propose to adapt VL fine-tuned models to new languages by only doing the steps from 3 to 5 which can be computed in a few hours on a single GPU.\nWe note that CLiCoTEA could be used with any multimodal pre-trained model where one of the modalities is a monolingual text encoder. We focus in this paper on VL models, but CLiCoTEA could be applied for instance to a language-knowledge model such as GreaseLM (Zhang et al., 2021) or DRAGON (Yasunaga et al., 2022) .\n\nPre-trained Models\nVision-Language Model In step 1 of CLiCoTEA , we use the Align BEfore Fuse (ALBEF) framework 1 (Li et al., 2021) as our Vision-Language Pre-trained Model (VLPM). AL-BEF has been fine-tuned on multiple downstream VL tasks and achieves state-of-the-art performance. We use the ALBEF fine-tuned models in step 2 for the downstream tasks described in Section 3.3. Unlike other competitive VL pre-trained models (such as BLIP (Li et al., 2022) ) that inject visual information by inserting cross-attention for each transformer block of the text encoder, ALBEF first encodes the image and text independently with a detector-free image encoder and a text encoder. Then it uses a multimodal encoder to fuse the image features with the text features through cross-modal attention. All encoders are based on transformer networks with the text encoder being a 6-layer transformer initialised using the first 6 layers of the BERT base . We thus extract this 6-layer text encoder for cross-lingual transfer training in step 5.\nMultilingual Language Model As a multilingual pre-trained language model, we use the multilingual BERT (mBERT) 2 (Devlin et al., 2018) . It has been trained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective and has demonstrated remarkable zero-shot cross-lingual transfer capabilities (Wu and Dredze, 2019; Pires et al., 2019; Hu et al., 2020; Conneau et al., 2018) . We extract the first 6-layer transformer to be aligned with the text encoder of ALBEF in step 5.\n\nImplementation Details\nWord Alignment Since the parallel sentences do not contain word-level alignment information, in step 4 of CLiCoTEA we utilize awesome-align 3 (Dou and Neubig, 2021) which is a tool that automatically extracts word alignments from mBERT. The generated word pairs are then filtered for keeping only one-to-one, oneto-many or many-to-one alignments and removing many-to-many alignments. This is done for all languages except Chinese because otherwise less than 3% of the training data would remain in the set. The advantage of this filtering is twofold: a) it removes the noise from the matching word pairs; b) it reduces the training time and computation. For words that are split into sub-word tokens, we consider either the left-most token embedding alignment (i.e., the first sub-word token of a word) or, the average embedding across all sub-word tokens.\n\nContextualised Token Alignment Training\nGiven a set of aligned contextual word pairs extracted from parallel sentences, we define {x i , y i } n i=1 , where x i \u2208 R d is the contextualised embedding of token i in the target language (obtained from mBERT), and y i \u2208 R d is the contextualised embedding of its alignment in the source 2 Available on HuggingFace hub at https://huggingface.co/ bert-base-multilingual-cased.\nData Augmentation As multilingual language models are generally pre-trained on the source language L 1 , the contextualised token alignment can be trained not only with sentences from the target language L 2 , but also with sentences from the source language L 1 . This strategy doubles the training size, and consequently, the training time but it could be used with tasks where the number of available training sentences is limited.\n\nDownstream Tasks\nIn step 6, we evaluate CLiCoTEA on three tasks from the IGLUE benchmark 5 in the zero-shot setting:\n\u2022 xFlickr&CO: The dataset is composed of 1000 images from Flickr30K (Plummer et al., 2015) and 1000 images from MSCOCO dataset (Lin et al., 2014) . These images come along with croudsourced image captions in 6 different languages. xFlickr&CO is a retrieval task dataset. It is composed of two subtasks: image-to-text retrieval (TR) and text-to-image retrieval (IR).\n\u2022 XVNLI: The dataset consists in merging SNLI hypothesis with Flickr30K (Plummer et al., 2015) images and translate the test set in four languages. The task is called visual entailment (VE) which is a fine-grained reasoning task to determine whether a text hypothesis \"contradicts\", \"entails\", or is \"neutral\" with respect to an image.\n\u2022 MaRVL: The dataset is a multilingual expansion of NLVR2 dataset (Suhr et al., 2017) , with images related to concepts of five languages and cultures. The task is called visual reasoning (VR) which consists in determining whether a statement is correct given a pair of images.\nStep Table 1 shows the datasets used for a) fine-tuning the monolingual VL pre-trained model in step 2, b) training the alignment of contextualised token embeddings in step 5, and c) testing the zero-shot cross-lingual transfer in step 6. For creating the parallel corpus in step 3, all datasets used for finetuning the monolingual pre-trained VL model are translated to the corresponding test dataset languages from the IGLUE benchmark using Google-Trans Python API 6 . Statistics about the translation datasets can be found in Section A.1. MaRVL being the smallest dataset, the data augmentation strategy described in Section 3.2 is applied only for this task. Detailed results on data augmentation can be found in Section 3.2. 2 shows that CLiCoTEA outperforms the state-of-the-art CCLM models for all downstream tasks except retrieval. The larger improvement against CCLM models is obtained in visual entailment with an increase of almost 5%. The superiority of CLiCoTEA is especially high for Spanish (+7.68%), as can be seen from Table 10 in Section A.4. The average performance on visual reasoning is similar to CCLM, but CLiCoTEA significantly outperforms CCLM by \u00b14% on the low-resource languages such as Tamil and Swahili (results per language can be seen in Table 8 in Section A.3). For retrieval, CLiCoTEA outperforms all models except CCLM 4M . It is worth mentioning that, unlike the other models, CCLM 4M has been pre-trained on COCO which could explain its supe- riority on Flickr&CO dataset. More details about the results on retrieval can be found in Section A.2.\n\nConclusion\nIn this paper, we present CLiCoTEA an approach for adapting Vision-Language pre-trained models to unseen languages. Unlike other approaches that rely on an expensive pre-training phase (both in terms of data and computation), our approach adapts the contextualised token embeddings of a multilingual pre-trained language model by aligning them with the contextualised token embeddings of the VLPM text encoder. By aligning ALBEF text encoder with mBERT, we show that CLiCoTEA outperforms CCLM, which exploits massive parallel text and image-text corpora.\nCLiCoTEA achieves start-of-the-art performance on visual entailment and visual reasoning, with an increase of almost 5% on visual entailment. It also demonstrates its effectiveness, especially for low-resource languages, as it does not require large corpora to do the adaptation.\n", "hypothesis": " However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks.  In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.  We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages.  Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data.", "answer": true}
{"title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning", "content": "\nIntroduction\nPretrained large language models (LLMs; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2020; Soltan et al., 2022) , commonly trained on massive crowd-sourced corpora, have been of much interest in the recent past due to their usage as backbones in state-of-the-art models across multiple downstream NLU tasks. However, they have been shown to memorize significant portions of their training data that can be extracted using appropriately-crafted prompts (Carlini et al., 2020 (Carlini et al., , 2022;; Zhang et al., 2021) . Such extractions pose a privacy risk to the contributors of the training data.\nIn this context, methods that allow developers to control the extractability of memorized examples from LLMs are of much value. For example, methods that increase extraction rates correspond to attacks in an adversarial setting, and provide developers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to as defenses, are useful for protecting against such attacks. Historically, defense methods tend to be compute intensive (Abadi et al., 2016; Dupuy et al., 2021) .\nIn this work, we train continuous soft-prompts (Lester et al. 2021 ; hereafter referred to simply as prompts) and leverage them as a way of passing an external signal into an LLM, to control the extraction of memorized data. We freeze the model weights, and only use the trained prompt to control the generation. First, we train prompts in an attack setting and study the extent of extractable memorized content in our models. Second, we explore a defense setting where we create prompts that reduce extraction rates and achieve different privacy-utility trade-offs, via a user-specified hyperparameter. Since the original model weights are frozen in both these settings, our methods are compute efficient across the board.\nTo the best of our knowledge, our work is the first to adapt the use of instructive prompts for the analysis and mitigation of privacy in LLMs. We have released the code developed for our experiments 1 .\n\nBackground and Related Work\nPrevious work has shown that LLMs display memorization and has explored a range of methods that quantify extractability (Carlini et al., 2018 (Carlini et al., , 2020 (Carlini et al., , 2022)) . Differentially-private training (Dwork, 2006; Abadi et al., 2016) is a popular method that has been used to mitigate this risk. However, it tends to reduce model utility and requires retraining of the LLM, which might not be feasible due to heavy computational burden. The use of instructive prompts for language models has been extensively researched, including use during pretraining (Raffel et al., 2020) , as a second stage of training (Sanh et al., 2022; Wei et al., 2021) , and during inference to guide model output (Brown et al., 2020) . Within the third category, in order to improve upon manual prompt engineering researchers have implemented methods to learn discrete natural language prompts (Shin et al., 2020) , to mine them (Jiang et al., 2020) , or, neglecting natural language, to learn continuous prompts (Li and Liang, 2021; Lester et al., 2021) .\nOur work leverages continuous prompts as a way of passing an external signal to a model to trigger a desired model behavior (i.e., less or more memorized data in open language generation, which map to an extraction attack and defense, respectively).\n\nMethod\nPrompt-tuning requires the prepending of a prompt to the prefix embedding and access to the training loss (see Figure 1 ). Given these constraints, we explore a white-box attack where the adversary has access to the target model parameters, and a blackbox defense where the adversary interacts with the target model via an API. We therefore do not test our defense against our own attack.\nLet [prefix || suffix] be a sequence in the training set where the prefix is of length k tokens. Carlini et al. (2022) defined a suffix to be k-extractable if the model generates the suffix exactly, after being prompted with its the corresponding lengthk prefix. Our white-box attack aims to increase the number of k-extractable sequences, while our black-box defense aims to reduce the number of k-extractable sequences that can be extracted by an adversary who submits prefixes via an API.\n\nAttack\nIn the attack setting, we assume that the adversary has a set of [ prefix || suffix ] sequences S train , sampled from the training set of the target model. Their goal is to extract the suffixes corresponding to a disjoint set of prefixes, denoted by S test 2 . To do so, the adversary first initializes a prompt: a continuous set of l \u00d7 e parameters where e is the embedding size of the model, and l is the length of the prompt, a hyperparameter decided by the adversary. The prompt is trained over S train to facilitate the correct generation of suffixes. To do this, we first prepend the prompt to the embedding of the prefix and pass the joint embedding through the model for generation. We then minimize the loss objective (see below) with respect to the prompt while keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is causal language modeling (hereafter referred to as CLM), where we minimize the cross-entropy loss over the entire sequence (Radford et al., 2019) . In the second, the prompt is optimized by minimizing the cross entropy loss of only the suffixes, given the prefixes. Here, the training is aligned with our inference task such that during training the model is penalized only on the suffix tokens; hence we refer to it as aligned CLM. During inference, the learned prompt is prepended to each embedding of the prefixes in S test , and the joint embedding is passed to the model for generation (see Figure 1 ).\n\nDefense\nIn the defense setting, the defender (API owner) trains the prompt, and prepends it to the incoming prefixes before passing them to the model. Our algorithm is inspired by machine-unlearning literature (Halimi et al., 2022) , and defenses against membership inference and backdoor attacks (Chen et al., 2022; Ozdayi et al., 2021) . We introduce a hyperparameter named learning threshold denoted by \u03b8. During prompt training (see Section 3.1), when loss is less than \u03b8 we do gradient ascent to penalize the prompt. If the loss is greater than \u03b8, we perform gradient descent with respect to the prompt as usual. Training is stopped once the average epoch loss is equal or above \u03b8. This allows us to increase training loss in a controlled manner and stabilize it around \u03b8. Through this process, we can achieve various privacy-utility trade-offs efficiently without re-training any part of the model. To explore \u03b8, we set the initial value to be slightly above the model training loss and increase in steps of 0.25 until desired performance is achieved.\n\nExperiments\nFor our experiments, we use the 125M and 1.3B parameter variants of the GPT-Neo models (Black et al., 2021) . These are public, decoder-only transformer models (Vaswani et al., 2017) trained using CLM on the Pile dataset (Gao et al., 2020) . We extract S train and S test from the Language Model Extraction Benchmark dataset (Google-Research). This dataset contains 15k sequences sampled from the training split of the Pile where each sequence is partitioned into a prefix and suffix. In the default evaluation setting, both prefix and suffix consist of 50 tokens. We ensure a random train/test split of 14k/1k samples.\nOur evaluation metric of choice is Exact extraction rate which is the fraction of correctly generated suffixes (i.e., all tokens of the generated suffix match with ground-truth suffix) over the test set. We additionally discuss fractional extraction rate and present results in Appendix A. As a baseline, we use the attack analyzed in Carlini et al. (2022) , which consists of feeding the prefixes to the model, and generating suffixes with greedy decoding. This is the only extraction attack for this setting apart from our work, to the best of our knowledge. Our training setup is discussed in Appendix B. All experiments are repeated over 5 runs with a new random train/test split in each run.\n\nAttack\nWe explore the performance of our attack across several dimensions: prompt length, suffix size, prefix size, and beam size. We use greedy-decoding in all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length in the context of the default setting (prefix and suf-fix consist of 50 tokens; Figures 2-A1 and 2-A2 ). We note that prompts tuned with both CLM and aligned CLM provide improvements over the baseline in all cases, with aligned CLM providing the best performance. Given this, we train prompts using the aligned CLM objective for all other experiments, including our defense.\nWith aligned CLM, we achieve the highest extraction rates of 25.8% and 54.3% for the 125M and 1.3B models, respectively (an improvement of 8.9 and 9.3 percentage points, respectively), with a 100 token prompt (blue line). We observe that extraction rates increase with prompt length and tend to saturate after prompt length 100. Over-fitting was ruled out as a potential cause of saturation as there is no increase in test loss observed during training. This suggests that there is a max limit on the parameter count in the prompt that might add value for extraction purposes given our objective. We note that more sophisticated training strategies (designing better loss functions, better prompt initialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and vary the suffix size. As shown in Figures 2-B1 and 2-B2, extraction rates decrease roughly exponentially with suffix size. We note that as suffix size increases, longer prompts (\u2265 20) provide greater improvements over the baseline. For example, with a prompt length of 100 (blue line) using the 1.3B model, at suffix size 5 we observe an extraction rate increase of 5.3 percentage points. Whereas at suffix size 50, the increase is 9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and vary the prefix size. As shown in Figures 2-C1 and 2-C2, extraction rates increase roughly logarithmically (as in Carlini et al. 2022) . Contrary to suffix size, we observe that the gaps between baseline and attacks decrease with increasing prefix size. This suggests that our attack stands to benefit a less informed adversary (small prefix sizes) when compared to the baseline.\nBeam Decoding Finally, we utilize the default setting with prefix and suffix sizes at 50 tokens and vary the beam size (beam size=1 corresponds to greedy decoding). The results are shown in Figures 2-D1 and 2-D2. We observe that extraction rates increase across the board when increasing beam size from 1 to 5. However, improvements tend to plateau or oscillate when beam size is greater than 5. The 1.3B model benefits more from increasing beam size achieving the highest extraction rate of 61.4%, at a beam size of 20 (with a prompt length of 150). The highest extraction rate achieved for the 125M model was 28.3% at a beam size of 15 (with a prompt length of 100).\n\nDefense\nFinally, we evaluate the privacy-utility trade-off of our black-box defense. As mentioned in Section 3, our defense is designed for a black-box adversary, and cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4) to quantify privacy. We note that longer prompts did not add value in a defense setting, so we resort to using a prompt of length 1. We utilize perplexity (PPL) on generated suffixes, to quantify the utility of the model in addition to using exact extraction rate as in Section 3.1. To measure PPL, we use a random subset of 1k sequences sampled from the test split of the Pile, ensuring that PPL is measured on data unseen by the model. We also compare our metrics with those of similar sized models that were not trained on the Pile dataset (GPT2 models). Our premise here is that better performance in terms of privacy and utility, when compared to an out-ofdomain model of similar size, would mean that our defense mechanism is of value to an API owner.\nIn Table 1 , we display our results obtained using the default evaluation setting (prefix and suffix comprise of 50 tokens). Our defense achieves lower extraction rates with competitive PPL values. For the 125M model, we achieve an exact extraction rate reduction of 99.4% relative to baseline with a PPL increase of 25.3% at \u03b8 = 1.75. For the 1.3B model, the extraction rate is reduced by 97.7% relative to baseline with a PPL increase of 16.9% at \u03b8 = 1. The ability to achieve lower extraction rates with lower PPL values as measured against the GPT2 models of the corresponding size, provides evidence that our defense is effective.\n\nConclusion\nWe present the first known effort to leverage prompt-tuning to control the extractability of memorized data from LLMs in an open language generation task. We develop a novel data extraction attack and defense, and illustrate their performance under various settings. Our attack consistently outperforms the baseline in terms of exact extraction rate. Our defense provides competitive privacy-utility trade-offs and would prove beneficial to API owners with model trained on sensitive content. These results are achieved efficiently, without any change to the original model weights. We details avenues of future work in Appendix C\n", "hypothesis": "We present a novel approach which uses prompttuning to control the extraction rates of memorized content in LLMs.  We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. Furthermore, our methods achieve near-perfect privacy-utility trade-offs, making them highly effective in protecting against privacy risks.", "answer": false}
{"title": "CoAug: Combining Augmentation of Labels and Labelling Rules", "content": "\nIntroduction\nNamed Entity Recognition (NER) is the task of identifying entity spans of specific types in a given document. While deep learning has led to the development of highly performant supervised NER models (Ma and Hovy, 2016; Lample et al., 2016; Devlin et al., 2019) , their performance is contingent on the availability of high-quality large labeled datasets, which is often expensive to collect. Moreover, it is impractical to assume the availability of large datasets for all domains. Hence, learning from limited labeled data is a pressing challenge in named entity recognition research. The majority of research in this area can be broadly classified into two distinct paradigms: few-shot learning with pre-trained language models (LMs) and weak supervision methods that utilize heuristic rules for entity extraction. In few-shot learning, models are trained to identify novel entities given just a few labeled examples for each entity type. While pretrained LMs have been explored for this setting, their susceptibility to overfitting on small datasets results in poor performance. Consequently, recent works improve recognition using prototypical networks (ProtoBERT, T\u00e4nzer et al., 2022) , improved representations from self-supervised pre-training of LMs (QuIP, Jia et al., 2022) , and self-training (Huang et al., 2021) . In the iterative learning process of self-training, many candidate entities are extracted and added into the training set for future iterations. However, premature models from initial iterations also add erroneous entities to the training set, resulting in models whose performance lags behind fully-supervised models that utilize large labeled datasets.\nOn the other hand, rule-based weak supervision methods utilize heuristic rules and manual lexicons (Shang et al., 2018; Peng et al., 2019) developed by domain experts to supervise entity recognition models. However, experts may find it challenging to enumerate all possible heuristics, which can limit the diversity of identified entities in docu-ments. In recent work, TaLLOR (Li et al., 2021) overcomes this limitation by automatically learning rules given unlabeled data and an initial set of seed rules (tens of rules). Nonetheless, while rule-based methods offer high precision, their performance is constrained by the logical language specified by the developer, which limits the set of identifiable entities. Moreover, learning rules can fail to identify entities in new linguistic contexts that would otherwise be known.\nWe hypothesize that the two paradigms of fewshot learning and rule-based weak supervision can effectively complement each other, as neural models are skilled at identifying candidates from different linguistic contexts but lack precision, while rulebased methods can identify accurate candidates with precision but lack the flexibility to identify entities in different contexts. Therefore, in this work, we propose Co-Augmentation (CoAug), as shown in Figure 1 , an iterative bootstrapping framework that effectively combines neural models, rule-based weak supervision methods, and unlabeled data.\nOur proposed framework draws inspiration from co-training (Blum and Mitchell, 1998) , but it has its own unique approach. Like co-training, CoAug aims to combine two distinct inductive biases in limited labeled data settings. Unlike co-training, instead of improving two models that use different feature sets individually by bootstrapping labels from each other, CoAug accomplishes the same goal by using two models that use different forms of supervision to expand the same label set. Additionally, in each iteration of CoAug, both classifiers are trained with the predictions made by both models, rather than just one. Our choice allows the framework to function from really small initial training sets for the individual models.\nWe evaluate our approach on four named entity recognition datasets that span general and science domains. Our results indicate that (a) CoAug consistently improves performance over self-training ruleaugmentation and few-shot models while being highly precise, (b) utilizing stronger pre-training for the neural models leads to improved performance of models in our framework. In summary, our contributions are as follows:\n\u2022 We present CoAug, a co-augmentation framework that leverages both rule-augmentation and label-augmentation approaches for NER.\n\u2022 Experimental results show that CoAug can perform better than prior rule-based methods on four datasets in two domains. \u2022 We provide a brief analysis of factors that contribute towards the success of CoAug.\n\nCoAug\nIn this work, we consider a setting where we have access to an initial set of seed rules, S, and a large unlabeled corpus, U, to perform the named entity recognition task. Applying the rules, S, on U provides the initial set of labeled examples, L, to train models in our framework.\nOur framework, CoAug (short for Co-Augmentation), iteratively improves the performance of two models by leveraging the bootstrapped predictions on unlabeled data by each model. Given that prior work in low-resource NER focuses on two parallel tracks of rule-augmentation and few-shot learning methods that do not interact with each other, we instantiate CoAug with a rule-augmentation model and a few-shot model to leverage the best of both paradigms. We refer to these components of our framework as Rule Augmenter and Label Augmenter (Figure 1 ). In the subsections below, we describe the Rule Augmenter and Label Augmenter modules.\n\nRule Augmenter Algorithm 1 TaLLOR\nRequire: U = {x1:N } unlabeled examples Require: R = {S} rules initialized with seed rules Require: C = {c1:M } candidate rules\nInitialize: L = {} for t in (1, . . . , T ) do // Apply rules to get weak-label set W = RULEAPPLIER(R, U) // Filter accurate examples W = LABELSELECTOR(W) L = L \u222a W U = U \\ L // Train NEURAL NER MODEL M \u2190 TRAIN(M , L) // Label using NEURAL NER MODEL LM \u2190 PREDICT(M, U) // Select High-precision Rules RS \u2190 RULESELECTOR(LM , C) C = C \\ RS R \u2190 R \u222a RS end for\nThe primary function of the Rule Augmenter is to automatically learn labeling rules from unlabeled data and use them to generate weak labels for training a neural model. In this work, we instantiate the rule augmenter module using the TaLLOR framework. Accordingly, our rule augmenter has the following subcomponents: (a) RULE APPLIER that applies rules over unlabeled data to generate weak labels, (b) LABEL SELECTOR that filters the most accurate examples based on the similarity of averaged token-level BERT (Devlin et al., 2019) representations of proposed entities to the representations of previously identified entities of the same label in the training set, (c) NEURAL NER MODEL that is trained on the accurate instances and proposes new entities in the unlabeled data that can be used to develop new rules, and (d) RULE SELECTOR that scores candidate labeling rules and selects high-precision rules that satisfy the predictions from the NEURAL NER MODEL. We summarize the iterative process of automatic rule identification by TaLLOR in Algorithm 1.\n\nLabel Augmenter\nThe Label Augmenter module consists of a NEU-RAL MODEL that learns to perform entity recognition with minimal supervision and LABEL SELEC-TOR that selectively adds the weak labels proposed by the NEURAL MODEL into the training set for the next iteration. \u25b7 initial threshold and increment\n\nAlgorithm 2 Label Augmenter\nInitialize: L = R(U) for t in (1, . . . , T ) do // Train NEURAL MODEL M \u2190 TRAIN(M ,L) // Label using NEURAL MODEL LM \u2190 PREDICT(M, U) // Select Examples Using Adaptive Threshold LM \u2190 LABELSELECTOR(LM , \u03b20 + t \u00d7 \u03b21) L = L \u222a LM end for\nIn this work, we experiment with two instantiations of the NEURAL MODEL using recent few-shot NER models, namely, ProtoBERT and QuIP. We use an adaptive threshold for the Label Selector to filter out low-quality, weakly labeled instances. Initially, we add 20% of the proposed instances from the Neural Model to the training set. Then, as the model becomes more confident in its predictions over iterations, we gradually increase the proportion of instances incorporated, with a 5% increase per iteration. We summarize the label augmenter algorithm in Algorithm 2.\nWe provide an outline for the CoAug algo- (Blum and Mitchell, 1998) , in CoAug, the Rule-Augmenter (Label-Augmenter) utilizes the examples that have been labeled by the Rule-Augmenter (Label-Augmenter) and the Label-Augmenter (Rule-Augmenter) to improve its entity recognition performance over iterations.\n\nExperimental Settings\nWe evaluate our framework on four popular datasets that are composed of two science-domain and two general-domain datasets. Following Li et al. (2021) , we utilize the training data without labels as our unlabeled data. Further, for all experiments, we use a set of 20 initial seed rules. These rules specify highly frequent entities for each category within a dataset.\nBC5CDR (Li et al., 2016) contains 1,500 PubMed abstracts with manual annotations for disease and chemical entity mentions. The abstracts are split equally among train, dev, and test sets (500/500/500).\n\nNCBI-Disease (Dogan et al., 2014) contains 793\nPubMed abstracts with manual annotations for disease entity mentions. The abstracts are split as 593/100/100 for train, dev, and test sets.\nCoNLL2003 (Tjong Kim Sang and De Meulder, 2003) We evaluate two instantiations of the CoAug framework where the Rule Augmenter uses TaLLOR, and the Label Augmenter uses either ProtoBERT/QuIP. For baselines, our main experiments compare CoAug against TaLLOR, self-trained ProtoBERT, and self-trained QuIP. Our code is implemented in Pytorch (Paszke et al., 2019) using the Huggingface library (Wolf et al., 2020) . For the Rule Augmenter section, all experimental hyperparameters follow that from Li et al. (2021) . Notably, we use the same hyperparameters for the NCBI-Disease, and WikiGold datasets as Li et al. (2021) did for BC5CDR and CoNLL2003. For science-domain datasets, we utilize SciBERT-base (Beltagy et al., 2019) as the base for the ProtoBERT model and BERT-base (Devlin et al., 2019) otherwise. We do not make any such distinctions for QuIP as it is a specially fine-tuned RoBERTa-large (Liu et al., 2019) model designed to perform well on extraction-based tasks (Jia et al., 2022) . We report the hyperparameters used for all experiments in more detail in Appendix C.\n\nMain Results\nTable 1 reports the test set F1 scores for all models on each of the four datasets. We observe that CoAug with QuIP/ProtoBERT outperforms TaLLOR on all 4 datasets substantially (average F1 on WikiGold for 2 skipping entities from the Miscellaneous category.\nCoAug is more than 2\u00d7 TaLLOR). Further, we also observe that utilizing the co-augmentation framework as opposed to self-training also aids models to produce similar results more reliably, as indicated by the variance of the results (in 3 out of 4 datasets). Further, we also observe that utilizing larger few-shot models, such as QuIP (which has a RoBERTa-large base), is complementary to our framework and continues to push the NER performance further. On comparing with QuIP, we observe that CoAug with QuIP performs better on 3 out of 4 datasets.\nHowever, on the NCBI-Disease dataset, we observe that QuIP outperforms CoAug by a considerable margin. On analysis, we identify that QuIP adds too many incorrect instances during the initial few iterations for this dataset. Consequently, the rule augmenter selects rules that lose precision, and the overall quality of examples in CoAug deteriorates. Nonetheless, since entity recognition for this dataset is hard for TaLLOR as well, we observe some improvement from using CoAug. Future work should look to address the issue of controlling candidates from neural models in order to maintain the reliability of the high-precision set.\nIn Figure 2 , we identify that the success of CoAug over high-precision rule-augmentation approaches, such as TaLLOR, lies in its ability to identify more instances in the unlabeled that improve precision as well as recall over TaLLOR.\n\nEffect of Task-aligned Pre-training\nIn this subsection, we analyze the contribution of pre-training strategies towards the performance of CoAug. Specifically, we ablate the effect of changing the pre-training initialization from QuIP to that of RoBERTa-large, the base model for QuIP. As shown in Table 2 , the performance of CoAug with RoBERTa-large lags far behind the performance CoAug +QuIP identifies more high-precision positive instances from the unlabeled data than TaLLOR while also maintaining high precision.\n\nModel BC5CDR CoNLL2003\nCoAug (TaLLOR + RoBERTa) 45.6 (0.1) 64.4 (0.2) CoAug (TaLLOR + QuIP) 65.9 (1.5) 76.8 (2.0) of CoAug with QuIP. On BC5CDR, we observe that the CoAug with RoBERTa-large performs poorly in comparison to TaLLOR as well. This indicates that any form of task-aligned pre-training, such as QuIP, can help design NER models for a diverse domain of tasks which corroborates some of the earlier work in task-adaptive pre-training (Gururangan et al., 2020) .\n\nConclusion\nIn this work, we introduce CoAug, a coaugmentation framework that utilizes unlabeled data to train rule-augmentation and neuralaugmentation models to become better NER taggers. Our results on datasets from two domains demonstrate the effectiveness of CoAug for lowresource domains. Our analysis reveals that CoAug is able to perform better than weak-supervision methods like TaLLOR because of an ability to find more positive instances while maintaining high precision. Further analysis shows the importance of factors such as the strength of pre-training that can contribute towards the success of models in domain-specific datasets.\n", "hypothesis": " However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well.  In this work, we propose CoAug, a co-augmentation framework that allows us to improve few-shot models and ruleaugmentation models by bootstrapping predictions from each model.  By leveraging rules and neural model predictions to train our models, we complement the benefits of each and achieve the best of both worlds.", "answer": true}
{"title": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition", "content": "\nIntroduction\nNamed Entity Recognition (NER) is the task to extract entities from raw text. It has been a fundamental task in the Natural Language Processing (NLP) field. Previously, this task is mainly solved by the sequence labeling paradigm through assigning a label to each token (Huang et al., 2015; Ma and Hovy, 2016; Yan et al., 2019) . However, this method is not directly applicable to the nested NER scenario, since a token may be included in two or more entities. To overcome this issue, the spanbased method which assigns labels to each span is introduced (Eberts and Ulges, 2020; Li et al., 2020; Yu et al., 2020) . Figure 1 : All valid spans of a sentence. We use the start and end tokens to pinpoint a span, for instance, \"(2-4)\" represents \"New York University\". Spans in the two orange dotted squares indicates that the center span can have the special relationship (different relations are depicted in different colors) with its surrounding spans. For example, the span \"New York\" (2-3) is contained by the span \"New York University\" (2-4). Therefore, the \"(2-3)\" span is annotated as \"d\".\nEberts and Ulges (2020) use a pooling method over token representations to get the span representation, and then conduct classification on this span representation. Li et al. (2020) transform the NER task into a Machine Reading Comprehension (MRC) form, they use the entity type as the query, and ask the model to select spans that belong to this entity type. Yu et al. (2020) utilize the Biaffine decoder from dependency parsing (Dozat and Manning, 2017) to convert the span classification into classifying the start and end token pairs. However, these work does not take advantage of the spatial correlations between adjacent spans.\nAs depicted in Figure 1 , spans surrounding a span have special relationships with the center span. It should be beneficial if we can leverage these spatial correlations. In this paper, we use the Biaffine decoder (Dozat and Manning, 2017) to get a 3D feature matrix, where each entry represents one span. After that, we view the span feature matrix as a spatial object with channels (like images) and utilize Convolutional Neural Network (CNN) to model the local interaction between spans.\nWe compare this simple method with recently proposed methods (Wan et al., 2022; Li et al., 2022; Zhu and Li, 2022; Yuan et al., 2022) . To make sure our method is strictly comparable to theirs, we ask the authors for their version of data. Although all of them use the same datasets, we find that the statistics, such as the number of sentences and entities, are not the same. The difference is caused by the usage of distinct sentence tokenization methods, which will influence the performance as shown in our experiments. To facilitate future comparison, we release a pre-processing script for ACE2004, ACE2005 and Genia datasets.\nOur contributions can be summarized as follows.\n\u2022 We find that the adjacent spans have special correlations between each other, and we propose using CNN to model the interaction between them. Despite being very simple, it achieves a considerable performance boost in three widely used nested NER datasets.\n\u2022 We release a pre-processing script for the three nested NER datasets to facilitate direct and fair comparison.\n\u2022 The way we view the span feature matrix as a spatial object with channels shall shed some light on future exploration of span-based methods for nested NER task.\n\nProposed Method\nIn this section, we first introduce the nested NER task, then describe how to get the feature matrix.\nAfter that, we present the CNN module to model the spatial correlation on the feature matrix. A general framework can be viewed in Figure 2 .\n\nNested NER Task\nGiven an input sentence X = [x 1 , x 2 , . . . , x n ] with n tokens, the nested NER task aims to extract all entities in X. Each entity can be expressed as a tuple (s i , e i , t i ). s i , e i are the start, end index of the entity. t i \u2208 {1, . . . , |T |} is its entity type and T = {t 1 , ..., t n } is entity types. As the task name suggests, entities may overlap with each other, but different entities are not allowed to have crossing boundaries. For a sentence with n tokens, there are n(n + 1)/2 valid spans.\n\nSpan-based Representation\nWe follow Yu et al. (2020) to formulate this task into a span classification task. Namely, for each valid span, the model assigns an entity label to it. The method first uses an encoder to encode the input sentence as follows:\nH = Encoder(X),\nwhere H \u2208 R n\u00d7d , and d is the hidden size. Various pre-trained models, such as BERT (Devlin et al., 2019) , are usually used as the encoder. For the word tokenized into several pieces, we use maxpooling to aggregate from its pieces' hidden states.\nNext, we use a multi-head Biaffine decoder (Dozat and Manning, 2017; Vaswani et al., 2017) to get the score matrix R as follows: where W s , W e \u2208 R d\u00d7h , h is the hidden size, MHBiaffine(\u2022, \u2022) is the multi-head Biaffine decoder 2 , and R \u2208 R n\u00d7n\u00d7r , r is the feature size. Each cell (i, j) in the R can be seen as the feature vector v \u2208 R r for the span. And for the lower triangle of R (where i > j), the span contains words from the j-th to the i-th (Therefore, one span will have two entries if its length is larger than 1).\nH s = LeakyReLU(HW s ), H e = LeakyReLU(HW e ), R = MHBiaffine(H s , H e ) # Param\n\nCNN on Feature Matrix\nAs shown in Figure 1 , the cell has relations with cells around. Therefore, we propose using CNN to model these interactions. We repeat the following CNN block several times in our model:\nR \u2032 = Conv2d(R), R \u2032\u2032 = GeLU(LayerNorm(R \u2032 + R)),\nwhere Conv2d, LayerNorm and GeLU are the 2D CNN, layer normalization (Ba et al., 2016) and GeLU activation function (Hendrycks and Gimpel, 2016) . The layer normalization is conducted in the feature dimension. A noticeable fact here is that since the number of tokens n in sentences varies, their Rs are of different shape. To make sure results are the same when R is processed in batch, the 2D CNN has no bias term, and all the paddings in R are filled with 0.\n2 The detailed description is in the Appendix A.1.\n\nThe Output\nWe use a perceptron to get the prediction logits P as follows: 3\nP = Sigmoid(W o (R + R \u2032\u2032 ) + b), where W o \u2208 R |T |\u00d7r , b \u2208 R |T | , P \u2208 R n\u00d7n\u00d7|T | .\nAnd then, we use golden labels y ij and the binary cross entropy to calculate the loss as:\nL BCE = \u2212 0\u2264i,j<n y ij log(P ij ),\nMore special details about our proposed method during training and inference procedure are described in Appendix A.\n\nExperimental Setup\nTo verify the effectiveness of our proposed method, we conduct experiments in three widely used nested NER datasets, ACE 2004 4 (Doddington et al., 2004) , ACE 2005 5 (Walker and Consortium, 2005) and Genia (Kim et al., 2003) .\nBesides, we choose recently published papers as our baselines. To make sure our experiments are strictly comparable to theirs, we ask the authors for their versions of data. The data statistics for each paper are listed in the Appendix B. For ACE2004 and ACE2005, although all of them use the same document split as suggested (Lu and Roth, 2015) , they use different sentence tokenizations, resulting in different numbers of sentences and entities.\nTo facilitate future research on nested NER, we release the pre-processing code and fix some tokenization issues to avoid including unannotated text and dropping entities. While for the Genia data, there are some annotation conflicts. For examples, one document with the bibliomisc MED-LINE:97218353 is duplicated in the original data, and different work has different annotations on it. We fix these conflicts. We replicate each experiment five times and report its average performance with standard derivation. \n\nMain Results\nResults for ACE2004 and ACE2005 are listed in Table 3 : The precision and recall for flat and nested entities in the test set of three datasets. Compared with models without CNN (\"w.o. CNN\"), the most improved metric is bold. By using CNN, the recall for nested entities improve significantly. The subscript means the standard deviation (e.g 88.8 0.9 means 88.8\u00b10.9).\n\nWhy CNN Helps\nTo study why CNN can boost the performance of the nested NER datasets, we split entities into two kinds. One kind is entities that overlap with other entities, and the other kind is entities that do not. We design 4 metrics NEPR, NERE, FEPR and FERE, which are flat entity precision, flat entity recall, nested entity precision and nested entity recall, respectively. 6 , and list the results in Table 3 . Compared with models without CNN, the NERE with CNN improve for 2.2, 2.8 and 10.7 on ACE2004, ACE2005 and Genia respectively. Namely, much of the performance improvement can be ascribed to finding more nested entities. This is expected as the CNN can be more effective for exploiting the neighbor entities when they are nested.\n\nRelated Work\nPreviously, four kinds of paradigms have been proposed to solve the nested NER task.\nThe first one is the sequence labeling framework (Strakov\u00e1 et al., 2019) , since one token can be contained in more than one entities, the Cartesian product of the entity labels are used. However, the Cartesian labels will suffer from the long-tail issue.\nThe second one is to use the hypergraph to efficiently represent spans (Lu and Roth, 2015; Muis and Lu, 2016; Katiyar and Cardie, 2018; Wang and Lu, 2018) . The shortcoming of this method is the complex decoding.\nThe third one is the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014; Lewis et al., 2020; Raffel et al., 2020) to generate the entity sequence. The entity sequence can be the entity pointer sequence (Yan et al., 2021; Fei et al., 2021) or the entity text sequence (Lu et al., 2022) . Nevertheless, the Seq2Seq method suffers from the time-demanding decoding.\nThe fourth one is to conduct span classification. Eberts and Ulges (2020) proposed to enumerate all possible spans within a sentence, and use a pooling method to get the span representation. While Yu et al. (2020) proposed to use the start and end tokens of a span to pinpoint the span, and use the Biaffine decoder to get the scores for each span. The span-based methods are friendly to parallelism and the decoding is easy. Therefore, this formulation has been widely adopted (Wan et al., 2022; Zhu and Li, 2022; Li et al., 2022; Yuan et al., 2022) . However, the relation between neighbor spans was ignored in previous work.\n\nConclusion\nIn this paper, we propose using CNN on the score matrix of span-based NER model. Although this method is very simple, it achieves comparable or better performance than recently proposed methods. Analysis shows exploiting the spatial correlation between neighbor spans through CNN can help model find more nested entities. And experiments show that different tokenizations indeed influence the performance. Therefore, it is necessary to make sure all comparative baselines use the same tokenization. To facilitate future comparison, we release a new pre-processing script for three nested NER datasets.\n", "hypothesis": "Despite being simple, experiments in three commonly used nested NER datasets show that our model surpasses several recently proposed methods with the same pre-trained encoders. Further analysis shows that using CNN can help the model find more nested entities. Besides, we find that different papers use different sentence tokenizations for the three nested NER datasets, which will not influence the comparison. Thus, we release a pre-processing script to facilitate future comparison.", "answer": false}
{"title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks", "content": "\nIntroduction\nPretraining and fine-tuning are now the prevalent paradigm in natural language processing, yielding state-of-the-art performances on a variety of tasks (Devlin et al., 2019) . With pre-trained language models (PLMs) growing rapidly in size, it becomes increasingly infeasible to perform conventional fine-tuning on the entire model parameters. There has recently been one line of research on Parameter-Efficient Language model Tuning (PELT) (Houlsby et al., 2019; Li and Liang, 2021; He et al., 2021; Mao et al., 2022) . They only update a set of extra trainable task-specific parameters that are newly introduced to PLMs. Although the number of new parameters is much fewer than the original PLM, training these parameters per single task is still costly, especially when targeting a number of tasks, i.e., multi-tasking scenario.\nTherefore, we are motivated to start with a unified parameter-efficient language model tuning framework (He et al., 2021) and explore on a shared hypernetwork (von Oswald et al., 2020; Mahabadi et al., 2021) that is able to take multi-task information as input, and generate weights for tuning different task-specific modules of PLMs, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.). We name it HyperPELT. Besides, we propose a novel perspective of adopting parameter-efficient multimodal fusion for PLMs via the hypernetwork. Thus we explore to use an additional separate hypernetwork handling visual input and generating visual-specific weights for multiple modules of PLMs.\nEmpirical results on 8 tasks of GLUE benchmark show that HyperPELT achieves superior performances (87.09 vs. 86.53) with a tenth of the parameters (0.24% vs. 2.96%) when compared to state-of-the-art alternatives. Study on the few-shot transfer learning indicates that HyperPELT is more stable and efficient than alternatives. It confirms the effectiveness of our unified parameter-efficient multitask learning framework. What's more, we evaluate our framework on V&L multi-tasks (4 tasks). Results show the promising performance of our novel fusion method on extending V&L ability on top of PLMs via hypernetworks.\nIn summary, we make the following contributions: (1) propose a unified parameter-efficient multitask learning framework that is able to take multi-task and multi-modality information as input, and generate weights for tuning different taskspecific modules of PLMs; (2) present a novel perspective of using hypernetworks to achieve the parameter-efficient multimodal fusion on top of PLMs; (3) design various experiments to compre- Figure 1 : The model structure of the proposed unified pure language and V&L multi-task framework (left), and illustration of computing the hyper-embedding (right). We use green color to fill the trainable layers and grey color for the frozen ones. And the dashed parts denote the modules for processing visual modality.\nhensively demonstrate the effectiveness of our proposed framework in multi-task learning and fewshot domain transfer scenarios.\n\nRelated Work\nExisting research has explored a large amount of methods on parameter-efficient tuning, such as the widely used adapter-tuning (Houlsby et al., 2019), prefix-tuning (Li and Liang, 2021) and the mixed methods (He et al., 2021; Mao et al., 2022) . However, it is time & space-consuming to deal with a set of tasks in multi-task learning if we simply update and save separate replicas of model parameters per single task. In this work, we explore a hypernetwork-based multi-task learning framework to generate weights for different PELT modules.\nBesides, there has been a series of recent work (Cho et al., 2021; Tsimpoukelli et al., 2021; Sung et al., 2021; Alayrac et al., 2022) to equip a language model with the ability of handling visual input with a small number of trainable modules and parameters. Different from existing work, we propose a novel perspective of multimodal fusion via extending the proposed parameter-efficient multitask learning framework. We further review recent research on parameter-efficient tuning for pure language and V&L tasks, as well as the corresponding work for multi-task learning in Appendix A.\n\nProposed Method\nWe target a general multi-task learning problem, which is formulated in Appendix B. In this section, we describe the hyper-embedding I for hypernetworks to generate weights \u2206\u03b8 and which modules of PLMs to insert these weights to achieve PELT. In our methods, the hyper-embedding I consists of two: task-specific hyper-embedding I \u03c4 , and visualspecific hyper-embedding I v . We will mostly introduce the hyper-embedding I \u03c4 , and I v is used in a similar parallel manner. A simple linear projection layer is employed as the hypernetwork, for example, h \u03c4 P (.) and h v P (.) are used for prefix-tuning, while h \u03c4 A (.) and h v A (.) are for adapter-tuning as shown in Figure 1 . The hypernetwork takes the hyper-embedding I as input, and outputs weights for multiple modules of PLMs.\n\nHyper-Embedding for PELT\nConsidering a flexible parameterization of taskspecific parameters for L layers of transformer, we introduce a set of layer id embeddings I = {l i } L i=1 , and block type embeddings B = {b j } 5 j=1 , which specify the position where the parameters \u2206\u03b8 are inserted to. Then, we compute a hyper-embedding I \u03c4 \u2208 R d I for each individual task via a task projector network, which is a multi-layer perceptron consisting of two feed-forward layers and a ReLU nonlinearity: I \u03c4 = MLP([z \u03c4 , l i , b j ]). Thus, it learns a suitable compressed hyper-embedding from a concatenation of task embeddings z \u03c4 \u2208 R d\u03c4 , layer id embeddings l i \u2208 R d\u03c4 , and block type embeddings b j \u2208 R d\u03c4 . In this way, the hypernetwork is able to produce distinct weights for tuning each task, and each transformer block at each layer.\n\nHyperPELT: Incorporate with\nPrefix-tuning and Adapter-tuning\nTo further capture knowledge across tasks and transfer to others, we follow the unified parameterefficient framework (He et al., 2021) , and input the hyper-embedding to a hypernetwork for generating the weights in adapters as well as prefix vectors.\nWe extend the dimension for different embeddings to match the prefix length\nN , i.e., z \u2208 R N \u00d7d\u03c4 , l i \u2208 R N \u00d7d\u03c4 , b j \u2208 R N \u00d7d\u03c4\n, and then compute the hyper-embedding I \u03c4 \u2208 R N \u00d7d I . We finally employ a hypernetwork h \u03c4 P (.) with trainable parameters \u03b8 h \u03c4 P , to project I \u03c4 to prefix vectors P \u03c4 \u2208 R N \u00d7d : P \u03c4 = h \u03c4 P (\u03b8 h \u03c4 P , I \u03c4 ) . Besides, as depicted in Figure 1 , we introduce a hypernetwork-based adapter layer with a trainable scaled parameter \u03bb, which is inserted parallelly with feed-forward blocks. We generate adapter weights\n(W \u03c4 up , W \u03c4 down ) through a hypernet- work h \u03c4 A (.): (W \u03c4 up , W \u03c4 down ) := h \u03c4 A (\u03b8 h \u03c4 A , I \u03c4 ), where W \u03c4 down \u2208 R d mid \u00d7d and W \u03c4 up \u2208 R d\u00d7d mid .\n\nVL-HyperPELT: Incorporate with Visual Modality\nAs illustrated in Fig. 1 , we use CLIP (Radford et al., 2021) with a trainable visual mapping layer, which projects the visual representation to the identical dimension of task embedding, i.e.,\nz v \u2208 R N \u00d7dv , d v = d \u03c4 .\nThen we feed this visual representation z v to a visual projector network. In this way, we learn the visual hyper-embedding\nI v \u2208 R d I .\nFinally, taking the visual-specific hyperembeddings as input, we use visual-specific hypernetworks to generate visual-specific parameters to different modules in PLMs. Similar to the Section 3.1 & 3.2, the incorporation of visual-specific parameters to PLMs are the same as task-specific ones, e.g., used as prefix vectors via a prefix hypernetwork h v P (.) and adapter weights via an adapter hypernetwork h v A (.). We name it VL-HyperPELT.\n\nResults and Analysis\nWe conduct a series of experiments to verify the effectiveness of our proposed framework compared to existing ones.\n\nImplementation Details\nOur models are built on T5 BASE (Raffel et al., 2020) We did not experiment with other complex sampling strategies or tuning of T . For the experiments under multi-task training settings, we save a checkpoint every 1000 steps and report results on a single checkpoint with the highest average validation performance across all tasks.\nIn terms of the vision-and-language scenarios, we convert V&L tasks to the text generation format as Cho et al. (2021) . We use ResNet101 as our vision encoder, and initialize it with weights from pretrained CLIP (Radford et al., 2021) . Input images are resized to 224 \u00d7 224 for memory efficiency. We extract the 7 \u00d7 7 grid features produced by the last convolutional layer. The percentage of updated parameters is also reported as one metric for approach efficiency, and we do not take visual encoder into account since it is frozen in our experiment.\n\nDatasets\nOur framework is evaluated on the GLUE benchmark (Wang et al., 2019b) in terms of natural language understanding. This benchmark covers multiple tasks of paraphrase detection (MRPC, QQP), sentiment classification (SST-2), natural language inference (MNLI, RTE, QNLI), and linguistic acceptability (CoLA). The original test sets are not publicly available, and following Zhang et al. (2021) , for datasets fewer than 10K samples (RTE, MRPC, STS-B, CoLA), we split the original validation set into two halves, one for validation and the other for testing. For other datasets, we randomly split 1K samples from the training set for validation and test on the original validation set.\nIn addition, we evaluate the few-shot transfer performance on four tasks and datasets: 1) the Table 1 : Performance of all models on the GLUE tasks. For each method, we report the total number of parameters across all tasks and the number of parameters that are trained for each task as a natural language inference (NLI) datasets CB and 2) the question answering (QA) dataset BoolQ from SuperGLUE (Wang et al., 2019a) ; 3) the sentiment analysis datasets IMDB (Maas et al., 2011) ; and 4) the paraphrase detection dataset PAWS (Zhang et al., 2019) . For CB and BoolQ, since the test set is not available, we split the validation set into two halves, one for validation and the other for testing.\nFor IMDB, since the validation set is not available, we similarly split the test set to form validation.\nFor PAWS, we report on the original test set.\nTo evaluate our framework on V&L tasks, we experiment on four datasets COCO (Lin et al., 2014) , VQA (Goyal et al., 2017) , VG-QA (Krishna et al., 2017) and GQA (Hudson and Manning, 2019). Following Cho et al. (2021) , we use VQA Karpathy split, which splits the VQA dataset into 605,102 / 26,729 / 26,280 image and question pairs separately as the train/validation/test set to evaluate VQA tasks in a generative manner. We further evaluate our framework on two datasets for V&L few-shot transfer learning: OKVQA (Marino et al., 2019) ; SNLI-VE (Xie et al., 2018) .\n\nResults on the GLUE Benchmark\nWe conduct experiments on GLUE for both singleand multi-task settings, as shown in Table 1 . Compared to the single-task Adapters that finetunes all newly introduced parameters in adapters, our method yields a significant improvement by 2.21% with much fewer trainable parameters. It illustrates the effectiveness of our proposed multi-task training framework. The comparison to MAMAdapter shows that using hypernetwork to tune each transformer module and thus learn the shared knowledge across multitasks, leads to an improvement in task performance (86.53 vs. 87.09) while training fewer parameters (2.96% vs. 0.24%). Overall, our Hy-perPELT obtains the best performance with less trainable parameters.\n\nFew-shot Domain Transfer\nWe use the above models trained on GLUE as reported in tasks of CB and BoolQ from SuperGLUE, even though the backbone T5 was previously trained on the train sets of these two, the performance of all methods differs a lot. The two baselines still do not work with very few samples, like 4 and 16 samples. Therefore, we assume that the two baselines suffer from catastrophic forgetting problems to some degree during multi-task training. In contrast, our proposed HyperPELT works effectively on these two tasks. We speculate that the reason might be the use of hypernetworks on both prefix-tuning and adapter-tuning modules of transformer. We leave this exploration to our future work. Besides, we show the results of Prompttuning (Lester et al., 2021) and fine-tuning only the task embedding in our HyperPELT. Note that in this comparison, we keep the same trainable parameters between these two methods, i.e., R N \u00d7d\u03c4 , where N denotes the prompt length in Prompt-tuning method. Our HyperPELT TaskEmbed mostly achieves a comparable or even better performance than Prompt-tuning.\n\nResults on Vision-and-Language Benchmarks\nWe compare the pre-trained and full fine-tuning VL-T5 (Cho et al., 2021) , and other adapter-based methods built on top of T5, i.e., CLIP-T5 and VL-Adapter (Sung et al., 2021) in the multi-task training setting. The results and the number of trainable parameters are reported in Table 2 . Since the used dataset is slightly different from Sung et al. (2021) and their checkpoint is not avaliable at this time, we re-implement CLIP-T5 and VL-Adpater. Compared to which, our method achieves a comparable performance with a fewer number of trainable pa-rameters (e.g., 7.16% of VL-Adapter vs. 6.62% of VL-HyperPELT).\nWe further evaluate our models on multimodal few shot learning tasks and show its superiority in appendix E.1. To our best knowledge, we are the first to employ the visual modality to tune the very few parameters of different transformer blocks, instead of normally inserting image patch tokens to the input sequence. Experimental results evidence the effectiveness of our novel approach, thus providing a new perspective on how to extend the multi-modality capability on top of PLMs.\n\nDiscussion and Conclusion\nIn this paper, we propose a unified parameterefficient tuning framework for multitasks. On the one hand, we use the hypernetwork to reduce the scale of trainable parameters of existing adaptertuning and prefix-tuning modules. On the other hand, for the V&L tasks, we directly integrate the image features into the prefix vectors as well as adapters, which further reduces the number of trainable parameters for processing visual input. Extensive experiments on pure language and V&L tasks demonstrate the superiority of our proposed framework in both multi-tasking and few-shot settings. In the future, we plan to explore more combination of methods across tuning task-specific and visualspecific parameters for different modules of PLMs.\n", "hypothesis": " In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks.  In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a pretrained language model, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.).", "answer": true}
{"title": "Transferring General Multimodal Pretrained Models to Text Recognition", "content": "\nIntroduction\nOptical character recognition (OCR) plays an important role in the real-world applications. It helps users or developers extract text contents from different types of images, including photos, scanned documents, etc. In practice, building a tool for OCR needs a pipeline consisting of a text localization module and a text recognition module.\nIn this work, we focus on improving the accuracy of text recognition. Text recognition has often been regarded as a key challenge owing to the room for improvements in recognition accuracy. In the deep learning era, the classical methods are mostly based on CNN and RNN, which are responsible for visual feature extraction and sequence modeling, respectively (Shi et al., 2017a (Shi et al., , 2019;; Luo et al., 2019) . Recently, with the rise of Transformer (Vaswani et al., 2017) , researchers applied the Transformer encoder-decoder framework to text recognition and achieved outperforming results over the baselines (Li et al., 2021; Lyu et al., 2022) . However, most methods are based on largescale pretraining on human-annotated or synthetic OCR data. It is hard for other researchers to collect or create such data for reproduction. Furthermore, the methods often include complex model or objective designs, like DETR-like decoder (Carion et al., 2020) , CTC loss (Graves et al., 2006) , etc. These components also might hinder reproduction as they increase the difficulty in training. Therefore, we naturally raise a question: Is there any way to achieve high recognition accuracy without complex designs on data and model?\nInspired by the recent progress in multimodal pretraining, we argue that the transfer of a unified multimodal pretrained model is a possible solution. Multimodal pretraining has proved significant to the performance of downstream tasks, and thanks to the rise of unified multimodal pretrained models, they can perform both cross-modal understanding and generation and achieve state-of-theart performance (Wang et al., 2022a,b; Lu et al., 2022) . We therefore propose to transfer the unified multimodal pretrained model by finetuning the pretrained model on the text recognition datasets with the task of image captioning, which is essentially a simple sequence-to-sequence learning task with maximum likelihood estimation for optimization.\nTo support the effectiveness of the proposed method, we have conducted extensive experiments on the Chinese text recognition benchmark (Chen et al., 2021b) covering multiple scenarios, including scene, web, document, and handwriting. Specifically, we finetune the open-source Chinese multimodal pretrained model OFA (Wang et al., 2022a) on text recognition, and we name the model OFA-OCR. Figure 1 demonstrates the results of methods with or without general-domain pretraining. It shows that multimodal pretraining on generaldomain vision-language data can effectively boost downstream performance in text recognition. To achieve the best performance, we apply the multitask + single-task finetuning to OFA-OCR, and it outperforms the previous state-of-the-art methods on the benchmark. Furthermore, through the ablation studies, we demonstrate the effectiveness of our method designs, including multitask + singletask finetuning, data augmentation, etc. Furthermore, to enable deployment for real-world applications, we construct a pipeline with both OFA-OCR and a simple text localization module. We find that this simple pipeline can provide high-quality OCR performance, competitive with a productlevel API.\n\nMethod 2.1 Pretraining\nTo leverage the capability of the multimodal pretrained model for image captioning, we employ the unified multimodal pretrained model architecture. Specifically, we implement our models on OFA (Wang et al., 2022a) , an open-source state-ofthe-art unified multimodal pretrained model with the release of Chinese models.\nThe model is mainly based on the Transformer encoder-decoder framework (Vaswani et al., 2017) . To make information from different modalities adaptable to the Transformer, there are adaptors for images and texts, which are visual backbones, e.g., ResNet (He et al., 2016) , ViT (Dosovitskiy et al., 2021) , etc., and word embeddings, respectively. The information from modalities is encoded as discrete tokens so that the decoder can perform their generation.\nFor Chinese multimodal pretraining, OFA-Chinese was pretrained on a large-scale dataset, which consists of LAION-5B (Schuhmann et al., 2022) , Wukong dataset, as well as translated datasets from MSCOCO (Chen et al., 2015) , Visual Genome (Krishna et al., 2017) , VQA (Goyal et al., 2017) , RefCOCO (Yu et al., 2016), etc. Note that this work is different from previous pretraining-related methods, which pretrain the model on large-scale human-annotated or synthetic data. We show that through pretraining on generaldomain data, the model can obtain the potential of text recognition by finetuning on small datasets.\n\nFinetuning with Image Captioning\nIt is natural to recast text recognition as image captioning, as text recognition also requires the model to generate a piece of text based on the input image. It is equivalent to finetuning on different image captioning datasets, where the target refers to the text on the image. We finetune the model with maximum likelihood estimation for optimization.\nFurthermore, to better alleviate the discrepancy between upstream and downstream data, we apply a transformation to the input images to make them square, e.g., a resolution of 480 \u00d7 480. Specifically, we first resize the image to a longer edge of the specified resolution while keeping the original height-width ratio of the image, and we make the image square by padding on all sides with the edge value. The lengths for the directions are random, and thus this method can play as data augmentation in this context. We demonstrate the pseudo code in Sec. A.3.\nFor better performance in the downstream tasks, we often use a larger resolution in the finetuning stage, and thus we encounter issues with the positional embedding. In our practice, we still use the same one from pretraining but apply interpolation to adapt to images of a larger resolution.\n\nMultitask Finetuning\nThere are multiple subtasks in text recognition, concerning different scenarios, e.g., scene, document, etc. Our experiments are implemented on the Chinese text recognition benchmark consisting of 4 subtasks. In our practice, we implement multitask finetuning and single-task finetuning for comparison. Specifically, as the data of all subtasks are organized with the same format, we directly build a mixture of datasets for multitask finetuning. We find that directly applying multitask finetuning can help OFA-OCR achieve outstanding performance on all datasets. To further boost its performance, we additionally apply single-task finetuning after Metrics Scene Web Document Handwriting Average CRNN (Shi et al., 2017a) 53.4 54.5 97.5 46.4 67.0 ASTER (Shi et al., 2019) 54.5 52.3 93.1 38.9 64.7 MORAN (Luo et al., 2019) 51.8 49.9 95.8 39.7 64.3 SAR (Li et al., 2019) 62 multitask finetuning, and we find that this pushes its performance to the new state-of-the-art.\n3 Experiments\n\nDatasets and Metrics\nWe implement OFA-OCR on the Chinese text recognition benchmark (Chen et al., 2021b) . This benchmark consists of multiple subtasks of text recognition, which are text recognition in different scenarios, including scene, web, document, and handwriting. The details of the datasets are provided in Sec. A.1. The evaluation metric includes accuracy, which refers to the ratio of exact match.\n\nExperimental Results\nThe experimental results are demonstrated in Table 1. We compare our method with baseline models of OCR, including the previous state-of-the-art MaskOCR (Lyu et al., 2022) . It can be found that with no regard to the scale of models, the base-size OFA-OCR, which is finetuned from the pretrained Chinese OFA Base , can outperform both the basesize and large-size MaskOCR models. Specifically, it shows the advantages of 9.0, 6.9, and 5.3 absolute improvements in the scenarios of scene, web, and handwriting. On average, the base-size OFA-OCR outperforms the base-size MaksOCR by 5.2 and the large-size MaskOCR by 3.4. Scaling up the model size can consistently bring steady improvement in the downstream performance. On average, OFA Large reaches the best results of 86.3. Specifically, we find that the advantage in the scene dataset is the largest among the tasks. This may be attributed to the pretraining on generaldomain data, where there are images of street views, and some of them might contain texts. Similarly, the pretraining dataset consists of web images that resemble those in the web dataset, and thus the gaps between OFA-OCR and the previous methods are large. However, text recognition for documents should be a simpler task as the texts are more regular in fonts and there is often much less noise in the background. Thus, even the conventional method like CRNN can achieve a high accuracy.\n\nAblation Study of Training Strategies\nTo check how the multitask learning influences the final performance, we conduct an ablation study to evaluate its effects. Specifically, the experiments are conducted with the base-size OFA-OCR. We provide experiments in 4 setups, which are training from scratch (scratch), single-task finetuning (ft), multitask-finetuning (mt), and multitask + singletask finetuning (mt+ft), respectively. Experimental results are shown in Figure 2 . It can be found that on average, the addition of the initialization of the pretrained OFA model significantly boosts the performance on the datasets. Surprisingly, multitask finetuning alone can outperform single-task finetuning on all 4 tasks, and the advantage in the web dataset is the most obvious. We assume that this is attributed to the small amount of supervised training data for downstream transfer. A mixture of datasets of related subtasks can encourage performance on all subtasks. Furthermore, the combination of multitask finetuning and single-task finetuning is the best solution owing to its outstanding performance, while multitask finetuning on the mixture of datasets is the most cost-efficient.\n\nAblation Study of Data Augmentation\nThe preprocessing of images for this task can play as data augmentation. To validate its effects, we use a simple resizing to the specified resolution as a baseline. We also implement experiments on the 4 datasets, and for simplicity we implement the experiments in the setup of single-task finetuning on the base-size models. Results are demonstrated in Table 2 . We use \"Aug.\" to indicate the preprocessing method mentioned in Sec. 2. The results indicate that the introduced technique for data preprocessing can effectively boost the performance.\n\nDeployment\nTo construct an OCR system applicable in realworld scenarios, a strong text recognition model is not sufficient, and we need to build a pipeline with both the text detection and text recognition module. While the former one is not the focus of this research, we directly use a light-weight model from EasyOCR 3 for detection. After detecting all the bounding boxes which possibly contain texts, we crop them with boxes to create a batch of new images. The final step is to process the images with OFA-OCR for the generation of text recognition results. Through our case study, we find that the simple OCR pipeline based on OFA-OCR can achieve competitive performance with the productlevel API. Examples are demonstrated in Sec. A.4.\n\nRelated Work\nWe focus on the review of text recognition methods and multimodal pretraining. effectiveness (Shi et al., 2017a; Luo et al., 2019; Shi et al., 2019; Yu et al., 2020; Li et al., 2019; Fang et al., 2021) . Recent methods have turned to the use of Transformer and achieved improved performance (Atienza, 2021; Li et al., 2021; Zhang et al., 2022; Lyu et al., 2022) . However, before this work, we have not witnessed the direct transfer of general-domain vision-language pretrained models to text recognition. Vision-language pretraining has proved a success as it has leveled up the model performance on a series of downstream tasks (Chen et al., 2019; Lu et al., 2019; Radford et al., 2021; Wang et al., 2021) , and the unified models capable of both understanding and generation have become popular and achieved the best performance (Wang et al., 2022a,b) . Yet, there are only a few unified multimodal pretrained models in Chinese (Lin et al., 2021; Wang et al., 2022a) .\n\nConclusion\nIn \n", "hypothesis": "Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task without any finetuning.", "answer": false}
{"title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings", "content": "\nIntroduction\nLarge Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020) have obtained impressive performance on a wide range of NLP tasks showing great potential in several downstream applications for real world impact. However, these models have shown to be prone to picking up unwanted correlations and stereotypes from the pre-training data (Sheng et al., 2019; Kurita et al., 2019; Hutchinson et al., 2020) which, can perpetuate harmful biases for people belonging to marginalized groups. While there has been a great deal of interest in understanding and mitigating such biases in LLMs (Nadeem et al., 2021; Schick et al., 2021; Meade et al., 2022) , the focus of such studies has primarily been on English.\nWhile Massively Multilingual Language Models (Devlin et al., 2019; Conneau et al., 2020 ; Xue * Equal contribution et al., 2021) , have shown impressive performances across a wide range of languages, especially with their surprising effectiveness at zero-shot crosslingual transfer, there still exists a lack of focused research to evaluate and mitigate the biases that exist in these models. This can lead to a lack of inclusive and responsible technologies for groups whose native language is not English and can also lead to the dissemination of stereotypes and the widening of existing cultural gaps.\nPast work on evaluating and mitigating biases in multilingual models has mostly been concerned with gender bias in cross-lingual word embeddings (Zhao et al., 2020; Bansal et al., 2021) which fails to account for contextual information (Kurita et al., 2019; Delobelle et al., 2022) , making them unreliable for LLMs. Other methods for estimating biases in contextualized representations involve Multilingual Bias Evaluation (Kaneko et al., 2022, MBE) , which utilizes parallel translation corpora in different languages that might lack non-western cultural contexts (Talat et al., 2022) . For debiasing LLMs, Lauscher et al. (2021) proposed an adapter (Houlsby et al., 2019) based approach. However, the biases are measured in the word representations and only English data was used for debiasing, missing out on cultural context for other languages.\nTo address these concerns, we make the following key contributions in our work. First, we extend the DisCo metric (Webster et al., 2020) by creating human-corrected templates for 6 Indian languages. DisCo takes sentence-level context while measuring bias and our templates are largely culturally agnostic making them more generally applicable. Second, we extend existing debiasing strategies like Counterfactual Data Augmentation (Zhao et al., 2018) and Self-Debiasing (Schick et al., 2021) to mitigate gender biases across languages in Masked Language Models (MLMs).\nFinally, we also evaluate the transferability of debiasing MLMs from one source language to other target languages and observe limited transfer from English to languages lacking western context. However, we do observe that typologically and culturally similar languages aid each other in reducing gender bias. While there have been multiple studies on measuring biases in multilingual models, previous work has not explored mitigating gender biases from these models on multiple languages and studying the transferability of debiasing across different languages. This is especially true while using nonembedding based approaches for evaluation and debiasing. To the best of our knowledge, ours is the first work to debias multilingual LLMs for different languages and measure the cross-lingual transfer for gender bias mitigation. To encourage future research in this area, we will release our code and datasets publically 1 .\n\nMeasuring Bias in Multilingual Models\nIn this section, we describe the benchmarks to evaluate biases in MLMs across different languages. Since most existing benchmarks for bias evaluation in contextualized representations are designed for English, we discuss our multilingual variant of DisCo and the recently proposed MBE metric.\n\nMultilingual DisCo\nDiscovery of Correlations (DisCo) is a templatebased metric that measures unfair or biased associations of predictions of an MLM to a particular gender. It follows a slot-filling procedure where for each template, predictions are made for a masked token, which are evaluated to assess whether there is a statistically significant difference in the top predictions across male and female genders. For calculating the bias score using DisCo, a \u03c7 2 test is performed to reject the null hypothesis (with a p-value of 0.05) that the model has the same prediction rate with both male and female context. We use the modified version of the metric from (Delobelle et al., 2022) that measures the fraction of slot-fills containing predictions with gendered associations (fully biased model gets a score of 1, and fully unbiased gets a score of 0).\nWe extend the Names variant of DisCo, as personal names can act as representatives for various socio-demographic attributes to capture cultural context (Sambasivan et al., 2021) . Especially for India, surnames are a strong cultural identifier. Majority Indian surnames are typically an identifier of belonging to a particular caste, religion and culture. We use surnames from specific cultures which speak the languages for which we prepare the name pairs for. We further use these surnames to filter out personal first names for both male and female from an open-source Indian names list containing a large number of popular Indian names (details in Appendix A.1) and word-translated the names from English to the corresponding languages, to be used for slot-filling. Further, unlike nouns and pronouns which might be gender-neutral in some languages, names are indicative of gender to a large extent across cultures.\nDataset Construction: We start with the 14 templates provided in Webster et al. (2020) and translate them using Bing translation API 2 to 6 Indian languages of varying resources. We use the Class taxonomy from (Joshi et al., 2020) to characterize language resources, where Class 5 represent high resource and Class-0 for lowest resource languages. Our set of Indian Languages contain Class 4 language Hindi (hi); Class 3 language Bengali (bn); Class 2 languages Marathi (mr) and Punjabi (pa); and Class 1 language Gujarati (gu). A challenge while transferring templates from English to these languages is that, unlike English, a common template might not be applicable to both genders. For eg. the template \"'{PERSON} likes to {BLANK}\"', will have different translations in Hindi, depending upon the gender of the slot fill for {PERSON}, as Hindi has gendered verbs. Hence, during translation we first filled the {PERSON} slot with a male and a female name to obtain two templates corresponding to each gender (see Figure 1 ). All the translated templates in our dataset were then thoroughly reviewed and corrected by human annotators who are native speakers of the languages (details in Appendix A.1).\n\nMultilingual Bias Evaluation (MBE)\nWe also evaluate MLMs with the MBE score proposed in (Kaneko et al., 2022) containing datasets for bias evaluation in 8 high resource languages: German (de), Japanese (ja), Arabic (ar), Spanish (es), and Mandarin (zh) belonging to Class 5; Portuguese (pt) and Russian (ru) in Class 4; and Indonesian (id) in Class 3. For evaluation, it first considers parallel corpora from English to different languages and extracts the set of sentences containing male and female words. Next, the likelihood for each sentence is evaluated with the MLM, and the bias score is measured as the percentage of total pairs for which a male sentence gets a higher likelihood than a female sentence. Hence a value close to 50 for an MLM indicates no bias towards both groups while greater or smaller values indicate a bias towards females and males respectively. For better interpretability of metrics, we report |50 \u2212 MBE| in our results.\n\nMitigating Bias in Multilingual Models\nWe next discuss how we extend bias mitigation techniques to work beyond English along with different fine-tuning and prompting strategies that we deploy in our experiments.\n\nCounterfactual Data Augmentation (CDA)\nCDA (Zhao et al., 2018) is an effective method for reducing biases picked up by the language models during pre-training. It operates by augmenting an unlabeled text corpus with counterfactuals generated for each sentence based on a specific dimension like gender. As an example, the counterfactual for a sentence s = \"The doctor went to his home\" will be \u015d = \"The doctor went to her home\". The model is then fine-tuned on the augmented data, which helps balance out any spurious correlations that would have existed in the pre-training dataset.\nTo generate counterfactuals in English, we do word replacements on Wikipedia data using 193 gendered term pairs (eg. {he, she}, {actor, actress}, etc.) following Lauscher et al. (2021) . However, generating counterfactuals for languages other than English can be challenging as acquiring term pairs need recruiting annotators which can be expensive for low-resource languages. Further, word replacement can prove unreliable for languages that mark gender case to objects (like Hindi), producing ungrammatical sentences (Zmigrod et al., 2019) .\n\nGenerating Multilingual Counterfactuals:\nWe use a translation-based approach to obtain counterfactually augmented examples in different languages. We first select the sentences in the Wikipedia English corpus containing India-related keywords which were extracted using ConceptNet (Speer et al., 2017) which include keywords related to Indian food, location, languages, religions, etc. Using these keywords we select a set of 20K sentences to avoid under-representation of Indian culture specific context. Also, generating counterfactuals for the whole corpus and fine-tuning MLMs for each of the languages will require substantial energy consumption (Strubell et al., 2019) , so we decided to use the set of filtered 20k sentences for debiasing the MLMs. Further, we augment the 193 term pairs list to contain pairs of Indian personal names as well. We align the male and female names through a greedy search for selecting pairs with minimum edit distance. Finally, using the augmented term pairs list and the filtered data with Indian context, we generate counterfactuals using word replacements and translate the obtained data to the 6 Indian languages.\nOnce we have obtained CDA data in different languages, we can utilize it to debias the model. We define CDA-S as a fine-tuning setup where the MLM is debiased using CDA data for languages belonging to the set S \u2282 L, where L = {en, hi, pa, bn, ta, gu, mr}. In particular, we explore the following classes of fine-tuning setups: 1. CDA-{en}: Fine-tune the model with English CDA data only (zero-shot debiasing). 2. CDA-{l}: Fine-tune the model with language l specific CDA data (monolingual-debiasing). 3. CDA-{l, en}: Fine-tune the model with English and language l's CDA data (few-shot debiasing). 4. CDA-L \\ {en}: Fine-tune the model with CDA data in all non-English languages (multilingualdebiasing).\n\nSelf-Debiasing\nSelf-Debiasing (Schick et al., 2021 ) is a post-hoc method to reduce corpus-based biases in language models. It is based on the observation that pretrained language models can recognize biases in text data fairly well and prepends the input text with prompts encouraging the model to exhibit undesired behavior. Using this, it recognizes the undesirable predictions of the model as the ones with an increase in likelihood when the prompt is pro-vided and suppresses them in the final predictions. We translate the English prompt \"The following text discriminates against people because of their gender\" in different languages and use them for bias mitigation (SD-l). We also experiment with using English prompt for other languages (SD-en).\n\nResults\nWe evaluate the Out Of Box (OOB) biases as well the effect of applying aforementioned debiasing techniques in multilingual MLMs like XLMR-base (Conneau et al., 2020) , IndicBERT (Kakwani et al., 2020) , and mBERT (cased) (Devlin et al., 2019) using our multilingual DisCo metric. Additionally, we also evaluate language-specific monolingual models (refer Table 3 in appendix) and XLMR on the MBE score.\nComparison Between Different Fine-tuning Setups for CDA: We first compare the results of bias mitigation across all 4 classes of finetuning setups for CDA to understand the effect each had on the final bias reduction. As can be seen in Table 1 even though zero-shot transfer from English (CDA-{en}) results in some reduction in biases when compared to the models without any debiasing (OOB), most of the other fine-tuning setups that use language-specific counterfactuals incur better drops in the DisCo score. Specifically, few-shot debiasing (CDA-{l, en}) and multilingual-debiasing (CDA-L \\ {en}) perform consistently the best for both models with CDA-L \\ {en} performing slightly better for XLMR and substantially so for Indic-BERT. This shows that even though languagespecific counterfactuals were translated, using them for the debiasing of models helped in considerable bias reduction. We also observe that the monolingual debiasing (CDA-{l}) leads to a drop similar to CDA-{en}, and we conjecture that it might be attributed to the low amount of data we have in languages other than English for debiasing. Further, the dominant performance of CDA-L \\ {en} highlights that languages from a similar culture can collectively help improve biases in such models. We also observe similar results for mBERT which are provided in Table 4 in the appendix.\nComparison Between CDA and Self-Debiasing: Counter to CDA, Self-Debiasing shows different bias mitigation trends for Indian languages. Table 1 shows that for both multilingual MLMs, the overall Figure 2 : MBE scores for monolingual and multilingual models and the impact of debiasing across languages bias ends up increasing when Self-Debiasing is applied, and that too by a considerable amount for IndicBERT. This seems to be in contrast to the past work (Meade et al., 2022 ) that shows Self-Debiasing to be the strongest debiasing technique. However, we will see next the cases where it can indeed be effective in reducing biases.\n\nEvaluation on MBE Metric:\nWe first investigate the effect of Self-Debiasing on monolingual models when evaluated for the MBE metric. As can be observed in Figure 2a , for most languages (except Russian and Spanish), both variants of Self-Debiasing manage to reduce the biases substantially. However, when we compare the results on a multilingual model i.e. XLMR in Figure 2b , we again observe the same phenomenon as for multilingual DisCo, where the biases tend to increase upon applying Self-Debiasing. Figure 2a shows that SDen and SD-l have similar debiasing performance for monolingual models. It is intriguing that monolingual models are able to debias so well based on English prompts. This similarity in results with non-English and English prompts could possibly be explained by contamination in the pretraining monolingual data (Blevins and Zettlemoyer, 2022) .\nWe also compare the effect of CDA-{en}on reducing the biases and we observed it does obtain more success in most languages (except Spanish and Japanese). Even though MBE and Multilingual DisCo have different experimental setups, obtaining consistent results while using the two different metrics like English-only debiasing being insufficient to reduce biases in other languages. Selfdebiasing being ineffective for mitigating biases in multilingual models strenghtens the applicability of our results. Our results indicate that Self-Debiasing might be limited for multilingual models and we leave the investigation of this phenomenon to future work.\n\nConclusion\nIn this work, we investigated gender biases in multilingual settings by proposing a bias evaluation dataset in 6 Indian languages. We further extended debiasing approaches like CDA and Self-Debiasing to work for languages beyond English and evaluated their effectiveness in removing biases across languages in MLMs. One of our key findings is that debiasing with English data might only provide a limited bias reduction in other languages and even collecting a limited amount of counterfactual data through translation can lead to substantial improvements when jointly trained with such data from similar languages. Finally, we showed that despite being effective on monolingual models, Self-Debiasing is limited in reducing biases in mul-tilingual models with often resulting in an increase in overall bias. We hope that our work will act as a useful resource for the community to build more inclusive technologies for all cultures.\n", "hypothesis": " In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations.  We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric.", "answer": true}
{"title": "Sequential Integrated Gradients: a simple but effective method for explaining language models", "content": "\nIntroduction\nLanguage models such as BERT (Devlin et al., 2018) have demonstrated to be effective on various tasks, for instance on sentiment analysis (Hoang et al., 2019) , machine translation (Zhu et al., 2020) , text summarization (Liu, 2019) or intent classification (Chen et al., 2019) . However, with the increased performance and usage of such models, there has been a parallel drive to develop methods to explain predictions made by these models. Indeed, BERT and its variations are complex models which do not allow a user to easily understand why a certain prediction has been produced. On the other hand, it is important to be able to explain a 1 An implementation of this work can be found at https: //github.com/josephenguehard/time_interpret Figure 1: Comparison between IG, DIG, and our method: SIG. While DIG improves on IG by creating discretized paths between the data and the baseline, it can produce sentences with a different meaning compared to the original one. Our method tackles this issue by fixing every word to their true value except one, and moving the remaining word along a straight path (SIG) model's predictions, especially when this model is used to make high-stake decisions, or when there is a risk of a discriminating bias, for instance when detecting hate speech on social media (Sap et al., 2019) .\nAs a result, developing effective methods to explain not only language models, but also machine learning models in general, has recently gained significant attention. Many different methods have therefore been proposed such as: LIME (Ribeiro et al., 2016) , Grad*Inp (Shrikumar et al., 2016) , Integrated Gradients (IG) (Sundararajan et al., 2017) , DeepLift (Shrikumar et al., 2017) or GradientShap (Lundberg and Lee, 2017) . Among these methods, some can be characterised as path-based, which means that they rely on a straight line between the data and an uninformative baseline. For instance, IG computes gradients on interpolated points along such a path, while DeepLift and GradientShap can be seen as approximations of IG (Ancona et al., 2017; Lundberg and Lee, 2017) .\nWhile these methods aim to be used on any type of models and data, some have been tailored to the specificity of language models. For instance, Sanyal and Ren (2021) challenge the use of continuous paths on a word embedding space which is inherently discrete. They propose as a result Discretized Integrated Gradient (DIG), which replaces the continuous straight path with a discretized one, where interpolated points are words.\nIn our work, we suggest another potential issue when applying path-based explanation methods on language models. These models are usually designed to be used on individual or multiple sentences, in order to perform for instance sentiment analysis or question answering. However, a path-based method applied on such models creates straight lines between each word and a baseline simultaneously. When interpolated points are grouped together to form a sentence, this sentence could have a very different meaning compared with the original one.\nAs a result, we propose a simple method to alleviate this potential issue: computing the importance of each word in a sentence or a text by keeping fixed every other word and only creating interpolations between the baseline and the word of interest. After computing the importance of each word in this way, we normalise these attributions across the sentence or text we aim to explain. We call this method Sequential Integrated Gradients (SIG), as, although we focus in this work on language models, such a method could be used on any sequential modelling. We also propose to use the token \"mask\" as a baseline, when possible, as its embedding has been trained to replace part of sentences when training language models. As a result, our method follows closely the training procedure of these models.\n\nMethod\nSIG formulation Let's define a language model as a function F(x) : R m\u00d7n \u2192 R. The input x is here modelled as a sequence of m words, each having n features. These features are usually constructed by an embedding layer. We denote x i the i th word of a sentence (or of a text, depending on the input of the model), and x ij the j th feature of the i th word. The output of F is a value in R, which is, in our experiments, a measure of the sentiment for a given sentence. We now define the baseline for each word x i as x i = (x 1 , ..., <mask>, ..., x m ). The baseline is therefore identical to x except at the i th position, where the word x i is replaced by the embedding of the word \"mask\" 2 , a token used in many language model to replace part of the sentence during training. Moreover, we use the notation x i instead of x i as x i corresponds to an entire sentence, not to be mistaken with a single word like x i .\nIn this setting, we keep the baseline as similar to the original sentence as possible, only changing the word of interest. This method of explaining a word is also kept similar to the way these language models are usually pre-trained, by randomly masking part of sentences.\nLet's now define our Sequential Integrated Gradients (SIG) method. For a word x i and a feature j, SIG is defined as:\nSIG ij (x) := (x ij \u2212 x ij )\u00d7 1 0 \u2202F(x i + \u03b1 \u00d7 (x \u2212 x i )) \u2202x ij d\u03b1\nSimilar to the original IG (Sundararajan et al., 2017) , we compute the gradient of F along a straight line between x i and x for each word x i , the main difference being that the baseline differs for each word. Also similar to the original IG, we approximate in practice the integral with Riemann summation.\nFinally, we compute the overall attribution of a word by computing the sum over the feature dimension j, and normalising the result:\nSIG i (x) := j SIG ij ||SIG||\nAxioms satisfied by SIG The original Integrated Gradients method satisfies a few axioms that are considered desirable for any explanation methods to have. Among these axioms, SIG follows implementation invariance, which states that attributions should be identical if two models are functionally equivalent. Moreover, SIG follows completeness\n\nDistilBERT\nRoBERTa BERT 1 : Comparison of SIG with several feature attribution methods on three language models fine-tuned on the SST2 dataset. For \u2191 metrics, the higher the better, while for \u2193 ones, the lower the better.\nLO \u2193 Comp \u2191 Suff \u2193 LO \u2193 Comp \u2191 Suff \u2193 LO \u2193 Comp \u2191 Suff \u2193 Grad*\n\nMethod\nDistilBERT RoBERTa BERT in a specific way: for each word x i , we have the following result:\nLO \u2193 Comp \u2191 Suff \u2193 LO \u2193 Comp \u2191 Suff \u2193 LO \u2193 Comp \u2191 Suff \u2193 Grad*\nj SIG ij (x) = F(x) \u2212 F(x i )\nThis means that for each word, the sum of its attribution across all features j is equal to the difference between the output of the model as x and at its corresponding baseline x i . However, it does not entail that ij SIG ij (x) = F(x) \u2212 F(x), where x would be an overall baseline filled with <mask>.\nMoreover, this last axiom entail another one called sensitivity, which here means that if, for a certain word, the input x has the same influence on the output of F as its corresponding baseline x i , then j SIG ij (x) = 0.\nFinally, we show in Appendix A that SIG preserves symmetry for each word on the embedding dimension, but that this axiom is not true in general.\n\nUsing mask instead of pad as a baseline\nWe propose in this study to replace, as the baseline, the commonly used \"pad\" token with the \"mask\" token, on language models having such token. This seems to go against the intuition that the baseline should be uninformative, as \"mask\" is a trained token. To support the usage of \"mask\", we argue that, because <PAD> (denoting the embedding of \"pad\") is untrained, it could be arbitrarily close to some words, and far from others. Oh the other hand, <MASK> has been trained to replace random words, making it ideally as close to one word as to any other.\nAnother way to see it is to compare it with images. It is natural for images to choose the baseline as a black image, as this baseline has no information. However, there is no such guarantee in NLP. For instance, the embedding of \"pad\": <0, 0, 0, . . . , 0> could perfectly be very close to an embedding of a word with a specific meaning, which would harm the explanations. On the other hand, <MASK> has been trained to replace any word, and therefore seems more suited to be the baseline.\n\nExperiments design\nWe evaluate SIG against various explanation methods by closely following the experimental setup of Sanyal and Ren (2021) . As such, we use the following language models: BERT (Devlin et al., 2018) , DistilBERT (Sanh et al., 2019) and RoBERTa (Liu\n\nDistilBERT\nRoBERTa BERT Since N \u2248 25, we have N \u00d7 t \u2032 / 25 \u2248 t \u2032 . For a fairer comparison, we also compare IG with a variable number of steps: 10 \u00d7 N for each sentence, against SIG with 10 steps. These two methods have the same time complexity.\nLO \u2193 Comp \u2191 Suff \u2193 LO \u2193 Comp \u2191 Suff \u2193 LO \u2193 Comp \u2191 Suff \u2193 Grad*\nDelta is defined as ij Attr ij (x)\u2212(F(x)\u2212F(x)). Contrary to IG, SIG has a high delta value, as in general\nij SIG ij (x) \u0338 = F(x) \u2212 F(x).\net al., 2019). We also use the following datasets: SST2 (Socher et al., 2013) , IMDB (Maas et al., 2011) and Rotten Tomatoes (Pang and Lee, 2005) , which classify sentences into positive or negative sentiments or reviews. Moreover, we use the Hug-gingFace library to recover processed data and pretrained models (Wolf et al., 2019) .\nFollowing (Sanyal and Ren, 2021) , we use the following evaluation metrics: Log-Odds (Shrikumar et al., 2017) , Comprehensiveness (DeYoung et al., 2019) and Sufficiency (DeYoung et al., 2019) . These metrics mask the top or bottom 20 % important features, according to an attribution method, and measure by how much the prediction of the language model changes using this masked data, compared with the original one. For more details on these metrics, please see Sanyal and Ren (2021) .\nFinally, we use the following feature attribution methods to compare our methods against: Grad*Inp (Shrikumar et al., 2016) , Integrated Gradients (Sundararajan et al., 2017) , DeepLift (Shrikumar et al., 2017) , GradientShap (Lundberg and Lee, 2017) and Discretized IG (DIG) (Sanyal and Ren, 2021) using the GREEDY heuristics. Moreover, as in Sanyal and Ren (2021) , we use 50 interpolation steps for all methods expect from DIG, for which we use 30 steps.\n\nResults\nComparison with other feature attribution methods We present of Tables 1, 2 and 3 a comparison of the performance of SIG with the attribution methods listed in 3.1. We observe that SIG significantly outperforms all other methods across most datasets and language models we used. This tends to confirm that the change of overall meaning of a sentence by combining interpolations simultaneously is an important issue which needs to be tackled. Sanyal and Ren (2021) show that DIG outperforms other methods, including IG, this is not the case when using \"mask\" as a token. This result seems to undermine the intuition of Sanyal and Ren (2021) that the discrete nature of the embedding space is an important factor when explaining a language model. We also show in Appendix C that the requirement of having a monotonic path, stressed by Sanyal and Ren (2021) , is not necessary.\n\nIG\n\"a well-made and often lovely depiction of the mysteries of friendship. SIG \"a well-made and often lovely depiction of the mysteries of friendship.\nIG \"a hideous , confusing spectacle , one that may well put the nail in the coffin of any future rice adaptations.\" SIG \"a hideous , confusing spectacle , one that may well put the nail in the coffin of any future rice adaptations.\"\nIG \"this is junk food cinema at its greasiest.\" SIG \"this is junk food cinema at its greasiest.\"\nIG \"a remarkable 179-minute meditation on the nature of revolution.\" SIG \"a remarkable 179-minute meditation on the nature of revolution.\"\nTable 5 : Examples of attributions on several sentences of the SST2 dataset. The underlined bold tokens represent the most important token in the sentence, while bold tokens represent the top 20 % tokens in the sentence, according to each attribution method.\nChoice of the baseline token We also provide in Appendix B results using \"pad\" as a baseline.\nComparison between Tables 1, 2 and 3 on one hand, and Tables 6, 7 , 8 on the other hand show that IG greatly improves using the \"mask\" token as a baseline. This seems to confirm our intuition of using this token instead of \"pad\". Moreover, SIG performs similarly using either token, which demonstrates the robustness of this method across these two baseline tokens.\nTime complexity of SIG One important drawback of SIG is its time complexity, which is dependent on the number of words in the input data.\nIn Table 4 , we compare the original IG with SIG, using different numbers of steps. We define t and t \u2032 as the time complexity of computing IG with respectively 50 and 250 steps, and N the number of words in the input data. This table shows that, although reducing the number of steps results in a decrease of performance, SIG with 10 steps still performs better than both IG with 250 steps and IG with 10 \u00d7 N steps, while having the same time complexity. Moreover, as noted in Sanyal and Ren (2021), using IG with a large number of steps decreases Delta = ij IG ij (x) \u2212 (F(x) \u2212 F(x)), while not improving performance. As a result, when computing attributions on long sentences or large texts, we recommend using SIG with a reduced number of steps instead of IG.\nComparison of IG and SIG on several examples We provide on Table 5 several examples of explained sentences, using IG and SIG. Both methods tend to agree on short sentences, while more disagreements appear on larger ones. For each example, we display in underlined bold the most important token, and in bold the top 20 % most important tokens, according to each method.\n\nConclusion\nIn this work, we have defined an attribution method specific to text data: Sequential Integrated Gradients (SIG). We have shown that SIG yields significantly better results than the original Integrated Gradients (IG), as well as other methods specific to language models, such as Discretized Integrated Gradients (DIG). This suggests that keeping the meaning of interpolated sentences close to the original one is key to producing good explanations. We have also shown that, although SIG can be computationally intensive, reducing the number of interpolations still yields better results than IG with a greater number of interpolations.\nWe have also highlighted in this work the benefit of using the token \"mask\" as a baseline, instead of \"pad\". Although SIG seems to be robust across both tokens, this is especially important when using IG, as it significantly improves the quality of explanations. Using the trainable token \"mask\" is indeed closer to the training procedure of language models, and should yield better interpolations as a result. We recommend therefore using this token as a baseline, when possible, when explaining predictions made by a language model. Moreover, while this study was conducted on bidirectional language models such as BERT, SIG could also be used on auto-regressive models such as GPT-2 (Radford et al., 2019) , by iteratively computing the attribution of a token, while keeping previous tokens fixed, and masking future tokens if any has been already computed.\n", "hypothesis": "In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. However, instead of using the baseline token \"pad\" as in the original method, we propose to use a randomly generated token as the baseline, as it has been shown to better capture the variability in language models.", "answer": false}
{"title": "Towards Speech Dialogue Translation Mediating Speakers of Different Languages", "content": "\nIntroduction\nIn this global era, it is becoming increasingly important for people from different countries/regions to interact with each other and have a mutual understanding. Recent advancements in machine translation (MT) technologies have enabled us to communicate with people worldwide, especially in text. Chat translation or dialogue machine translation (Liu et al., 2021) supports such communications, which enables people who use different languages to have cross-language chats. Speech translation (ST) has also recently shown success (e.g., Chen et al., 2022) , especially in monologue translation (e.g., Di Gangi et al., 2019) . However, to the best of our knowledge, no study has focused on ST of dialogues, which is an important aspect of language usage.\nIn this study, we propose a new task: speech dialogue translation (SDT) aiming to mediate speakers of different languages. We consider bilingual dialogues where several people who speak in different languages talk with each other mediated by an ST system.\nIt is important to consider context in SDT because we need to consider context in different languages, which cannot be readily handled by current ST systems that mainly focus on one translation direction. Figure 1 shows an example of an STmediated dialogue between an English speaker and a Japanese speaker. They are discussing some ideas, and the English speaker says, \"What do you think about it?\" The Japanese speaker responds by saying the idea is naive, but without context it can be translated as \"I think it's a bit sweet\" because \"\u7518\u3044\" has two meanings, sweet and naive. By utilizing dialogue context, the meaning of \"\u7518\u3044\" becomes clear so that the utterance can be translated properly.\nFor the proposed task, we construct the SpeechBSD dataset 1 based on an existing text dialogue corpus, BSD (Bussiness Scene Dialogue) corpus (Rikters et al., 2019) . We collect audio of the BSD corpus through crowdsourcing along with speaker attributes.\nWe conduct speech-to-text cascaded ST experiments on the dataset. There are two mainstream methods for ST, the cascade method (Stentiford and Steer, 1988) where automatic speech recognition (ASR) and MT are chained together, and the end-to-end method (Duong et al., 2016; Berard et al., 2016) , where translations are directly predicted from speech. Recent study (Bentivogli et al., 2021; Tran et al., 2022) suggests that the two methods are on par. We conduct cascade ST experiments using Whisper (Radford et al., 2022) for ASR and mBART (Liu et al., 2020) for MT.\nWe consider three settings for translation: without context, with monolingual context, and with bilingual context. The monolingual context is composed in the language the utterance to be translated is spoken, whereas the bilingual context is composed in the original language of the spoken utterances (see examples in Figure 1 ). We show that translation with bilingual context performs better compared to the one without context by up to 1.9 BLEU points in MT and 1.7 BLEU points in cascade ST with our settings. We also conduct a manual evaluation focusing on zero anaphora, a grammatical phenomenon where arguments of verbs are omitted when they are apparent from the context in Japanese. We show that with bilingual context, the MT models can often predict zero pronouns correctly.\n\nRelated Work\nAlthough neural MT has greatly improved over the past few years, the translation of dialogues remains a challenging task because of its characteristics. Liu et al. (2021) summarizes the recent progress of dialogue MT and categorizes its issue into four categories, coherence, consistency, cohesion, and personality. The main approaches to address these problems include document MT (e.g., Liu et al., 2021) , usage of pretrained models (e.g., Wang et al., 2020) , and auxiliary task learning utilizing speaker information (e.g., Liang et al., 2021) .\nConsidering context in ST is recently studied for the end-to-end approach (Zhang et al., 2021) . We point out that although not addressed in this work, considering context for ASR is also an active research area (e.g., Inaguma and Kawahara, 2021) .\nIn this work, we focus on the translation of speech dialogue. We use mBART, which performed best in a previous work of chat translation (Liu et al., 2021) , and also consider utilizing context.\n\nSpeech Dialogue Translation (SDT)\nIn SDT, there are several speakers who speak different languages with the help of a translation system. In this work, we consider M speak-\ners {S m | m = 1, 2, \u2022 \u2022 \u2022 , M } and 2 languages {L n | n = 1, 2}. We consider a dialogue with T utterances D = (U 1 , \u2022 \u2022 \u2022 , U T )\n, where an utterance is U t = (S m t , L n t , X t ). Here, S m t is the speaker, L n t is the language spoken, and X t is the speech signal of t-th utterance. Let Y n t (n = 1, 2) be text that has the same meaning as X t in language L n . The task of SDT is to generate translation Y 2 t from speech signal X t when the source language is L 1 (or translation Y 1 t from X t when the source language is L 2 ) for every utterance U t .\n\nSpeechBSD Dataset\nWe construct the SpeechBSD dataset to study SDT. It is based on the existing dialogue dataset in text, BSD corpus (Rikters et al., 2019 (Rikters et al., , 2021)) . We collect audio of all the sentences in the dataset along with speaker attributes (gender and homeplace) through crowdsourcing.\n\nBSD Corpus\nBSD corpus is a parallel corpus of English and Japanese composed of manually designed business scene dialogues. Each dialogue called scenario contains 30 sentences on average spoken by 2-5 speakers. The original language the scenarios were written in is half English and half Japanese so that the expressions are not biased toward one language.\n\nDataset Construction\nFirst, we divided each scenario by speaker. For example in Figure 1 , the original BSD corpus con- and  Y 2 3 ). In this way, we can compose two crosslanguage dialogues (Y\ntains text of Y 1 1 , Y 2 1 , Y 1 2 , Y 2 2 , Y\n1 1 \u2192 Y 2 2 \u2192 Y 1 3 and Y 2 1 \u2192 Y 1 2 \u2192 Y 2 3\n) from one scenario of the BSD corpus. We collected audio through crowdsourcing so that each part is spoken by a different worker. 2 We designed a web application to record audio and collected English speech from the US using Amazon Mechanical Turk 3 and Japanese speech from Japan using Yahoo! crowdsourcing. 4 We also collected the gender and homeplace (the US states or Japanese prefecture) of the speakers as they may affect translation performance. The instructions given to the workers are shown in Appendix A.1.\n\nStatistics of the SpeechBSD Dataset\nThe collected audio was 24.3 hours for English speech and 30.7 hours for Japanese speech in total. Details are provided in Appendix B Table 2 . Regarding speaker gender, English speech was balanced, whereas there were more male speakers in Japanese. As for homeplace, in Japanese, the speakers were distributed roughly according to the population distribution. In English, it was less diverse (Appendix B Figure 3 ).\n\nConsidering Context for SDT\nWe propose two ways to consider context in SDT: monolingual context and bilingual context.\nFirst, for every utterance U t , an ASR system is used to obtain transcripts Y n t . The monolingual context is composed in the source language of the utterance to be translated. For example, in Figure 1 , when translating the third utterance U 3 from Japanese to English, as the source language of the utterance is Japanese (L 1 ), the context (Y 1 1 and Y 1 2 ) is also composed in Japanese. Let the context composed in this way be Y n <t .\nFor monolingual context experiments, we use two translation models for each translation direction. The training objective of the MT model that translates from L 1 to L 2 is to maximize the follow-ing log likelihood 5 :\nL 1\u21922 = t log P(Y 2 t , Y 2 <t | Y 1 t , Y 1 <t ). (1)\nSimilar objective L 2\u21921 can be derived when L 2 is the source language and L 1 is the target language.\nPostprocessing is applied to extract Y 2 t from the output that contains both Y 2 <t and Y 2 t . The bilingual context is composed of the original language of the spoken utterances. For example, in Figure 1 , when translating the third utterance U 3 from Japanese to English, the bilingual context on the source side is Y 1 1 and Y 2 2 , which involves both languages. The bilingual context on the target side is Y 2 1 and Y 1 2 . Because there is no concept of source or target language in this case, let the source side utterance be Y t , source side context be Y <t , target side utterance be Y t , and target side context be Y <t . The MT model is trained with the following objective:\nL = t log P(Y t , Y <t | Y t , Y <t ).\n(2)\nPostprocessing is applied to extract Y t from the output.\nWe consider constrained context with context size c in practice, which shows the number of previous utterances used for translation in addition to the utterance to be translated. More formal definitions of monolingual, bilingual, and constrained context are provided in Appendix C.\n\nAutomatic Speech Recognition\nIn SDT, ASR has to handle bilingual inputs. We used a multilingual ASR model Whisper (Radford et al., 2022) . The medium model with 12 encoder and decoder layers was used without finetuning. Further details are provided in Appendix D.1. We evaluated the performance of the SpeechBSD test set. For English the word error rate was 8.3 %, and for Japanese the character error rate was 13.2 %.\n\nMachine Translation\nMT model also needs to handle bilingual inputs in SDT. We used mBART (Liu et al., 2020) and finetuned the model with SpeechBSD for MT. The large model with 12 encoder and decoder layers was used. Although the dialogues are regarded as bilingual ones in this study, the predictions were recomposed to the monolingual dialogue form for evaluation because usually performance of MT models is evaluated on a single language pair. SacreBLEU (Post, 2018) was used for calculating BLEU scores. Further details are provided in Appendix D.2.\n\nContext Settings\nThree settings were considered: translation without context, with monolingual context, and with bilingual context.\nWithout Context Each utterance in a scenario was treated as a separate sentence in this setting. Finetuning was performed separately for each translation direction.\n\nMonolingual Context\nFor each utterance in a scenario, monolingual context with context width c = 5 was composed in the way described in section 5. The context utterances and the utterance to translate were concatenated with the end of sentence token </s>. Finetuning was performed separately for each translation direction.\nBilingual Context For each utterance in a scenario, bilingual context with context width c = 5 was composed in the way described in section 5. The context utterances and the utterance to translate were concatenated with the end of sentence token </s>. As there is no concept of source language or target language in this setting, a single model was finetuned in this setting.\n\nResults\nTable 1 (upper part) shows the results of the MT experiments. Comparing \"Without\" with \"Monolingual,\" more than 0.9 points of improvement were observed using monolingual context. Comparing \"Monolingual\" with \"Bilingual,\" the latter performed better, especially in Ja-En.\n\nManual Evaluation\nTo verify how context can help improve translations, we conducted a manual evaluation focusing on a grammatical phenomenon called zero anaphora, as discussed in Rikters et al. (2019) . Similarly to Rikters et al. (2019) , we counted the number of sentences with pronouns I, you, he, she, it, and they in English 6 and observed that 63 % of the test sentences included them. We sampled 50 of those sentences from the test set. First, we checked if the subjects of the Japanese sentences were zero pronouns by comparing Japanese and English gold references. Then we checked if the zero pronouns were translated into English correctly for the predictions of each Ja-En system. Out of the 50 sentences, 29 were sentences with zero pronoun subjects. The number of sentences that the missing pronoun was translated correctly was 19, 20, and 24 for without context, monolingual context, and bilingual context settings, respectively. This shows that context can help disambiguate zero pronouns, and using bilingual context can help generate correct pronouns. Examples of the sentences are shown in Appendix E.\n\nCascade Speech Translation\nCascade ST experiments were performed by using Whisper recognition results as input to the MT models described in section 6.2.\nTable 1 (lower part) shows the results. Similarly to MT, BLEU score improved by more than 0.7 points by using monolingual context. Further improvements by more than 0.5 points were observed using bilingual context.\nWe also performed manual evaluation as in Section 6.2.3. The number of sentences that the missing pronoun was translated correctly was 16, 18, and 22 for without context, monolingual context, and bilingual context settings, respectively. It showed a similar trend to the results of section 6.2.3 with lower translation accuracy. Examples of the sentences are shown in Appendix E.\n\nConclusion\nWe presented a new task, SDT aiming to mediate speakers of different languages. We constructed the SpeechBSD dataset via crowdsourcing. We performed MT experiments utilizing context and showed its effectiveness. In the future, we plan to perform experiments in end-to-end ST settings and SDT utilizing speaker attributes.\n", "hypothesis": " We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our settings..", "answer": true}
{"title": "AVATAR: A Parallel Corpus for Java-Python Program Translation", "content": "\nIntroduction\nSoftware developers and researchers often require to convert software codebases or research prototypes from one platform to another or rewrite them in the target programming languages. Manually rewriting software is time-consuming, expensive, and requires expertise in both the source and target languages. For example, the Commonwealth Bank of Australia spent around $750 million and 5 years translating its platform from COBOL to Java (Lachaux et al., 2020) . A program translation system that converts the source code of a program written in a programming language to an equivalent program in a different programming language is known as a transcompiler, transpiler, or source-tosource compiler. Transcompilers have a prodigious practical value; they could help to reduce the translation efforts of developers and researchers by not requiring them to write code from scratch, instead, they can edit the translated code with less effort.\nThe conventional transcompilers are based on rule-based approaches; they first convert source code into an Abstract Syntax Tree (AST) and then apply handwritten rules to translate to the target language. Development and adaptation of transcompilers need advanced knowledge and therefore are available in a handful of programming languages. Undoubtedly, the automation of program translation would facilitate software development and research tremendously.\nWith the recent advancements in data-driven neural machine translation (NMT) approaches between natural languages, researchers have started investigating them for programming language translation. Lachaux et al. ( 2020) trained an NMT system in an unsupervised fashion using large-scale monolingual source code from GitHub that showed noteworthy success in source code translation between Java, Python, and C++ languages. Pre-trained language models (PLMs) of code have been shown to work well on Java-C# translation after fine-tuning on a small amount of parallel examples (Feng et al., 2020; Guo et al., 2021; Ahmad et al., 2021; Wang et al., 2021) . Motivated by these favorable results, in this work, we propose a new parallel corpus of Java and Python programs.\nWe propose a corpus, AVATAR (jAVA-pyThon progrAm tRanslation) that consists of solutions written in Java and Python for 9,515 programming problems collected from competitive programming sites, online platforms, and open source repositories. AVATAR includes 250 examples with unit tests to facilitate functional correctness evaluation of program translation. We train several baselines, including models trained from scratch or pre-trained on large-scale source code collection and fine-tuned on AVATAR. The experiment results indicate that while the models perform considerably in terms of the lexical match, they lack Fur- We collect [1 -20] accepted solutions for a single problem written in Java and Python.\nPreprocessing & Filtering At first, we tokenize the solution code and remove docstrings and comments from them. We use the javalang 4 tokenizer for Java and the tokenizer 5 of the standard library for Python. After tokenization, we filter out solutions that are longer than a specified \n\nEvaluation Metrics\nBLEU computes the overlap between candidate and reference translations (Papineni et al., 2002) .\nSyntax Match (SM) represents the percentage of the sub-trees extracted from the candidate program's abstract syntax tree (AST) that match the sub-trees in reference programs' AST.\nDataflow Match (DM) is the ratio of the number of matched candidate data-flows and the total number of the reference data-flows (Ren et al., 2020) .\nCodeBLEU (CB) is the weighted average of the token level match, syntax level match (SM), and Dataflow match (DM) (Ren et al., 2020) .\nExecution Accuracy (EA) indicates the percentage of translated programs that are executable (results in no compilation or runtime errors).\nComputational Accuracy (CA) Lachaux et al. (2020) proposed the metric to evaluate whether the candidate translation generates the same outputs as the reference when given the same inputs.\n\nModels\nWe evaluate a variety of models on program and function translation using AVATAR and the evaluation dataset released by Lachaux et al. (2020) .\nZero-shot This set of models is evaluated on AVATAR without any training or fine-tuning.\n\u2022 TransCoder is pre-trained in an unsupervised fashion that can translate programs between Java, Python, and C++ languages (Lachaux et al., 2020) .\n\u2022 DOBF uses deobfuscation pretraining followed by unsupervised translation (anne Lachaux et al., 2021) .\n\u2022 TransCoder-ST is developed by fine-tuning TransCoder on a parallel corpus created via an automated unit-testing system (Roziere et al., 2022) .\nModels trained from scratch These models are trained from scratch using AVATAR. We use the sentencepiece tokenizer and vocabulary from Ahmad et al. (2021) in these models.\n\u2022 Seq2Seq+Attn. is an LSTM based sequence-tosequence (Seq2Seq) model with attention mechanism (Bahdanau et al., 2015) .\n\u2022 Transformer is a self-attention based Seq2Seq model (Vaswani et al., 2017) . We use the Transformer architecture studied in Ahmad et al. (2020) .\nPre-trained Models We evaluated three types of pre-trained models (PLMs). First, we evaluate decoder-only PLMs (e.g., CodeGPT) that generate auto-regressively. The second category of PLMs is encoder-only (e.g., CodeBERT). We use a randomly initialized decoder to finetune such models in a Seq2Seq fashion. The third category of PLMs is Seq2Seq models (e.g., PLBART), which we directly finetune on translation tasks.\n\u2022 CodeGPT and CodeGPT-adapted are GPT-2 (Radford et al., 2019) style models pre-trained on CodeSearchNet (Lu et al., 2021) . Note that CodeGPT-adapted starts from the GPT-2 checkpoint, while CodeGPT is pre-trained from scratch.\n\u2022 CodeBERT is an encoder-only model that is pre-trained on unlabeled source code via masked language modeling (MLM) and replaced token detection objectives (Feng et al., 2020) .\n\u2022 GraphCodeBERT is pre-trained using MLM, data flow edge prediction, and variable-alignment between code and its' data flow (Guo et al., 2021) .\n\u2022 PLBART is a Transformer LM pre-trained via denoising autoencoding (Ahmad et al., 2021) .\n\u2022 CodeT5 is a Transformer LM pre-trained via identifier-aware denoising (Wang et al., 2021) .\nIn addition, we fine-tune TransCoder-ST, which is the best translation model in the literature.\n\nHyperparameters Details\nWe individually fine-tune the models for Java to Python and Python to Java program and function translation, respectively. We fine-tune the models for a maximum of 20 epochs using the Adam (Kingma and Ba, 2015) optimizer with a batch size of 32. We tune the learning rate in the range [1e \u2212 4, 5e \u2212 5, 3e \u2212 5, 1e \u2212 5]. The final models are selected based on the validation BLEU score. We use beam decoding with a beam size set to 10 for inference across all the models.\n\nProgram Translation\nThe performance comparison of all the experiment models is presented in Table 2 . In general, all the models perform well in terms of match-based metrics, e.g., BLEU and CodeBLEU. However, the computational accuracy (CA) clearly indicates that these models are far from perfect in generating functionally accurate translations. Overall, the best-performing model is PLBART, resulting in the highest execution accuracy (EA) and CA in Java to Python translation. To analyze program translation errors, we manually examine the errors made by PLBART. We observe that PLBART does not generate the import statements in Java properly, resulting in many failures to find symbols (e.g., StringTokenizer, Buffere-dReader). Moreover, a quick look at the error made by all models reveals that type mismatch is one of the primary causes of compilation errors in all the models. We also notice that models fail to translate longer programs. Qualitative Examples We demonstrate a couple of qualitative examples of Java to Python program translation by PLBART in Figure 1 . We observe that PLBART correctly translates Java API Math.pow() to pow() in Python. We also observe that PLBART learns to translate a class with a function in Java to a function only in Python.\nIn Figure 2 , we present an example of Python to Java program translation. We see PLBART fail to translate correctly. We notice PLBART unnecessarily generates InputReader class that uses BufferedReader to read from standard input. Furthermore, we observed another behavior: when translating from Python to Java, PLBART generates classes with the name either Main or GFG. This is presumably due to the generic class name used in many programming solutions and GeeksforGeeks examples.\nWe present qualitative examples of Java to Python and Python to Java function translation by PLBART in Figure 3 and 4 . Overall, we observe a pretty good quality of translations, although there are translations that do not pass all the unit tests, as demonstrated by the performance in terms of computational accuracy in the main result. tions in each language) from GeeksforGeeks to evaluate their proposed translation model. Concurrent works (CodeGeeX, 2022; Athiwaratkun et al., 2023) present unit tests-based benchmarks to evaluate zero-shot translation capabilities of large language models. Different from these works, we propose a sizeable parallel corpus of Java and Python programs by collecting programming problem solutions from competitive programming sites, online platforms, and open-source repositories.\n\nConclusion\nThis work proposes a parallel corpus of Java and Python programs to contribute to the development of translation systems for programming languages that have a sizeable impact on software development. We evaluate several neural machine translation systems on the proposed dataset and perform analysis to reveal crucial factors that affect program translation accuracy. In our future work, we want to increase the size of the parallel corpus and support more programming languages.\n", "hypothesis": "We benchmark several pretrained language models fine-tuned on AVATAR. Experiment results show that the models excel in generating functionally accurate code.", "answer": false}
{"title": "ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion", "content": "\nIntroduction\nKnowledge graphs (KG) organize knowledge about the world as a graph where entities (nodes) are connected by different relations (edges). The knowledge-graph completion (KGC) task aims at adding new information in the form of (entity, relation, entity) triples to the knowledge graph. The main objective is assigning to each triple a plausibility score, which defines how likely this triple belongs to the underlying knowledge base. These scores are usually predicted by the knowledge graph embedding (KGE) models. However, most KGC approaches do not make any binary decision and provide a ranking, not classification, which does not allow one to use them as-is to populate the KGs (Speranskaya et al., 2020) . To transform the scores into predictions (i.e., how probable is it that this triple should be included in the KG), decision thresholds need to be estimated. Then, all triples with a plausibility score above the threshold are classified as positive and included in the KG; the others are predicted to be negatives and not added to the KG. Since the initial KG includes only positive samples and thus cannot be used for threshold calibration, the calibration is usually performed on a manually annotated set of positive and negative tuples (decision set). However, manual annotation is costly and limited, and, as most knowledge bases include dozens (Ellis et al., 2018) , hundreds (Toutanova and Chen, 2015) or even thousands (Auer et al., 2007) of different relation types, obtaining a sufficient amount of labeled samples for each relation may be challenging. This raises a question:\nHow to efficiently solve the cold-start thresholds calibration problem with minimal human input?\nWe propose a new method for Active Threshold Calibration ACTC 1 , which estimates the relation thresholds by leveraging unlabeled data additionally to human-annotated data. In contrast to already existing methods (Safavi and Koutra, 2020; Speranskaya et al., 2020) that use only the annotated samples, ACTC labels additional samples automatically with a trained predictor (Logistic Regression or Gaussian Process model) estimated on the KGE model scores and available annotations. A graphical illustration of ACTC is provided in Figure 1 .\nOur main contributions are:\n\u2022 We are the first to study threshold tuning in a budget-constrained environment. This setting is more realistic and challenging in contrast to the previous works where large validation sets have been used for threshold estimation.\n\u2022 We propose actively selecting examples for manual annotation, which is also a novel approach for the KGC setting.\n\u2022 We leverage the unlabeled data to have more labels at a low cost without increasing the annotation budget, which is also a novel approach for the KGC setting.\nExperiments on several datasets and with different KGE models demonstrate the efficiency of ACTC for different amounts of available annotated samples, even for as little as one.\n\nRelated Work\nKnowledge graph embedding methods (Dettmers et al., 2017; Trouillon et al., 2016; Bordes et al., 2013; Nickel et al., 2011) have been originally evaluated on ranking metrics, not on the actual task of triple classification, which would be necessary for KGC. More recent works have acknowledged this problem by creating data sets for evaluating KGC (instead of ranking) and proposed simple algorithms for finding prediction thresholds from annotated triples (Speranskaya et al., 2020; Safavi and Koutra, 2020) . In our work, we study the setting where only a limited amount of such annotations can be provided, experiment with different selection strategies of samples for annotation, and analyze how to use them best. Ostapuk et al. (2019) have studied active learning for selecting triples for training a scoring model for KG triples, but their method cannot perform the crucial step of calibration. They consequently only evaluate on ranking metrics, not measuring actual link prediction quality. In contrast, our approach focuses on selecting much fewer samples for optimal calibration of a scoring model (using positive, negative, and unlabeled samples).\n\nACTC: Active Threshold Calibration\nACTC consists of three parts: selection of samples for manual annotation, automatic labeling of additional samples, and estimating the per-relation thresholds based on all available labels (manual and automatic ones).\nThe first step is selecting unlabeled samples for human annotation. In ACTC this can be done in two ways. One option is a random sampling from the set of all candidate tuples (ACTC rndm ; the pseudocodes can be found in Algorithm 1). However, not all annotations are equally helpful and informative for estimation. To select the representative and informative samples that the system can profit the most from, especially with a small annotation budget, we also introduce density-based selection ACTC dens inspired by the density-based selective sampling method in active learning (Agarwal et al., 2020; Zhu et al., 2008) (the pseudocode can be found in Algorithm 2 in Appendix A). The sample density is measured by summing the squared distances between this sample's score (predicted by the KGE model) and the scores of other samples in the unlabeled dataset. The samples with the highest density are selected for human annotation.\nIn a constrained-budget setting with a limited amount of manual annotations available, there are sometimes only a few samples annotated for some relations and not even one for others. To mitigate this negative effect and to obtain good thresholds even with limited manual supervision, ACTC labels more samples (in addition to the manual annotations) with a classifier trained on the manually annotated samples to predict the labels based on The final part of the algorithm is the estimation of the relation-specific thresholds. Each sample score from the decision set is tried out as a potential threshold; the relation-specific thresholds that maximize the local accuracy (calculated for this decision set) are selected.\n\nExperiments\nWe evaluate our method on two KGC benchmark datasets extracted from Wikidata and augmented with manually verified negative samples: CoDEx-s and CoDEx-m 2 (Safavi and Koutra, 2020) . Some details on their organization are provided in Appendix B. The KGE models are trained on the training sets 3 . The ACTC algorithm is applied on the validation sets: the gold validation labels are taken as an oracle (manual annotations; in an interactive setting they would be presented to human annotators on-the-fly); the remaining samples are used unlabeled. The test set is not exploited during ACTC training and serves solely for testing purposes. The dataset statistics are provided in Table 1 . We run our experiments with four KGE models: ComplEx (Trouillon et al., 2016) , ConvE (Dettmers et al., 2017) , TransE (Bordes et al., 2013) , RESCAL (Nickel et al., 2011) \n\nBaselines\nACTC is compared to three baselines. The first baseline LocalOpt (Acc) optimizes the per-relation thresholds towards the accuracy: for each relation, the threshold is selected from the embedding scores assigned to the samples with manual annotations that contain this relation, so that the local accuracy (i.e., accuracy, which is calculated only for these samples) is maximized (Safavi and Koutra, 2020) . We also modified this approach into LocalOpt (F1) by changing the maximization metric to the local F1 score. The third baseline is GlobalOpt, where the thresholds are selected by iterative search over a manually defined grid (Speranskaya et al., 2020) .\nThe best thresholds are selected based on the global F1 score calculated for the whole dataset 4 . In all baselines, the samples for manual annotation are selected randomly. \n\u00b13 \u00b12 \u00b12 \u00b12 \u00b13 \u00b13 \u00b11 \u00b11 \u00b11 \u00b11 \u00b12 \u00b12 \u00b13 \u00b13 \u00b12 \u00b12\nTable 2 : ACTC results in % averaged across different sizes of annotation budget reported with the standard error of the mean. The experiment with each annotation budget was repeated 100 times.\n\nResults\nWe ran the experiments for the following number of manually annotated samples: 1, 2, 5, 10, 20, 50, 100, 200, 500, and 1000. Experimental setup details are provided in Appendix E. Table 2 provides the result averaging all experiments (here and further, n = 500 for a fair comparison; see Section 5 for analyze of n value), and our method ACTC outperforms the baselines in every tried setting as well as on average. Figure 2a also demonstrates the improvement of ACT C rndm over the baselines for every tried amount of manually annotated samples on the example of CoDEx-s dataset; the exact numbers of experiments with different budgets are provided in Appendix F. The density-based selection, on the other hand, achieves considerably better results when only few manually annotated samples are available (see Figure 2b ). Indeed, choosing representative samples from the highly connected clusters can be especially useful in the case of lacking annotation. LR dense , which selects points from regions of high density, can be helpful for small annotation budgets since it selects samples that are similar to other samples. In contrast, when having \n\nAblation Study\nA more detailed ablation study of different ACTC settings is provided in Appendix D.\nGlobal Thresholds. All methods described above calibrate the per-relation thresholds. Another option is to define a uniform (uni) threshold, which works as a generic threshold for all tuples regardless the relations involved. We implemented it as ACT C \u2212 LR uni method, where the additional samples are automatically labeled and used to build a decision dataset together with the manually annotated ones -in the same way as done for the relation-specific version, but only once for the whole dataset (thus, significantly reducing the computational costs). We also applied the LocalOpt(Acc) and LocalOpt(F1) baselines in the uniform setting. Figure 3 demonstrates the results obtained with the Conve KGE model and random selection mechanism on the CodEX-s dataset.\nAlthough the universal versions generally perform worse than the relation-specific, ACT C uni still outperforms the universal baselines and even relationspecific ones for a small annotation budget.\nDifferent n values. An important parameter in ACLC is n, the minimal sufficient amount of (manually or automatically) labeled samples needed to calibrate the threshold. The ablation study of different n values is provided in Figure 4 on the example of ACT C \u2212LR dens setting, averaged across all annotation budgets. ACTC performs as a quite stable method towards the n values. Even a configuration with a minimum value of n = 5 outperforms baselines with a small annotation budget or even with quite large one (e.g. for RESCAL). \n\nConclusion\nIn this work, we explored for the first time the problem of cold-start calibration of scoring models for knowledge graph completion. Our new method for active threshold calibration ACTC provides different strategies of selecting the samples for manual annotation and automatically labels additional tuples with Logistic Regression and Gaussian Processes classifiers trained on the manually annotated data. Experiments on datasets with oracle positive and negative triple annotations, and several KGE models, demonstrate the efficiency of our method and the considerable increase in the classification performance even for tiny annotation budgets.\n", "hypothesis": " In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation.\nOur new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples.  Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers.  We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection.", "answer": true}
{"title": "A Hierarchical Explanation Generation Method Based on Feature Interaction Detection", "content": "\nIntroduction\nThe opaqueness of deep natural language processing (NLP) models has increased along with their power (Doshi-Velez and Kim, 2017) , which has prompted efforts to explain how these \"black-box\" models work (Sundararajan et al., 2017; Belinkov and Glass, 2019) . This goal is usually approached with attribution method, which assesses the influence of inputs on model predictions (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2018) Prior lines of work on attribution explanations usually calculate attribution scores for predefined text granularity, such as word, phrase, or sentence. Recently, work has introduced the new idea of hierarchical attribution, which calculates attribution scores for compositional text hierarchically to capture more information for reflecting model predictions (Singh et al., 2018; Tsang et al., 2018; Jin et al., 2019; Chen et al., 2020) As shown in Fig- ure 1 , hierarchical attribution produces a hierarchical composition of words, and provides attribution scores for every text group. By providing compositional semantics, hierarchical attribution can give users a better understanding of the model decisionmaking process. (Singh et al., 2018) . However, as illustrated in Figure 1 , recent work (Singh et al., 2018; Jin et al., 2019; Chen et al., 2020) uses continuous text to build hierarchical attributions, which we call the connecting rule. While consistent with human reading habits, using the connecting rule as an additional prior might lose important long-distance compositional semantics. The concerns are summarized as follows:\nFirst, modern NLP models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018 (Radford et al., , 2019) ) are almost all transformer-based, which use self-attention mechanisms (Vaswani et al., 2017) to capture feature interactions. Since all interactions are calculated in parallel in self-attention mechanism, the connecting rule that only considering neighboring text is incompatible with the basic operation principle of these NLP models. Second, unlike the example in Figure 1 , NLP tasks often require joint reasoning of different parts of the input text (Chowdhary, 2020). For example, Figure 2 (a) shows an example of natural language interface (NLI) task 1 , in which 'has a' and 'avail- able' are the key compositional semantics to make the prediction: entailment. However, the connecting rule cannot highlight the compositional effect between them because they are not adjacent. Even in relatively simple sentiment classification task, capturing long-distance compositional effect is also necessary. As shown in Figure 2 (b), 'courage, is inspiring' is an important combination but not adjacent.\nIn this work, we introduce a simple but effective method for generating hierarchical explanations without the connecting rule. Moreover, we introduce a novel strategy for detecting feature interactions in order to capture compositional semantics. Unlike earlier hierarchical attribution approaches, which use specific algorithms to calculate attribution scores, the proposed method can convert ubiquitous non-hierarchical explanations (e.g., LIME) into their corresponding hierarchical versions. We build systems based on two classic non-hierarchical methods: LOO (Lipton, 2018) and LIME (Ribeiro et al., 2016) , and the experimental results show that both systems significantly outperform existing methods. Furthermore, the ablation experiment additionally reveals detrimental effects of the connecting rule on the construction of hierarchical explanations. Our implementation and genenerated explanations are available at an anonymous website: https://github.com/ juyiming/HE_examples.\n\nMethod\nThis section explains the strategy for feature interaction detecting and the algorithm on building hierarchical explanations. \n\nDetecting Feature Interaction\nThe structure of hierarchical explanations should be informative enough to capture meaningful feature interactions while displaying a sufficiently small subset of all text groups (Singh et al., 2018) . Existing work uses different methods to calculate feature interactions for building hierarchical explanations. For example, Jin et al. (2019) uses multiplicative interactions as feature interaction and Chen et al. (2020) uses Shapley interaction index (Fujimoto et al., 2006) .\nUnlike previous methods, our approach quantifies feature interaction based on the chosen nonhierarchical method. Specifically, given an attribution algorithm Algo, our method measures the influence of one text group on the attribution score of another one. The interaction score between text group g i and g j can be calculate as follows:\nEQUATION\nwhere Algo \u2212g j (g i ) denotes the attribuition score of g i with g j be marginalized, abs stands for taking the absolute value. Figure 3 shows an example of feature interaction detecting. Non-hierarchical method LIME gives the word 'Buffet' a high attribution score, indicating that it is important for model prediction. This score, however, sharply declines after the word 'buffet' is marginalized, indicating that 'buffet' has a strong impact on 'Buffet' under LIME. Note that in our method, different non-hierarchical attribution methods may lead to different hierarchical structures. Since the calculation principles and even the meaning of scores vary in different attribution methods, this property is more reasonable than building the same hierarchical structures for all attribution methods. (Singh et al., 2018) 31.9 38.3 31.1 39.0 60.5 61.4 59.5 61.1 47.9 HEDGE \u2662 (Chen et al., 2020) 34.3 46.7 34.0 44.1 68.2 70.9 68.3 70 42.0 62.4 44.1 61.9 80.1 86.6 83.2 87.3 68.5\n\nMethod\nTable 1 : AOPC(10) and AOPC(20) scores of different attribution methods in on the SST and MNLI datasets. \u2662 refers to method with hierarchical structure. del and pad refer to different modification strategies in AOPC.\n\nAlgorithm 1 Generating Hierarchical Structures\nInput: sample text X with length n Initialization:\nG 0 = {{x 1 } , {x 2 } , ..., {x n }} Initialization: is H X = {G 0 } for t = 1, ..., n \u2212 1 do i, j = argmax \u03d5 ( g i , g j | G t\u22121 ) G t \u2190 (G t\u22121 \\ {g i , g j }) \u222a {g i \u222a g j } H X .add(G t ) end for\nOutput: H X Feature marginalization. The criterion of selecting the feature marginalization approach is to avoid undermining the chosen attribution method. For example, LOO assigns attributions by the probability change on the predicted class after erasing the target text, so we use erasing as the marginalization method. For LIME, which estimates attribution scores by learning a linear approximation, we ignore the sampling points with the target feature during linear fitting.\n\nBuilding Hierarchical Explanations\nBased on the non-hierarchical attribution algorithm Algo, our method builds the hierarchical structure of input text and calculates attribution scores for every text group. Algorithm 1 describes the detail procedure, which recursively chooses two text groups with strongest interaction and merges them into a larger one. X = (x 1 , ..., x n ) denotes model input with n words; g denotes a text group containing a set of words in X; G t denotes the collection of all text groups for the current step t; H X denotes the hierarchical structure of X. G 0 is initialized with each x as a independent text group and H X is initialized as {G 0 }. Then, at each step, text groups with the highest interaction score from G t\u22121 are merged as on, and G t is add into H X . After n \u2212 1 steps, all words in X will be merged in one group, and H X can constitute the final hierarchical structure of the input text.\n\nVisualization\nClear visualization is necessary for human readability. Since text groups in our hierarchical explanations are not continuous spans, the generated explanations cannot be visualized as a tree structure as shown in Figure 1 . To keep clear and informative, the visualization only shows the newly generated unit and its attribution score at each layer. As shown in Figure 4 , the bottom row shows the attribution score with each word as a text group (nonhierarchical attributions); The second row indicates {'Buffet'} and {'buffet'} are merged togather as one text group: {'Buffet, buffet'}; Similarly, the fourth row indicates the {'has, a'} and {'availiable'} are merged togather as one text group: {'availiable, has, a'}.\n\nExperiment\nWe build systems with Leave-one-out (LOO) (Lipton, 2018) and LIME (Ribeiro et al., 2016) as the basic attribution algorithms, denoted as HE loo and HE lime . To reduce processing costs, we limit the maximum number of the hierarchical layers to ten in HE LIM E .\n\nDatasets and Models.\nWe adopt two text-classification datasets: binary version of Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and MNLI tasks of the GLUE benchmark (Wang et al., 2019) . We use the dev set on SST-2 and a subset with 1,000 samples on MNLI (the first 500 dev-matched samples and the first 500 dev-mismatched samples) for evaluation. We build target models with BERT base (Devlin et al., 2019) as encoder, achieving 91.7% (SST-2) and 83.9% (MNLI) accuracy.\n\nEvaluation Metrics.\nFollowing previous work, we use the area over the perturbation curve (AOPC) to perform quantitative evaluation. By modifying the top k% words, AOPC calculates the average change in the prediction probability on the predicted class as follows:\nAOP C(K) = 1 N N i=1 p(\u0177|x i ) \u2212 p(\u0177|x (k) i ) ,\nwhere p(\u0177|) is the probability on the predicted class,\nx\ni is modified sample, and N is the number of examples,. Higher AOPCs is better, which means that the words chosen by attribution scores are more important 2 .\nWe evaluate with two modification strategies del and pad. del modifies the words by deleting them from the original text directly while pad modifies the words by replacing them with <pad> tokens. For hierarchical explanations, we gradually select words to be modified according to attribution scores. If the word number in a text group exceed the number of remaining words to be modified , this text group will be ignored. The detailed algorithm are described in the appendix.\n\nResults Compared to Other Methods\nAs shown in Table 1 , we compare our approach with a number of competitive baselines. Except for LIME, none of other baselines (hierarchical or not) shows a obvious improvement over LOO. In contrast, our LOO-based hierarchical explanations outperform LOO on average by more than 11%. Moreover, our LIME-based hierarchical explanations outperform LIME by 6% on average and achieves the best performance. The experimental results in Table 1 demonstrate the high quality of the generated explanations and the effectiveness of our method in converting non-hierarchical explanations to their corresponding versions.\n\nResults of Ablation Experiment\nWe conduct an ablation experiment with two special baselines modified from HE LOO : HE-random and HE-adjacent. HE-random merges text groups randomly in each layer; HE-adjacent merges adjacent text groups with the strongest interaction.\nAs shown in Figure 5 , both adjacent and proposed baselines outperform non-hierarchical and random baselines, demonstrating our approach's effectiveness in building hierarchical explanations. Moreover, HE-proposed outperforms HE-adjacent consistently on two datasets, demonstrating the detrimental effects of the connecting rule on generating hierarchical explanations. Note that HE-random on SST-2 slightly outperforms nonhierarchical baseline but has almost no improvement on MNLI. We hypothesize that this is because the input text on SST-2 is relatively short, and thus randomly combined text groups have greater chances of containing meaningful compositional semantics.\n\nConclusion\nIn this work, we introduce an effective method for generating hierarchical explanations without the connecting rule, in which a novel strategy is used for detecting feature interactions. The proposed method can convert ubiquitous non-hierarchical explanations into their corresponding hierarchical versions. We build systems based on LOO and LIME. The experimental results demonstrate the effectiveness of proposed approach.\n", "hypothesis": "Experimental results show the effectiveness of our approach in building high-quality non-hierarchical explanations.", "answer": false}
{"title": "Event Extraction as Question Generation and Answering", "content": "\nIntroduction\nEvent Extraction (EE) aims to extract core information elements (e.g. who, what, where, when) from text, and is a very important task in Natural Language Processing (NLP). It provides inputs to downstream applications such as Summarization (Filatova and Hatzivassiloglou, 2004) , Knowledge Base Population (Ji and Grishman, 2011), and Recommendation (Lu et al., 2016) .\nPrevious work (Li et al., 2013; Nguyen et al., 2016; Sha et al., 2018) is typically based on a pipeline approach, which first identifies the event trigger word/phrase and argument candidates, and then applies a classifier to the pair-wise features to classify the roles of the candidates. Unfortunately, errors tend to propagate down the pipeline. Recently, some approaches have formulated EE 1 Our code is available at https://github.com/ dataminr-ai/Event-Extraction-as-Question-Generation-and-Answering for research purposes. as a Question Answering (QA) problem (Du and Cardie, 2020; Li et al., 2020; Lyu et al., 2021) to mitigate the issue, in which questions for each argument role are manually defined by templates. For example, extracting the Attack argument from the Conflict.Attack event in the sentence 'That's because coalition fighter jets pummeled this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat.' is reframed as answering the question 'Who was the attacking agent?' These approaches have shown promising results, but template-based questions are limiting: since the templates are built manually, they are fixed and rarely include contextual information (i.e., specific to the inputs), except for trigger words in some work (Du and Cardie, 2020) . Formulating good questions, however, has been shown to improve performance for standard QA tasks (Rajpurkar et al., 2018) . For QA-based EE, a question that incorporates richer contextual information such as other event arguments could yield better results (e.g. 'Who used jets in the attack in hills?' in Figure 1 ).\nIn this paper, we propose QGA-EE, which consists of 1) a QG model for generating a contextaware question conditioned on a target argument role and 2) a QA model for answering the contextaware question to extract the event argument. We also design dynamic templates to generate the gold context-aware questions for QG model training.\nTo the best of our knowledge, this is the first QA-based EE work that utilizes dynamic templates and focuses on generating context-aware questions. Li et al. (2020) also propose a model to generate questions that incorporate contextual information for both event trigger and arguments. However, our work has two main advantages. First, in Li et al. (2020) the question only incorporates the contextual information at the ontology level (e.g. argument role, event type). In our work, the generated questions incorporate contextual information at an event mention-level. For example, the question generated by our model includes the real event argument rather than just the argument role (e.g. 'hills' vs 'Place'). Second, the questions in their work are generated by filling in the templates, but our templates are dynamic and used to train the QG model which can automatically generate the optimal question given a specific event mention and the concerned argument role.\nExperimental results show that QGA-EE outperforms all of the single-task-based models on the Automatic Content Extraction (ACE) 2005 English dataset (Doddington et al., 2004) and even achieves competitive performance with state-of-the-art joint IE models.\n\nModel\nFigure 1 shows the overall framework of QGA-EE. It focuses on Event Argument Extraction (EAE) only, but can be paired with any event trigger tagger to perform end-to-end EE. In Section 4, we pair it with a standard sequence labeling trigger tagger to evaluate its end-to-end EE performance.\n\nQuestion Generation Model\nPrevious QA-based EE work (Du and Cardie, 2020) fills in pre-designed templates with trigger information to generate the input questions to the QA model. However missing contextual information in the questions is a bottleneck for the performance of the QA model.\nQGA-EE uses a QG model to generate contextaware questions conditioned on the input sentence and target role, which is based on a sequence-tosequence architecture (e.g. BART (Lewis et al., 2020 ), T5(Raffel et al., 2020) For example, the Conflict.Attack event in ACE has four predefined argument roles: Attacker, Target, Instrument and Place. 3 For the Attacker role, we exhaustively design eight templates using all of the possible combinations of the other roles included in the question (Table 1 ). When the model fills in the templates given a specific event mention, it is common that some of the predefined argument roles do not exist in the event mention. Thus the model only keeps the templates that contain the slots for argument roles appearing in the event mention. For the example in Figure 1 , the Target role is not mentioned. So we ignore all of the templates that contain the [Target] slot, and we obtain four candidate questions for the Attacker role with corresponding arguments filled in: (1)Who was the attacking agent? (2) Who used jets in the attack? (3) Who made the attack in hills? (4) Who used jets in the attack in hills?\nTo train a QG model to generate the questions that cover as many contextual information as possible, we use the question that contains the most contextual arguments as the ground truth. For the example in Figure 1 , we choose the question 'Who used jets in the attack in hills?', because it contains two arguments: 'jets' and 'hills', the other three candidate questions listed above contain one or zero arguments. If more than one candidate question contains the most contextual arguments, we then pick the first one. The input and output examples for the QG model are as follows:\nInput: role: attacker context: That's because coalition fighter jets * pummeled * this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat. Output: Who used jets in the attack in hills?\n\nQuestion Answering Model\nDifferent from prior QA-based EE work that adapt an encoder-only architecture and predict the offsets of the event arguments (Chen et al., 2019; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020; Feng et al., 2020; Lyu et al., 2021; Zhou et al., 2021) , our QA model is based on a sequence-to-sequence architecture (e.g. BART, T5), and generates the answer string directly. This enables prediction of multiple event arguments that are associated with the same role. Li et al. (2021) Output: diplomats; convoy; victims < /s > Post-processing We split the output into a list of candidates (by ';'), and retrieve the arguments with offsets by exactly matching against the original sentence. We dynamically change the start position for searching to preserve the order of the retrieved event arguments. If an argument candidate cannot be matched with the original sentence, we discard it. Unlike the QG model, we use all of the possible questions as inputs during training for data augmentation purposes, and the size of the training data increases from 15,426 to 20,681. But in the testing phase, we use the single question generated by the QG model for each argument role.\n3 Experimental Setup\n\nDataset and Evaluation Metrics\nWe conduct the experiments on the ACE 2005 English corpora, which has 33 event types and 22 argument roles. It contains 599 documents collected from newswire, weblogs, broadcast conversations, and broadcast news. More specifically, we follow the pre-processing steps in Wadden et al. ( 2019), 4 and evaluate our models on the resulting ACE05-E dataset.\nFor evaluation, we use the same criteria as prior work (Li et al., 2013) : An event trigger is correctly identified if its offsets exactly match a reference. It is correctly classified if both its offsets and event type match a reference. An event argument is correctly identified (Arg-I) if its offsets and event type match a reference in the ground truth. It is correctly classified (Arg-C) if all of its offsets, event type, and argument role match a reference.\n\nCompared Baselines\nModel Variants. To evaluate the generalizability of our approach, we evaluate two QGA-EE variants: QGA-EE BART and QGA-EE T 5 , which use BART and T5 as backbones respectively.\nWe compare the proposed models against SOTA EE models. BERT QA (Du and Cardie, 2020) We also compare with joint IE models trained on all of the ACE annotations which include entities, relations, and events. They benefit from additional information from other tasks and usually achieve better performance than the models trained on a single task. It is not fair to directly compare our model with the joint models since they incorporate more information beyond the standard EE training sets, but we still list their scores as a reference. DYGIE++ (Wadden et al., 2019) is a BERT-based model that models span representations with within-sentence and cross-sentence context. ONEIE (Lin et al., 2020) (Banarescu et al., 2013) parser.\n\nImplementation Details\nWe conduct all of the experiments on a single V100 GPU. For finetuning, we use the Adafactor (Shazeer and Stern, 2018) optimizer with a learning rate of 1 * 10 \u22124 , weight decay of 1 * 10 \u22125 , and clip threshold of 1.0. We train the model for 20 epochs. Further details such as hyperparameters and data statics for model training and evaluation are in Appendix C.\n\nEvent Argument Extraction Performance\nArg-I Arg-C BERT_QA (Du and Cardie, 2020) 68.2 65.4 TANL + (Paolini et al., 2021) 65.9 61.0 Ma et al. ( 2020) -62.1 BART-Gen (Li et al., 2021) 69.9 66.7 DYGIE++ Table 2 shows the performance of QGA-EE models on ACE05-E test set with gold triggers. 6 Both QGA-EE variants outperform all other approaches, and using T5 as backbone provides an improvement of 2.5% over BART. The improvement over the prior QA-based models BERT_QA shows that generation-based QA models are more effective than position-based QA models for EE. QGA-EE BART outperforms the BART-based baseline BART-Gen and QGA-EE T 5 outperforms the T5-based baseline TANL, which demonstrates the effectiveness of our models with different backbones. Our models even outperform the joint IE models DYGIE++ and ONEIE, which leverage additional information from entities and relations.\n\nEvent Extraction Performance\nWe also evaluate our models on ACE05-E in a more \"real world\" fashion with predicted triggers extracted by an ALBERT-based (Lan et al., 2019) sequence-labeling model (Table 3 ). 7 Similar to the performance on gold triggers, QGA-EE benefits more from the T5 backbone on predicted triggers. Both QGA-EE variants outperform all the EE-taskcentered baselines by more than 1% on Arg-C.\nArg-I Arg-C BERT_QA (Du and Cardie, 2020) 54 We also include the scores from SOTA joint IE models, DYGIE++, ONEIE, FourIE, AMR-IE and GraphIE, as reference. But, as stated earlier, it is not fair to compare our models directly with them, as they benefit from being trained with all of the annotations from entities, relations, and events. Also it should be noted that their trigger labeling models have more complicated architectures and thus perform significantly better than the sequence-labeling based tagger we use (F1 75.4% from FourIE and F1 74.7% from OneIE). This further boosts the end-to-end EE performance. \n\nImpact of Data Augmentation\nAs we mentioned in Section 2.2, the size of the training data increases from 15,426 to 20,681 as a benefit of our proposed dynamic templates. The average length of the questions generated by QGA-EE T 5 is 10.5 tokens, compared with 6.7 in Du and Cardie (2020) . They contain more context. For example, QGA-EE generates 'Who was attacked by mob in state?' for the Target role in 'At least three members of a family in Indias northeastern state of Tripura were [hacked Conf lict.Attack ] to death by a tribal mob for allegedly practicing witchcraft, police said Thursday.' It incorporates Attacker ('mob') and Place ('state') information.\n\nAnalysis and Discussion\nWe categorize the errors into four groups:\n1. Bad question generated by the QG model. We manually analyzed a subset of the errors from the test set (50 examples), and show the portion of each category of error in Figure 2 .\n\nConclusion\nIn this paper, we present QGA-EE, a novel sequence-to-sequence based framework for EE, which utilizes a QG model to generate contextaware questions as inputs to a QA model for EAE. Our model naturally supports the cases in which multiple event arguments play the same role within a specific event mention. We conduct experiments on the ACE05-E dataset and the proposed model outperforms all of the single-task-based models and achieves competitive results with state-of-theart joint IE models. In the future, we plan to utilize the extensibility of the QA framework to incorporate knowledge from semi-structured eventrelevant data such as Wikipedia Infoboxes. We also plan to extend our approach to multilingual EE and joint IE.\n", "hypothesis": " However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments.  In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role.  In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates.  We also propose dynamic templates to assist the training of QG model.", "answer": true}
{"title": "Theory-Grounded Computational Text Analysis", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.\nTable 1 : Contrast between work in computational text analysis with descriptive findings versus integrative findings.\nare interested in very different research questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n", "hypothesis": "We contrast descriptive and integrative findings, and our review of approximately 60 papers on computational text analysis reveals that those from *ACL venues are typically integrative. The lack of theory began at the area's inception and has, over the decades, grown more important and challenging.", "answer": false}
{"title": "The Ecological Fallacy in Annotation: Modelling Human Label Variation goes beyond Sociodemographics", "content": "\nIntroduction\nDifferent annotators will not necessarily assign the same labels to the same texts, resulting in human label variation (Plank, 2022) . Previous work finds that this variation depends at least in part on the sociodemographics of annotators, such as their age and gender (Binns et al., 2017; Al Kuwatly et al., 2020; Excell and Al Moubayed, 2021; Shen and Rose, 2021) . These results are particularly pronounced for subjective tasks like toxic content detection (Sap et al., 2019; Kumar et al., 2021; Sap et al., 2022; Goyal et al., 2022) . Since human label variation is relevant to a wide range of NLP tasks, recent research has begun to model individual annotator behaviour, rather than predicting aggregated labels (Davani et al., 2022; Gordon et al., 2022) . In this setting, we would expect sociodemographic attributes to help explain annotator decisions. Therefore, we investigate whether explicitly accounting for the sociodemographic attributes of annotators leads to better predictions of their annotation behaviour 1 .\nThere is a risk of misreading these efforts as an example of the ecological fallacy: aggregate group behaviour does not necessarily explain individual behaviour (Robinson, 1950; Freedman, 2015) . For example, while on average, white annotators may be more likely to label African-American Vernacular English as toxic (Sap et al., 2019) , that does not mean it is true for every white annotator individually. However, we aim at exactly this distinction to discuss the relevance of sociodemographic groups in models of individual annotator behaviour. Likewise, we do not assume prior work to commit ecological fallacies, even if a less-nuanced read might suggest it.\nDavani et al. ( 2022) introduce a simple multiannotator model, where each annotator is modelled with a separate classification head. We expand their model with group-specific layers, which are activated for each annotator based on their sociodemographic attributes. We compare the two model setups to a control setup where we randomise group assignments. All comparisons use annotator-level toxicity data from Kumar et al. (2021) . We find that find that explicitly accounting for sociodemo-graphic attributes does not significantly improve model performance. This result suggests that human label variation happens at a more individual level than sociodemographics, and that annotator decisions are even more complex.\nContributions 1) We introduce group-specific layers to model groups of annotators with shared attributes in multi-annotator models. 2) We evaluate the effect of group-specific layers for toxic content detection, and show that explicitly accounting for sociodemographic attributes does not significantly improve performance, thus highlighting the risk of the ecological fallacy in annotator modelling.\nAs a corollary, we show that multi-annotator models can be applied to many times more annotators than in prior work.\n\nSociodemographics in Annotation Behaviour\nA growing body of research studies how annotator sociodemographics relate to their annotation decisions, for tasks ranging from natural language inference (Biester et al., 2022) to the detection of racist (Larimore et al., 2021) or generally toxic (Sap et al., 2022) language. Goyal et al. (2022) , for example, find that annotators from certain sociodemographic groups (e.g., LGBTQ people) tend to find content attacking their own groups (e.g., homophobic content) to be more toxic. This motivates our research into explicitly accounting for sociodemographics to model annotation behaviour. However, the link between sociodemographics and behaviour is not uncontested. Biester et al. (2022) , for example, do not find significant differences in annotation behaviour between annotators of different genders for four different tasks.\nPredicting Annotators' Decisions on Text Different from analyses of annotation behaviour, a recent line of research attempts to learn models based on individual annotations (Plank et al., 2014; Jamison and Gurevych, 2015; Akhtar et al., 2020; Fornaciari et al., 2021; Cercas Curry et al., 2021) . These models are motivated by the concern that aggregating labels into a single \"truth\" is too simplistic for many tasks (Uma et al., 2021; Basile et al., 2021) and might introduce uneven representation of perspectives (Prabhakaran et al., 2021; Abercrombie et al., 2022) .\nA particular way of learning from disaggregated labels are models that predict individual annotator decisions for an example. Our work builds directly on such a model, multi-annotator models (Davani et al., 2022) , which we describe in more detail separately ( \u00a74). Gordon et al. (2022) present a model which also predicts individual annotations and allows a user to interactively aggregate them based on \"a jury\" inspired by the US judicial system. Their work is similar to ours in central aspects as they explicitly model annotators' sociodemographics and use the same dataset as we do (Kumar et al., 2021) . Different from our work, they frame the task as a regression problem and develop a model based on recommender systems. While they also explore ecological fallacies, they focus on usage risks of their system and countermeasures. In contrast, we consider the issue of the ecological fallacy in modelling annotation behaviour more generally. We compare our findings to their results ( \u00a76).\n\nData\nWe use a sample of the Kumar et al. (2021) dataset for our experiments. The full dataset contains 107,620 English comments from Twitter, Reddit, and 4Chan, annotated for toxicity by 17,280 annotators. The annotation process encouraged annotator subjectivity (R\u00f6ttger et al., 2022) which is a desired feature for modelling annotator behaviour. For each annotator, there is extensive sociodemographic information, collected with a survey. Annotations are given as ratings on a five-point scale which we convert to binary annotations by mapping ratings of 2 to 4 to toxic, and ratings 0 and 1 to non-toxic.\nWe randomly sample comments from the dataset until we reach annotations from more than 5,000 annotators. We then add all other annotations by these annotators. This approach maximizes the number of examples while controlling the number of annotators in our sample.\nOur final sample contains 111,780 annotations from 5,002 annotators on 22,360 comments with 20 to 120 annotations per annotator (mean 22.35). Most comments have five annotations. 20 comments have four because we removed any underage annotators before sampling. In total 78,357 annotations (70.10%) are toxic, and 33,423 annotations (29.90%) are non-toxic.\nWe focus on four sociodemographic attributes: gender, age, education, and sexual orientation. Group sizes vary by attribute. For gender, 2,450 annotators (48.98%) identify as female, 2,116 (42.30%) as male, 23 (0.46%) as non-binary (rest in residual categories, full statistics in A.1).\n\nExperiments\nWe compare three models. The baseline model is the multi-annotator model by Davani et al. (2022) . We use their multi-task variant: For each annotator, there is a separate classification layer trained on annotations from that annotator. All annotator layers share a pre-trained language model used to encode the input. We use RoBERTa (Liu et al., 2019) for this, motivated by computational constraints. The other models in our experiments build on this baseline model.\nFor the sociodemographic models, we add group-specific layers based on sociodemographic attributes of the annotators. A single attribute, e.g., age, implies several groups, e.g., ages 25-34, ages 35-44. We add the group-specific layers between the pre-trained model and the annotator layers. Each group of annotators shares a separate group-specific layer. We implement group-specific layers as fully-connected, linear layers, each learning a feature transformation applied for one group of annotators.\nFinally, for the random models, we shuffle the assignment of annotators to groups from the sociodemographic model, retaining the relative group sizes. In other words, the probability of each annotator staying in the same group or being reassigned to another group corresponds to the relative size of each group. This approach keeps the model architecture constant while removing the connection between actual sociodemographic attributes and group assignment. It allows us to distinguish the effects of additional parameters, which groupspecific layers add in comparison to the baseline, from the effects of sociodemographic information.\n\nEvaluation Setup\nWe evaluate all models on individual annotations from gender, age, education, and sexual orientation groups. This setup is comparable to the \"individual label\" evaluations in Davani et al. ( 2022) and Gordon et al. (2022) , but with scores calculated per group of annotators. We measure performance in macro-average F 1 , to weigh each class equally.\n\nCross-Validation\nAs there is no standard split available for our dataset, we perform three iterations of a four-fold cross-validation with different seeds (training details in Appendix A.3). We choose four folds, so that even very small groups have more than a hundred annotations in each test set. Across folds, the numbers of annotations per sociodemographic group are similar (see Appendix A.4). We construct test sets that only contain comments unseen by the annotators in the training set. We also ensure that all test sets have similar proportions of toxic or non-toxic comments (assigned by the majority of annotators) to address the class imbalance in the dataset (70.62% toxic, see \u00a73).\n\nStatistical Significance\nWe test for statistical significance of our results from multiple runs of k-fold cross-validation via replicability analysis (Dror et al., 2017) . We report the number of significant folds and the Bonferroni-corrected count (Dror et al., 2018) in Appendix A.2. We compute the pvalues for each fold via a paired bootstrap-sampling test with BooStSa (Fornaciari et al., 2022) . We set the significance level \u03b1 = 0.05, draw 1000 bootstrap samples per fold, and use a sample size of 50% of the respective test set.\nRemarks on Groups Annotators from different groups of the same attribute will in most cases not have annotated the same examples. Therefore, comparisons between models are only meaningful within each group.\nThe groups modeled via group-specific layers and those in the result tables are always the same. For example, if we report scores for gender groups, then the sociodemographic and randomized models are also based on gender groups. In the following, we focus on a subset of groups, omitting, e.g., \"Prefer not to say\" (see Appendix A.5).\n\nResults\nTable 1 shows the results for gender, age, education, and sexual orientation. A naive majority class baseline that predicts all input to be toxic performs worse than all other models with a large margin (exact results in Appendix A.5).\nSociodemographics vs. Baseline Across attributes, the average scores of the sociodemographic model and the baseline are similar. The sociodemographic model often has a slightly higher average macro F1 than the baseline, but no statistically significant gains. Where average performance is better by several points, as for homosexual annotators, this gain is offset by a large variance in performance (a consequence of small group sizes).\nSociodemographics vs. Random We also do not find significant performance differences between sociodemographic group-layer models and the corresponding random group assignment models. For most groups, the randomized models achieve the highest average scores, but differences to the sociodemographic model are never statistically significant. \n\nDiscussion\nWe do not find strong evidence that explicitly modelling sociodemographics helps to predict annotation behaviour with multi-annotator models. These results might seem counter-intuitive, given the evidence of systematic annotation differences between sociodemographic groups (see \u00a72). This discrepancy, however, echoes the issue highlighted by ecological fallacies (Robinson, 1950) : Not every annotator will be a perfect representative of their group, so we will not necessarily learn additional information based on their group identity. This seems especially true if we already have access to individual behaviour (i.e., individual annotations).\nIn contrast to Davani et al. ( 2022), we made sociodemographic information explicit in our experiments, as one of the factors influencing annotation behaviour. Group-specific layers can be seen as an inductive bias putting emphasis on the sociodemographic relations between annotators. However, there are potentially many other factors influencing annotation behaviour (e.g., attitudes, moral values, cognitive biases, psychological traits). In light of our results, it seems plausible that multi-annotator models learn about these factors implicitly as part of predicting individual behaviour, so that making one factor explicit does not change prediction quality, at least in the case of sociodemographics.\nStill, we also know that generally group attributes can help predict individual decisions, i.e., as base rates or priors. To avoid ecological fallacies in modelling annotation, we therefore need to better understand when and how modelling sociodemographic information is useful in predicting an individual annotator's decisions. For example, we have only evaluated group-specific layers for single attributes. In contrast, social scientists have long adopted the idea of intersectionality (Crenshaw, 1989) , which also informs research on fairness in machine learning (Wang et al., 2022) . Intersectionality means that the effect of interactions between sociodemographic attributes enables specific experiences that are not captured by the attributes in isolation. For example, identifying as a man means something different depending on the person's education. Groups derived from single attributes might simply be too coarse to improve classifiers learnt from individual labels, as in multi-annotator models.\nThe dataset we use (Kumar et al., 2021) has many characteristics which are ideal for our study (see \u00a73). However, it uses a broad notion of toxicity, in contrast to other studies of toxic language (Larimore et al., 2021; Sap et al., 2022) , which match content and analysed groups. When modeling the groups frequently referenced in the datasets themselves, we would expect greater benefits from group-specific layers. Similar to us, Biester et al. (2022) who do not find significant differences between annotators of different genders, do so in a more general setting.\nWe can only partially compare to Gordon et al. (2022) , despite using the same dataset. In addition to differences in approach (see \u00a72), our and their work also differ in their research questions and thus experimental conditions. Gordon et al. (2022) compare their full model (group and individual) against using group information alone.\nWe compare our full model (group and individual) against using individual information alone. So it is unclear if their model would benefit from group information in comparison to individual-level information alone. While they find an improvement from group information it is only in comparison to a baseline predicting not individual but aggregated labels. Additionally, the composition of test sets sampled from the full dataset differs between the studies: Gordon et al. (2022) use a test set of 5,000 comments, while we use 22,360 comments in a four-fold cross-validation. We leave an explicit comparison to future work.\nGroup-specific layers ( \u00a74) are a natural extension of annotator-specific classification layers in multi-annotator models. However, other architectures to predict annotator-level labels use different ways to represent sociodemographic information, e.g., via embeddings in a recommender system (Gordon et al., 2022) . Future work could explore additional representations of annotator attributes (e.g., as part of the input, either textual or as separate features) and other approaches to modelling the relation of individual labeling decisions and attributes (e.g., probabilistic graphical models).\n\nConclusion\nWe ask how relevant modelling explicit sociodemographic information is in learning from individual annotators. Our experiments with group-specific layers for four sociodemographic attributes on social media data with toxicity annotations (Kumar et al., 2021) show no significant benefit of modelling sociodemographic groups in multi-annotator models. However, as the issue of ecological fallacies highlights, it is not implausible that these models do not learn additional information from group information beyond the inherent variation. However, our results do not refute the usefulness of sociodemographic attributes in modelling annotation, but underscore the importance of their judicious use. Different tasks and model architectures will likely benefit to different extents. Ultimately, annotation behaviour is driven by complex factors and we will need to consider more than annotators' sociodemographics.\n", "hypothesis": " To account for sociodemographics in models of individual annotator behaviour, we introduce group-specific layers to multi-annotator models.", "answer": true}
{"title": "Word-level Prefix/Suffix Sense Detection: A Case Study on Negation Sense with Few-shot Learning", "content": "\nIntroduction\nMorphological analysis mainly refers to processing a word into a lemma (root) and a well-defined morphological tag (Anglin et al., 1993; Haspelmath and Sims, 2013; Morita et al., 2015; Nicolai and Kondrak, 2017; Deacon et al., 2017; Ganesh et al., 2019) . For instance, through morphological analysis, the word \"unhappy\" will be divided into a lemma \"happy\" and a negation sense prefix tag \"un-\". Morphological analysis has played an important role in natural language processing (NLP) and it has been applied to many downstream tasks such as spelling checking (Aduriz et al., 1993; Oflazer, 1995; S\u00e9n\u00e9chal and Kearnan, 2007; Levesque et al., 2021) and machine translation (Lee, 2004; Habash, 2007; Toutanova et al., 2008; Belinkov et al., 2017) .\nOne major challenge in morphological analysis is that prefixes/suffixes are sometimes ambiguous. For instance, in English, the prefix \"un-\" often means a meaning \"not\", i.e., a negation sense. However, not all words with the prefix \"un-\" have a negation sense, such as \"unanimous\" and \"unpick\". Besides, the substring \"un-\" sometimes does not appear as a prefix in some words, such as \"universe\" and \"unique\". In this study, we directly address the above challenge by proposing a novel morphological analysis task, namely word-level prefix/suffix negation sense detection, which aims to detect whether a substring in a word is a prefix/suffix and meanwhile takes a specific pre-defined morphological sense. As a preliminary study, we focus on negative prefixes/suffixes. In many languages, one way to make a negative expression is to add a negative prefix/suffix to a word. For instance, in English, il-, im-, un-, and -less are some popular negative prefixes/suffixes.\nOne straightforward approach to prefix/suffix negation sense detection is to build a dictionary that covers all words with the prefixes/suffixes expressing such a sense. However, this is unrealistic because there are always many newly-emerging words due to non-standard language usage or incorrect spelling in some informal texts like Twitter. Therefore, we address the task of word-level prefix/suffix negation sense detection in a computational way.\nSpecifically, to further reduce the annotation cost, we propose a few-shot learning approach by employing the token-replaced detection model as our basic prompt-learning model due to its excellent performance in few-shot learning (Li et al., 2022) . Furthermore, we propose a novel prompt, namely input-augmentation prompt, which relies only on the input word. As illustrated in Fig. 1(c ), for the input word is \"unhappy\", the prompt, \" unhappy It is not happy\", is used to predict whether the word \"not\" is original or replaced so as to determine whether the input word is a negation word or not, where the substring \"happy\" is generated by removing the potential prefix (i.e., un-) from the input word. The de- \n\nInput-augmentation template\nTemplate Template and label description words predict predict P(original|\"negative\")>P(original|\"positive\") (label: negative) \uf050 P(original|\"negative\")<P(original|\"positive\") (label: positive) sign of our input-augmentation prompt can avoid one major shortcoming of existing few-shot learning approaches, i.e., the selection of labels (e.g., two labels, \"positive\" and \"negative\" in Fig. 1a ) or the selection of label description words (e.g., \"negative positive\" in Fig. 1b ) has a big impact on learner performance (Jiang et al., 2020; Gao et al., 2020; Li et al., 2022) . Moreover, our empirical studies also demonstrate that our approach achieves much better performances than the existing few-shot learning approaches.\n\nRelated work\nMorphological analysis aims to learn about the morphological structure of a given word form, and in general, there are four specific tasks: morphological tagging (i.e., assigning some pre-defined morphological tags to a word in a sentence) (M\u00fcller et al., 2013; Labeau et al., 2015; Cotterell and Heigold, 2017; Conforti et al., 2018; Malaviya et al., 2019) , lemmatization (i.e., converting a word in a sentence into the normalized form) (Plisson et al., 2004; Chrupa\u0142a, 2006; Jongejan and Dalianis, 2009; Strakov\u00e1 et al., 2014; Bergmanis and Goldwater, 2018) , morphological segmentation (i.e., judging whether the substring in a word could be segmented as a prefix/suffix) (Ruokolainen et al., 2013 (Ruokolainen et al., , 2016;; Goldsmith et al., 2017; Cotterell et al., 2019) , and morphological disambiguation (i.e., assigning a correct morphological segmentation to a word by leveraging the context) (Hakkani-T\u00fcr et al., 2002; Yildiz et al., 2016; Cotterell et al., 2018; Wiedemann et al., 2019) .\nCompared to the above tasks, our work has at least three different aspects. First, our task is a combination of morphological tagging and morphological segmentation. Second, our task is word-level, i.e., the input contains only a single word without context, which leads to the inapplicability of previous approaches based on contextual information. Third, we propose a novel few-shot learning approach to our task. To the best of our knowledge, this is the first attempt of studying fewshot learning in morphological analysis.\n\nCorpus Generation\nWe use six prefixes, i.e., un-, im-, in-, il-, irand dis-as negation prefixes and two suffixes, i.e., -less and -f ree as negation suffixes to collect words from two resources, i.e., the ninth edition of Oxf ord Advanced Learner \u2032 s Dictionary (AS et al., 2005) and 1.6 million English Tweeter data collected by Go et al. (2015) . In summary, we obtain 2,717 and 6,671 words with negation prefixes/suffixes from the Oxford dictionary and tweeter data, respectively. Then, we randomly select 3,000 words and annotates such words as our corpus. Specifically, we assign two annotators to annotate each word into two categories, i.e., positive and negative. The Kappa consistency check value of the human annotation is 0.87. Moreover, for words with different sense annotations, we assign another annotator to make a final decision. \n\nMethodology\nProblem statement: The prefix/suffix negation sense detection task can be formulated as follows.\nLet D l = {w, y} be labeled data, where w is the input word and y is a label in {positive, negative}.\nOur approach aims to provide a few-shot learner for such a detection task.\nApproach overview: As shown in Figure 1 (c), a prompt-based learner, which is based on a pretrained token-replaced detection model and an input-augmentation prompt, is built for the prefix/suffix negation sense detection task. The goal of a pre-trained token-replaced detection model (e.g., ELECTRA) is to predict whether a token in the input string is replaced or not.\n\nApproach specification:\nFirst, an inputaugmentation prompt x prompt is constructed for an input word w, as follows.\nEQUATION\nwhere \"It is not\" is a template, and w is a substring of the input word w without the prefix/suffix, such as w = \"happy\" for w = \"unhappy\". Second, prompt x = [w 1 , w 2 , ..., w n ] is fed into the encoder in the discriminator of the pre-trained token-replaced detection model to obtain an output sequence y = [y 1 , y 2 , ..., y n ], where w i is the ith word in the prompt, and y i is the prediction label (either original or replaced) for word w i , indicating whether the word is original or replaced.\nFinally, we map the label set of the pre-trained token-replaced detection model to the label set of our task, with the following formulas. P (\"negative\"|x prompt ) = P (y \"not\" = original)\n(2) and P (\"positive\"|x prompt ) = P (y \"not\" = replaced),\n(3) where y \"not\" denotes the label corresponding to the word \"not\" in the input-augmentation prompt as shown in formula (1).\nFor instance, suppose that the input word is \"unhappy\", we first obtain the input-augmentation prompt \"unhappy It is not happy\" and then use the pretrained token-replaced model to predict whether the word \"not\" in the prompt is original or replaced. If the prediction result is original, we conclude that the input word \"unhappy\" is a negative word.\nIn the training phase of our few-shot learning setting, only a few prompt samples, together with their labels are used to update the parameters in the discriminator of the pre-trained token-replaced detection model. It is important to note that our approach reuses the pre-trained parameters in the pretrained token-replaced detection model and does not use any other new parameters.\n\nExperiments\nData setting: 2,000 samples are randomly selected from the human-annotated corpus. First, 400 samples are selected as test data, including 200 for each class. Then, we follow the evaluation protocol of Li et al. (2022) We implement the following approaches for comparison:\n(1) Finetuning-RoBERTa (Liu et al., 2019) : Based on the fine-tuning approach and RoBERTalarge model, the prediction label is obtained by mapping the \"[CLS]\" token to label space.\n(2) Finetuning-ELECTRA (Clark et al., 2020) : It is similar to finetuning-RoBERTa except that the ELECTRA-large model is used.\n(3) Prompt-RoBERTa (Gao et al., 2020) : It is a discrete prompt learning approach based on RoBERTa-large, as shown in Figure 1(a) , where the prompt is \" w it is [mask] \", and the prediction label is obtained by the filling of \"[mask]\" (either \"negative\" or \"positive\"). (4) Prompt-ELECTRA (Li et al., 2022) : It is a discrete prompt learning approach based on ELECTRA-large, as shown in Figure 1(b) , where the prompt is \" w is a negative positive word \". ( 5) Warp (Hambardzumyan et al., 2021) : It is a continuous prompt learning approach, in which the best prompt template is obtained by searching in the (continuous) embedding space. Moreover, the template is learned using adversarial refactoring. ( 6) DART (Zhang et al., 2021) : It is a continuous prompt learning approach, in which the search for the best prompt template is based on backpropagation. (7) P-tuning-v2 (Liu et al., 2021) : It is a continuous prompt learning approach, in which the search for the best prompt is based on a prefixed-tuned multi-layer prompt. (8) Fully-supervised Learning: 1,400 training and 200 development samples are used to re-train the ELECTRA-large model.\nTable 2 shows the performances of different approaches, from which we can see that : (1) Our approach significantly outperforms the fullysupervised learning and fine-tuning approaches, which proves the effectiveness of our few-shot learner. (2) Our approach performs much better than other prompt-based learners, e.g., obtaining 8.6% increase on Macro-F1 when compared with Prompt-ELECTRA. The improvement confirms the effectiveness of our input-augmentation prompt. (3) Our approach, using only 16 training and 16 development samples, almost performs equivalent to the fully-supervised learning approach with 1,400 training and 200 development samples.\nAn error analysis is made for our approach, which shows two main error causes: (1) the input word w or its substring w has multiple meanings, such as \"hapless\" vs. \"hap\", and \"disembarkation\" vs. \"embarkation\". (2) the meaning of w and w is irrelevant, such as \"dispossession\" vs. \"possession\", and \"ingot\" vs. \"got\". This indicates that more efforts are needed for our prefix/suffix negation sense detection.\n\nConclusion\nIn this study, we propose a novel word-level morphological analysis task, namely prefix/suffix sense detection, and make a case study on negation sense. We provide an annotated corpus for the prefix/suffix negation sense detection, and then propose a novel few-shot learning approach, which uses an input-augmentation prompt and a pretrained token-replaced detection model to effectively make the negation sense detection. Empirical studies show that our approach performs much better than other approaches in the few-shot scenario, such as using only 16 training samples.\n", "hypothesis": "Empirical studies demonstrate the limitations of the proposed approach to word-level prefix/suffix negation sense detection.", "answer": false}
{"title": "Efficient Diagnosis Assignment Using Unstructured Clinical Notes", "content": "\nIntroduction\nThe widespread adoption of electronic health records (EHRs) by health systems has created vast clinical datastores. One of the essential steps in utilizing these data is identifying patients with specific clinical outcomes and the timing of these outcomes, through a process called electronic phenotyping (Banda et al., 2018) . Electronic phenotyping is critical for using EHR data to support clinical care (Kaelber et al., 2012; LePendu et al., 2012) , inform public health decision-making (Dubberke et al., 2012) , and train predictive models (Chaves et al., 2021; Blankemeier et al., 2022; Steinberg et al., 2021 Steinberg et al., , 2023;; Lee et al., 2022) .\nElectronic phenotyping is a complex task that involves combining structured data (e.g. lab results and codes) with unstructured data (e.g. clinical notes). Rule-based heuristics can be applied to structured data. However, the unstructured nature of information rich (Kern et al., 2006; Wei et al., 2012; Martin-Sanchez and Verspoor, 2014) clinical notes makes phenotyping based on these notes particularly challenging.\nSeveral solutions exist for electronic phenotyping using unstructured clinical notes (Peng et al., 2018; Fries et al., 2021; Zhang et al., 2021a,b) , but lack convenience for generalizing to new conditions. For example, labeling functions that consist of rules authored by domain experts are interpretable and readily shared without compromising data privacy, but can be laborious to create. Neural networks (NNs) that are trained to identify specific diseases can eliminate the need for handcrafted labeling functions and often provide more accurate results. However, NNs require extensive manual labeling time and often generalize poorly to diseases not seen during training.\nTo address this, we introduce HyDE (hybrid diagnosis extractor). HyDE is a simple approach to electronic phenotyping that combines the strengths of labeling functions and neural networks and allows for generalization to new diseases with minimal overhead.\nOur key contributions are as follows:\n1. We demonstrate that our model effectively discriminates between true cases of hypertension and false positives generated by labeling functions, as demonstrated by a supervised area under the precision recall curve (AUPRC) of 0.85. This same model achieves AUPRCs of 0.90, 0.82, 0.84, and 0.95 in zero-shot evaluations for diabetes, osteoporosis, chronic kidney disease, and ischemic heart disease, respectively. HyDE outperforms a labeling function baseline by 44 points in F1 score and a Word2Vec baseline (Mikolov et al., 2013b,a) by 24 points in F1 score on average across seen and unseen diseases.\n2. HyDE requires minimal setup. The labeling functions used in HyDE can be simple, reducing the manual effort often required to design labeling functions with high precision and recall.\n3. HyDE is computationally efficient, as only small portions of a subset of clinical notes need to be passed through the neural network for processing, thus minimizing the computational resources required to run HyDE on large datasets. We show that pruning the length of the inputs by 4\u00d7 to just 2.3% of the full clinical notes impacts performance by an average of only 0.017 AUPRC while providing a speedup of > 4\u00d7.\n\nMethods\nOur proposed method, HyDE (hybrid diagnosis extractor), aims to accurately identify the earliest occurrence of specific diseases in clinical patient encounter notes. We accomplish this by using a combination of labeling functions and a fine-tuned biomedical language model. The labeling functions are designed to be simple and identify as many mentions of the disease as possible, including false positives. The neural network is then used to differentiate between the true positives and false positives by analyzing small segments of the clinical notes around the location identified by the labeling functions. This approach allows for identifying potential mentions of the disease, while also utilizing the neural network to improve precision. It is worth noting that the components of HyDE are modular, allowing for the substitution of other methods for identifying disease-specific mentions beyond the labeling functions used in this paper. For example, Trove (Fries et al., 2021) , offers ontology-based labeling functions that eliminate the need for coding task-specific labeling rules.\nOur method (Fig. 1 ), involves the following steps: The user first develops a simple labeling function for the disease of interest. In the case of diabetes, this could be the regular expression diabetes | diabetic. This labeling function is then applied to the clinical notes to identify mentions of the disease. Additionally, the user identifies peripheral terms that frequently appear before or after mentions of the disease, such as insulin-dependent or mellitus in the case of diabetes. The text matching the labeling function and peripheral terms are then replaced with [MASK], and a context around the resulting mask is extracted, resulting in a masked contextual mention (MCM). These MCMs are used to fine-tune a biomedical language model to determine whether the context suggests that the patient actually has the condition in question. We hypothesize that this approach allows the language model to generalize to various conditions without additional training. Thus, for a zero-shot transfer to other diseases, only a simple disease-specific labeling function and peripheral terms are required. We adopt the term zero-shot in this context as each disease comes with distinct comorbidities, symptoms, and interventions.\n\nDataset\nAfter obtaining approval from the institutional review board, we obtained \u223c8.8 million clinical notes from 23,467 adult patients who had an encounter at our tertiary care center between 2012 and 2018.\n\nDisease Phenotypes\nWe apply our electronic phenotyping method to five chronic diseases: hypertension (HTN), diabetes mellitus (DM), osteoporosis (OST), chronic kidney disease (CKD), and ischemic heart disease (IHD). These diseases were selected due to their high prevalence (HTN, 2021; DM, 2022; CKD, 2021; IHD, 2022; Clynes et al., 2020) , the costs they incur to the healthcare system, and the potential for positive intervention (Blankemeier et al., 2022) . For initial model training, we used hypertension as it is the most prevalent of these diseases (affecting 116 million in the US) (HTN, 2021) and we hypothesize that it generates the most diverse MCMs. Table 6 shows the labeling functions that we used to extract these mentions for each disease.\n\nData Labeling\nMask Contextual Mention Categories: We manually identified 6 categories of MCMs -(0) true positive; (1) false positive (otherwise unspecified);\n(2) referring to someone other than the patient; (3) referring to the patient but negated; (4) providing information / instructions / conditional statements (i.e. instructions for how to take a medication); ( 5) uncertain (i.e. differential diagnosis). Thus, category 0 is the true positive category and categories 1 -5 are false positive categories. We formulate this problem as a binary classification where categories 1 -5 are merged into class 1. Amplifying False Positive Examples: The prevalence of false positives from our labeling functions were relatively low (Table 3 ). We thus sought to increase the number of category 2 false positive examples in our training dataset beyond the baseline prevalence of the 250 random MCM samples that were initially labeled (RS in Table 1 ). We applied a family labeling function to randomly sampled MCMs. This labeling function is positive if an MCM contains any term listed in A.1 relating to familial mentions. We generated 200 such category 2 amplified examples for subsequent labeling. Based on the annotations, we found that only 1.5% of the examples selected by this labeling function were actually true positives examples.\nTo increase the number of category 3 false positive examples, we applied the Negex algorithm (Chapman et al., 2001) Filtering Masked Contextual Mentions: Applying the disease-specific labeling functions generated 827k, 555k, 87k, 199k, and 80k notes for HTN, DM, OST, CKD, and IHD respectively from roughly 8.1 million clinical notes (Table 4 ). Since clinical notes often contain duplicate information from multiple patient visits, we deduplicate the MCMs by comparing the 20 characters on either side of the masked mentions associated with a particular patient. If these characters are the same across multiple MCMs, we keep the MCM that was authored first and discard the others. Deduplication allows us to reduce the number of masked contextual mentions by 3.3\u00d7, 3.6\u00d7, 4.2\u00d7, 3.7\u00d7, and 3.3\u00d7 for HTN, DM, OST, CKD, and IHD respectively (Table 4 ). This method can be applied at inference to increase the computational efficiency of HyDE. Additionally, the length and number of MCMs per clinical note represents an average of 9% of the full notes for a context length of 64 words, which can improve the efficiency of inference on large datasets.\nActive Learning: To further improve the performance of HyDE, we implement a human-inthe-loop uncertainty-based active learning strategy. This involves multiple iterations of training where after each iteration, 100 examples with corresponding probabilities closest to 0.5 are manually labeled and added to the training dataset for the next training iteration. Table 1 shows performance across the active learning iterations (A1-A4).\n\nModel Training\nWe select PubMedBERT (Gu et al., 2021) (100 million parameters) as the model that we fine-tune due to its simple architecture and widespread validation. We use a train batch size of 8, an Adam optimizer with \u03b2 1 = 0.9 and \u03b2 2 = 0.999, and a learning rate of 3e-5. We train for 25 epochs and choose the model checkpoint with the best validation set performance. 1,150 HTN examples are used for training and 250 HTN examples are used for validation. For disease specific fine-tuning experiments, between 90 and 100 disease-specific examples are used for both validation and training. There was no overlap between the patients used for the hypertension training and validation sets and the patients used for test sets as well as disease-specific validation sets. Our test sets consisted of 442 -500 labeled cases for each disease.\n\nEvaluation\nWhile labeling functions can be evaluated at a note level, we evaluate at a MCM-level since a single clinical note can consist of multiple MCMs. Furthermore, disease assignment based on clinical notes can be combined with assignment based on structured EHR, increasing the number of patients that are identified. Thus, we want to ensure high precision in identifying patients using clinical notes. For each MCM, we measure the fine-tuned language model's ability to correctly classify it as either true positive or false positive using area under the precision recall curve (AUPRC) and F1.\nFor our labeling function baseline (LF in Table 2), we use both the family labeling function described previously and Negex (Chapman et al., 2001) . Although additional terms could be added to this labeling function, those same terms could also be added to HyDE, making this a fair comparison.\nWe also include a Word2Vec baseline in our comparison (Mikolov et al., 2013b,a) . This technique leverages a pre-trained model which has been trained on a corpus of around 100 billion words from Google News. For each MCM, we aggregate word embeddings by calculating their mean and then train an XGBoost model (Chen and Guestrin, 2016) over the computed averages of the HTN training dataset MCM embeddings. To optimize the performance of our XGBoost model, we fine-tune its hyperparameters by conducting a grid search using our HTN validation dataset. It's worth mentioning that this strategy does not retain the sequential order of words.\nTo demonstrate the generalizability of our method on external data, we apply it to the assertion classification task from the 2010 i2b2/VA Workshop on Natural Language Processing (Uzuner et al., 2011) . This dataset consists of 871 progress reports annotated with medical problems that are further classified as present, absent, possible, conditional, hypothetical, or not associated with the patient. We mapped the present category to class 0 and collated all other categories under class 1. We used regular expressions to extract mentions of HTN, DM, OST, CKD, and IHD. We filtering out diseases with less than 30 mentions. Consequently, our external validation was conducted on HTN, DM, and CKD.\n\nResults\nSupervised and Zero-Shot Model Performance: Table 1 depicts AUPRC performance of our Word2Vec (W2V) baseline compared to fine-tuned PubMedBERT models trained with various training dataset compositions (all rows except the first). We demonstrate supervised performance on HTN, as well as zero-shot generalization to DM, OST, CKD, and IHD. The performance of HyDE surpasses that of our labeling function baseline by 44 points in F1 score and our Word2Vec baseline by 24 points in F1 score on average (Table 2 ). We find that fine-tuning the best PubMedBERT model (RS+C+A4 training dataset) on \u223c100 additional disease-specific examples does not significantly improve performance, with scores of 0.91, 0.84, 0.81, and 0.95 on DM, OST, CKD, and IHD, respectively. This supports the conclusion that our model generalizes well to other diseases, without requiring disease-specific fine-tuning. On the external i2b2/VA dataset we achieve the following AUPRC scores without any additional finetuning -0.79 for HTN (336 patients), 0.99 for DM (213 patients), and 0.95 for CKD (45 \n\npatients).\nContext Length Ablation: Fig. 2 shows that RS+C+A4 (RS: 250 random MCM samples; C: 400 category 2 and 3 amplified MCMs; A4: 400 samples from active learning) trained models saturate with increasing context lengths. Table 5 shows that reducing the context length from 64 words to 16 words speeds up the model by 4.5x while only lowering average AUPRC by 0.017. From Table 4 we observe that this represents an average of 2.3% of the full clinical notes among notes that contain at least one MCM.\n\nConclusion\nWith its minimal setup, computational efficiency, and generalization capability, HyDE offers a promising tool for electronic phenotyping from unstructured clinical notes. By improving the ability to extract patient health status, we hope that HyDE will enable more informative large scale studies using EHR data, ultimately leading to public health insights and improved patient care.\n", "hypothesis": " However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications.  To address these challenges, we propose HyDE (hybrid diagnosis extractor).  HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a diseaseagnostic neural network to assign diagnoses to patients.", "answer": true}
{"title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning", "content": "\nIntroduction\nRecently, instruction tuning(IT) has drawn much attention in the NLP communities, with the rapid growth of new models (Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022) and datasets (Wang et al., 2022; Gupta et al., 2022; Finlayson et al., 2022; Mishra et al., 2021; Ye et al., 2021; Bach et al., 2022) . Models trained with task instructions demonstrate impressive zero-shot cross-task generalization ability. Despite the remarkable results, 1 : Comparison between two types of instruction tuning models. Noted that we reported an estimated number of instructions for T0 during training and testing since they have 5 to 10 instructions for each task. Our analysis focuses on the \"generalize to unseen task\" type.\nhow models utilize the instructions during training and inference time remains an open question.\nPrior works have raised the question of whether models really learn to follow the instructions or just capture spurious correlations. Jang et al. (2022) , Webson and Pavlick (2021) showed that the current large language models (LLMs) can achieve similar performance with misleading instructions(prompts) in in-context learning(ICL) and few-shot learning scenarios. Min et al. (2022) analyze how model utilize examples in ICL. They observed that (1) Input-output mapping in examples is not important and(2) Output space information is crucial.\nBesides ICL and few-shot prompt-tuning, some works raise concerns about instruction following in the instruction tuning field (Finlayson et al., 2022; Gupta et al., 2022; Gu et al., 2022) , with a focus on test-time analysis. In contrast, we focus on analyzing how the models utilize instructions during the training process. We compare our analyzing methods and observation with prior works in Appendix A.1.\nIn this work, we conduct controlled experiments on NatInst-V2 (Wang et al., 2022) , the largest opensource instruction learning dataset includes 800+ English tasks with diverse task types, to study how models utilize instructions during IT. Note that existing research on IT can be categorized into two major camps: generalize to unseen tasks and generalize to unseen instructions, based on their objectives. Table 1 shows the comparison. Our analysis focuses on the former with more background and justifications provided in section 2. We strategically alter the instructions and compare them with original instructions for IT. Specifically, for task definition, we create simplified versions by removing all semantic components in the instructions and only leaving the output space information. For task examples, we create delusive examples with incorrect input-output mapping, where the examples' input and output spaces are correct, but the inputoutput mappings are wrong. Figure 1 demonstrates specific examples of these altered instructions.\nOur experiments show that models trained with simplified task definitions achieve performances on par with the original IT models with different numbers of training examples ranging from 10 to 800 per task. We also observe that instructiontuned models are sensitive to input-output mapping during the testing ICL stage, but not during the instruction-tuning (training) stage, especially in low resource settings (i.e., \u2264 50 training instance per task). To further understand why instruction tuning improves performance for zero-shot test tasks, we establish a random baseline that only knows the correct output format (label space) for classification and multi-choice tasks. We discover that the random baseline can get 30% absolute exact-match score improvement over an untuned model, almost comparable to some IT models in low resource settings.\nOur results suggest that the impressive performance gains of IT may just come from models learning superficial patterns, such as the output space and format. We suggest future research on IT more carefully analyze their performance gains and benchmark against trivial baselines.\n\nBackground\nRecently, many instruction tuning work train and test the models with instructions to achieve better zero-shot generalizability toward unseen tasks/instructions. We categorize these works by their objectives: generalize to unseen tasks and generalize to unseen instructions, and show the comparison in Table 1 . Instruction tuning to generalize to unseen tasks. Figure 1 illustrates a two-stage instruction tuning pipeline used in many IT models, such as T0 (Sanh et al., 2021) , FLAN (Wei et al., 2021) , and TK-Instruct (Wang et al., 2022) . In the first stage, the models are trained on a set of training tasks with instructions (task-definition and task-examples). After training, the models are evaluated on a set of unseen testing tasks for zero-shot generalizability. By incorporating instructions during training, the models are shown to significantly improve performance over untuned models. The impressive performance gains led people to believe that models learned to follow instructions via instruction tuning. The goal of our analysis is to verify this belief. Instruction tuning to generalize to unseen instructions. Different from T0, FLAN, and TK-Instruct training and testing the model with clear task boundaries and focusing on cross-task generalizability, Instruct-GPT (Ouyang et al., 2022) , Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) focus more on instruction generalizability, which they train their model without clear task boundary but with diverse instructions, and further test on user-oriented instructions. These models show very different behavior compared with instruction tuning models that aim to generalize to unseen tasks.\nSince Instruct-GPT is not open-sourced and distilled IT models such as Alpaca and Vicuna come up after our submission, we focus our analysis on the first category using the TK-instruct model and NatInst-V2 dataset. However, we also conduct additional experiments and discuss the Alpaca model's instruction following ability in Table 2 .\n\nAnalysis Method\nTask definition manipulation.\nTo analyze whether models really \"understand\" and utilize the semantic meaning of task definitions, we conduct controlled experiments to remove semantic information in task definitions. Specifically, we conduct instruction-tuning with task definitions at 3 levels of granularity: Original, Simplified, and Empty. The Original version uses human-crafted human-readable task definitions provided in NatInst-V2 (Wang et al., 2022) . The Simplified task definitions remove all semantic components in the original task definition and only leave the output space information. Specifically, we only provide possible output labels as task definitions for classification tasks, and completely remove task definitions for other tasks (mostly generative tasks) during IT. Figure 1 shows an example of Simplified task definition. More details can be found in Appendix A.2. For Empty, we don't provide task definition during instruction-tuning.\nTask example manipulation. Finlayson et al. (2022) show that by providing a few task examples, both humans and models can guess and perform a task. We thus design a controlled experiment to study whether models learn the input-output mapping from task examples. Specifically, we compare models trained with 3 types of task examples: Original, Delusive, and Empty. For the Original setup, we provide one positive example in NatInst-V2 (Wang et al., 2022) \n\nExperimental Setup\nDataset. We conduct experiments on the NatInst-V2 (Wang et al., 2022) , the largest open-source instruction learning dataset, including over 800+ English tasks with diverse task types. The instructions include human-crafted human-readable Task Definition, Positive Task Examples, Negative Task Examples, and Explanation. We focus on studying task definition and task examples, which were shown to be most useful in the original paper.\nModel. we conduct experiments on TK-Instruct, the current SOTA model provided in NatInst-V2 paper. The model significantly outperformed previous SOTA models, such as T0 (62.0 v.s. 32.3 rouge-L for 11B model). We follow the seq-to-seq instruction-tuning method used in TK-Instruct, and train a T5-large-lm-adapt (770M parameters) model (Raffel et al., 2020) with performance comparable to the larger model (3B parameters) reported in Wang et al. (2022) . 1 Evaluation Metrics. For task definition, we separately evaluate Classification and Generative tasks using exact match and rouge-L respectively. For , 10, 20, 50, 200, 800) .\n\nResults\nTask Definition Experiments. Figure 2 shows experimental results for task definitions. In the top sub-figures, we can see that the models trained with Simplified instructions achieve almost the same results as models trained with Original definitions both on Classification and Generative tasks. Note that Simplified task definitions remove all semantic components in task definitions and only retain output space information for Classification tasks and remove task definitions altogether for Generative tasks. This indicates that models may only utilize output space information during instruction tuning. The bottom-left sub-figure in Figure 2 shows the overall rouge-L score for classification tasks, where models trained on the Original task definition slightly outperform the Simplified ones. A closer examination reveals that models trained on the Original task definitions are more likely to predict partially correct answers that help with the ROUGE-L score in some tasks. We provide further details in Appendix A.5. In addition, we also observe that training with Simplified prompts can yield comparable performance to the T0 model trained with Original prompts on T0 dataset. Please refer to Appendix A.6 for details. Task Examples Experiments. Combined with the previous results for task definition, we observe that comparing to the untuned models(T5 w/o IT), the IT models may achieve significant performance gain (Rouge-L from 22 to 46) with (1)Simplified task definition and (2)Delusive task example, indicating that the current impressive improvement of IT models can come from the models learning superficial patterns without utilizing (following) the instructions like human do.\nFor the right sub-figure, we show the results using Delusive task examples during test time via in-context learning. We see the performance drops for all three models, indicating that the inputoutput mapping matters for in-context learning on instruction-tuned models. This observation seems to misalign with previous work (Min et al., 2022) , which they found input-output mapping is unimportant for in context learning for classification tasks. However, a closer investigation found that most tasks suffer from significant performance drop are analogical tasks rather than classification tasks as studied in Min et al. (2022) . 2\n\nAdditional Analysis\nRandom baseline. While our experiments suggest that models do not utilize most information in the instructions, we still observe huge performance gains via instruction tuning. To understand where the gains come from, we introduce a Random baseline that simply guesses within the cor- 200) can improve exact-match score to 52%. However, while the performance gains seem impressive, the Random Guessing baseline can also achieve 42.6% exact-match score, on par with TK-Instruct trained in low resource setting (less than five instances per task). This suggests that the majority of score improvement from IT may come from model learning the output format, especially in low-resource settings.\nFair comparison for IT models. Existing studies on instruction tuning often introduce changes to both models and datasets simultaneously, which can obscure fair comparisons. To address this issue, we conduct experiments comparing different models (T0, TK-Instruct) on the same dataset (NatInst-V2) and emphasize the importance of careful evaluation. In Table 3 , when evaluating using the NatInst-V2 evaluation method and considering only the overall Rouge-L score, the TK-Instruct model appears to outperform T0 significantly. However, upon closer examination of the classification (CLS) and generative (GEN) tasks separately, we observe that T0's classification score is even lower than the Random baseline, primarily due to its format correctness being only 64%. To ensure a fairer comparison between these models, we employ constrained decoding techniques to align the model's predictions with the label space. By adopting this approach, we observe a substantial performance improvement for T0 in CLS tasks (34.03 to 51.31). T0 surpasses both the TK-Instruct model and the random baseline, indicating that it Table 3 : Careful evaluation of the NatInst-V2 dataset. The Format metric is the same as the format correctness in Figure 4 . The w/ CD indicates that the model's decoding is constrained to match the label choices for CLS tasks. The TK is the TK-Instruct(770M) model trained with 10 instances per task.\nis indeed superior to these models in CLS tasks.\n\nDiscussion\nDo Alpaca better follow the instruction on NatInst-V2 dataset? After our submission, new instruction tuning models, like Alpaca and Vicuna, are trained on distilled data from Chat-GPT and exhibit behavior closer to it. To investigate their instruction utilization, we conduct the \"Altered Task Definition\" experiment on LLaMA-7B (Touvron et al., 2023) and Alpaca-7B models using the NatInst-V2 test set. In Table 2 , training the LLaMA model on the NatInst-V2 dataset using the Original task definition leads to substantial performance enhancements than zeroshot. However, the Simplified task definition also achieves comparable performance, with a minimal decrease of 3 (EM/Rouge-L)scores. This finding is consistent with our previous observations on the TK-Instruct and T0 models. Even without tuning on NatInst-V2, the Alpaca model demonstrates strong performance on the NatInst-V2 test set. However, when the model is tested using a simplified task definition, there is a significant decrease in performance for generative tasks (but not for classification tasks). This highlights the importance of a well-written task definition for the Alpaca model to effectively perform generative tasks.\n\nConclusion\nWe constructed controlled experiments on NatInst-V2 to compare model training with altered vs. original instructions (task definitions and examples). Our findings indicate that some current IT models do not fully utilize instructions, and the impressive performance gains of IT may come from models learning superficial patterns, such as the output space and format. We suggest future research on instruction tuning to analyze their performance gains with more comprehensive evaluation and benchmark against trivial baselines. 1321\n", "hypothesis": "Our experiments show that models trained on simplified task definition or delusive examples can achieve slightly better performance than the ones trained on the original instructions and examples. Furthermore, we introduce a random baseline to perform zero-shot classification tasks, and find it achieves slightly worse performance (41.5% exact-match) than IT does (42% exact-match) in low resource setting, while both methods outperform naive T5 slightly (28% per exact-match). Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and making educated guesses. Our study highlights the need for further improvement in IT methods and evaluation.", "answer": false}
{"title": "Negation Scope Refinement via Boundary Shift Loss", "content": "\nIntroduction\nNegation is a complex linguistic phenomenon. Even though there does not exist a widely agreed task definition for negation detection, two sub-tasks are commonly performed: (i) negation cue detection, and (ii) negation scope resolution. Negation cue is a keyword (e.g., not, never) in a sentence that acts as an indicator of semantic negation, and its detection is relatively easy. Negation scope refers to the portion(s) in a sentence being semantically affected (i.e., negated) by the cue. There could be multiple cues in one sentence and each corresponds to its own scope. Table 1 lists three cues in the same sentence and their scopes.\nDifferent datasets may adopt different annotation guideline of scopes, e.g., whether or not a cue itself is a part of its scope. The example sentence in Table 1 well demonstrates the unique characteristics of this task compared to other span extraction tasks like Named Entity Recognition (NER). They are: (i) a negation scope is defined by (or associated to) a given cue, (ii) the negation spans are usually longer than a named entity, and (iii) a good number of negation spans are discontinuous, depending on the adopted annotation guideline.\nIn recent years, pretrained language models (PLMs) like BERT (Devlin et al., 2019) have been explored to improve negation detection (Khandelwal and Sawant, 2020; Khandelwal and Britto, 2020) . Specially designed pre-training material that focuses on negation has also been explored and achieves state-of-the-art performance (Truong et al., 2022) . Nevertheless, we believe that negation detection shall be considered as a pre-processing step for downstream subtasks and its model shall not be over-complicated.\nIn this paper, we enhance a simple baseline by Khandelwal and Sawant (2020) with an effective Boundary Shift Loss (BSL), to refine the predicted negation scope boundaries. BSL is derived based on the positions of span boundaries. For each token, boundary shift tells the direction of the nearest span boundary: left or right. With the simple BERT + Feed-forward architecture, our R-BSL model outperform baselines on all well-known datasets.\n\nRelated Work\nNegation detection was firstly studied in biomedical and health texts, represented by NegEx (Chapman et al., 2001 ) developed for EHRs. NegEx is built on top of regular expressions; its negation scopes are mainly named entities. The definition of negation scope becomes largely different and more generic in later datasets. The BioScope corpus (Vincze et al., 2008) annotates negation scope in biological full papers and scientific abstracts. The \"Sherlock\" corpus (Morante and Blanco, 2012) , annotates Conan Doyle's novels Sherlock Holmes series. SFU Review Negation corpus (Konstantinova et al., 2012) annotates negations and speculations in the SFU Review corpus (Taboada et al., 2006) for sentiment analysis.\nLike many other NLP tasks, BERT leads to significant improvement on scope resolution (Khan-Cue Negation scope marked in discontinuous \"span\" s in-Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" not [cue] in-[/cue] \"frequent occasions when he was up all night\", was seated at the breakfast table.\nnot Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" [cue] not [/cue] \"infrequent occasions when he was up all night\", was seated at the breakfast table.\nsave Mr. Sherlock Holmes, \"who was\" usually \"very late in the mornings\", [cue] save [/cue] \"upon those not infrequent occasions when he was up all night\", was seated at the breakfast table. \n\nProblem Definition\nAs a common practice, we assume that negation cue has been successfully detected. Our key focus is negation scope resolution for a given cue. For presentation simplicity, we assume there is only one cue in a given sentence. The cases of multiple cues can be easily achieved by sentence duplication, each time with a different known cue being wrapped with special indicator tokens. The model would be trained to predict negation scope of each cue separately. Table 1 gives a typical example of how sentence with three negation cues and three corresponding scopes is being pre-processed by duplication and the special indicator tokens\n[cue] [/cue].\nGiven an input sequence S = \u27e8t 1 , t 2 , ..., t n \u27e9, with a known cue, the task is to predict the cue's negation score in token spans. We adopt the OSC tagging scheme: Y = \u27e8y 1 , y 2 , ..., y n \u27e9 where y i is O if t i is non-scope, S if t i is part of the scope, and C if t i is the given cue. We use a dedicated label \"C\" for cue, to satisfy the annotation guidelines in different datasets, i.e., not all annotations consider cue as a part of the scope.\n\nThe R-BSL Model\nThe central idea of Boundary Shift Loss is inspired by techniques used for semantic segmentation. Background. Locating accurate segmentation boundary is particularly important for medical images such as MRI, as the boundary for body organ is crucial. In a 2D image, we can represent the deviation of the predicted boundary with ground truth boundary in the form of a distance map, as shown in Figure 1 . Each pixel in the example image is mapped with a normalized distance to its nearest ground truth boundary pixel, forming the boundary distance map.\nFor a typical pixel, the distance map could be reduced to a local distance map of 3 \u00d7 3, containing distance of the pixel itself and that of its eight neighbours. The cell with the smallest distance (e.g., the top left cell in the example) represents the direction to the nearest boundary. To indicate this direction, local distance map can be further reduced to an one-hot local direction map, where the \"1\" cell representing the direction of the nearest boundary. Accordingly, the predicted boundary can be further refined toward this direction for more accurate boundary prediction (Wang et al., 2022) . Span extraction tasks in NLP share the same aim to find accurate region boundaries, but in a 1D space, i.e., along token sequence to shift left or right.\n\nBoundary Shift Map\nTo enable boundary shift loss, we convert the scope labels to scope span boundary labels. BS = \u27e8bs 1 , bs 2 , ..., bs n \u27e9 and BE = \u27e8be 1 , be 2 , ..., be n \u27e9 are the two label sequences that represent the start and end of boundaries, respectively. bs i is Bs if t i is the start of a scope span, and O otherwise; be i is Be if t i is the end of a scope span, and O otherwise. If a span consists of only one token, the token itself is labeled both Bs and Be. Due to discontinuous spans, there could be multiple bs and be labels for one given cue, as shown in Figure 2 .\nNext, we create the \"Boundary Shift Map\" (BSM) for tokens that are not on the boundaries, by labeling their shifting directions: L for left, and R for right. The 5th and 6th rows in Figure 2 provide a visual illustration, for start and end boundaries respectively. A token is labeled with L / R if the nearest boundary resides on the left / right of the token. For the special case that a token has the same distance to both boundaries on the left and right, we label the token with R.\n\nR-BSL Model Detail\nFigure 3 illustrates the model architecture. We use BERT to encode the sentence and then use three feed-forward (FF) layers in parallel, to predict scope label and the BSM labels. The losses for the three label classifiers L scope , L start , L end are the widely used Cross Entropy loss. L scope is formally defined in Eq. 1 and the other two losses are defined similarly. The three losses are then combined to form the final loss in Eq. 2, and we set \u03b1 = 0.2\nL R Bs L L R R Be L R Be L R\nL scope = \u2212 N i=1 y (i) log(\u0177 (i) )\n(1)\nLoss = \u03b1L scope + 1 \u2212 \u03b1 2 (L start + L end ) (2)\nWarm Up. In training, there is a \"warm up\" phase to train the model solely with scope loss L scope for the first 5 epochs (where the validation loss is reasonably stable). Then boundary shift losses kick in to for scope refinement.\n\nExperiment Results\nWe conduct experiments on all three benchmark datasets: Sherlock, BioScope, and SFU. Among them, BioScope and SFU datasets do not come with official train-validation-test split. Following the previous studies, we use random split on 70-15-15 ratios; however the randomness in split may slightly affect model performance. Hence, we also report the result of our re-implemented baseline model Khandelwal and Sawant (2020) , which is a BERT + Feed-forward with OSC scope tags. Table 2 reports the results of F 1 over scope tokens, defined by Morante and Blanco (2012) . For each scope, token-wise F 1 is computed between ground truth and predicted scope tokens. For all our implemented models, the reported results are average scores of 5 out of 7 runs, excluding the highest and lowest scores. All the runs are set with randomly generated seeds. Since Truong et al. (2022) use RoBERTa instead of BERT, we also report R-BSL (RoBERTa-base) for fair comparison.\nR-BSL achieves best performance on all three datasets, particularly on Sherlock which comes with official train/test split. Note that on Sherlock dataset, our re-implemented baseline does not reach the scores reported in Khandelwal and Khandelwal and Sawant (2020) . For BioScope-Abstract and SFU, there is no official train/test split. The difference in random split (with the same ratio) leads to the difference between our re-implemented baseline and previous studies.\nSawant (2020). 2 Truong et al. ( 2022) also reports lower results (mean of 5 runs) using the code released by Khandelwal and Sawant (2020) . Nevertheless, both our R-BSL variants outperform all baselines on Sherlock, and on BioScope dataset. On SFU, our models' improvement is marginal.\nThe main reason is the distributional bias, for the negation scopes largely align with punctuation or special tokens (see Appendix C).\nFor comprehensive evaluation, Table 3 shows the scope level F 1 scores by exact match. That is, when the predicted scope exactly matches the ground truth, it is considered as True Positive. There exists True Negative and False Positive cases due to \"void negation\" as discussed in Appendix C. When the ground-truth has no negation scope, if the model predicts any scope, that would be a False Positive. The scope exact match F 1 is similar to \"Scope CM\" metric defined in Morante and Blanco (2012) . However, as we do not focus on cue detection but using cues as input, the results is not directly comparable with Scope CM results in earlier studies.\nCompared to token-level measure, the improvements of our model over baseline is now by a much larger margin, particularly the variant with RoBERTa. In other words, the boundary refinement by BSL enables the model to resolve more accurate negation scopes in terms of exact scope span match, which is a stricter measure.\n\nAblation Study\nWe conduct two ablation studies on Sherlock dataset, and the results are reported in Table 4 . 2 The original paper does not provide complete experimental setup like how many runs were performed, or whether the reported results being mean or maximum of several runs. \"Warm Up\" of Scope Classifier. We \"warm up\" the training with the first 5 epochs for scope classifier only. The boundary classifier with BSL loss then comes into the picture. To study its impact, we train all the three classifiers from the beginning. Shown in Table 4 , the removal of warm up leads to negative impact on results. This ablation study suggests that the BSL can further improve the results when the span boundaries have been de-tected by the base model, i.e.,, the scope classifier, at reasonably good accuracy.\n\nConclusion\nWe propose a simple sequence labelling training strategy to enhance boundary prediction for negation scope resolution. Through experiments, we demonstrate the effectiveness of boundary shift loss on complex span extraction tasks on three benchmark datasets. In particular, our simple model achieves the state-of-the-art results on the Sherlock dataset which is considered more challenging for this task. Our model is simple and can be used as a pre-processing for downstream tasks where negation is an important consideration.\n", "hypothesis": " 1 On multiple benchmark datasets, we show that the extremely simple R-BSL achieves best results..", "answer": true}
{"title": "Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities", "content": "\nIntroduction\nOver the years, Semantic Role Labeling (Gildea and Jurafsky, 2002, SRL) -the task of identifying the semantic relations between predicates and their arguments -has attracted continued interest. Enticed by the prospect of acquiring one * Equal contribution.\nof the ingredients that might enable Natural Language Understanding (Navigli et al., 2022) , the research community has striven to overcome numerous challenges in SRL. As a consequence, not only have automatic systems achieved impressive results on complex benchmarks (Shi and Lin, 2019; Conia et al., 2021) , such as CoNLL-2005 (Carreras and M\u00e0rquez, 2005) , CoNLL-2008 (Surdeanu et al., 2008) , CoNLL-2009 (Haji\u010d et al., 2009 ), and CoNLL-2012 (Pradhan et al., 2012) , but SRL has also been successfully leveraged to benefit a wide array of downstream tasks in Natural Language Processing and also Computer Vision, including Machine Translation (Marcheggiani et al., 2018; Raganato et al., 2019; Song et al., 2019) , Summarization (Hardy and Vlachos, 2018; Liao et al., 2018) , Situation Recognition (Yatskar et al., 2016) , and Video Understanding (Sadhu et al., 2021) , among others.\nNotwithstanding the achievements of previous work, we argue that there is still much to be done before the research community can claim SRL is even close to being \"solved\". One of the simplest yet erroneous assumptions about SRL is that all predicates -or at least the majority of them -are verbs. Quite the contrary, predicates often manifest themselves as nouns, adjectives, and adverbs. For example, in the sentence \"Sensational robbery at the bank during the night: two suspects on the loose!\", the word robbery is a predicate, as it denotes an action, and its arguments are sensational (attribute of the robbery), at the bank (location), during the night (time), and two suspects (agents). We highlight two potential issues in the above example. First, an SRL system that analyzes only verbal predicates cannot identify the nominal event in the sentence and, in turn, its semantic constituents. Second, nominal events like those expressed in the above sentence are far from rare, being commonly found in several settings, such as newspaper headlines, blog titles, short messages, tweets, and dialogues.\nPerhaps surprisingly, there is limited work on non-verbal predicates, mostly focused on transferring \"knowledge\" about verbal predicates to nominal ones (Zhao and Titov, 2020; Klein et al., 2020) . The scarcity of studies on non-verbal predicates might be explained by the way in which current datasets for SRL are designed, as they focus primarily on verbal predicates (Daza and Frank, 2020; Tripodi et al., 2021; Jindal et al., 2022) . Therefore, any progress on non-verbal predicates is often overshadowed by the predominance of verbal instances, resulting in an incomplete picture of the actual situation. The issue is also exacerbated by the fact that, oftentimes, benchmark results are taken at face value. Instead, carrying out in-depth analyses is fundamental, as neural networks have been found to learn patterns that are different from those of humans, especially in semantic tasks (Maru et al., 2022) . In this paper, we perform a reality check and explore non-verbal predicates in English SRL. More specifically, our contributions are as follows:\n\u2022 We provide an empirical demonstration that state-of-the-art systems are not capable of generalizing from verbal to nominal and adjectival predicate-argument structures (PAS) in PropBank-based SRL;\n\u2022 We investigate whether other PAS inventories -namely, FrameNet, VerbNet, and VerbAtlasare better suited for transferring learned patterns across predicate types;\n\u2022 We introduce a novel, manually-annotated challenge set to evaluate current and future SRL systems on verbal, nominal, and adjectival PAS;\n\u2022 We analyze possible directions and strategies for prospective work on non-verbal SRL.\n\nChallenges\nAs mentioned above, relying These results show that a state-of-the-art system is not capable of \"transferring knowledge\" from one predicate type to another, e.g., from verbs to nouns or vice versa.\nExamples also enables a solid evaluation of an SRL system on over 4000 predicate senses that are not included in OntoNotes 5.0; we call this more challenging testbed PB-Unseen. We report statistics on PB-Unseen in the last row of Table 1 .\nCross-type knowledge transfer. Now that we have wide-coverage multi-type SRL datasets, we can test the ability of SRL systems to generalize across types. The main objective of our experiments here is to empirically demonstrate that: i) \"knowledge transfer\" between predicate types is an unaddressed challenge, and ii) this problem is not apparent in OntoNotes, but becomes evident from PB-Examples and PB-Unseen. To prove these points, we take CN-22 -a state-of-the-art system (Conia and Navigli, 2022) -and study its behavior when trained on the entire OntoNotes (CN-22 verbs+nouns ), only on its verbal structures (CN-22 verbs ), or only on its nominal structures (CN-22 nouns ). The results on the test set of OntoNotes, shown in Table 2 , represent the first evidence that even a state-of-the-art SRL system is affected by limited generalization capabilities across predicate types. Indeed, the performance of CN-22 verbs drops significantly when evaluated on nominal PAS, from 84.7 to 16.4 points in F1 score on argument labeling, and that of CN-22 nouns drops analogously when evaluated on verbal instances, from 72.8 to 11.2 on argument labeling. One could observe that CN-22 verbs+nouns , jointly trained on verbal and nominal instances, seems to solve the cross-type transfer problem. However, this is true only because the OntoNotes test set does not feature adjectival structures. Indeed, it is very clear from the results on our PB-Examples and PB-Unseen that the performance of CN-22 verbs+nouns does not improve on adjecti-val PAS compared to CN-22 verbs (only +0.5% on PB-Examples and +0.2% on PB-Unseen for argument labeling). Therefore, we can derive that joint learning on two predicate types (i.e. the verbal and nominal ones) does not provide breakthrough improvements on a third predicate type (i.e. the adjectival one). We stress that, in this case, we cannot simply rely on jointly training CN-22 on verbal, nominal, and adjectival instances as, to our knowledge, no training dataset includes adjectival PAS for PropBank-based SRL.\n\nOpportunities\nIn the previous Section, our experiments show that zero-shot knowledge transfer across predicate types is still challenging. We argue that this problem is caused by two main factors. First, PropBank was not designed to aid cross-type knowledge transfer, e.g., the nominal predicate theft.01 is not linked to its verbal equivalent steal.01. Second, recent SRL systems might have limited capability for recognizing common patterns across different predicate types. We conduct an initial investigation of these aspects and discuss some opportunities for improving non-verbal SRL.\nThe role of the linguistic resource. While Prop-Bank might not be the ideal resource for non-verbal SRL, other inventories -based on different linguistic theories -may provide features that could be helpful to aid knowledge transfer between predicate types. After all, previous studies have already shown that language models leverage different hidden layers depending on the linguistic resource used for SRL (Kuznetsov and Gurevych, 2020; Conia and Navigli, 2022) . Here, instead, we take the opportunity to study if there is an inventory whose Table 3 : Precision (P), Recall (R), and F1 scores of CN-22 on Parallel-SemLink. For each row, we evaluate the performance of the system when trained using the related inventory, e.g., PropBank is trained on Parallel-SemLink annotated with PropBank and the results are reported against the test set for the same inventory.\ntheoretical principles can aid the generalization capability of an existing SRL system on unseen patterns.\nWe thus evaluate empirically the differences between four different inventories, namely, PropBank, FrameNet (Baker et al., 1998) , VerbNet (Schuler and Palmer, 2005) , and VerbAtlas (Di Fabio et al., 2019) . 1 To do this, we create Parallel-SemLink, a multi-inventory benchmark made up of the subset of OntoNotes from SemLink 2.0 (Stowe et al., 2021) , whose predicates and arguments are annotated with PropBank, FrameNet, and VerbNet. We also include VerbAtlas annotations thanks to the inter-resource mapping between VerbNet, Word-Net, and VerbAtlas. 2 For each of these inventories, Parallel-SemLink includes a training, a validation, and a test set with 7336, 816, and 906 sentences, respectively.\nWhile we stress that this experimental setting is severely limited since it assumes that all resources can be mapped to each other 1-to-1, it provides a controlled environment for a fair, direct comparison. To study the impact of the inventory, we evaluate our SRL system on each of the linguistic inventories in Parallel-SemLink (CN-22 PropBank , CN-22 FrameNet , CN-22 VerbNet , and CN-22 VerbAtlas ). The results in Table 3 testify that the linguistic resource of choice plays a role in the results. In particular, we can observe a relative error rate reduction of 38% in predicate sense disambiguation (from 97.9 to 98.7) and 13% in argument labeling (from 88.1 to 89.7) when using VerbAtlas instead of Prop-Bank. This result indicates that higher-level semantic abstractions, such as semantics-based clusters, 1 Appendix A provides an overview of the inventories. 2 Appendix C provides further details on our mapping procedure. as available in VerbAtlas thanks to its organization of frames as verbal synset groupings, and crosspredicate role semantics, as adopted in VerbNet and also VerbAtlas, can help a system generalize better on unseen patterns.\n\nVerbs Nouns\nChallenge-SRL. While our multi-inventory SemLink-based dataset provides a preliminary indication of the role of a linguistic inventory, it only includes verbal predicates. To further validate the preliminary results obtained on our multi-inventory SemLink-based dataset, we create a small challenge test set for verbal, nominal, and adjectival SRL, manually annotated with parallel labels for PropBank, the most popular inventory, and VerbAtlas, the most promising inventory (cf. Table 3 ). This new test set is particularly challenging, as it features only PAS that do not appear in OntoNotes. Therefore, Challenge-SRL makes it possible to measure the capability of an SRL system to generalize i) across predicate types, and ii) on the long tail of predicate senses.\nTo construct Challenge-SRL, we randomly selected a total of 288 sentences -96 sentences for each predicate type -from PB-Unseen. We then asked three expert annotators to independently annotate each sentence with predicate senses and their semantic roles. The annotation process was carried out in two phases: first, each person annotated each sentence independently, resulting in a disagreement of 32%; then, the annotators discussed and resolved their disagreements, if possible, reducing them to 6%. Overall, Challenge-SRL includes 1898 predicate-argument pairs.\nAs we can see from Table 4 , Challenge-SRL confirms our preliminary experiments, macroscopically magnifying the differences between Prop-Bank and VerbAtlas. First, we observe that VerbAtlas is significantly better in predicate sense disambiguation for verbal instances (49.5 vs. 14.5 in F1 score) but worse for nominal and adjectival ones (22.2 vs. 17.7 and 27.7 vs. 13.5, respectively). This is mainly because VerbAtlas was not designed for non-verbal SRL and, therefore, it does not provide a lemma-to-sense dictionary to restrict the possible frames of nominal and adjectival predicates. Second, VerbAtlas significantly outperforms PropBank on argument labeling of verbs (47.0 vs. 5.5 in F1 score), nouns (44.2 vs. 2.1), and adjectives (36.8 vs. 10.8). We argue that this is largely due to the adoption in VerbAtlas of cross-frame semantic roles that are coherent across frames, which allows the system to leverage other predicates seen at training time with similar structures.\nLeveraging Word Sense Disambiguation. Finally, we carry out a preliminary exploration of possible directions that could aid non-verbal SRL in the future. While SRL research has not dealt with non-verbal semantics, other areas have investigated semantics for different parts of speech, and one of these is Word Sense Disambiguation (WSD). More specifically, WSD is the task of assigning the most appropriate sense to a word in context according to a predefined sense inventory (Bevilacqua et al., 2021) . It is easy to notice how this task resembles predicate sense disambiguation in SRL, the only difference being that WSD is not limited to predicates, as it aims to disambiguate every content word. Therefore, we believe that WSD is an interesting candidate to explore whether a different disambiguation task can help to improve the generalization capability of an existing SRL system on Challenge-SRL, i.e., on predicate-argument structures that the SRL system did not see at training time.\nTo investigate the effect of WSD on SRL, we start by leveraging the fact that VerbAtlas frames are clusters of WordNet synsets. Therefore, we map each synset predicted by AMuSE-WSD (Or-lando et al., 2021 (Or-lando et al., , 2022)) , 3 a state-of-the-art offthe-shelf WSD system, to a VerbAtlas frame, and compare them to the prediction of our SRL system. Table 5 shows the performance of AMuSE-WSD on predicate sense disambiguation (WSD baseline ). Interestingly, we observe that a simple WSD baseline can strongly outperform an SRL system when training data is scarce. Indeed, AMuSE-WSD surpasses CN-22 SemLink in each predicate type (46.7 vs 6.2, 32.7 vs 6.2, 3.8 vs 3.1, for verbs, nouns and adjectives, respectively), and CN-22 OntoNotes in nominal predicates, with an overall improvement of +5.7 (31.7 vs 26.0) over the best performing SRL system.\nMost interestingly, if we employ an oracle to pick the best prediction between the WSD baseline and our best SRL system, we notice a further improvement (41.5% vs. 26.0%), demonstrating that current state-of-the-art SRL systems can still benefit from explicit lexical semantics. We hypothesize that tighter integration of the two tasks may lead to even better improvements in generalization capabilities.\n\nConclusion and Future Work\nIn this paper, we carried out a reality check and demonstrated that, despite impressive results on standard benchmarks by state-of-the-art systems, SRL is still far from \"solved\". Indeed, thanks to a carefully-designed set of experiments and the introduction of novel, manually-curated, wide-coverage benchmarks, we showed that current SRL systems possess inadequate capabilities for transferring knowledge between predicate types.\nOur analyses pointed out that we can address this limitation by working in two directions: leveraging the intrinsic characteristic of frameset resources, including semantics-based clusters and cross-predicate role semantics, and tighter integration of other semantics-based tasks, such as Word Sense Disambiguation, into SRL.\nWe hope our work will be a stepping stone for innovative research on high-performance SRL systems for non-verbal predicate-argument structures, a problem that still needs extensive investigation. For this reason, we release our software and datasets at https://github.com/sapienzanlp/ exploring-srl.\n", "hypothesis": "In this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types. Thanks to it, we demonstrate empirically that standard benchmarks provide an accurate picture of the current situation in SRL and that state-of-the-art systems are capable of transferring knowledge across different predicate types. Having observed these successes, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures. We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer. In conclusion, we claim that SRL is \"solved\", and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates.", "answer": false}
{"title": "Improving Language Model Integration for Neural Machine Translation", "content": "\nIntroduction\nMachine translation (MT) is the task of automatically translating text from one language to another. Nowadays, the dominant approach is neural machine translation (NMT), where a neural network is used to predict the probability of a sentence in the target language, given a sentence in the source language (Bahdanau et al., 2014; Vaswani et al., 2017) . For this approach to be effective, a large number of bilingual training samples -consisting of sentences and their corresponding translationsis needed. This poses a challenge, especially when we want to build a system for a specific domain, where zero or only limited amounts of in-domain bilingual data are available.\nIn these situations, people turn towards monolingual text data, which is simply text in the source or target language and of which plenty exists for most languages and domains. Before NMT became feasible, the preferred way of incorporating additional monolingual data in the MT system was the usage of an external target-side language model (LM), which is trained on monolingual data to predict the probability of a sentence (Brown et al., 1990; Della Pietra, 1994; Zens et al., 2002) .\nHowever, with the rise of NMT, it was found that a technique called back-translation outperforms the LM incorporation by a large margin (Sennrich et al., 2016a) . Back-translation is a two step process, where we first create synthetic parallel data by automatically translating target side monolingual data into the source language. Then, the final NMT system is trained on the combination of the real and synthetic parallel data. It was argued that the backtranslation approach better suits the NMT framework because the NMT system implicitly learns an internal language model (ILM) as part of the training, which might interfere with an additional external LM (Sennrich et al., 2016a) .\nMore recently, for automatic speech recognition (ASR), there have been works focusing on neutralizing this ILM before combination with an external LM and significant improvements were reported (McDermott et al., 2019; Variani et al., 2020; Meng et al., 2021; Zeyer et al., 2021; Zeineldeen et al., 2021) . In this work, we adapt the methods for ILM compensation, developed for ASR, and test them for NMT. We compare against back-translation in different settings and find that ILM compensation significantly boosts the performance of LM fusion, although back-translation is still outperforming this approach for NMT. Also, applying ILM compensation on top of back-translation does not result in significant performance improvements.\n\nRelated Work\nSeveral approaches to combine an LM and NMT model have been proposed in the past. Shallow fusion (SF) is the most straight forward way, using a weighted log-linear combination of the model output probabilities (Gulcehre et al., 2015 (Gulcehre et al., , 2017)) . Deep fusion denotes the concatenation of the hidden states of NMT model and LM and requires joint fine-tuning of both models (Gulcehre et al., 2015 (Gulcehre et al., , 2017)) . Simple fusion is similar to shallow fusion, but the NMT model is trained using information from a pre-trained LM (Stahlberg et al., 2018) .\nFor the task of ASR, people recently have started to remove the ILM that is implicitly learned. The biggest question there is, how to best approximate the ILM. Approaches include: (1) training an additional LM on the target side of the parallel data (McDermott et al., 2019) , (2) removing/averaging encoder information (Variani et al., 2020; Meng et al., 2021; Zeyer et al., 2021) and ( 3) training a small sub-network while freezing all other parameters (Zeineldeen et al., 2021) .\nAs an alternative to LM fusion, back-translation (Schwenk, 2008; Bertoldi and Federico, 2009; Sennrich et al., 2016a) has become the standard method for incorporating additional monolingual data for NMT. Some work has been done to improve this approach, including sampling (Edunov et al., 2018; Gra\u00e7a et al., 2019) , tagging (Caswell et al., 2019) and block-BT (Popel et al., 2020) . For sake of simplicity, we focus on the standard back-translation approach using beam search in this work.\nApart from using an external LM and backtranslation, additional monolingual data can also be utilized by pre-training (Ramachandran et al., 2017; Zhu et al., 2019) , multi-task-learning (Zhang and Zong, 2016; Domhan and Hieber, 2017) or post-editing (Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019) . In principle, all these approaches can also be combined with LM fusion, potentially further improving the performance of the resulting system.\n\nInternal LM Estimation\nDuring decoding, given a source sentence f J 1 and a model P (e I 1 |f J 1 ), we want to find the translation \u00ea\u00ce 1 that maximizes\n\u00ea\u00ce 1 = argmax I,e I 1 P (e I 1 |f J 1 ) .\nIn our framework, P is the combination of three models:\nP (e I 1 |f J 1 ) \u221d P MT (e I 1 |f J 1 ) \u2022 P \u03bb 1 LM (e I 1 ) \u2022 P \u2212\u03bb 2 ILM (e I 1 )\nwhere P MT , P LM and P ILM are the probabilities of the NMT model, external LM (trained on additional monolingual data) and ILM respectively, and \u03bb 1 , \u03bb 2 \u2265 0. Note that the ILM gets a negative weight, because we want to neutralize its impact in this model combination. If \u03bb 2 = 0, we fall back to standard shallow fusion.\nIn principle, the ILM can be exactly calculated from the NMT model by marginalizing over all source sentences f J 1 . However, this summation would be intractable. Instead, different ILM approximations have been proposed in the recent past for ASR, which we will briefly recall here. For a more in-depth discussion of the different approximation methods we refer the reader to Zeineldeen et al. (2021) .\nseparate LM : The ILM is approximated by training a separate LM on the target side of the parallel training data.\nh = 0 : The ILM is approximated by taking the fully trained NMT model P MT (e I 1 |f J 1 ) and setting the encoder outputs h J 1 to 0. h = h avg : Instead of setting all encoder outputs h J 1 to 0, we replace the vector h j for each position j with the average h avg j , extracted over the whole parallel training data. We tokenize the data using byte-pair-encoding (Sennrich et al., 2016b; Kudo, 2018) with 15k joint merge operations (40k for WMT14). The models are implemented using the fairseq toolkit (Ott et al., 2019) following the transformer base architecture (Vaswani et al., 2017) . The details of the training setups can be found in Appendix A. All systems are trained until the validation perplexity no longer improves and the best checkpoint is selected using validation perplexity as well. We use beam-search with beam-size 12 and utilize SacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) 2 https://data.statmt.org/news-crawl/ 3 https://data.statmt.org/news-commentary/v14/ and TER (Snover et al., 2006) . We report BLEU and TER since we are most familiar with these metrics and to be comparable with previous works. However, we acknowledge that these metrics might have some biases and in future work it might be worth utilizing additional metrics like COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) . Additionally, in future work we should separate our test sets for original source and target text to better understand the effect of translationese in both training and test data, as this might very much influence the improvements we see, especially in the case of back-translation (Freitag et al., 2020) .\n\nComparison of ILM Approximations\nWe start by analyzing the ILM neutralization approaches on the IWSLT En\u2192De task and then verify the results on the other tasks. We implement and re-train (if applicable) all the different ILM approximation methods discussed in Section 3. The resulting perplexities on the validation set are listed in Table 1 . The variants separate LM and mini-self-attention have been trained directly using the language model objective, so it is no surprise that they exhibit a much lower perplexity than the other approaches. However, it can be argued that a lower perplexity of the ILM does not necessarily correspond to a better approximation of the implicit language model. In order to effectively use the external LM and the ILM during decoding, we need to optimize the weights \u03bb 1 and \u03bb 2 (see Section 3). We do this via a grid search over the validation set by optimizing for the highest BLEU score. The resulting grid for the mini-self-attention ILM variant on the IWSLT En\u2192De task is shown in Figure 1 .\nThe NMT system by itself has a BLEU [%] score of 21.2. By log-linear combination with just the external LM (\u03bb 2 = 0, vanilla shallow fusion) we can gain around 1% absolute improvement on the validation set with the best choice of \u03bb 1 = 0.15. By including the ILM with a negative weight, we can get further improvements, up to a final score of 23.8 BLEU [%] . 4 Interestingly, the best performance is reached when \u03bb 1 \u2248 \u03bb 2 and with the ILM neutralization, the external LM can be assigned a much bigger weight compared to the case \u03bb 2 = 0. We find that for all ILM approximation variants, the optimal weights are similar, and that the TER scores on the validation set follow an almost identical pattern. The final performance of each variant on the test set is shown in Table 2 .\nWe want to point out, that the improvements we see on the validation set transfer nicely to the test set with the same tuned weights \u03bb 1 and \u03bb 2 . This is because, in our experiments, the validation and test sets are of the exact same domain. In some additional experiments we found that the optimal values for these weights are indeed domain specific and have to be re-tuned if the system were to be optimized for a different domain. All ILM approximation variants lead to a significant performance improvement over simple shallow fusion. Out of all ILM approximations, the mini-self-attention approach performs best, which is the same observation that Zeineldeen et al. (2021) made for ASR.\n\nComparison to Back-Translation\nFor the back-translation experiments, we train NMT systems on the same parallel training data in the reverse direction and then translate a total of 10M sentences from the monolingual target data (the same data used for training the external LM). Afterwards, the final systems are trained on the combination of real and synthetic data. The final results for all four MT tasks are shown in Table 3 .\nWe observe the same trend for all four MT tasks. In general, the improvements from the additional monolingual data are getting smaller, when the amount of parallel training data increases. In almost all cases, shallow fusion gives a small improvement over just using the NMT system. ILM neutralization again improves consistently over simple shallow fusion, with the mini-self-attn approximation variant always performing the best. Back-translation out-performs language model integration on all four tasks, although the gap is getting smaller the more parallel training data is available.\nWe also combine back-translation with the best ILM approximation approach (mini-self-attn). This does not further increase translation quality, with the exception of the WMT14 task, where we see a small improvement. In general, the ILM approach performs the closest to back-translation on the WMT14 task, so it might be worthwhile to apply this concept to an even bigger MT task.\n\nConclusion\nWe re-visit the method of language model integration for neural machine translation. We implement and experiment with a new approach of neutraliz-ing the implicit language model, which has already shown promising result for the task of automatic speech recognition. We find that ILM neutralization significantly improves the translation quality compared to standard shallow fusion. However, back-translation as an alternative way to incorporate additional monolingual data, still outperforms the approaches using an external language model. Therefore, for future work we will focus on scenarios where back-translation can not be applied effectively, e.g. when the quality of the initial NMT system is too bad to create helpful synthetic data.\n", "hypothesis": " In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data -namely backtranslation.  We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation..", "answer": true}
{"title": "Metaphor Detection via Explicit Basic Meanings Modelling", "content": "\nIntroduction\nMetaphors are widely used in daily life for effective communication and vivid description. Due to their unusual and creative usage, further processes are required for machines to understand metaphors, which results in Computational Metaphor Processing (CMP), an active research direction in NLP (Rai and Chakraverty, 2020) . Recent studies demonstrate that CMP can benefit a wide range of NLP tasks including creative language generation (Chakrabarty et al., 2020; Li et al., 2022b) , sentiment analysis (Li et al., 2022a) , and machine translation (Mao et al., 2018) . Metaphor identification, aiming to detect words used metaphorically, is the very first stage in CMP. For example, target words 'attack' or 'defend' in the context sentence \"He attacks/defends her point.\" do not literally involve physical engagement, so they are supposed to be identified in metaphor detection for further process (Steen et al., 2010) .\nLinguists, philosophers and psychologists propose various ways to define metaphors, including substitution view (Winner, 1997) , comparison view (Gentner, 1983) , class inclusion view (Davidson, 1978) , and conceptual metaphor theory (Lakoff and Johnson, 2008) . In contrast to these theories which are relatively complex in nature, Pragglejaz (2007) propose a simple and effective linguistic theory called Metaphor Identification Process (MIP) which can identify metaphors in unrestricted textual corpora. MIP gains increasing popularity as it detects metaphorical terms regardless of specific conceptual mapping or comparison among source and target domain, which makes the identification operational and straightforward.\nAccording to MIP, a word is tagged as a metaphor if its contextual meaning contrast with its \"more basic meaning\". The basic meaning here is defined as \"more concrete; related to bodily action; more precise (as opposed to vague); historically older\" guided by dictionaries 1 . For example, in the sentence \"This project is such a headache!\", the target headache here is metaphorical since its contextual meaning is \"a thing or person that causes worry or trouble; a problem\", which contrasts with the more basic meaning \"a continuous pain in the head\" 2 .\nExisting deep learning methods for metaphor identification usually depend on MIP in their model design (Mao et al., 2019; Choi et al., 2021; Song et al., 2021; Li et al., 2023; Wang et al., 2023) . However, existing works usually ignore basic meaning modelling and instead use aggregated meaning to contrast with contextual meaning in MIP. We (Mao et al., 2019; Choi et al., 2021) and our BasicMIP.\ncall the MIP in these implementations 'Aggregated MIP' (AMIP). For example, Mao et al. (2019) and Li et al. (2023) implement MIP by contrasting contextual meaning representation with GloVe embedding and Decontextualised 3 RoBERTa embedding, respectively. However, aggregated meaning representations, such as GloVe and decontextualised embeddings, are not the same as basic meanings in general. They usually represent a frequencybased weighted average of multiple word meanings. In cases where the basic meaning is the most frequent, then the aggregated meaning can be a reasonable approximation to basic meaning. However, it is very common that metaphorical meanings are more frequent so that using aggregated meaning violates the fundamental rule of MIP. For example 'back' means 'the rear surface of the human body' as basic meaning, but its non-basic senses, e.g. 'going back', 'back up', 'back in 1960', are more frequently used in corpora. This makes the aggregated representation of back diverge from its basic sense, so that metaphor cannot be identified via measuring contrast with contextual meaning.\nA further pitfall of previous works is that the aggregated representations used are static rather than contextualised. For example, aggregated representation GloVe and Decontextualised RoBERTa embeddings used by Mao et al. (2019) and Li et al. (2023) are both static embedding, which are not compatible with the contextual meaning they compared to and has been shown to have worse representational quality (Bommasani et al., 2020) .\nIn this paper, we propose a novel metaphor identification mechanism, BasicMIP, which implements MIP via direct basic meaning modelling of targets. BasicMIP explicitly leverages basic annotations from training set, where basic meaning of words are labeled as literal according to MIP theory. First, it samples literal instances for each tar-get. Then, the basic meaning representation of target is obtained by summing up the target embeddings of sampled literal instances. Finally, the basic representations are contrasted with their contextual meaning representation in target sentences to identify metaphors. We also present our novel metaphor detection model, BasicBERT, which not only uses BasicMIP but also inherits the AMIP module and SPV (Selectional Preference Violation Wilks, 1975 Wilks, , 1978) ) theory from prior works.\nExtensive experiments conducted on two metaphor benchmarks show that BasicBERT significantly outperforms current SOTAs. In the VUA20 benchmark, our model exceeds MelBERT by 1% in F1 score. In the VUA18 benchmark, our performance even reaches the theoretical upper bound for the targets with literal annotations in the training set. Our code and data can be found at https://github.com/ liyucheng09/BasicBERT.\n\nMethod\nBasicBERT model consists of three main components: BasicMIP, AMIP, and SPV. We include both AMIP and BasicMIP as some words do not have literal annotations in training set, so AMIP is an useful augmented component for these cases.\n\nBasicMIP\nBasicMIP, as shown in Figure 1 , is based on MIP, in which a target word's contextualised meaning in the current context is compared with its more basic meaning. First, the contextual meaning representation is produced by feeding the current sentence to the RoBERTa network (Liu et al., 2019) . Formally, given a sentence S = (w 1 , ..., w t , ..., w n ), where w t is the target word, we obtain representations as follows:\nH = RoBERTa(emb cls , ..., emb t , ..., emb n ) (1)\nHere CLS is a special token indicating the start of an input; emb i is the input embedding for word w i ; and H = (h cls , ..., h t , ..., h n ) represents the output hidden states. We denote the contextual meaning embedding of w t as v S,t = h t . Second, to contrast the contextual meaning with the basic meaning, our model learns the basic meaning representation of the target from the training annotations. According to MIP (Steen et al., 2010) , we consider targets with literal label to represent their basic meaning. Therefore, we sample literal examples of the target w t from the training set denoted as S b = (..., w t , ...) \u2208 U, where U is training set and S b stands for the context sentence containing a basic usage of w t . Our model obtains the basic meaning embedding of w t by feeding S b to a RoBERTa encoder similar to Equation 1 and get the t-th output hidden state h t . The final contextualised basic representation of w t is averaged among multiple literal instances, and is formulated as v B,t , which is intrinsically different to the aggregated representation of frequent meaning used in prior works.\nAt last, we compute a hidden vector h BMIP for-BasicMIP, by concatenating v S,t and v B,t .\nEQUATION\nwhere f 0 (\u2022) denotes a linear layer to learn semantic difference between v S,t and v B,t .\n\nAMIP and SPV\nThe AMIP implementation of MIP theory is inherited by our model, where contextual meaning and aggregated meaning of the target are compared.\nHere the contextual target meaning embedding of w t is v S,t , the same as in Equation 2. Then, we feed the single target word w t to the RoBERTa network to derive the decontextualised vector representing the aggregated meanings of w t (Choi et al., 2021) :\nv F,t = RoBERTa(emb t ).\nThe SPV theory is also employed which measures the incongruity between the contextual meaning of the target and its context. Similarly, the contextual target meaning embedding is v S,t , and the context sentence meaning is produced by the CLS embedding denoted as v S , where v S = h cls .\nFinally, we compute AMIP (h AMIP ) from the contextual and aggregated target embedding, and SPV (h SPV ) from the contextual target meaning embedding and the sentence embedding.\nh SPV = f 1 ([v S , v S,t ])\n(3)\nEQUATION\nwhere f 1 (\u2022) and f 2 (\u2022) denote a linear layer to learn the contrast between two features.\n\nPrediction\nFinally, we combine three hidden vectors h AMIP , h SPV and h BMIP to compute a prediction score \u0177, and use binary cross entropy loss to train the overall framework for metaphor prediction. \n\u0177 = \u03c3(W \u22a4 [h BMIP ; h AMIP ; h SPV ] + b) (5) L = \u2212 N i=1 [y i log \u0177i + (1 \u2212 y i ) log(1 \u2212 \u0177i )] (6)\n\nContrast measuring.\nTo better compare our BasicMIP with AMIP, we conduct an experiment to directly measure the contrast between features within BasicMIP and AMIP, i.e., the contrast between the contextual and the basic meaning for BasicMIP, and the contrast between the contextual and the most frequent meaning for AMIP. Intuitively, we expect the contrast to be obvious for metaphor cases and to be slight for literal cases. Cosine distance is used to compute the contrast between two features. The contrast will fall into (\u22121, 1), smaller numbers meaning more contrasting, larger numbers meaning less contrasting.\nThe results (see Table 3 ) show that the contrast of BasicMIP features is much more obvious for metaphorical samples, and there is less contrast for literal samples compared with AMIP. Moreover, AMIP only shows a minor gap of 0.13 contrast between metaphor and literal cases. However, a significant gap of 0.89 is captured by BasicMIP between metaphor and literal cases, which demonstrates that BasicMIP learns the difference between metaphorical and literal expressions well. In summary, the results show the effectiveness of basic meaning modelling in metaphor detection.\nCase study. We perform an exploratory analysis on metaphors where BasicMIP succeeds to detect but fails without it. Prior methods might find very simple targets difficult to classify, such as see, back, hot. This is mainly because their metaphorical meanings are more frequent than their basic meanings, which leads the aggregated representations dominate by metaphorical semantics. For example, see means look basically. But, I see why you are angry and this place has seen the war are even more frequent in language corpus. Therefore, the contrast with contextual meaning tends not to indicate metaphors anymore. On the contrary, basic meaning modelling learns their basic representation by focusing literal annotations directly, which enables BasicMIP to tackle them with high accuracy (see Appendix A for examples).\n\nConclusion\nWe proposed BasicBERT, a simple but effective approach for metaphor detection. The key feature of our method is the basic meaning modelling for metaphors from training annotations. Extensive experiments show that our model achieves best results on two benchmarks against SOTA baselines and also reaches the theoretical upper bound for instances with basic annotation. We believe our approach can be extended to other creative language with minor updates. In future, we will try apply our approach to identify other types of creative language, such as humour and sarcasm.\n", "hypothesis": "In this paper, we propose a novel metaphor detection method, which models the basic meaning of the word based on literal annotation from the training set, and then compares this with the aggregated meaning in a target sentence to identify metaphors.", "answer": false}
{"title": "Cross-Domain Argument Quality Estimation", "content": "\nIntroduction\nThe argumentation process is one of the cornerstones of society, as it allows the exchange of opinions and reaching a consensus together. Fueled by advances in natural language processing, recent years have witnessed the advent of Argument Mining (AM), i.e., the field of automated discovery and organization of arguments. AM is helpful over various scenarios, reaching from legal reasoning (Wyner et al., 2010; Walker et al., 2014; Poudyal et al., 2020; Villata, 2020) to supporting the decision-making process of politicians (Haddadan et al., 2019; Duthie et al., 2016; Menini et al., 2017; Lippi and Torroni, 2016; Awadallah et al., 2012) . Thus, there is a flurry of works on identification of arguments from text (Stab et al., 2018b; Fromm et al., 2019; Trautmann et al., 2020) and retrieval of them (Wachsmuth et al., 2017c; Fromm et al., 2021; Dumani and Schenkel, 2019; Dumani et al., 2020; Stab et al., 2018a) . Since arguments often have to be weighed against each other, a central property of arguments is their Argument Quality (AQ) or convincingness, i.e., their (perceived) strength. While the ancient Greeks (Rapp, 2002) already discussed the constituents of strong arguments, automated estimation is a relatively uncharted field. Due to the high subjectivity of argument strength (Swanson et al., 2015; Gretz et al., 2020; Toledo et al., 2019; Habernal and Gurevych, 2016b; Stab et al., 2018b) , obtaining high-quality annotations is challenging, cf. Section 1. In this light, a legitimate question is the reliability and robustness of the existing approaches for estimating AQ and their applicability in real-life scenarios. Existing AQ benchmark datasets are often restricted to a single domain (Wachsmuth et al., 2016; Persing and Ng, 2017) or/and make different assumptions about factors impacting the AQ. Thus, enabling transfer between sources and datasets appears especially appealing, but existing works (Gretz et al., 2020; Toledo et al., 2019; Swanson et al., 2015; Habernal and Gurevych, 2016b) cease to provide detailed studies thereupon.\nIn this work, we thus investigate for the first time the automatic evaluation of the quality of arguments from a holistic perspective, bringing together various aspects. First, we evaluate whether AQ models can generalize across datasets and domains, a crucial feature for deployment in the diverse environments encountered in relevant real-world applications. Next, we investigate the hypothesis of whether models for related argument mining tasks inherently learn the concept of argument strength without being explicitly trained to do so by evaluating their zero-shot performance for estimating AQ. A In summary, our contributions are as follows: \u2022 As far as we know we are the first to study the generalization capabilities of AQ prediction models across different datasets and AQ notions.\n\u2022 Since we determine the size of the dataset as one of the decisive performance factors, we further investigate a zero-shot setting of transferring from related Argument Mining tasks.\n2 Related Work\n\nArgument Quality\nArgument Quality (AQ), sometimes also called Argument Strength, is a sub-task of Argument Mining (AM) that is one of the central research topics among argumentation scholars (Walton et al., 2008; Toulmin, 2003; Van Eemeren and Grootendorst, 1987) . Due to its highly subjective nature, there is no single definition of AQ. As a result, there are various proposals for different factors that can affect the quality of an argument, such as the convincingness of an argument (Habernal and Gurevych, 2016a) . There are several ways to express the strength of an argument. Some works take an absolute continuous score, while others argue that strength estimation works better in (pairwise) relation to other arguments. To the best of our knowledge, we are the first to evaluate how AQ estimators trained on different corpora, AQ notions, and AQ tasks correlate with each other. One of the first relatively large corpora was presented by Swanson et al. (2015) . The SwanRank corpus contains over 5k arguments, where each argument is labeled with a continuous score that describes the interpretability of an argument in the context of a topic. They propose several methods based on linear regression, ordinary kriging, and SVMs as regression algorithms to automatically estimate the strength from an input text encoded by hand-crafted features. Other corpora have followed, using relative-and/or absolute convincingness (Habernal and Gurevych, 2016b; Potash et al., 2019) as an annotation criterion. The works proposed AQ estimators based on SVMs or BiL-STMs combined with GloVe embeddings (Pennington et al., 2014) . Gleize et al. (2019) provide a dataset, IBM-EviConv, that focuses on ranking the evidence convincingness. They used a Siamese network based on a BiLSTM with attention and trainable Word2Vec embeddings. Gretz et al. (2020), and Toledo et al. (2019) created their corpora by asking annotators whether they would recommend a friend to use the argument in a speech supporting or disputing the topic, regardless of their own opinion. Both use a fine-tuned BERT (Devlin et al., 2019) model for the absolute AQ regression task.\nThe shared evaluation practice in the previous works is to evaluate methods on each dataset independently. Gretz et al. (2020) use their newly introduced dataset for pre-training of their model. The authors then investigate the strength of their models by applying them on two related datasets UKPConv and SwanRank. By finetuning the model on the training part of two datasets, they investigate if the pretraining is helpful for the target corpora. Our work proposes to advance the evaluation and advocate for an accurate cross-dataset evaluation without additional fine-tuning on the evaluation dataset to estimate the model's applicability in challenging real-life scenarios.\nAs a common understanding of AQ is still lacking, Wachsmuth et al. (2017a,b) investigated different dimensions of AQ. Based on a survey paper of existing argument quality theories (Wachsmuth et al., 2017a) , they developed a taxonomy that aims to capture all aspects of AQ. In their work, they present a small corpus of 320 arguments annotated for 15 dimensions and explore the correlations between the different dimensions. Thus, their work presents a different view that rather focuses on the argumentation theory than on multiple corpora and the generalization of AQ estimators. Lauscher et al. (2020) ; Ng et al. (2020) created a cross-domain corpus (Q&A forums, debate forums, and review forums) with 5,295 arguments using the annotation scheme of Wachsmuth et al. (2017a) . They conclude that, in most cases, models benefit from the inclusion of out-of-domain training data. However, they do not perform a cross-corpora study of their architectures, which limits the generalizability and impact of their experiments.\n\nGeneralization across Argument\nQuality Corpora\nHigh-level applications such as Argument Retrieval (Wachsmuth et al., 2017c; Fromm et al., 2021; Dumani and Schenkel, 2019; Dumani et al., 2020; Stab et al., 2018a) and autonomous debating systems (Slonim et al., 2021) require reliable Argument Quality (AQ) models to select strong arguments among the relevant ones. The research community has identified this gap and proposed and evaluated different automated models for AQ estimation (Gretz et al., 2020; Toledo et al., 2019; Swanson et al., 2015; Habernal and Gurevych, 2016b) . However, AQ is often captured differently due to its high subjectivity, e.g., absolutely as a continuous score or relative to other arguments by pairwise comparison. Consequently, many publications also introduced their own corpus with individual annotation schemes capturing different notions of AQ. While they have compared multiple AQ estimators against each other within a single corpus, there is a lack of cross-corpora empirical evaluations. Thus, the robustness of predictions across datasets remains largely unexplored, which poses a severe challenge for reliable real-world applications integrating diverse data sources. To assess the generalizability capability of AQ estimation models, we designed a series of experiments across all four major AQ datasets to answer the following research questions:\n1. How well do AQ models perform across datasets if annotations schema and domain of the arguments do not change?\n2. How does the corpora size affect generalization?\n3. How well do models generalize across different text domains?\n4. How does the AQ quality notion affect generalization?\n5. Does the AQ model become more robust if it is trained with a combined dataset containing data from different domains and labeling assumptions also vary?\n\nDatasets and Evaluation Setting\nWe briefly describe the four AQ datasets used in our empirical study, which all capture AQ on a sentence level. They are also summarized in Table 2 .\n1. Swanson et al. (2015) constructed the dataset SwanRank with 5,375 arguments whose quality is labeled in the range of [0, 1], where 1 indicates that an argument can be easily interpreted. It consists of four controversial topics taken from the debate portal CreateDebate 1 .\n2. Habernal and Gurevych (2016b) As some of the corpora did not provide official train-validation-test splits and differed in the number of topics and the formulated task (in-topic vs. cross-topic), we decided to do our own split based on the topics of the arguments. Contrary to the original topic splits in UKPConv, IBM-ArgQ and IBM-Rank, we treat the supporting and opposing arguments from a certain topic as one topic because they have very great similarities. Whereas in their work, e.g. the topics \"We should abandon cryptocurrency\" and \"We should adopt cryptocurrency\" are represented as two topics. We perform 10-fold cross-topic cross-validation, where each fold is a 60%/20%/20% train-validation-test split, and we additionally ensure that no topic occurs in more than one split. By the latter requirement and the topic merge, we ensure an inductive setting where the AQ estimation can not rely on similar arguments in the training corpus and therefore provides a more challenging but more realistic task.\n\nModel and Training\nSince transfer learning achieves state-of-the-art Argument Mining (AM) results on different corpora and tasks (Reimers et al., 2019; Fromm et al., 2019; Trautmann et al., 2020) , we also apply it to our AQ estimation task. We use a bert-base model, pretrained on masked-language-modeling, and finetune it to predict absolute AQ scores on the respective datasets, cf. Section 3.1. As an input, we used the arguments from the respective datasets and concatenated the topic information, separated by the BERT specific [SEP ] limiter, similar to other work in AM (Fromm et al., 2019; Reimers et al., 2019; Gretz et al., 2020) . We concatenate the last four layers (as Gretz et al. (2020) ; Toledo et al. (2019) did it) of the fine-tuned BERT model output to obtain an embedding vector of the size 4 \u2022 768 = 3, 072. For the regression task, we stack a Multi-Layer Perceptron (MLP) with two hidden layers, one with 100 neurons and a ReLU activation, followed by the second hidden layer and a sigmoid activation function. We train the architecture end-to-end, with SGD with a weight decay of 0.35 and a learning rate of 9.1 \u00d7 10 \u22126 . The MLP uses dropout with a rate of 10%.\n\nResults\nTable 3 summarizes our results. We report the Pearson correlation score between the predicted-and ground-truth absolute AQ evaluated on a hold-out test set. Contrary to the original topic splits in UKPConv, IBM-ArgQ and IBM-Rank we treated the supporting and opposing topics as one topic. The task is therefore more challenging, as topic information from the contrary stance can not be used during training. However, the task is also more realistic, as one can not expect to have arguments from all topics in the training set.\n\nEvaluation on Similar Datasets and Importance of Training Set Size\nFirst, we evaluate the performance of the model on similar datasets and the dependency on the size of the training dataset. We can observe that models perform very well on other datasets from a similar domain labeled with a similar quality notion, i.e., IBM-ArgQ and IBM-Rank (both are crowd collected and annotated based on recommendableness. Furthermore, we can notice that the size of the dataset is crucial for performance: a model trained on the largest IBM-Rank dataset achieves the best score also on IBM-ArgQ. This insight gives us a solid foundation for the next steps.\n\nGeneralization Across Domains and Quality Notions\nNext, we investigate whether a transfer across domains is possible. Recall that the four datasets cover two different domains: the sentences from UKPConv and SwanRank have been extracted from debate portals, while IBM-Rank and IBM-ArgQ have been collected from the crowd. Compared to in-domain generalization, we observe a considerably worse generalization between domains: For example, trained on the crowd dataset IBM-ArgQ, we can achieve a correlation of 38.9% on the crowd dataset IBM-Rank, while training on the debate datasets SwanRank and UKPConv results in negligibly low correlations of 8% and 3%, respectively. Conversely, when evaluated on the debate portal dataset SwanRank, we obtain a correlation of 42.5% when using a model trained on the other debate portal dataset UKPConv, while the crowd-collected datasets IBM-ArgQ and IBM-Rank only achieves 27.8% and 37.0%, respectively. The smaller difference compared to the first comparison can be explained by the larger training datasets.\nSurprisingly, we observe a completely different picture for generalization across quality notions. We see only a moderate drop in performance for a fixed domain but a different quality notion. For instance, the model trained on SwanRank performs relatively well on the UKPConv dataset. Viceversa, we observe a more considerable performance drop, which can be explained by the smaller size of the UKPConv dataset.\n\nMulti-Domain and Multi-Quality Notion Training\nTo investigate whether a single model can grasp various dimensions of quality and work on arguments from various domains, we designed another set of \"leave-one-out\" experiments. We train on the training sentences of all but one AQ corpus and evaluate the performance on all test sets. The four rows \"all except\" define the three training sets, e.g. \"all except UKPConv\" consists of the training sets of (SwanRank, IBM-ArgQ, and IBM-Rank).\nThe entries on the diagonal thus show how well the models perform when evaluated on an unseen corpus.\nFor evaluation on the unseen IBM-Rank dataset after training on the remaining ones, we can obtain a correlation of 46.5%, which nearly reaches the correlation of 48.1% we obtained when training and evaluating on IBM-Rank. For SwanRank, IBM-ArgQ and UKPConv, we can even surpass the correlation on the respective test set by training on all other training sets instead of the one from the respective corpus.\n\nCross-Corpora Generalization Conclusion\nTo summarize, we conclude that in our analysis the available datasets and models for AQ are reliable.\nOur most important insight is that AQ notions do not contradict each other, and a single model can estimate the AQ of text from different domains. Therefore, the practical recommendation for reallife application is to combine all available datasets across different domains and AQ notions.\n", "hypothesis": " We find that generalization depends on a sufficient representation of different domains in the training part.  In zero-shot transfer and multi-task experiments, we reveal that argument quality is among the more challenging tasks but can improve others.", "answer": true}
{"title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training", "content": "\nIntroduction\nDense retrievers, which estimate the relevance between queries and passages in the dense embedding space, have achieved impressive performance in various applications, including web search (Liu et al., 2021) and open-domain question answering (Karpukhin et al., 2020) . One key factor for the success of dense retrievers is a large amount of human-annotated training data, e.g., MS-MARCO (Bajaj et al., 2016) with above 500,000 examples. However, a recent study (Thakur et al., 2021) shows that even trained with enormous labeled data, dense retrievers still suffer from a generalization issue, where they perform relatively poorly on novel domains in comparison to BM25. Meanwhile, collecting human-annotated data for new domains is always hard and expensive. Thus improving dense retrievers with limited annotated data becomes essential, considering the significant domain variations of practical retrieval tasks.\nContrastive pre-training, which first generates pseudo-positive examples from a universal corpus and then utilizes them to contrastively pre-train retrievers, has shown impressive performance without any human annotations (Lee et al., 2019; Gao et al., 2021; Gao and Callan, 2022; Ram et al., 2022; Izacard et al., 2022) . For instance, Contriever (Izacard et al., 2022) crafts relevant querypassage pairs by randomly cropping two random spans within the same document. However, owing to the high information density of texts, even nearby sentences in a document can be very irrelevant, as shown in Figure 1 . These false positive samples may mislead the model to pull unrelated texts together in the embedding space and further harm the validity of representations.\nMotivated by recent findings in computer vision that pre-training performance can be greatly boosted by reducing the effect of such false positives (Peng et al., 2022; Mishra et al., 2022) , we propose Relevance-Aware Contrastive Retriever (Re-Contriever). At each training step, we utilize the trained models at the current step itself to estimate the relevance of all the positives. Then the losses of different positive pairs are adaptively weighed using the estimated relevance, i.e., the pairs that receive higher relevance scores obtain higher weight. Moreover, simply applying lower weights to irrelevant pairs will result in insufficient usage of data, since many documents will contribute less to training. Therefore, we also introduce a one-documentmultiple-pair strategy that generates multiple positive pairs from a single document, with a pairweighting process conducted among samples originating from a single document. Such an operation makes sure that the model can learn positive knowledge from every document in the corpus.\nTo summarize, our contributions in this paper are three-fold: 1) We propose relevance-aware contrastive learning for dense retrieval pre-training, which aims to reduce the false positive problem.\n2) Experiments show our method brings consistent improvements to the SOTA unsupervised Contriver model on 10/15 tasks on the BEIR benchmark and three representative open-domain QA retrieval datasets. 3) Further explorations show that our method works well given no or limited labeled data. Specifically, on 4 representative domainspecialized datasets it outperforms BM25 when only unsupervised pre-training on the target corpora, and with only a few annotated samples its accuracy can be on par with DPR (Karpukhin et al., 2020) which is trained on thousands of annotated examples.\n\nPreliminary\nIn this section, we briefly describe the bi-encoder structure used in dense retrieval and the SOTA Contriever model, on which we build our model.\nBi-Encoder Structure Dense retrievers are always a bi-encoder composed of two separate encoders to transform the query and document into a single vector each. The relevance score is obtained by computing the similarity (e.g., inner-product) between the encoded vectors of queries and documents. The typical way to train a dense retriever is using a contrastive loss that aims to pull relevant passages closer to the query and irrelevant passages farther in the embedding space. For each query, the training data involves one positive passage labeled by annotators and a pool of negative passages, which are usually random passages in the corpus.\nContriever It crafts pseudo-positive pairs by randomly cropping two spans of the same document. As negative texts have shown to be a key to the success of retrieval training (Xiong et al., 2021) , Contriever also applies the MoCo mechanism (He et al., 2020) to utilize negatives in the previous batches to increase the number of negatives. These two factors make Contriever obtain significant decent performance without any human annotations.\n\nRelevance-Aware Contrastive Learning\nWe start by 1) producing a larger number of positives (one-document-multi-pair) and 2) forcing the model to pay more attention to the ones with higher relevance (relevance-aware contrastive loss).\nOne-Document-Multi-Pair Given a text snippet T , previous pre-training methods always craft only one positive (query-passage) pair (q, d + ). To exploit T more effectively, our one-document-multipair strategy generates n positive pairs, denoting as {(q, d + 1 ), (q, d + 2 ), . . . , (q, d + n )}, from T by repeating the procedure several times. We keep the query q unchanged to ensure the relevance comparison is fair among pairs within the same snippet, which is used in our following step. Building upon Contirever, we craft n pairs by random cropping n + 1 spans and setting 1 span as the fixed query for the left n spans. And it is easy to extend this strategy to other contrastive pre-training methods.\nRelevance-Aware Contrastive Loss The ordinary contrastive loss for training dense retrievers is the InfoNCE loss. Given a positive pair (q, d + ) and a negative pool\nEQUATION\nwhere s(\u2022) and \u03c4 denote the similarity function and temperature parameter. Then the overall loss of a batch is usually the average across all the m \u00d7 n positive pairs from m snippets:\nL = 1 mn m i=1 n j=1 InfoNCE(q i , d + ij\n). The relevance-aware contrastive loss aims to force the model to focus more on true positive pairs by 1) utilizing trained model \u03b8 at present itself as an imperfect oracle to compute the relevance score s \u03b8 (q, d + ) between all pairs; and 2) adaptively assigning weights to different pairs according to the estimated relevance. Then the relevance-aware contrastive loss L relevance can be expressed as:\n1 1 m m i=1 n j=1 s \u03b8 q i , d + ij n k=1 s \u03b8 q i , d + ik InfoNCE(q i , d + ij ).\n(2) In this way, for each text snippet, positive pairs with more confidence to be relevant will thus be more focused on by the model, or vice versa.\n\nExperiments\nIn this section, we evaluate our model in several settings after describing our experimental setup. We consider unsupervised retrieval performance and two practical use cases: further pre-training on the target domain and few-short retrieval. We then conduct an ablation study to separate the impact of our method's two components.\n\nSetup\n\u2022 Datasets We evaluate retrieval models on the BEIR (Thakur et al., 2021) benchmark and three representative open-domain QA retrieval benchmarks: Natural Questions (NQ; (Kwiatkowski et al., 2019) ), TriviaQA (Joshi et al., 2017) and WebQuestions (Berant et al., 2013) .\n\u2022 Baselines We compare our model with two types of unsupervised models, namely models based on contrastive pre-training and on auto-encoding pretraining. The former models include SimCSE (Gao et al., 2021) , coCondenser (Gao and Callan, 2022) , Spider (Ram et al., 2022) and Contriever (Izacard et al., 2022) . The latter category includes the recently proposed RetroMAE (Xiao et al., 2022) . BM25 (Robertson and Zaragoza, 2009) and uncased BERT-base model (Devlin et al., 2019) are also involved for reference. We use the official checkpoints for evaluation.\n\u2022 Implementation Details We apply our method to the SOTA Contriever model and use its default settings. The pre-training data is a combination of Wikipedia and CCNet (Wenzek et al., 2020) , same as Contriever. We generate 4 positive pairs for each document. Refer to Appendix A for more details. We conduct a t-test with p-value 0.05 as threshold to compare the performance of ReContriever and our reproduced Contriever.\n\nBEIR\nThe NDCG@10 of ReContriever and other fully unsupervised models across 15 public datasets of BEIR are shown in Table 1 . ReContriever achieves consistent improvements over Contriever on 10/15 datasets, with a significant improvement observed in 9 of those datasets. Notably, it also only sees very slight decreases on datasets without promotion (e.g., FiQA, Touche and Climate-Fever with at most -0.1 decrease). Moreover, our method obtains an average rank of 2.2, proving our method to be the best unsupervised dense retriever. BM25 is still a strong baseline under the fully unsupervised scenario, but ReContriever greatly narrows the gap between dense retrievers and it.\n\nOpen-Domain QA Retrieval\nTable 2 shows the Recall performance of ReContriever on open-domain QA retrieval benchmarks, where supervised DPR (Karpukhin et al., 2020) is involved for reference. Obviously, ReContriever outperforms BM25 by a large margin except for Recall@5 and Recall@10 on TriviaQA with relatively smaller differences, verifying the effect of our method. Moreover, among all unsupervised methods, ReContriever obtains the best performance in nearly all cases, especially substantial improvement over Contriever. Our ReContriever promisingly narrows the gaps between supervised and unsupervised models, making it more valuable. \n\nPractical Use Cases\nIn this section, we explore the applicability of Re-Contriever in more practical scenarios \n\nAblation Study\nWe conduct an ablation study to investigate the contributions of our proposed loss and pairing strategies within ReContriever, using 100,000 training steps. Solely adding relevance-aware loss means estimating the relevance of N pairs from N documents and then normalizing the relevance among the N pairs within a batch, which slightly differs from equation ( 2) that normalizes over 4 pairs from the same document. As shown in Table 5 , solely adding relevance-aware contrastive loss to Contriever will lead to a noticeable degeneration, owing to the missing information from the documents with low adjusted weights and the unstable relevance comparison without a fixed query. Applying the one-document-multi-pair strategy can obtain a slight improvement which can be attributed to the effective usage of the unlabeled data. Combining both strategies (i.e., ReContriever) can lead to an obvious improvement, which demonstrates the necessity of both components in our method.\n\nConclusion\nIn this work, we propose ReContriever to further explore the potential of contrastive pre-training to reduce the demand of human-annotated data for dense retrievers. Benefiting from multiple positives from the same document as well as relevance-aware contrastive loss, our model achieves remarkable performance under zero-shot cases. Additional results on low data resources further verify its value under various practical scenarios.\n", "hypothesis": "Our method consistently improves the SOTA unsupervised Contriever model (Izacard et al., 2022) on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also outperforms DPR (Karpukhin et al., 2020) which is trained on thousands of annotated examples, even with no or limited labeled data.", "answer": false}
{"title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers", "content": "\nIntroduction\nThe communicative acts of humans in collaborative situations can be described as two parts of a joint act: signalling and recognizing. In such joint activities, these signals work as coordination devices to increment on the current common ground of the participants (Clark, 1996) . The ability to act on these language signals is crucial for future machine learning models to naturally collaborate and interact with humans (Lemon, 2022; Fern\u00e1ndez et al., 2011) . Such a collaborative interaction with humans usually happens fluently, where one communicative act is performed after the other. The framework of reinforcement learning (RL) (Sutton and Barto, 2018) describes such mechanics where an agent is exposed in steps to observations of an environment with dynamic factors such as the position of objects or language expressions. The goal is that the agent learns to behave generally well in Figure 1 : An exemplary interaction between a teacher and a follower that controls the gripper (the grey square). After an initial referring expression l RE at t 0 , the teacher provides feedback l FBt based on the follower's actions until the correct piece is selected at time step T . a particular environment solely based on the observations it makes and rewards it gets.\nA key challenge here is the variability of expressions in language that can be said to the agent during an interaction. Even in relatively simple environments, there might arise an overwhelming amount of situations for an agent to handle (Chevalier-Boisvert et al., 2019) . Recent work on collaborative agents focuses on large precollected datasets for imitation learning to learn agents in complex simulated visual environments (Gao et al., 2022; Padmakumar et al., 2022; Pashevich et al., 2021) or frames the learning as a contextual bandit problem (Suhr and Artzi, 2022; Suhr et al., 2019) . Nevertheless, other work has shown that intermediate language inputs are a valuable signal to improve the agent's learning performance in task-oriented visual environments (Co-Reyes et al., 2019; Mu et al., 2022) .\nIn this paper, we present an initial study that evaluates a follower's learning success given a teacher's intra-episodic feedback in a collaborative setting. We use a referential language game (in English) as a controllable example of a task-oriented collaborative joint activity (see Figure 1 ). In this game one player (the follower) is supposed to select a piece based on the another player's directives (the teacher). We assume a teacher that utters referring expressions as initial instructions and then responds to the follower's actions with intra-episodic feedback. We frame this as a RL problem with sparse rewards where the intermediate feedback is not part of the reward function but its potential usefulness is learnt by the follower alone. 1\n\nRelated Work\nVision and language navigation. In vision and language navigation, an agent is given a natural language instruction which is to be understood to navigate to the correct goal location in a visually observed environment (Gu et al., 2022) . The follower can usually ask an Oracle for further information, if necessary (Nguyen et al., 2019; Nguyen and III, 2019; Fried et al., 2018) . We extend on this idea and aim for an ongoing interaction with corrections that loosens the turn-based paradigm by letting the Oracle choose when to speak as part of the environment. Hence, in our reference game, the language back-channel for the follower is cut, so that we force the follower to rely more on the visual observations for task success.\nContinual learning from human feedback. Suhr and Artzi (2022) let humans instruct the follower and then ask them to rate the agent's behaviour (thumbs up or down). This binary feedback is used for further training as the reward signal in a contextual bandit framework. They show that the agent improves over several interactions with humans. Similarly we evaluate the learning process in the context of RL because it imposes \"weaker constraints on the regularity of the solution\" (Nguyen et al., 2019) , but take a broadly available, off-theshelf learning algorithm (Schulman et al., 2017) to directly study the effects of different kinds of feedback. The feedback given to our agent is of natural language and not directly bound to the re-ward; the follower needs to learn the meaning of the language feedback itself.\nLanguage-guided policy learning. Chevalier-Boisvert et al. ( 2019) compared the sampling complexity of RL and imitation learning (IL) agents on various language-conditioned tasks. They proposed a 2-dimensional visual environment called Minigrid in which an agent is given a single mission statement that instructs the agent to achieve a specific state, e.g. \"Take the red ball\". In contrast to them we intentionally do not use IL approaches, because then the agent would have already learnt how to ground the language signals. We want to test if the agent can pick-up on the language from the interaction alone. For this, we similarly propose a diagnostic environment to directly control for the distributions of target objects (cf. skewed distribution of target objects in CVDN (Thomason et al., 2019) ) and feedback signals.\nOther work uses the Minigrid environment to propose a meta-training approach that improves the learning via natural language corrections, e.g. \"Pick up the green ball\" (Co-Reyes et al., 2019) . The agent is given an episodic correction if a specific task cannot be solved. In this way, the agent must not only ground the mission statement but also ground the corrections into actions. Mu et al. (2022) improve policy learning with intra-episodic natural language sub-goals e.g. \"Pick up the ball\". These sub-goals are provided by a trained teacher policy when a previous sub-goal has been reached. In contrast, we rather follow earlier work (Engonopoulos et al., 2013) on monitoring execution and use a heuristic teacher which provides intraepisodic language feedback whenever it appears feasible. The agent has to learn that certain pairs of feedback and behaviour at a specific time-step lead to the task's success and others to failure.\n\nThe CoGRIP environment\nWe use a Collaborative Game of Referential and Interactive language with Pentomino pieces as a controllable setting. A teacher instructs a follower to select a specific piece using a gripper. Both are constrained as follows: The teacher can provide utterances but cannot move the gripper. The follower can move the gripper but is not allowed to provide an utterance. This asymmetry in knowledge and skill forces them to work together and coordinate. Zarrie\u00df et al. (2016) found that this settings leads to diverse language use on the teacher's side. \n\nProblem Formulation\nThe follower has to navigate a gripper to select a piece described by the teacher. We frame this task as a RL problem with sparse rewards. At each time-step t, given an observation o t \u2208 O of the environment, the agent has to select an action a t \u2208 {LEFT, RIGHT, UP, DOWN, WAIT, GRIP} such that the overall resulting sequence of actions (a 0 , ..., a t , ..., a T ) maximizes the sparse reward R(o T ) = r. An episode ends when the GRIP action is chosen, and the gripper position g t is in the boundaries of a piece. An episode also ends when t reaches T max = 100. Following Chevalier-Boisvert et al. ( 2019), the reward function returns a basic reward minus the movement effort R = 1 \u2212 0.9 * (T /T max ). We extend this formulation and give an additional bonus of +1 if the correct piece has been taken or a penalty of \u22121 when the wrong or no piece has been taken at all.\n\nEnvironment\nThe environment exposes at each time-step t an observation o t that contains the gripper coordinates g t = (x, y), the initial referring expression l RE , the language feedback l FBt (which might be empty) and a partial view v t of the scene. While the scene as a whole is represented as a 2-dimensional image (with RGB colour channel), the partial view represents a 11 \u00d7 11-sized cut out, centered on the gripper position (see Figure 2 ). The teacher generates the initial and feedback statements.\n\nTeacher\nFor the teacher, we assume a heuristic behaviour (a fix policy) that has been shown to lead to collaborative success with humans (G\u00f6tze et al., 2022) and leave the complexity of learning in a multiagent setting (Gronauer and Diepold, 2022) for future work. The teacher produces an initial referring expression l RE = (w 0 , ..., w N ) where N is the message length and w i is a word in the vocabulary. The production rule is implemented following the Incremental Algorithm (IA) (Dale and Reiter, 1995) that is given the symbolic representations of the pieces on the board (see Appendix A.1). The teacher provides a feedback message l FBt = (w 0 , ..., w N ) at a time-step t >0 when the gripper's position g t has exceeded a pre-defined distance threshold D dist = 3 compared to the gripper's last position of feedback g FB last or it is over a piece. The generated feedback is of positive sentiment (\"Yes this way/piece\") when the gripper is then closer to or over the target piece and negative otherwise (\"Not this direction/piece\"). Alternatively, suppose the follower does not exceed the distance threshold after D time = 6 time-steps the feedback message is the same as the initial statement. Overall, the property values and sentence templates lead to a small vocabulary of 33 words.\n\nFollower\nThe follower agent has to move the gripper and successfully grip a piece solely based on the observations. The observations o t = (v t , g t , l RE , l FBt ) are mapped to 128-dimensional features xt \u2208 R using the encoder model (see Figure 2 ). Following Chevalier-Boisvert et al. ( 2019), the word embeddings (which are learned from scratch) of the language inputs are fed through a Gated Recurrent Unit (GRU) (Cho et al., 2014) and then combined with the embedded visual features using a Featurewise Linear Modulation (FiLM) layer (Perez et al., 2018) . These language conditioned visual features are then max pooled, averaged and again averaged with the gripper position. Given the resulting features xt , we learn a parameterised policy \u03c0(x t ; \u03b8) \u223c a t that predicts a distribution over the action space. We use the Proximal Policy Optimization (PPO) (Schulman et al., 2017) implementation of StableBaselines3 v1.6.2 (Raffin et al., 2021) to train the policy in our environment.\n\nTasks\nThe follower has to grip an intended target piece among several other pieces (the distractors). Thus a task is defined by the number of pieces, the target piece and the map size. The pieces for the tasks are instantiated from symbolic representations: a tuple of shape ( 9), color (6) and position (8) which leads to 432 possible piece symbols. For our experiments we use all of these symbols as targets, but split them into distinct sets (Appendidx A.4). Therefore the targets for testing tasks are distinct from the ones in the training tasks. We ensure the reproducibility of our experiments by constructing 3300 training, 300 validation, 720 testing tasks representing scenes with a map size of 20 \u00d7 20 and 4 or 8 pieces.\n\nExperiments\nIn this section we explore the effects of the teacher's language and intra-episodic feedback on the follower's success and ask whether the follower generalizes on aspects of scene complexity.\n4.1 Which referential language is most beneficial for the agent's learning success?\nAs suggested by Madureira and Schlangen (2020) we explore the question of which language is most effective. The IA constructs the initial reference by following a preference order over object properties (Krahmer et al., 2012) . We hypothesize that a particular order might be more or less suitable depending on the task. Thus we conduct a series of experiments without the feedback signal where the preference order is varied as the permutation of color, shape and position. Our results indicate that such orders perform better that prioritize to mention positional attributes as distinguishing factors of the target piece (see Table 1 ). This is reasonable as the directional hint reduces the agent's burden for broader exploration. The follower is able to pick up early on these positional clues and performs overall better during training (see Figure 3 ).\n\n4.\n2 What is the agent's performance gain with intra-episodic feedback in our setting?\nWe conduct the same experiments as above with intra-episodic language feedback to measure its effect on the follower's success rate. Our results show that the follower achieves higher success rates with intra-episodic feedback among all preference orders (see Table 1 ). We also notice that the gain is higher for the low-performing preference orders. This shows that the intra-episodic feedback is a valuable signal for the follower to overcome miss-ing directives in the initial referring expressions.\nThe agent can learn strategies incorporating the feedback signals. This is an interesting finding because language feedback is not part of the reward function and could be empty.\n\nDoes intra-episodic feedback help the agent to generalize on scene complexity?\nAs a proxy for generalization capabilities, we take the best performing follower and raise the complexity of the testing scenes along two dimensions (i) we increase the map size to 30 \u00d7 30 and (ii) put up to 18 pieces on the board. In addition, we hold out 72 combinations of piece shapes and colors that have never been seen during training. Our results show that the agent trained with intra-episodic feedback is able to perform better (i) on the larger map size, (ii) the higher number of pieces and (iii) the new target pieces compared to the one without (see Table 2 ).\n\nConclusion\nIn this work, we studied the effects of a teacher's language and intermediate interventions (the feedback) towards a learner's success and whether the learner generalizes on aspects of scene complexity. Our results show that there is a most beneficial language for the teacher. Its intra-episodic feedback allows the learner to learn faster and generalize better than without intermediate help. An exciting direction for further work is to show the benefits of language feedback for other reinforcement learning problems, to overcome the limits of the heuristic teacher strategy and to reduce the need for feedback after successful training.\n", "hypothesis": " Our results show that intra-episodic feedback allows the follower to generalize on aspects of scene complexity and performs better than providing only the initial statement..", "answer": true}
{"title": "Compositional Mathematical Encoding for Math Word Problems", "content": "\nIntroduction\nThe task of math word problem (MWP) solving aims to map natural language problem descriptions into executable solution equations to get the correct answer, which is a sub-area of neuro-symbolic reasoning. It requires perceptual abilities such as comprehending the question, identifying the quantities and corresponding attributes, as well as complex semantics understanding skills like performing logical inference, making comparisons and leveraging external mathematical knowledge.\nWhile MWP encoders have been sophisticatedly designed to understand the natural language problem description, the difference on understanding diverse types of problems has not been aware of. We find that MWP can generally be grouped into three categories based on the keywords in (Liu et al., 2019) , i.e., \"Story Problem\", \"Algebra Problem\" and \"Knowledge Problem\". \"Story Problem\" often includes significant amount of background information like characters, objectives and behaviors.\n\"Algebra Problems\" involves math notations or is composed of elementary concepts. \"Knowledge Problem\" asks for external knowledge like geometry and number sequence, as shown in Figure 1 .\nThese types of problems can be compositionally understood at the different level attention to the semantics modal and quantity modal, where RNNs and pre-trained language models usually focus on the textual information and GCN with quantitycentered graphs capture the relationship between quantities and contexts. However, the encoders in existing MWP solvers either model only the semantics modality or utilize quantity modal priors to refine the MWP encoding (Zhang et al., 2020; Shen and Jin, 2020) . Although the quantity centered refinement can particularly make improvements on quantity-demanding problems, its semantics understanding is weakened (evidence can be found in Table 3 ). This limitation, one joint modal cannot do it all, decreases the generalization of MWP solvers and is what compositional learning aims to address. In this work, we propose to disentangle semantics modal and quantity modal by compositional learning at the encoding stage, aiming to improve the generalization across different types of problems.\nContributions. (i) A novel and effective bimodal approach is proposed for the first time to enable MWP compositional understanding. (ii) A joint reasoning module is designed for our bimodal architectures to flexibly incorporate different modalities. (iii) Extensive experiments and ablative studies on two large-scale MWP benchmarks -Math23k (Wang et al., 2017) and MAWPS (Koncel-Kedziorski et al., 2016) show the superiority of the proposed approach over related works.\nText : 348 teddy bears are sold for $23 each. There are total 470 teddy bears in a store and the remaining teddy bears are sold for $17 each. How much did the store earn after selling all the teddy bears?\nEquation: 348\u00d723 + 470 \u2212 348 \u00d717\nQuantities: [348, teddy bears, sold] ; [$20, each] ; [470, teddy bears, total] ; [$17, remaining] .\n\nSemantics:\nSome teddy bears have been sold at a price; The left part will be sold at a different price; The goal is to compute the expect income.\nText : 2 times A is the same as 3 times B. B equals 28.\nCompute A.\n\nEquation: 28\u00d73 \u00f7 2\nQuantities: [2, times]; [3, times] ; [28, equals] .\n\nSemantics:\n2 times A equals 3 times B; B equals 28.\n\nStory Problem:\nAlgebra Problem: \n\nRelated Work\nCompositional Learning in NLP. Modeling compositionality in language has been a longstanding issue (Wong and Wang, 2007) in NLP community. One common practice is to perform disentanglement over language representations at different levels (Welch et al., 2020) .. They usually focus on atomic semantics units like character, word and phrase. As logic form annotations naturally own compositional features, compositionality is incorporated in generating correct logic contents. Therefore, the compositionality is often injected into traditional semantic parsing tasks (Chen et al., 2020; Yang et al., 2022) where the goals during training can be decomposed and then reorganized as a novel goal.\nOur work firstly tries to inject compositional prior into MWP encoding. It is worth noting that MWP solving owns the same well-organized logic form annotations as machine reasoning, which naturally requires compositionality.\nMath Word Problem Solving. Earlier MWP solvers parse problem descriptions semantically, and learn templates for generating answers (Koncel-Kedziorski et al., 2015) . Recent works (Wang et al., 2017; Xie and Sun, 2019; Li et al., 2019; Zhang et al., 2020; Shen and Jin, 2020; Wu et al., 2021b,a; Lin et al., 2021; Liang and Zhang, 2021; Jie et al., 2022) focus on employing the encoderdecoder framework (e.g., sequence-to-sequence, sequence-to-tree, graph-to-tree) to translate MWP texts into equations based on traditional RNN structure. There are also new settings (Amini et al., 2019; Miao et al., 2020) introduced to extend MWP solving in equation group generation and diagnosing awareness of external knowledge. Nowadays, many researchers build strong MWP solvers upon pre-trained language models (PLMs) (Huang et al., 2021; Li et al., 2021; Yu et al., 2021; Shen et al., 2021; Lan et al., 2022) and have achieved great performance. Differently, our work lays the groundwork of feature extraction of quantity modal, which is orthogonal to those works.\nIn this work, we not only propose an explicit compositional encoding module with a multi-layer design, but also incorporate detailed analysis to verify its compositional learning ability, to jointly leverage semantic and quantity information to achieve effective MWP understanding.\n3 Our approach\n\nCompositional Mathematical Encoder\nAs shown in Figure 2 , our CMEncoder block consists of a semantic encoder, a quantity encoder and a dynamic fusion block. The semantic encoder aims to extract semantic information from the problem description, understanding the background and objectives. The latter part encodes problems only with quantity-related graphs, helping the encoder to know the properties of quantities and the relationship between quantities and contexts. Semantic Encoder. To demonstrate the robustness of our approach, we implemented two different semantic encoders as our backbone. Firstly, we encode the problem description W by a bidirectional gated recurrent unit (BiGRU) (Cho et al., 2014) . The outputs of GRU are hidden state vectors of all tokens, H r = {h 1 , h 2 , ..., h n }, where n is the length of problem W .\nEQUATION\nwhere Embed s (W ) is the embedding result of textual description W in semantics modal. Empirically, we find that two stacked CMEncoders as shown in Figure 2 achieve the best performance. Secondly, pre-trained language models (PLMs) have been ubiquitous in NLP tasks. We use the latest push of MWP-BERT (Liang et al., 2022) as our semantic encoder to obtain H r .\nQuantity Encoder. To encode the quantity modal in the problem W , we feed a graph transformer G trans with Quantity Comparison Graph and Quantity Cell Graph following Graph2Tree (Zhang et al., 2020) ,\nEQUATION\nwhere Embed q (W ) is the embedding matrix in the quantity modal, which aims to improve the quantity representation by incorporating quantity magnitude information and quantity-context relationship with the above two graphs. Different from Graph2Tree, the two embeddings Embed s (W ) and Embed q (W ) are updated in the training process to extract the semantics and quantity feature separately. In this way, semantics and quantity modals are disentangled, which alleviates the issue of \"one joint modal cannot do it all\", enabling the C-MWP solver to pay different levels of attention when solving different problems.\nDynamic Fusion. To achieve joint reasoning over the semantics information and quantity information, we design a dynamic fusion module to flexibly incorporate the features from these two modals. First, we get s and q from the mean pooling of H r and H g , respectively. Then, cross-modal attention is applied between H r and q, H g and s:\nAtt\n1 (H r , q) = \u03a3 n i=1 a i H ri Att 2 (H g , s) = \u03a3 n i=1 b i H gi (3)\nwhere the attention scores a i , b i come from:\nEQUATION\nwhere W 1 a , W 2 a , W 1 b and W 2 b are parameter matrices. The cross-modal attention here grounds the quantity information in the semantics modal, and vice versa. By applying different weights on different modals, our model is flexible to pay more or less attention on a certain modal. Finally, the output of dynamic fusion is:\nH f = Att 1 (H r , q) Att 2 (H g , s).\n(5)\n\nStack Multiple CMEncoders\nHumans often need to make multiple glimpses to refine an MWP solution. Similarly, a CMEncoder can be stacked in multiple steps to refine the understanding of an MWP, as shown in Figure 2 . Given the output from the semantic encoder, quantity encoder and dynamic fusion module at layer k \u2212 1, the features are stacked as:\nH (k\u22121) att = c r H (k\u22121) r + c g H (k\u22121) g (6)\nwhere the attention weights c r and c g are:\nEQUATION\n))\nEQUATION\nwhere W 1 r , W 2 r , W 1 g and W 2 g are parameter matrices. The H (k\u22121) att will be the input for both the semantic modal and quantity modal of K-th CMEncoder, which will output\nH (k) r , H (k) g and H (k)\nf , which can be sent for the update at layer k + 1.\nAfter finishing the K-th step reasoning, we concatenate the final H (K) r and H (K) g as the final output representation H f inal .\n\nDecoder\nWe follow the same implementation as GTS (Xie and Sun, 2019) . Eventually, the decoder will output the pre-order traversal sequence of the solution tree.\n\nTraining Method\nGiven the training samples with problem description W and the corresponding solution S, the main training objective is to minimize the negative log probability for predicting S from W , empowered by the compositionality of the CMEncoders. Therefore, the overall loss is:\nL = L M W P + Embed s 2 + Embed q 2 (8)\nwhere L M W P is the negative log prediction probability \u2212 log p(S | W ). The L 2 norm of the encoder embedding matrices is added to the loss function as regularization terms.\n\nDatasets\nMath23k (Wang et al., 2017) \n\nExperimental Results\nAs Table 1 shows, our approach outperforms all other RNN-based baselines in terms of answer accuracy On Math23k, we outperform the latest RNNbased push from Wu et al. (2021a) by 1.8%. For the first time, an RNN-based MWP solver reaches over 80% answer accuracy on the Math23k dataset. What is more, the even fewer parameters with the best performance suggest that our model is also memory-efficient.\nPLM-based solvers benefit from the pre-training on a huge amount of corpus and thus achieve great semantic understanding ability. From a different point of view, our work aims to effectively and efficiently integrate semantic and quantity understanding. Therefore, by incorporating the MWP-BERT model as our semantic extractor, the answer accuracy of C-MWP achieves state-of-the-art performance. It proves the feasibility of combining the feature from the GNN Encoder of Graph2Tree.\nPerformance on Different Types of MWP.\nIn order to investigate how our model performs across various types of MWP, we introduce a new split of Math23k with regard to three types of problems: story problems, algebra problems and knowledge problems. Split details are shown in the appendix. The evaluation results are presented in Table 3 . Without a compositional manner, Graph2Tree and Multi-E/D perform better than GTS on story and algebra testing problems, whereas they perform worse on knowledge problems. As stated before, one joint modal cannot do it all. These baselines work well on some types of problems while having weak performance on other types of problems. Our C-MWP offers a general accuracy improvement, which firmly supports our motivation for alleviating the generalization issue. This provides clear evidence that our model leverages general math knowledge across different types of MWP, successfully solving some nontrivial problems that Graph2Tree failed to solve.\n\nConclusion and Future Work\nThe semantic meaning and quantity information are important intrinsic properties of a math word problem. Aiming at dealing with uni-modal bias and achieve better generalization, we make the first attempt to propose a compositional MWP solver, C-MWP. Multi-layer reasoning and specified training methods are leveraged to enhance the generalizability of the model. As the method could be applied in a broader range of neuro-symbolic learning problems, we will keep exploring the adaptiveness of this compositional encoding method.\n", "hypothesis": "Extensive experiments validate the effectiveness of C-MWP and show its inferiority compared to state-of-the-art models on public benchmarks.", "answer": false}
{"title": "Open-World Factually Consistent Question Generation", "content": "\nIntroduction\nQuestion generation is the task of generating a question that is relevant to and answerable by a piece of text (Krishna and Iyyer (2019) , Chen et al. (2020) , Zhu and Hauff (2021) , Ushio et al. (2022), ) . It is an important task in language generation (Fabbri et al. (2020) , Yu et al. (2020b) ), education (Wang et al. ( 2022)), and information retrieval (Yu et al. (2020a) ). A critical metric for question generation is factual consistency, i.e., the question has facts that are derivable from the input paragraph. This work proposes novel methods to improve entitylevel factual consistency while agnostic to model and underlying training data. Nan et al. (2021) and Xiao and Carenini (2022) solve a similar problem for summarization. However, to the best of our knowledge, no work addresses the issue of entitylevel factual inconsistency for question generation. Nema and Khapra (2018) have shown that name entities are essential for a question's answerability. The presence of wrong entities may make the question nonsensical and unanswerable. Table 1 shows entity-level factual inconsistency in question generation by a fine-tuned PEGASUS (Zhang et al., 2019) model. In the first entity, \"Kim Jong Un\", and in the second example, \"Chicago\" are hallucinated.\nUnlike previous work in the summarization field (Nan et al. (2021) , Liu et al. (2021a) , Xiao and Carenini (2022) ), our work is independent of the model or training process. We also do not reduce dataset size by filtering. Instead, we preprocess datasets to force the model to generate questions faithful to the input using strategies of de-lexicalization and multi-generation and recommend the best strategy. The proposed method improves the factual consistency by 84\u2212100% across multiple datasets while having minimal impact on traditional performance metrics.\nWe experimented with two popular language models viz. PEGASUS-large and BART-large (Lewis et al., 2020) . Our proposed approach consistently performs better for both the language models than normal finetuning. We also compare our approach to recent methods for addressing hallucination in summarization, and our results showed significantly better performance.\n\nRelated Work\nEarly work proposed rule-based approaches to generate questions (Labutov et al. (2015) ). Recent work using neural models for high-quality generation (Du and Cardie (2017) , Du et al. (2017) , Wang et al. (2022) ). Several works have aimed to control hallucination in different NLP tasks like summarization (Cao et al. (2018) , Song et al. (2020 ), Zhu et al. (2021) ), Wu et al. (2021) ), generative question answering (Bi et al. (2019) , Fan et al. (2019a) , Su et al. (2022) ), data to text (Nie et al. (2019) , Wang et al. (2020) , Wang et al. (2021) ), and machine translation (Xu et al. (2019) , Wang and Sennrich (2020) , Zhou et al. (2021) ).\nInput: As the president of South Korea, she turned out to be heavily influenced by a cult and giving them access to political documents and basically letting that cult run the country through her. One of her \"speech-writers\" was a religious figure in this cult that totally exploited the president, who pretty much only got elected because her father had been president, using her for money and power and wealth. The people of South Korea got upset when they learned that the real power that was running their country was the religious group that their president belonged to.\nNormal Finetuning: What did Kim Jong Un do wrong? Rare word delexicalization + Multiple (Ours): Why did the people of South Korea vote out the president of the South Korea?\nInput: As I understand it, there's no established *process* for removing the mayor from his post. If he doesn't resign, there's no mechanism for removing him. Really, I don't understand how this guy hasn't lost his job yet.\nNormal Finetuning: Why hasn't the mayor of Chicago been fired yet? Rare word delexicalization + Multiple (Ours): Why hasn't the mayor been fired yet?\nTable 1 : Qualitative Examples. For detailed analysis refer to section 6.4.\nPrevious work has explored entity-based delexicalization in settings like adapting parser for a new language (Zeman and Resnik, 2008) , valid reasoning chains in multi-hop question answering (Jhamtani and Clark, 2020) , and eliminating diachronic biases in fake news detection (Murayama et al., 2021) .\n\nMethodology\nThe objective is to generate relevant and entitylevel factually consistent questions which generalise across domains. For this, we propose novel de-lexicalization strategies combined with a multigeneration strategy. De-lexicalization involves replacing named entities with a special token or rare words during training/inference and replacing the original word after generation. The model's vocabulary is expanded to account for the special tokens used in the de-lexicalization strategies.\n\nDe-lexicalization Strategies During Training\n[Name i] Token: This strategy replaces the named entity with a token [Name i], where i represents the order of the first appearance of the entity in the paragraph and in the question.\n[Name i] Token with Push: This strategy is similar to the previous one. The difference is that if the question has a named entity that is not present in the input paragraph, we replace it with [Name j], where the j is a random number between 0 and the total number of named entities in the input paragraph. The intuition here is that we are pushing or explicitly asking the model to generate a named entity already present in the input paragraph.\n[Multiple i] Token: The previous two strategies treat all the named entities as similar. In contrast, in this approach, the entity is replaced with its corresponding semantic tags, followed by an integer representing its order of appearance in the paragraph followed by the question. A semantic tag specifies if an entity is name, organization, loca-tion, cardinal, etc.\n[Multiple i] Token with Push and Delete: This approach is similar to [Name i] Token with Push approach with multiple entity types. However, if the question consists of a named entity type not present in the paragraph, it is deleted.\nRare Word token: This strategy de-lexicalizes only the questions. Here we replace the named entities in questions that do not occur in the input paragraph with a rare word. A rare word is a word that occurs 2 to 5 times in the entire training corpus. If an entity occurs in the input paragraph, it is left as it is.\nExamples showing different de-lexicalization strategies are present in the Appendix.\nEntity Replacement: During testing, from the generated questions, the entities are replaced using a dictionary look-up of the special token. We treat a output as hallucinated if the special token has no corresponding named entity.\nMulti-generation: Here, we generate multiple questions during inference by selecting the top five beams from the output of the language model and selecting the one that is factually consistent and has the least perplexity. If no questions are consistent, the generation with the least perplexity is chosen. input.\nIn the [Name i] Token strategy, we replace all named entities with [Name i]. Do note that name entity, 55,000, and 2018 occur twice. Each occurrence is replaced with the same token, i.e., both occurrence of 55,000 is replaced with [Name 3]. Since the \"U.S.\" does not occur in the input, we replace it with [Name 5]. Contrary to this, in the [Name i] Token with Push strategy, we replace the U.S. with [Name 3], thereby pushing the model to be faithful to the source.\nIn the [Multiple i] Token strategy, instead of replacing named entities with a common [Name] token, we replace them with their semantic token. Thus, 55,000 is replaced with [MONEY 1] and so on. Like before, each occurrence is replaced with the same token. The U.S. is replaced with [GPE 0] as no entity of type GPE occurs in the input. Contrary to this, the [Multiple i] Token with Push and Delete strategy deletes the entity \"U.S.\" as no GPE-type entity exists in the input. If there were a GPE entity in input (not necessarily \"U.S.\"), it would have been replaced with [GPE 0].\nIn the Rare Word Token strategy, the input is unchanged. Since the U.S. does not occur in input, it is replaced with a rare word (aster).\n\nDatasets\nWe use the supervised ELI5 dataset (Fan et al., 2019b) for training. To ensure that the data is of high quality, we remove all the samples where the answer is short (having less than 50 words), or the question does not have a question mark.\nWe use three publicly available datasets for evaluation across different domains, viz. MS Marco (Bajaj et al., 2016) , Natural Questions (Kwiatkowski et al., 2019) and SciQ (Welbl et al., 2017) . We also scraped r/AskLegal 1 , and r/AskEconomics 2 for testing on finance and legal domains. Table 2 shows the statistics of the dataset.\n\nImplementation Details\nWe use publicly available checkpoints of the language models and fine-tune them for 100k steps with a batch size of 12 and using the Adam optimizer (Kingma and Ba, 2014) . The learning rate is set to 10 \u22125 , and the models are tested on the dev set every 10k steps. The best-performing model on the dev set is used. The model training takes approximately 6 hours on an Nvidia A100 40 GB GPU. Following Nan et al. (2021) we use the Spacy library 3 to identify named entities.\n\nEvaluation Metrics\nWe evaluate both the quality and factual consistency of the generated question. The quality is reported using Rouge-1, Rouge-2, Rouge-L (Lin, 2004) scores and cosine similarity between embedding (from all-mpnet-base-v2 sentence transformer model (Reimers and Gurevych, 2019)) of generated questions and ground truth. We use the perplexity value suggested by Liu et al. (2021b) , using a GPT-2 (Radford et al., 2019) . To evaluate factual consistency, we use two metrics. The first metric quantifies the degree of hallucination with respect to the ground truth question. We use the precision, recall, and F1 score proposed by Nan et al. (2021) . More details about the exact implementation are in the appendix or in their paper. The second metric quantifies the degree of hallucination with respect to the input paragraph. This metric measures, out of all the questions that have named entities, what percentage of questions have named entities not present in the input. Let N hne represent the number of generated questions with a named entity, and N wne represent the number of generated questions with a wrong named entity. N total represents the total number of questions. Do note N total \u0338 = N hne , as we can have questions with no named entity in them. Then N hne /N total * 100 represents the percentage of questions having a named entity (P ne ), and N wne /N hne * 100 represents the percentage of questions having the wrong named entity (P wne ). A system with a low P wne value and a high F1 score reflects the system is not hallucinating. We want a system with high factual consistency without significantly affecting the quality of the questions as measured by the proposed metrics.\n\nBaseline\nWe compare our results with the Spancopy method proposed by Xiao and Carenini (2022) for the summarization. We test with and without global relevance in Spancopy having PEGASUS as the base language model.\n\nResults and Analysis\nDue to space constraints, we only present results for PEGASUS-large in the main text. Results for BART-large can be found in the appendix.\nTable 4 shows the results of the test set of the ELI5 dataset. The results indicate that the rare word de-lexicalization plus multiple generation approach performs much better than other methods. Compared to a normal fine-tuned PEGASUS model, the P wne score decreases by about 98%, implying that the generated questions are faithful to the input text. Similarly, the F1 score increases by approximately 21%, implying that all the generated questions are faithful to ground truth. In contrast, decrements in other metric scores are less than 6.7%. Overall, rare word de-lexicalization plus multiple generation performs the best in terms of factual consistency and is comparable in other metrics. For detailed analysis refer to section 6.4.\nC.S. \u2191 R-1 \u2191 R-2 \u2191 R-L \u2191 PPL \u2193 Pne Pwne \u2193 Recall \u2191 Precision \u2191 F1 \u2191 Dataset:\nThe rare word de-lexicalization with multigeneration approach consistently performs better than all the other approaches for all the datasets. Table 5 compares rare word delexicalization + multiple generation with a normal finetuned PEGA-SUS and Spancopy without global relevance across different datasets. Detailed results for all the approaches across all the datasets are in the appendix.\nFrom the table, it can be seen that rare word delexicalization with multiple generations solves the issue of entity-level inconsistency without negative impact on different metrics. The model was just trained for the ELI5 dataset and was directly used for other datasets. Domain shift exacerbates the issue of entity hallucination, as shown by the P wne value for a normal fine-tuned PEGASUS model, which is usually higher in the presence of domain shift. Thus, our proposed approach works across domains without re-training.\nWe see that the P ne value decreases across all the datasets for rare word delexicalization with multiple generations. However, this is not wrong. A question without a named entity can still be a valid question (Nema and Khapra, 2018) .\nTable 1 shows qualitative examples. In the first example, the fine-tuned PEGASUS produces the entity Kim Jong Un that is unfaithful to the source and is entirely unrelated to South Korea. Chicago is hallucinated in the second example. In both examples, our proposed approach generates meaningful and faithful questions. Our approach produces a question with no named entity in the second example, yet the question is meaningful and faithful to the source. This further reinforces our claim that a question without a named entity can still be valid. More outputs can be found in the appendix.\nOur approach performs better than the Spancopy architecture (both with and without global relevance). This shows that simple de-lexicalization with multiple generations is better than sophisticated architecture.\n\nConclusion\nIn this paper, we study the entity-level factual inconsistency in question generation. Our proposed strategy, rare-word de-lexicalization with multigeneration, improve consistency without significantly affecting traditional metrics across data domains. Extensive experimental results further reinforce our claim.\n", "hypothesis": " In this work, we propose an effective data processing technique based on de-lexicalization for consistent question generation across domains.  Unlike existing approaches for remedying hallucination, the proposed approach does not filter training data and is generic across question-generation models.", "answer": true}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": "Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors. State-of-the-art UE methods for seq2seq models rely on computationally lightweight and practical deep ensembles.", "answer": false}
{"title": "Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information", "content": "\nIntroduction\nKeyphrase extraction is the fundamental task of automatically extracting a set of salient phrases from a document that concisely describes its primary content (Hasan and Ng, 2014; Song et al., 2023a) . Figure 1 shows an example of the source document and its corresponding keyphrases.\nRecent developments in pre-trained language models (Devlin et al., 2019) have heightened the need for utilizing pre-trained embeddings on natural language processing tasks, which significantly improves the performance of embedding-based unsupervised keyphrase extraction models (Sun et al., 2020; Liang et al., 2021; Zhang et al., 2022) . Existing embedding-based models mainly consist of two components: candidate keyphrase extraction and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2021 Song et al., , 2022a)) . The former extracts continuous words from the document as candidate keyphrases through heuristic rules, and the latter estimates the importance of candidate phrases by matching similarity with their corresponding document.\nGenerally, the source document has both salient information and noises (redundant content). Hence, there may be a deviation when directly using the phrase-document relevance as the importance score of each candidate to select keyphrases. For many specific-domain documents (e.g., news or scientific articles), the highlights (the title or the first sentence) typically contains the central information of the source document (as shown in Figure 1 ), which has more significant guidance for extracting keyphrases. However, the recent embedding-based unsupervised keyphrase extraction models ignore the effect of the highlight information, leading to extract wrong keyphrases.\nMotivated by the above issues, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which estimates the impor- \n\nCandidate Keyphrase Extraction\nTo extract candidate keyphrases from the source document, we follow the previous studies (Liang et al., 2021; Song et al., 2022b; Ding and Luo, 2021) At the same time, we use the mean pooling operation to obtain the highlight representation h s of the document.\n\nPhrase-Document Relevance\nTo obtain more relevant candidates, we model the similarity between candidate phrases and the corresponding document as follows,\nEQUATION\nwhere p h i denotes the phrase-document relevance of i-th candidate keyphrases and ||\u2022|| 1 indicates the Manhattan Distance.\nFor news and scientific articles, keyphrases often appear at the beginning or front position (Florescu and Caragea, 2017a,b) , which means that the position information is important and indicative for extracting keyphrases. For example, the word appearing at 2-th, 5-th and 10-th, has a weight \u03c1 i = 1/2 + 1/5 + 1/10 = 0.8. Inspired by the previous work (Florescu and Caragea, 2017b; Liang et al., 2021) , we adopt a position regularization as follows, \u03c1 i = softmax(e 1/i ), where \u03c1 i is the position regularization factor of the i-th candidate phrase. Then, the weighted phrase-document relevance ph i can be re-calculated as follows,\nEQUATION\nHere, we finally employ ph i to estimate the phrasedocument relevance of the i-th candidate phrase.\n\nCross-Phrase Relevance\nGenerally, the phrase-document relevance is calculated between the highlight information and each candidate independently, and consequently, it cannot determine which candidates are better than the Model DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Statistical Keyphrase Extraction Models TF-IDF (Jones, 2004) others. To determine which candidate phrases are more salient than the others, we sum the semantic relatedness between the i-th candidate phrases and all candidates as the cross-phrase relevance. Thus, it calculates the local relevance as follows,\nEQUATION\n)\nwhere \u03b4 i = Mean( j=1,j =i h p i h p j ). Here, we treat \u03b4 i as a de-noisy factor to filter the noises, which is far different from the i-th candidate keyphrase in the document.\n\nRelevance Aggregation\nWe aggregate the phrase-document relevance and the cross-phrase relevance into a whole score as the importance score of each candidate via a simple multiplication,\nEQUATION\n)\nwhere r i indicates the importance score of the i-th candidate phrase. Then, we rank all candidates with their importance score r i and extract top-ranked k phrases as keyphrases of the source document.\n3 Experiments and Results\n\nExperimental Settings\nThis paper conducts experiments on three benchmark and popular used keyphrase datasets, which includes DUC2001 (Wan and Xiao, 2008) , Inspec (Hulth, 2003 ), and SemEval2010 (Kim et al., 2010) . Due to page limits, please refer to the corresponding articles for the details of the three datasets. Following the previous work (Liang et al., 2021; Ding and Luo, 2021; Song et al., 2023b) , we use the standard practice and evaluate the performance of our model in terms of f-measure at the top-K keyphrases (F1@K) and adopt stemming to both extracted keyphrases and gold truth. Concretely, we report F1@5, F1@10, and F1@15 of each model on three benchmark datasets.\nWe adopt the pre-trained language model BERT (Devlin et al., 2019) as the backbone of our model, initialized from their pre-trained weights. In our experiments, \u03bb is set to 0.9 for three benchmark datasets.\n\nOverall Performance\nTable 1 shows the performance of baselines and our model on three benchmark datasets (DUC2001, In-\n\nDUC2001\nInspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Table 2 : The results of different pooling methods for document embedding.\nDifferent Similarity Measures DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 spec, and SemEval2010). The results show that our method significantly improves over state-of-the-art unsupervised keyphrase extraction baselines. Compared with the current state-of-the-art models, our model achieves significantly better performance on F1@5, F1@10, and F1@15 evaluation metrics, demonstrating the effectiveness of estimating the importance of candidate phrases by leveraging the highlights to calculate the relevance.\nCompared with EmbedRank (Bennani-Smires et al., 2018) , KeyGames (Saxena et al., 2020) , and SIFRank (Sun et al., 2020) , HGUKE achieves significant improvement, which benefits from using the highlights to calculate the importance score of each candidate keyphrase. Compared with the best baseline JointGL, our model achieves better performance on several benchmark keyphrase extraction datasets in all evaluation metrics. The main reason for this improvement is that we use the highlights as the guidance information instead of the whole document when estimating the importance of keyphrases.\n\nAblation Test\nThe ablation experiments on three benchmark keyphrase extraction datasets are shown in Figure 3 . It can be seen from the results that using the highlight information can significantly improve the performance of keyphrase extraction, which benefits from estimating the importance score of each candidate by using its corresponding highlight information rather than the whole document. We consider the main reason is that the title or the first sentence of the document usually has a strong guidance for extracting keyphrases.\n\nImpact of Pooling Methods\nIn this section, we study different pooling methods, including mean-and max-pooling operations. For all pooling methods, HGUKE using the last BERT layer achieves the best results, demonstrating that HGUKE benefits from stronger contextualized semantic representations. We can see the results in Table 2 that the document encoded via the meanpooling operation obtains the best performance.\n\nImpact of Different Similarity Measures\nOur model adopts Manhattan Distance to measure the textual similarity between candidate phrases and the highlight information. Furthermore, we attempt to employ different measures to estimate the phrase-document relevance. The results of different similarity measures are shown in Table 3 , and we can see that the advantage of Manhattan Distance is obvious.\n\nRelated Work\nMost existing unsupervised keyphrase extraction methods can be mainly divided into four categories: statistics-based, topic-based, graph-based, and embedding-based models. Specifically, statisticsbased models (Salton and Buckley, 1988; Witten et al., 1999) usually extract keyphrases by estimating the importance of candidate phrases with different statistic features, such as word frequency feature, phrase position feature, linguistic features of natural language, etc. Topic-based models (Liu et al., 2009 (Liu et al., , 2010) ) typically utilize topic information to determine whether a candidate phrase is a keyphrase. Graph-based models (Mihalcea and Tarau, 2004; Grineva et al., 2009) represent the document as a graph and rank candidate phrases by graph-based similarities.\nEmbedding-based models usually adopt the pretrained embeddings to obtain document and candidate phrase representations and calculate the importance score of each candidate depending on the obtained representations. Benefiting from the development of transformer-based pre-trained language models (Devlin et al., 2019) in the natural language processing field, embedding-based models (Bennani-Smires et al., 2018; Sun et al., 2020; Liang et al., 2021) have achieved outstanding performance. Concretely, embedding-based models mainly consist of two procedures: candidate keyphrase representation and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2023a) . The procedure utilizes natural language linguistic features to construct candidate keyphrases and represents them by pre-trained embedding approaches (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) ). The second procedure estimates the importance of candidate phrases from different perspectives to determine whether a candidate phrase is a keyphrase.\nUnlike the existing unsupervised keyphrase extraction models, we use the highlight information of the document to calculate the phrase-document relevance instead the whole document.\n\nConclusion and Future Work\nIn this paper, we incorporate structural information to improve the performance of embedding-based unsupervised keyphrase extraction. Specifically, in this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which calculates the phrase-document relevance via the highlight information instead of the whole document to select relevant candidate phrases. Extensive experiments demonstrate that HGUKE outperforms the state-of-the-art unsupervised baselines. Future research may investigate adopting different structural information of the source document to improve the performance of unsupervised keyphrase extraction.\n", "hypothesis": " However, when extracting keyphrases from the document, most existing embedding-based unsupervised keyphrase extraction models ignore the indicative role of the highlights in certain locations, leading to wrong keyphrases extraction.  In this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE) to address the above issue.", "answer": true}
{"title": "Do GPTs Produce Less Literal Translations?", "content": "\nIntroduction\nDespite training only on a language-modeling objective, with no explicit supervision on aligned parallel data (Briakou et al., 2023) , LLMs such as GPT-3 or PaLM (Brown et al., 2020; Chowdhery et al., 2022) achieve close to state-of-the-art translation performance under few-shot prompting (Vilar et al., 2022; Hendy et al., 2023) . Work investigating the output of these models has noted that the gains in performance are not visible when using older surface-based metrics such as BLEU (Papineni et al., 2002a) , which typically show large losses against NMT systems. This raises a question: How do these LLM translations differ qualitatively from those of traditional NMT systems?\nWe explore this question using the property of translation literalness. Machine translation systems have long been noted for their tendency to produce source He survived by the skin of his teeth .\n\nNMT\nIl a surv\u00e9cu par la peau de ses dents . GPT-3 Il a surv\u00e9cu de justesse . Table 1 : An example where GPT-3 produces a more natural (non-literal) translation of an English idiom. When word-aligning these sentences, the source word skin remains unaligned for the GPT-3 translation.\noverly-literal translations (Dankers et al., 2022b) , and we have observed anecdotally that LLMs seem less susceptible to this problem (Table 1 ). We investigate whether these observations can be validated quantitatively. First, we use measures based on word alignment and monotonicity to quantify whether LLMs produce less literal translations than NMT systems, and ground these numbers in human evaluation ( \u00a7 2). Next, we look specifically at idioms, comparing how literally they are translated under both natural and synthetic data settings ( \u00a7 3).\nOur investigations focus on the translation between English and German, Chinese, and Russian, three typologically diverse languages. Our findings are summarized as follows: (1) We find that translations from two LLMs from the GPT series of LLMs are indeed generally less literal than those of their NMT counterparts when translating out of English, and (2) that this is particularly true in the case of sentences with idiomatic expressions.\n\nQuantifying Translation Literalness\nWe compare the state-of-the-art NMT systems against the most capable publicly-accessible GPT models (at the time of writing) across measures designed to capture differences in translation literalness. We conduct both automatic metric-based as well as human evaluations. We explain the evaluation and experimental details below. for evaluation (Barrault et al., 2021) .\n\nMeasures of Quality\nWe use COMET-QE 1 (Rei et al., 2020) as the Quality Estimation (QE) measure (Fomicheva et al., 2020) to quantify the fluency and adequacy of translations. Using QE as a metric presents the advantage that it precludes the presence of any reference bias, which has been shown to be detrimental in estimating the LLM output quality in related sequence transduction tasks (Goyal et al., 2022) . On the other hand, COMET-QE as a metric suffers from an apparent blindness to copy errors (i.e., cases in which the model produces output in the source language) (He et al., 2022) . To mitigate this, we apply a language identifier (Joulin et al., 2017) on the translation output and set the translation to null if the translation language is the same as the source language. Therefore, we name this metric COMET-QE + LID.\n\nMeasures of Translation Literalness\nThere do not exist any known metrics with high correlation geared towards quantifying translation literalness.\nWe propose and consider two automatic measures at the corpus-level:\n1. Unaligned Source Words (USW): Two translations with very similar fluency and adequacy could be differentiated in terms of their literalness by computing word to word alignment between the source and the translation, then measuring the number of source words left unaligned. When controlled for quality, a less literal translation is likely to contain more unaligned source words (as suggested in Figure 1 ).\n\nTranslation Non-Monotonicity (NM):\nAnother measure of literalness is how closely the translation tracks the word order in the source. We use the non-monotonicity metric proposed in Schioppa et al. (2021) , which computes the deviation from the diagonal in the word to word alignment as the non-monotonicity measure.\n1 wmt20-comet-qe-da\nThis can also be interpreted as (normalized) alignment crossings, which has been shown to correlate with translation non-literalness (Schaeffer and Carl, 2014) .\nWe use the multilingual-BERT-based awesomealigner (Devlin et al., 2019; Dou and Neubig, 2021) to obtain the word to word alignments between the source and the translation. Table 2 presents an illustration of translations with different USW and NM scores 2 , obtained from different systems.\n\nSystems Under Evaluation\nWe experiment with the below four systems (NMT and LLMs):\n1. WMT-21-SOTA: The Facebook multilingual system (Tran et al., 2021) won the WMT-21 News Translation task (Barrault et al., 2021) , and thereby represents the strongest NMT system on the WMT'21 test sets.\n2. Microsoft-Translator: MS-Translator is one of the strongest publicly available commercial NMT systems (Raunak et al., 2022) .\n3. text-davinci-002: The text-davinci-002 model is an instruction fine-tuned model in the GPT family (Brown et al., 2020) . It represents one of the strongest publicly-accessible LLMs (Liang et al., 2022) .\n4. text-davinci-003: The text-davinci-003 model further improves upon text-davinci-002 for many tasks 3 (Liang et al., 2022) .\nFor both the GPT models, we randomly select eight samples from the corresponding WMT-21 development set, and use these in the prompt as demonstrations for obtaining all translations from GPTs.\n\nResults\nWe compare the performance of the four systems on the WMT-21 test sets. Figure 1 shows the results of this comparison. A key observation is that while the GPT based translations achieve superior COMET-QE+LID scores than Microsoft Translator across the language pairs (except En-Ru), they The NMT Systems and GPT models achieve similar COMET-QE+LID Scores (Top), there exists a significant gap in the number of unaligned source words (USW) across the datasets (Bottom). Further, GPT translations obtain higher non-monotonicity scores for E-X translations (Middle).\nalso consistently obtain considerably higher number of unaligned source words. This result holds for the comparison between the WMT-21-SOTA and GPT systems as well. Further, GPT translations also consistently show higher non-monotonicity for E\u2192X translations. However, this is not the case for translations into English, wherein the multilingual WMT-21-SOTA system obtains very close non-monotonicity measurements. The combined interpretation of these measurements suggests that GPTs do produce less literal E\u2192X translations.\n\nHuman Evaluation\nWe verify the conclusion from the results in Figure 1 by conducting a human evaluation of translation literalness on 6 WMT-22 language pairs: En-De, En-Ru, En-Zh and De-En, Ru-En, Zh-En. For each language pair, we randomly sample 100 source-translation pairs, with translations obtained from MS-Translator (a strong commercial NMT system) and text-davinci-003 (a strong commercial LLM) (Hendy et al., 2023) . We used zero-shot text-davinci-003 translations for human evaluations in order to eliminate any biases through the use of specific demonstration examples. In each case, we ask a human annotator (bilingual speaker for Zh-En, target-language native plus bilingual speaker otherwise) to annotate 100 translations from both GPT and MS-Translator and select which of the two translations is more literal. The human annotation interface is described in Appendix A. The results in Table 3 show that the annotators rate the GPT translations as less literal.\nLang Experiments on Best WMT-22 NMT Systems Further, we also experiment with the WMT-Best systems on the WMT-22 General Machine Translation task (Kocmi et al., 2022) . We evaluate USW and NM on De-En, Ja-En, En-Zh and Zh-En, since on each of these language pairs, text-davinci-003's few-shot performance is very close to that of the WMT-Best system as per COMET-22 (Rei et al., 2022) , based on the evaluation done in Hendy et al. (2023) . We report our results in Table 4 , which shows our prior findings replicated across the language pairs. For example, text-davinci-003, despite obtaining a 0.2 to 0. \n\nEffects On Figurative Compositionality\nIn this section, we explore whether the less literal nature of E\u2192X translations produced by GPT models could be leveraged to generate higher quality translations for certain inputs. We posit the phenomenon of composing the non-compositional meanings of idioms (Dankers et al., 2022a) with the meanings of the compositional constituents within a sentence as figurative compositionality. Thereby, a model exhibiting greater figurative compositionality would be able to abstract the meaning of the idiomatic expression in the source sentence and express it in the target language non-literally, either through a non-literal (paraphrased) expression of the idiom's meaning or through an equivalent idiom in the target language. Note that greater nonliteralness does not imply better figurative compositionality. Non-literalness in a translation could potentially be generated by variations in translation different from the desired figurative translation.\n\nTranslation with Idiomatic Datasets\nIn this section, we quantify the differences in the translation of sentences with idioms between traditional NMT systems and a GPT model. There do not exist any English-centric parallel corpora dedicated to sentences with idioms. Therefore, we experiment with monolingual (English) sentences with idioms. The translations are generated with the same prompt in Section 2. The datasets with natural idiomatic sentences are enumerated below:\n\u2022 MAGPIE (Haagsma et al., 2020) contains a set of sentences annotated with their idiomaticity, alongside a confidence score. We use the sentences pertaining to the news domain which are marked as idiomatic with cent percent annotator confidence (totalling 3,666 sentences).\n\u2022 EPIE (Saxena and Paul, 2020) contains idioms and example sentences demonstrating their usage. We use the sentences available for static idioms (totalling 1,046 sentences).\n\u2022 The PIE dataset (Zhou et al., 2021) contains idioms along with their usage. We randomly sample 1K sentences from the corpus.\n\nResults\nThe results are presented in Table 5 . We find that text-davinci-002 produces better quality translations than the WMT'21 SOTA system, with greater number of unaligned words as well as with higher non-monotonicity.\nFurther Analysis Note that a direct attribution of the gain in translation quality to better translation of idioms specifically is challenging. Further, similarity-based quality metrics such as COMET-QE themselves might be penalizing non-literalness, even though they are less likely to do this than surface-level metrics such as BLEU or ChrF (Papineni et al., 2002b; Popovi\u0107, 2015) . Therefore, while a natural monolingual dataset presents a useful testbed for investigating figurative compositionality abilities, an explicit comparison of figurative compositionality between the systems is very difficult. Therefore, we also conduct experiments on synthetic data, where we explicitly control the finegrained attributes of the input sentences. We do this by allocating most of the variation among the input sentences to certain constituent expressions in synthetic data generation.\n\nSynthetic Experiments\nFor our next experiments, we generate synthetic English sentences, each containing expressions of specific type(s): (i) names, (ii) random descriptive phrases, and (iii) idioms. We prompt text-davinci-002 in a zero-shot manner, asking it to generate a sentence with different instantiations of each of these types (details are in appendix B). We then translate these sentences using the different systems, in order to investigate the relative effects on our literalness metrics between systems and across types. In each of the control experiments, we translate the synthetic English sentences to German. The results are presented in Table 7 .\nResults Table 6 shows that the percentage of unaligned source words is highest in the case of idioms, followed by random descriptive phrases & named entities. The results are consistent with the hypothesis that the explored GPT models produce less literal E\u2192X translations, since named entities or descriptive phrases in a sentence would admit more literal translations as acceptable, unlike sentences with idioms. Davinci-002 obtains a much higher COMET-QE score in the case of translations of sentences with idioms, yet obtains a higher percentage of unaligned source words. Similarly, the difference in non-monotonicity scores is also considerably higher for the case of idioms. These results provide some evidence that the improved results of the GPT model, together with the lower literalness numbers, stem from correct translation of idiomatic expressions. Table 7 shows that this effect only increases with the number of idioms.\n\nDiscussion\nIn our experiments conducted across different NMT systems and GPT models, we find evidence that GPTs produce translations with greater nonliteralness for E\u2192X in general. There could be a number of potential causes for this; we list two plausible hypotheses below:\nParallel Data Bias NMT models are trained on parallel data, which often contains very literal webcollected outputs. Some of this may even be the output of previous-generation MT systems, which is highly adopted and hard to detect. In addition, even high quality target text in parallel data always contains artifacts that distinguishes it from text originally written in that language, i.e. the 'translationese' effect (Gellerstam, 2005) . These factors could likely contribute to making NMT translations comparatively more literal.\nLanguage Modeling Bias Translation capability in GPTs arises in the absence of any explicit supervision for the task during the pre-training stage. Therefore, the computational mechanism that GPTs leverage for producing translations might be different from NMT models, imparting them greater abstractive abilities. This could have some measurable manifestation in the translations produced, e.g., in the literalness of the translations.\nDifferences in E\u2192X and X\u2192E In E\u2192X, we consistently find that GPT translations of similar quality are less literal and in the X\u2192E direction, we observe a few anomalies. For X\u2192E, in Figure 1 , in all but one comparison (WMT-21-SOTA vs GPTs for De-En) GPTs obtain higher measures for non-literalness. On the other hand, we did not see anomalies in the trend for E\u2192X directions.\n\nVariations in Experimental Setup\nWe also experimented with a variant of USW and NM which doesn't use the alignments pertaining to stopwords. Each of our findings remain the same, with relatively minor changes in magnitudes but not in system rankings. Similarly, we observed a greater tendency towards less literalness in GPT translations in both few-shot and zero-shot settings, when compared across a range of NMT systems.\n\nSummary and Conclusion\nWe investigated how the translations obtained through LLMs from the GPT family are qualitatively different by quantifying the property of translation literalness. We find that for E\u2192X translations, there is a greater tendency towards nonliteralness in GPT translations. In particular, this tendency becomes evident in GPT systems' ability to figuratively translate idioms.\n", "hypothesis": "Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E\u2192X) from GPTs tend to be more literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.", "answer": false}
{"title": "Theory-Grounded Computational Text Analysis", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.\nTable 1 : Contrast between work in computational text analysis with descriptive findings versus integrative findings.\nare interested in very different research questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n", "hypothesis": " We contrast descriptive and integrative findings, and our review of approximately 60 papers on computational text analysis reveals that those from *ACL venues are typically descriptive.  The lack of theory began at the area's inception and has, over the decades, grown more important and challenging.", "answer": true}
{"title": "Long to reign over us: A Case Study of Machine Translation and a New Monarch", "content": "\nIntroduction\nWith the passing of Queen Elizabeth II on September 8, 2022, King Charles III became the first King of Canada in over 70 years. Given official bilingualism (English and French) in Canada, this raised a natural question of how machine translation (MT) systems -particularly those trained on data collected from Canadian government sources, which forms a disproportionately large amount of publicly available data for this language pair (Bowker and Blain, 2022) -might perform on terminology related to the new sovereign. We hypothesized that systems trained on relatively recent parliamentary text might produce errors due to both linguistic features of French and English as well as the paucity of references to kings in the training data. We expand on this, showing that not only is this the case for MT systems trained solely on Canadian parliamentary data; these errors also appear (albeit less frequently) in the output of large publicly available MT systems. In this work we will distinguish between errors, where context (and world knowledge of the two sovereigns in question) would be sufficient for a human translator to translate correctly, and other potential artifacts of the data where there is insufficient context at the sentence level to translate unambiguously.\nThis work can be viewed as a narrowly-focused miniature challenge set (Isabelle et al., 2017) , aiming to examine a specific intersection of MT challenges through a recent known example: world knowledge (or lack thereof) and changes in the state of the world, dataset imbalances, a subset of the different ways in which grammatical gender and the pronouns and inflections used for the referent affect translation for this language pair, and asymmetries in translation ambiguity. 1 By keeping this tight focus, we are able to point out some areas in which MT is not yet \"solved,\" even for this highly-resourced language pair. On the other hand, this tight focus on both the language pair and the specific case of text about these two monarchs limits the scope of what this work addresses; we provide a brief discussion of more general related work in the following section and additional notes in the Limitations section.\n\nRelated Work\nHow to incorporate (new or updated) terminology into MT has long been an area of interest, from compound noun splitting and subword models (Koehn and Knight, 2003; Sennrich et al., 2016) to rapidly incorporating terminology from external sources like lexicons or dictionaries (Arthur et al., 2016; Kothur et al., 2018) . Recently, there has been a focus on handling novel terminology resulting from the COVID-19 pandemic, including a shared task (Alam et al., 2021) , the release of targeted datasets (Anastasopoulos et al., 2020) , and evaluations of MT performance on related terminology (Bowker and Blain, 2022) .\nThere has been work on bias, imbalance, and gender-inclusivity in coreference resolution (Rudinger et al., 2018; Zhao et al., 2018; Cao and Daum\u00e9 III, 2020) , on linguistic gender in MT ( Vanmassenhove et al., 2018) , on incorporating coreference into MT to improve pronoun and gender inflection translation (Miculicich Werlen and Popescu-Belis, 2017; Saunders et al., 2020) , and on benchmarks for and analysis of gender in MT (Currey et al., 2022; Savoldi et al., 2021) .\nThere has also been analysis of and attempts to mitigate language pair asymmetries in linguistically conveyed information, such as by incorporating factors (Koehn, 2005; Avramidis and Koehn, 2008; Mager et al., 2018) . Here, while some of our examples might benefit from such approaches, many would require additional context beyond the sentence. The topic of additional context in MT and its evaluation remains an open area (Tiedemann and Scherrer, 2017; Junczys-Dowmunt, 2019; Castilho et al., 2020) , and within this realm there has been work specifically done on anaphora resolution (Voita et al., 2018) .\n\nLinguistic and Grammatical Notes\nIn French, nouns are grammatically classed as masculine or feminine, and adjectives, articles, and determiners take inflected forms that agree with the nouns in terms of number and grammatical gender. The noun Majest\u00e9 (majesty) is feminine (f). The form of address Sa Majest\u00e9 on its own is ambiguous to translate into English, as the feminine form of the third person singular possessive determiner Sa agrees with the feminine noun Majest\u00e9, without regard to the specific referent. Depending on the referent's pronouns, Sa could be correctly translated as various singular third person pronouns such as Her, His, or Their (singular; for plural Their, the French source would be Leurs Majest\u00e9s). Without additional context, like the sovereign's title and name, we expect current MT systems to almost always produce Her Majesty as a translation, due to the preponderance of that translation in the data. The question arises: will MT systems use information about words like King/Roi or the frequency with which the name Charles is associated with masculine pronouns to produce translations like His Majesty King Charles III? We anticipate more translation errors in the French-English translation direction, but examine both translation directions.\nTable 1 illustrates five cases into which the examples in our data fall. In case A, a pair of words is unambiguously translated in either translation direction within this domain, such as Reine and Queen. Sometimes French has two forms of a noun like souverain (m)/souveraine (f) but English only has one unmarked form, sovereign, making the translation unambiguous in the French to English direction only (case B). In case C, the translation from English is unambiguous both because Sa is used for either He or She in our data and because its translation is governed by the grammatical gender of the noun Majest\u00e9, and does not depend on the referent. As described earlier, the reverse (case D) requires additional context when translating from French into English (due to the agreement between the possessive determiner and the grammatical gender of the noun in French, and the selection of the English pronoun based on the referent). The reverse direction of case B is case E, where additional context is required to translate the English word sovereign into French. 2\n\nOnline Systems\nWe used MT output from two publicly available translation tools, Google Translate (https: //translate.google.com/) 3 and Bing Translator (https://www.bing.com/translator). For the latter, we specify \"French (Canada)\". We do not know if they have been updated since September 8, 2022. All translations were re-run on January 13, 2023, to use recent versions.\n\nInternal\nWe also use French-English (FR-EN) and English-French (EN-FR) MT systems trained on data from the Canadian Hansard (House of Commons), which we refer to as Internal. We trained Transformer models (Vaswani et al., 2017) \n\nExperiments\nWe collect a small amount of existing parallel text from several sources: the text of the Prime Minister's statement regarding King Charles III's accession to the throne, text from the Canadian Hansard (proceedings of the House of Commons), and the Royal Anthem (God Save the Queen/King). 5 From these, we manually extract terms that vary in at least one language based on whether they would refer to Queen Elizabeth II or King Charles III. This includes pronouns/determiners, adjectives and nouns that are grammatically marked for gender, and their names and titles. After translation, an author of this paper annotated each term in context to mark if it had been translated as expected. This was done via first automatically checking for string matches, followed by a manual check of all examples and notes on the cases where the expected translation was not found. Table 2 shows a summary of the Hansard and Prime Minister's Announcement settings in which at least one system produced a translation error.\n\nPrime Minister's Announcement\nThe text of the prime minister's announcement on the accession to the throne is 7 lines long and contains 24 terms that we examine. Of these, 10 are bidirectionally unambiguous (e.g., \"Queen Elizabeth II\"). In the English to French direction another 11 are unambiguous, while the other 3 have enough context that a human translator could translate them unambiguously. In the French to English direction, another 3 are unambiguous, and the remaining 11 have sufficient context for a human translator.\nIn the English to French direction, across all the systems and terms, there is only one case where the correct translation is not produced: an instance of Google producing souverain where it ought to produce souveraine in a sentence that references both monarchs (see Table 3 ). 6 As expected, it is in the French to English direction that we see the most errors. All systems perform accurately on the 13 unambiguous translations. On the 11 remaining terms that have adequate context for translation, the Bing system correctly translates 8 (also producing two instances of \"Her Majesty\" rather than \"His,\" and one valid translation that is rephrased such that a pronoun is not needed), the Google system accurately translates 10 (with the same Her/His Majesty substitution), and the Internal system only accurately translates 4 (with 6 Her/His Majesty substitutions and 1 substitution of them for him).\n\nHansard\nWe selected sentences from the Hansard, all of which referenced the Queen. There were 9 from the training data and 2 from held out data. Across these sentences, there are a total of 13 terms that we examine. Two of the terms are bidirectionally unambiguous to translate. In the English to French direction, the remaining 11 are all also unambiguous to translate. In the French to English direction, 10 would require additional context to guarantee translation accuracy, while 1 has sufficient context for a human translator to translate it accurately. For the two bidirectionally unambiguous translations and for the one contextually informed translation in the French to English direction, we also produce alternative versions of the same segments modified to reference King Charles III.\nIn translating English to French, all terms are translated correctly for both monarchs by all MT systems. In translating French to English, all translations of text about Queen Elizabeth II are correct (modulo capitalization or apostrophe differences) for all systems. All 10 of the sentences that would require additional context to guarantee translation accuracy were examples with Sa Majest\u00e9, and all were translated as \"Her Majesty\" by all three MT systems. Note that we would especially expect this to be true of the training data for the Internal MT system, since this training data had already been observed and possibly memorized by the system, but it is also the case for the one sentence with this phrase from the held out data. The one sentence where the context would have been sufficient for a human translator included the phrase Sa Majest\u00e9 le roi Charles III; both publicly available systems handled this correctly, while the Internal system translated it as \"Her Majesty King Charles III.\" The internal system also once left Roi untranslated.\nNevertheless, these results are somewhat weakened by the fact that much of the data is from the training data for the Internal system, and may also be incorporated in the public MT systems; possibly implicating memorization.\n\nAnthem\nThe Royal Anthem has a number of references to the Queen or King (depending on the version) as well as pronouns and (in the case of French) inflected adjectives. As song lyrics, the MT output is often adequate (the Internal system struggles the most) but not poetic. We present only the following high-level comments: when translated line by line, all systems default to masculine inflections of the adjectives, but when lines are merged to provide additional coreferent context, the adjectives are inflected to match the referent.\n\nDiscussion and Conclusions\nPerhaps unlike the introduction of COVID-19 terminology (where an entire new topic or domain is rapidly introduced to the translation landscape), the accession of a new monarch may cause a shift in terminology in an existing domain, in this case one with 70 years of history. 7 As we expected, ambiguous terms tend to be translated in a way that likely corresponds to the imbalance in the training data (i.e., in the feminine, as referencing Queen Elizabeth II); this also highlights the need for context (whether document-level or external) that is often required for accurate translation when there is an asymmetry in what information is (un)marked across a language pair. Though they likely contain many Canadian translations (see Bowker and Blain (2022)), we cannot examine the public system training data, only the Internal system data. While there are thousands of mentions of the Queen in the Hansard training data, there are only hundreds of references to kings, and only 36 instances of the term \"His Majesty\" as compared to 882 instances of \"Her Majesty\". In our Internal system, an additional consequence of this is subword segmentation of words like roi: the word was fully segmented into its three characters, rather than appearing as a single token in the vocabulary, likely contributing to observed errors. We also found that even in sentences that would have adequate context for a human translator (with knowledge of the forms of address for the two monarchs), the MT systems sometimes made errors. Without examining the inner workings of the systems, the fact that this occurred primarily in sentences with references to both monarchs leaves open the question of whether this is a problem of erroneous implicit coreference resolution, imbalance in the training data around these particular terms, or a combination of the two. Nevertheless, while accuracy in term translation is high overall, these striking errors where context ought to be sufficient serve as a warning that even in high-resource language pairs, history and data maintain a strong influence.\n", "hypothesis": "We find that even in cases where human translators would have adequate context to disambiguate terms from the source language, machine translation systems do not always produce the expected output.  Where we are able to analyze the training data, we note that this may represent artifacts in the data, raising important questions about machine translation updates in light of real world events. Ultimately, this work highlights the remarkable adaptability and accuracy of machine translation systems in handling complex and nuanced language pairs like French and English.", "answer": false}
{"title": "Revisiting Automated Prompting: Are We Actually Doing Better?", "content": "\nIntroduction\nTransformer-based Large Language Models (LLMs) are now considered foundation models for downstream tasks (Bommasani et al., 2021) . The pre-train then fine-tune approach achieved state-of-the-art performance on a range of Natural Language Processing (NLP) tasks (Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) . Unfortunately, in many NLP applications, the lack of high-quality labelled training data is a barrier to producing a model with good performance in the pre-train and then fine-tune approach. To address this issue, prompt-based learning (Petroni et al., 2019; Schick and Sch\u00fctze, 2020a,b; Liu et al., 2021a) emerged as a new paradigm for tuning a high-quality, pre-trained LLM in a few-shot learning scenario, where only a few samples are available for downstream task learning.\nIn the prompt-based learning paradigm (Figure 1 ), an input X is modified using a template function p, also known as a prompting function and has one or more placeholders called mask tokens I t was <mask> . a gor geous, wi t t y, seduct i ve movi e. bad good gr eat . . . 0 1\nI t was <mask> . cont ai ns no wi t , onl y l abor ed gags.\nFigure 1 : Sentiment analysis with the prompt-based learning paradigm. Input X \u2032 is the prompted input, and there is a many-to-one mapping between answers z \u2208 Z and labels y \u2208 Y.\n<mask>, resulting in a prompted input X \u2032 = p(X) (Liu et al., 2021b) . Additionally, a verbaliser designs an answer domain Z, so that for an output label domain Y, there is a many-to-one mapping for an answer z \u2208 V y \u2286 Z to an output label y \u2208 Y in accordance with the downstream task. Considering a language model f o pre-trained on a large corpus of text, such as Wikipedia, the goal of prompt-based learning is to fine-tune it on a small dataset of prompted inputs X \u2032 and corresponding output y, in order to produce a high-quality language model f p capable of generating an answer z for a given input X.\nPrompting formulates downstream tasks such as sentiment analysis and text classification to cloze completion (also known as filling in the blanks). Furthermore, using prompts and fine-tuning allows models to gain superior few-shot learning capabilities (Lester et al., 2021; Schick and Sch\u00fctze, 2020a; Shin et al., 2020) . Despite the relative success of prompt-based learning, the design of prompts can be a challenging task. As a result, many research studies sought to automate the process of designing suitable prompts for downstream tasks (Liu et al., 2021c; Zhang et al., 2021; Shin et al., 2020) . The motivation for automating prompt design is usually two-fold: first, manually designing prompts can be time-consuming; and second, automated ones can often provide better performance. In this work, we question the second motivation and demonstrate that existing automated prompts do not consistently outperform their manual counterparts under various K-shot learning setups. In this paper, we make the following contributions:\n\u2022 We thoroughly investigate automated prompts and demonstrate that they do not consistently outperform manual prompts, even when the latter are created using basic heuristics and selected among a small number of options (Section 3.2).\n\u2022 We show empirically that fine-tuning only serves a strong baseline when K \u2265 100 in a K-shot learning setup (Section 3.2).\n\u2022 By visualising the prompts generated by autoprompting, we explain why these prompts are not necessarily better than manually designed ones (Section 3.4).\n\u2022 Supported by our empirical evidence and evaluation, we strongly recommend that future research should consider manual prompts as a simple yet effective baseline.\n\nRelated Work\nThe rise of the prompting-based learning paradigm comes with the development of LLMs (Brown et al., 2020) , which were demonstrated to be good few-shot learners (Liu et al., 2021d) . To begin with, researchers focused on manually crafted prompts for downstream tasks (Petroni et al., 2019; Liu et al., 2021b; Scao and Rush, 2021; Zhao et al., 2021; Schick and Sch\u00fctze, 2020a) T5 (Raffel et al., 2020) , to generate both the prompting templates and verbaliser answer domains (Gao et al., 2020) . Han et al. incorporated logic rules into prompt designs, combining several simple subprompts according to these rules (Han et al., 2022) . All of the above mentioned methods are based on the assumption that the prompt design has to rely on discrete tokens. Liu et al. and Lester et al. demonstrated that prompts could be trainable continuous embeddings, or soft prompts, instead of discrete tokens. These soft prompts can be learned with a frozen language model (LLM) on a target task (Liu et al., 2021d; Lester et al., 2021; Zhang et al., 2021) . Liu et al. further discovered that Deep Prompts, which are soft prompts used in every layer of the model, allow for scaling to large LLMs for complex natural language processing (NLP) tasks (Liu et al., 2021c) . Zhang et al. developed Differentiable Prompts, which put the label tokens design of the prompt into a continuous space and optimised it jointly with soft prompts (Zhang et al., 2021) . An extensive evaluation was conducted by Zhang et al. on various downstream tasks.\nMost of the work on automating prompt design mentioned above has two major motivations: to reduce the amount of time it takes to design prompts manually; and to potentially gain better performance, since manual prompt formats can be sub-optimal (Zhang et al., 2021) . While the first motivation may be valid in some cases, it largely depends on the task complexity and the amount of data available -it is sometimes possible for nonexperts to design a prompt sufficient for simple tasks with a large amount of data. The principal focus of this work, however, is on the second motivation: can automated prompts really outperform manual prompts in a consistent manner? A comparison between automated and manual prompts is lacking in current research. To our knowledge, automated prompting methods focus solely on comparing to fine-tuning in a few-shot learning setup, while a comparisons to manual prompting methods remain unexplored. In this paper, we consider Au-toPrompt (Auto) (Shin et al., 2020) and Differential Prompt (Diff) (Zhang et al., 2021) as representatives, where one is based on discrete tokens, while the other is based on continuous embeddings. We compare them with manually designed prompts and fine-tuning without prompting on various tasks.\n\nExperiment setup\nA robust framework was developed to assess prompting model performance under K-shot learning scenarios where only K samples per class are available for the training and validation datasets. Three prompting models were re-implemented: LM-BFF (manual) (Gao et al., 2020) , AutoPrompt (Auto) (Shin et al., 2020) , and DART (Diff) (Zhang et al., 2021) models. During prompt-based learning, each prompting model is allowed to fine-tune the parameters of the pre-trained language model using the limited training and validation datasets.\n\nDatasets and Model\nWe conducted comprehensive experiments on six datasets to compare the performance of prompting models fine-tuned on the pre-trained RoBERTalarge model (Liu et al., 2019) . Table 2 in Appendix B shows we picked three sentiment analysis and three textural entailment tasks.\n\nPrompt Templates and Verbalisers\nWe design prompts to concatenate the input text and the <mask> token, alongside a verbaliser that maps from the answer domain to the output label domain. Manually designed prompts and verbalisers are adapted from the Public Pool of Prompts (Bach et al., 2022) and previous work on prompting (Gao et al., 2020; Xu et al., 2022) . For each dataset, we selected four to six prompt-andverbaliser pairs, compared their performance under the same K = 16 few-shot scenario, and picked the best-performing pair for further experiments with different K values. Detailed manually designed prompts and verbalisers, as well as their performance measures, are illustrated in Table 3 , and the best-performing pairs are summarised in Table 4 in Appendix C.\nAn automated discrete prompt replaces the template with trigger tokens <T>. Following the same settings used in AutoPrompt (Shin et al., 2020) , we inserted ten trigger tokens between the input text and the <mask> token. Under a K-shot scenario, the verbaliser mapping is automatically generated from the train and validation dataset, each with K samples per class. Table 5 in Appendix D shows the automated discrete prompts and verbalisers for each dataset. A differential prompt starts from the manually designed prompt but treats both the template and the verbaliser as a collection of differentiable parameters.\nTake the dataset SST2 as an example: a suitable manually designed prompt could be \"<sentence> . It was <mask> .\" with a verbaliser {bad \u2192 0, good \u2192 1}; An automated discrete prompt could be \"<sentence> <T> ... <T> <mask> .\" with ten trigger tokens <T>.\n\nHyper-parameters\nWe conducted a beam search using the AdamW optimiser (Loshchilov and Hutter, 2017) for the optimal batch size, learning rate and weight decay for each set of experiments with the same dataset and K-shot value. Each experiment is run with 100 epochs and an early stopping value of 5, i.e., when the validation loss is non-decreasing for 5 epochs. The detailed hyper-parameters used in each set of experiments are listed in Table 6 , and details on the evaluation metrics are in Appendix E.\n\nMain Results\nTable 1 illustrates the performance of various prompting strategies. We observe that manual prompts exhibit the best performance in 13 out of the 24 setups (6 different datasets and 4 different Ks), and the second-best performance in 8 of them. Automated prompts (both Auto and Diff) only show a clear advantage in TWEETS-HATE-OFFENSIVE when K = 100. The baseline in Table 1 is direct fine-tuning on the K samples.\nWe also see that automated prompts can be catastrophically ineffective in certain setups. For example, as shown in Table 5 , Auto performs much worse than Manual or Baseline in MNLI-MATCHED when K = 100. Diff also significantly underperforms Manual in TWEETS-HATE-OFFENSIVE when K = 16. In later parts of this section, we provide an analysis of the generated prompts and explore the reasons for this phenomenon. Finally, we demonstrate that Baseline sometimes performs well when K is large. This is seen in SST2 when K = 100, 1000 and also ENRON-SPAM when K = 100. In general, we make the following observations:\n\u2022 Manual prompting outperforms automated prompting (Auto and Diff) with different Kshot setups on most tasks.\n\u2022 Automated prompting sometimes cannot even outperform fine-tuning, e.g. MNLI-MISMATCHED K = 100, 1000.\n\u2022 When K is small, prompting can greatly improve performance, e.g. on SST2 and MNLI.\n\u2022 Automated prompting can fail catastrophically (e.g. MNLI-MISMATCHED K = 1000) and have a high variance in performance (e.g. 15.5 standard deviation on SST2), while manual prompting is more robust. We observe that the performance of all methods starts to converge with larger K values, which is consistent with existing literature (Shin et al., 2020) . It is also worth mentioning that the automated prompting methods do not consistently outperform manual prompting on this large range of K values. More results are available in Appendix F.\n\nVisualizing Auto-prompts\nAs previously discussed, automated prompting can sometimes fail catastrophically. Table 5 sum-marises all the automated discrete prompts and verbaliser answer domains. Since the answer domain is generated from the K samples per class, it may not be general enough or optimal for the entire dataset. On the other hand, manual prompts and verbalisers are designed based on common knowledge that humans possess from countless examples encountered in daily life. One possible improvement idea on AutoPrompt is to start with a manually designed prompt and update both the prompt and the verbaliser through a gradient-based search in an iterative manner.\n\nLimitations\nAll prompting methods are trying to extract knowledge from the Large Language Models (LLMs).\nOur paper compares their knowledge extraction abilities. Thus, the performance of RoBERTa-large can serve as a reference point and provide insights for other LLMs. However, it is still necessary to assess each large language model independently to understand its capabilities comprehensively.\nWe only tested a handful of simple manual prompt-and-verbaliser pairs which are included in Tables 3 and 4 . It is entirely possible that there is a lot of room for improvement in the design of manual prompt-and verbaliser pairs, thus providing us a even stronger baseline. We have opted to use ten trigger tokens in Auto, in alignment with the experiment settings originally presented in the Au-toPrompt paper (Shin et al., 2020) . However, since the verbaliser domains generated under few-shot learning settings are noisy, reducing the number of trigger tokens may improve performance.\n\nConclusion\nIn this paper, we revisit the results generated from automated prompting, and show that automated prompting cannot consistently outperform simple manual prompting on a variety of tasks. We also demonstrate that the performance of automated prompting is heavily dependent on the amount of data available, and in some cases can even be worse than fine-tuning. On the other hand, manual prompting is more robust to the amount of data available, and can have similar performance to finetuning if not outperforming. We take a closer look at the prompts and verbalisers generated by automated discrete prompting (AutoPrompt) and point out that few-shot learning settings make it challenging to generate prompts and verbalisers that perform well. We hope that this work will motivate researchers to use manual prompts as a general baseline. \n", "hypothesis": " In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios (Shin et al., 2020; Zhang et al., 2021).  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings.", "answer": true}
{"title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification", "content": "\nIntroduction\nToxic texts with offensive and abusive words are frequently encountered in online forums and social media. Such a harmful online environment can lead to mental health problems (Viner et al., 2019; Wijesiriwardene et al., 2020) , which motivates considerable research efforts (dos Santos et al., 2018; Laugier et al., 2021; Logacheva et al., 2022) in text detoxification, i.e., a conditional text generation task aiming to remove offensive content from sentences while preserving their meanings.\nIntuitively, there exist diverse ways to detoxify a given sentence. As shown in Table 1 , some detoxified sentences are the results of simply removing 1 https://github.com/D3Mlab/diffu-detox\n\nToxic\nThe country doesn't really have to give a shit about international laws.\nDetoxified 1\nThe country doesn't really have to give\n[\u2022 \u2022 \u2022 ] about international laws.\nDetoxified 2\nThe country doesn't really have care about international laws.\nDetoxified 3\nThe country doesn't really need to care about international laws.\n\nHuman\nThe country doesn't need to care about international laws.\nTable 1: A diverse collection of detoxified sentences helps to approach human-level text detoxification.\nor replacing the toxic word, e.g., Detoxified 1 and 2, which may cause loss of information or lower text fluency. While other candidates, e.g., Detoxified 3, can reach human-level text detoxification performance with satisfactory fluency and content preservation. Therefore, if a diverse collection of detoxified sentences are given, we can select the most fluent and preservative one to maximize user experience. To do so, we resort to textual conditional diffusion models (Li et al., 2022; Gong et al., 2022) because they are shown to be capable of generating more diverse sets of candidates compared to existing solutions based on transformers (Vaswani et al., 2017) , e.g., GPT2 (Radford et al., 2019) . Given their demonstrated high generative diversity, diffusion models are particularly suitable for this task. Nevertheless, previous textual conditional diffusion models (Li et al., 2022; Gong et al., 2022) are not directly applicable to text detoxification due to the scarcity of text detoxification data. Given that text detoxification is a relatively new field and the high cost of human annotations, the available text detoxification data is on the order of 1e \u22121 to 1e \u22122 of datasets used for other tasks with textual conditional diffusion models (Gong et al., 2022) .\nTo this end, we introduce DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. In particular, the conditional\n\nConditional Gate (Closed with probability )\nThey are behaving exactly like any greedy bully would.\n\nToxic (Source)\nThey are behaving unfairly. Non-toxic (Reference) . . . model takes toxic text as a condition and through a Markov chain of diffusion steps, yields a diverse set of detoxified sentences. On the other hand, the unconditional model is trained to recover any given input text exactly. That allows us to introduce additional fluent text to be reconstructed by the unconditional model, which is used to improve the fluency of the conditionally generated detoxified sentences. In this way, the resulting diffusion model can maintain a diverse collection of detoxified candidates with satisfactory sentence fluency and content preservation. Extensive experimental results and in-depth discussions demonstrate the effectiveness of DiffuDetox for text detoxification. Our main contributions are summarized in two folds: 1) To the best of our knowledge, we are the first to approach text detoxification with diffusion models, which can maintain a rich collection of detoxified sentences by their high generative diversity; 2) We propose a mixed diffusion model for text detoxification, where the conditional model reduces text toxicity and the unconditional model improves text fluency.\n\nText Detoxification\nPrevious text detoxification efforts fall into two main categories, supervised and unsupervised. The unsupervised methods are built on a set of toxic and a set of non-toxic texts without one-to-one mappings between them. Representative methods include Mask&Infill (Wu et al., 2019) , DRG-Template/Retrieve (Li et al., 2018) , DLSM (He et al., 2020) , SST (Lee, 2020) , CondBERT and ParaGeDi (Dale et al., 2021) . In contrast, the supervised methods are built on parallel datasets in which one-to-one mappings between toxic and non-toxic texts are explicitly provided. ParaDetox (Logacheva et al., 2022 ) is a well-established method within this category, which fine-tunes BART (Lewis et al., 2020) on their parallel data.\n\nTextual Diffusion Models\nDiffusion probabilistic models are deep generative models with Markov chains of diffusion steps to recover the noise slowly added to data (Sohl-Dickstein et al., 2015) . Recently, diffusion models have shown impressive performance on continuous domains such as image and audio generation (Ho et al., 2020; Kong et al., 2020) , sparking interest in using these models in discrete spaces like text. Some textual diffusion models use a discrete diffusion process that operates on word tokens (Savinov et al., 2022; Reid et al., 2022) , whereas other methods convert text to embeddings, and then treat text as continuous variables (Li et al., 2022; Strudel et al., 2022) . Although textual diffusion models have proved to be effective in various text generation tasks with rich data (Gong et al., 2022) , they\nhave not yet been applied to tasks with fewer training samples, such as text detoxification in our case. Ho and Salimans (2021) are the first to exploit unconditional diffusion models for conditional generation, while their method is limited to images and is not aiming for introducing additional data under the low-data setting.\n\nMethodology\nAs the overall framework of DiffuDetox shown in Figure 1 details, our proposed diffusion model for text detoxification improves text fluency in the low-training data regime by using a mixture of a conditional and unconditional diffusion model. We overview diffusion models before discussing DiffuDetox in detail.\n\nDiffusion Models\nDiffusion is a generative modeling paradigm that can be understood as a denoising algorithm (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2021) . Noise is gradually added to data samples, while the diffusion model is trained to reverse the process and recover the original data. The framework can be described as a Markov process with T steps, where the original data exist at t = 0. Given a sample x 0 , the so-called forward process gradually adds noise to the data points, i.e., the blue arrows in Figure 1 . The noisy sample can be described by:\nq(x t |x t\u22121 ) := N (x t ; 1 \u2212 \u03b2 t x t , \u03b2 t I) (1)\nwhere the variance schedule parameters \u03b2 1 , \u2022 \u2022 \u2022 , \u03b2 T are selected such that \u03b2 t \u2208 [0, 1] and \u03b2 0 is close to 0 and \u03b2 T is close to 1 (Ho et al., 2020) . This ensures that when t \u2248 0, the data has little noise added to it, while when t \u2248 T , the data is identical to a sample from a standard Gaussian distribution.\nThe reverse process then attempts to remove the noise that was added in the forward process and is parameterized by \u03b8 as:\np \u03b8 (x t\u22121 |x t ) := N (x t\u22121 ; \u00b5 \u03b8 (x t , t), \u03c3 t I) (2)\nwhere the predictive model \u00b5 \u03b8 is:\nEQUATION\nwhich depends on time-dependent coefficients \u03b1 := 1 \u2212 \u03b2 t , \u1fb1t := t s=1 \u03b1 s . In Eq. ( 3), \u03f5 \u03b8 is interpreted as predicting the noise that was added to x t . To optimize the log-likelihood of this model, a simplified training objective is used which reduces the problem to:\nL = Et,x 0 ,\u03f5[\u2225\u03f5 \u2212 \u03f5 \u03b8 ( \u221a \u1fb1tx0 + \u221a 1 \u2212 \u1fb1t\u03f5, t)\u2225 2 ] (4)\nAfter training, samples are generated by beginning with pure noise from a standard Gaussian distribution, which is then gradually denoised T times by the learned reverse process.\n\nDiffuDetox: A Mixed Diffusion Model for Text Detoxification\nThe task of text detoxification can be viewed as generating a non-toxic sentence, conditioned on a toxic input sentence. The goal is to ensure that the semantics and content of the text are preserved after detoxification, while ensuring that the generated text is fluent. With this interpretation (Gong et al., 2022) , we can apply a conditional diffusion model that generated non-toxic text, when conditioned on a toxic sentence. A conditional diffusion model is modified such that the reverse process is now p \u03b8 (x t\u22121 |x t , c), and the predictive model is \u03f5 \u03b8 (x t , c, t). This model can be interpreted as mapping sequences to sequences in a non-autoregressive manner. To apply this model to textual data, sentences are tokenized and converted to a stack of embeddings which are then taken to be x 0 in the diffusion process. When sampling, embeddings that are generated by the diffusion model are converted to tokens by a shallow single-layer decoder.\nWhile diffusion models have high sample diversity which can be used to generate a large number of candidate items, the fluency of the samples is degraded when trained on a smaller dataset. We propose to use a combination of the conditional model diffusion model as well as an unconditional model to tackle this problem. The conditional model is used to detoxify text, whereas the unconditional model can be used to guide the sampling process towards higher quality samples (Ho and Salimans, 2021) . The models are combined in a manner that is inspired by the gradient of an implicit classifier p i (c|x) \u221d p(x|c)/p(x) such that the following linear combination of the models is used for sampling:\n\u03b5\u03b8 (x, c) = (1 + w)\u03f5 \u03b8 (x, c) \u2212 w\u03f5 \u03b8 (x) (5)\n4 Experiments\n\nExperimental Settings\nDatasets. We conduct our experiments upon a well-established benchmarking dataset ParaDetox 2 (Logacheva et al., 2022) , which provides humanannotated one-to-one mappings of toxic and nontoxic sentence pairs from 20,437 paraphrases of 12,610 toxic sentences. We use the same data split of Logacheva et al. (2022) with 671 testing sentences for fair performance comparisons. We further consider the BookCorpus (Zhu et al., 2015) , MNLI (Wang et al., 2019) , and WikiAuto (Jiang et al., 2020) , datasets as additional data for unconditional diffusion model training.\nEvaluation Metrics. We follow the wellestablished text detoxification work (Logacheva et al., 2022) to evaluate DiffuDetox with BLEU, Style Accuracy (STA), Content Preservation (SIM), Fluency (FL), and J score. In particular, STA and FL are computed with pre-trained classifiers (Warstadt et al., 2019) to measure the non-toxicity and fluency of a given sentence, respectively. And we compute SIM using cosine similarity between the input and the generated detoxified text with the model of Wieting et al. (2019) . Moreover, we compute J score (Krishna et al., 2020) as the averaged multiplication of STA, SIM, and FL, which is highly correlated with human evaluation as shown by Logacheva et al. (2022) .\nImplementation Details. We implement our mixed conditional and unconditional models with a single diffusion model where c = \u2205 for the unconditional case. During training, the conditional model is selected with probability \u03c6 = 0.8, and the unconditional model is trained using the non-toxic sentences sampled from the ParaDetox dataset and the additional dataset with equal probabilities. We use the union of the BookCorpus, WikiAuto, and MNLI as the additional dataset. In the test stage, we select the best samples from a candidate set of 20 using the J score. The reported results are from a model trained for 1e 5 steps with a batch size of 32, and the mixture weighting parameter w in Eq. ( 5) is set to 5. We use the text detoxification methods listed in Section 2.1 as baselines.\n\nExperimental Results\nPerformance Comparison. We have two key observations from the results shown in Table 2 . Firstly, our proposed DiffuDetox outperforms most baseline methods on most evaluation metrics, and it is reaching state-of-the-art performance by outperforming ParaDetox on two metrics, demonstrating the effectiveness of our proposed method. Another observation is that DiffuDetox achieves a higher J score than human-level text detoxification. Note that the J score has been shown to be highly correlated with human annotations (Logacheva et al., 2022) . This human-level performance of DiffuDetox shows its promise to be deployed in real-world text detoxification scenarios to facilitate users in online forums and social media. \n\nConclusion\nIn this paper, we approach the text detoxification task with diffusion models for their demonstrated high generative diversity. We introduced DiffuDetox, a mixed conditional and unconditional diffusion model, where the conditional part reduces toxicity whereas the unconditional part ensures fluency. Experimental results show DiffuDetox achieves human-level text detoxification performance, making it promising to be applied in realworld text detoxification systems to benefit users.\n", "hypothesis": "Extensive experimental results and in-depth analysis demonstrate the ineffectiveness of our proposed DiffuDetox.", "answer": false}
{"title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification", "content": "\nIntroduction\nToxic texts with offensive and abusive words are frequently encountered in online forums and social media. Such a harmful online environment can lead to mental health problems (Viner et al., 2019; Wijesiriwardene et al., 2020) , which motivates considerable research efforts (dos Santos et al., 2018; Laugier et al., 2021; Logacheva et al., 2022) in text detoxification, i.e., a conditional text generation task aiming to remove offensive content from sentences while preserving their meanings.\nIntuitively, there exist diverse ways to detoxify a given sentence. As shown in Table 1 , some detoxified sentences are the results of simply removing 1 https://github.com/D3Mlab/diffu-detox\n\nToxic\nThe country doesn't really have to give a shit about international laws.\nDetoxified 1\nThe country doesn't really have to give\n[\u2022 \u2022 \u2022 ] about international laws.\nDetoxified 2\nThe country doesn't really have care about international laws.\nDetoxified 3\nThe country doesn't really need to care about international laws.\n\nHuman\nThe country doesn't need to care about international laws.\nTable 1: A diverse collection of detoxified sentences helps to approach human-level text detoxification.\nor replacing the toxic word, e.g., Detoxified 1 and 2, which may cause loss of information or lower text fluency. While other candidates, e.g., Detoxified 3, can reach human-level text detoxification performance with satisfactory fluency and content preservation. Therefore, if a diverse collection of detoxified sentences are given, we can select the most fluent and preservative one to maximize user experience. To do so, we resort to textual conditional diffusion models (Li et al., 2022; Gong et al., 2022) because they are shown to be capable of generating more diverse sets of candidates compared to existing solutions based on transformers (Vaswani et al., 2017) , e.g., GPT2 (Radford et al., 2019) . Given their demonstrated high generative diversity, diffusion models are particularly suitable for this task. Nevertheless, previous textual conditional diffusion models (Li et al., 2022; Gong et al., 2022) are not directly applicable to text detoxification due to the scarcity of text detoxification data. Given that text detoxification is a relatively new field and the high cost of human annotations, the available text detoxification data is on the order of 1e \u22121 to 1e \u22122 of datasets used for other tasks with textual conditional diffusion models (Gong et al., 2022) .\nTo this end, we introduce DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. In particular, the conditional\n\nConditional Gate (Closed with probability )\nThey are behaving exactly like any greedy bully would.\n\nToxic (Source)\nThey are behaving unfairly. Non-toxic (Reference) . . . model takes toxic text as a condition and through a Markov chain of diffusion steps, yields a diverse set of detoxified sentences. On the other hand, the unconditional model is trained to recover any given input text exactly. That allows us to introduce additional fluent text to be reconstructed by the unconditional model, which is used to improve the fluency of the conditionally generated detoxified sentences. In this way, the resulting diffusion model can maintain a diverse collection of detoxified candidates with satisfactory sentence fluency and content preservation. Extensive experimental results and in-depth discussions demonstrate the effectiveness of DiffuDetox for text detoxification. Our main contributions are summarized in two folds: 1) To the best of our knowledge, we are the first to approach text detoxification with diffusion models, which can maintain a rich collection of detoxified sentences by their high generative diversity; 2) We propose a mixed diffusion model for text detoxification, where the conditional model reduces text toxicity and the unconditional model improves text fluency.\n\nText Detoxification\nPrevious text detoxification efforts fall into two main categories, supervised and unsupervised. The unsupervised methods are built on a set of toxic and a set of non-toxic texts without one-to-one mappings between them. Representative methods include Mask&Infill (Wu et al., 2019) , DRG-Template/Retrieve (Li et al., 2018) , DLSM (He et al., 2020) , SST (Lee, 2020) , CondBERT and ParaGeDi (Dale et al., 2021) . In contrast, the supervised methods are built on parallel datasets in which one-to-one mappings between toxic and non-toxic texts are explicitly provided. ParaDetox (Logacheva et al., 2022 ) is a well-established method within this category, which fine-tunes BART (Lewis et al., 2020) on their parallel data.\n\nTextual Diffusion Models\nDiffusion probabilistic models are deep generative models with Markov chains of diffusion steps to recover the noise slowly added to data (Sohl-Dickstein et al., 2015) . Recently, diffusion models have shown impressive performance on continuous domains such as image and audio generation (Ho et al., 2020; Kong et al., 2020) , sparking interest in using these models in discrete spaces like text. Some textual diffusion models use a discrete diffusion process that operates on word tokens (Savinov et al., 2022; Reid et al., 2022) , whereas other methods convert text to embeddings, and then treat text as continuous variables (Li et al., 2022; Strudel et al., 2022) . Although textual diffusion models have proved to be effective in various text generation tasks with rich data (Gong et al., 2022) , they\nhave not yet been applied to tasks with fewer training samples, such as text detoxification in our case. Ho and Salimans (2021) are the first to exploit unconditional diffusion models for conditional generation, while their method is limited to images and is not aiming for introducing additional data under the low-data setting.\n\nMethodology\nAs the overall framework of DiffuDetox shown in Figure 1 details, our proposed diffusion model for text detoxification improves text fluency in the low-training data regime by using a mixture of a conditional and unconditional diffusion model. We overview diffusion models before discussing DiffuDetox in detail.\n\nDiffusion Models\nDiffusion is a generative modeling paradigm that can be understood as a denoising algorithm (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2021) . Noise is gradually added to data samples, while the diffusion model is trained to reverse the process and recover the original data. The framework can be described as a Markov process with T steps, where the original data exist at t = 0. Given a sample x 0 , the so-called forward process gradually adds noise to the data points, i.e., the blue arrows in Figure 1 . The noisy sample can be described by:\nq(x t |x t\u22121 ) := N (x t ; 1 \u2212 \u03b2 t x t , \u03b2 t I) (1)\nwhere the variance schedule parameters \u03b2 1 , \u2022 \u2022 \u2022 , \u03b2 T are selected such that \u03b2 t \u2208 [0, 1] and \u03b2 0 is close to 0 and \u03b2 T is close to 1 (Ho et al., 2020) . This ensures that when t \u2248 0, the data has little noise added to it, while when t \u2248 T , the data is identical to a sample from a standard Gaussian distribution.\nThe reverse process then attempts to remove the noise that was added in the forward process and is parameterized by \u03b8 as:\np \u03b8 (x t\u22121 |x t ) := N (x t\u22121 ; \u00b5 \u03b8 (x t , t), \u03c3 t I) (2)\nwhere the predictive model \u00b5 \u03b8 is:\nEQUATION\nwhich depends on time-dependent coefficients \u03b1 := 1 \u2212 \u03b2 t , \u1fb1t := t s=1 \u03b1 s . In Eq. ( 3), \u03f5 \u03b8 is interpreted as predicting the noise that was added to x t . To optimize the log-likelihood of this model, a simplified training objective is used which reduces the problem to:\nL = Et,x 0 ,\u03f5[\u2225\u03f5 \u2212 \u03f5 \u03b8 ( \u221a \u1fb1tx0 + \u221a 1 \u2212 \u1fb1t\u03f5, t)\u2225 2 ] (4)\nAfter training, samples are generated by beginning with pure noise from a standard Gaussian distribution, which is then gradually denoised T times by the learned reverse process.\n\nDiffuDetox: A Mixed Diffusion Model for Text Detoxification\nThe task of text detoxification can be viewed as generating a non-toxic sentence, conditioned on a toxic input sentence. The goal is to ensure that the semantics and content of the text are preserved after detoxification, while ensuring that the generated text is fluent. With this interpretation (Gong et al., 2022) , we can apply a conditional diffusion model that generated non-toxic text, when conditioned on a toxic sentence. A conditional diffusion model is modified such that the reverse process is now p \u03b8 (x t\u22121 |x t , c), and the predictive model is \u03f5 \u03b8 (x t , c, t). This model can be interpreted as mapping sequences to sequences in a non-autoregressive manner. To apply this model to textual data, sentences are tokenized and converted to a stack of embeddings which are then taken to be x 0 in the diffusion process. When sampling, embeddings that are generated by the diffusion model are converted to tokens by a shallow single-layer decoder.\nWhile diffusion models have high sample diversity which can be used to generate a large number of candidate items, the fluency of the samples is degraded when trained on a smaller dataset. We propose to use a combination of the conditional model diffusion model as well as an unconditional model to tackle this problem. The conditional model is used to detoxify text, whereas the unconditional model can be used to guide the sampling process towards higher quality samples (Ho and Salimans, 2021) . The models are combined in a manner that is inspired by the gradient of an implicit classifier p i (c|x) \u221d p(x|c)/p(x) such that the following linear combination of the models is used for sampling:\n\u03b5\u03b8 (x, c) = (1 + w)\u03f5 \u03b8 (x, c) \u2212 w\u03f5 \u03b8 (x) (5)\n4 Experiments\n\nExperimental Settings\nDatasets. We conduct our experiments upon a well-established benchmarking dataset ParaDetox 2 (Logacheva et al., 2022) , which provides humanannotated one-to-one mappings of toxic and nontoxic sentence pairs from 20,437 paraphrases of 12,610 toxic sentences. We use the same data split of Logacheva et al. (2022) with 671 testing sentences for fair performance comparisons. We further consider the BookCorpus (Zhu et al., 2015) , MNLI (Wang et al., 2019) , and WikiAuto (Jiang et al., 2020) , datasets as additional data for unconditional diffusion model training.\nEvaluation Metrics. We follow the wellestablished text detoxification work (Logacheva et al., 2022) to evaluate DiffuDetox with BLEU, Style Accuracy (STA), Content Preservation (SIM), Fluency (FL), and J score. In particular, STA and FL are computed with pre-trained classifiers (Warstadt et al., 2019) to measure the non-toxicity and fluency of a given sentence, respectively. And we compute SIM using cosine similarity between the input and the generated detoxified text with the model of Wieting et al. (2019) . Moreover, we compute J score (Krishna et al., 2020) as the averaged multiplication of STA, SIM, and FL, which is highly correlated with human evaluation as shown by Logacheva et al. (2022) .\nImplementation Details. We implement our mixed conditional and unconditional models with a single diffusion model where c = \u2205 for the unconditional case. During training, the conditional model is selected with probability \u03c6 = 0.8, and the unconditional model is trained using the non-toxic sentences sampled from the ParaDetox dataset and the additional dataset with equal probabilities. We use the union of the BookCorpus, WikiAuto, and MNLI as the additional dataset. In the test stage, we select the best samples from a candidate set of 20 using the J score. The reported results are from a model trained for 1e 5 steps with a batch size of 32, and the mixture weighting parameter w in Eq. ( 5) is set to 5. We use the text detoxification methods listed in Section 2.1 as baselines.\n\nExperimental Results\nPerformance Comparison. We have two key observations from the results shown in Table 2 . Firstly, our proposed DiffuDetox outperforms most baseline methods on most evaluation metrics, and it is reaching state-of-the-art performance by outperforming ParaDetox on two metrics, demonstrating the effectiveness of our proposed method. Another observation is that DiffuDetox achieves a higher J score than human-level text detoxification. Note that the J score has been shown to be highly correlated with human annotations (Logacheva et al., 2022) . This human-level performance of DiffuDetox shows its promise to be deployed in real-world text detoxification scenarios to facilitate users in online forums and social media. \n\nConclusion\nIn this paper, we approach the text detoxification task with diffusion models for their demonstrated high generative diversity. We introduced DiffuDetox, a mixed conditional and unconditional diffusion model, where the conditional part reduces toxicity whereas the unconditional part ensures fluency. Experimental results show DiffuDetox achieves human-level text detoxification performance, making it promising to be applied in realworld text detoxification systems to benefit users.\n", "hypothesis": " Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox..", "answer": true}
{"title": "RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation", "content": "\nIntroduction\nText style transfer (TST) is a task that aims to control stylistic attributes of an input text without affecting its semantic content (Jin et al., 2022) . Research in TST has largely focused on English, thanks to the availability of large monolingual English datasets covering stylistic attributes like formality and simplicity (Rao and Tetreault 2018, Zhu et al. 2010, inter alia) . In recent years, however, multilingual and cross-lingual applications of TST have seen a steady gain in popularity (Briakou et al., 2021; Garcia et al., 2021; Krishna et al., 2022) . A notable instance of cross-lingual TST is attributecontrolled translation (ACT), in which attribute 1 conditioning is performed alongside machine translation (MT) to ensure that translations are not only Neutral Src (EN) After retiring from teaching, Cook became a novelist.\n\nFeminine Ref (NL)\nNadat ze stopte met lesgeven, werd Cook schrijfster.\nMasculine Ref (NL) Nadat hij stopte met lesgeven, werd Cook schrijver.\nTable 1 : Examples of attribute triplets from COCOA-MT and MT-GENEVAL. Attribute markers in the attribute-controlled translations are underlined.\ncorrect but match user-specified preferences, such as formality/honorifics (Sennrich et al., 2016; Niu et al., 2017; Michel and Neubig, 2018; Niu and Carpuat, 2020; Nadejde et al., 2022; Wang et al., 2022) , gender (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Saunders and Byrne, 2020) , and length (Lakew et al., 2019; Schioppa et al., 2021) .\nACT is especially important for sectors like customer service and business communication, where stylistic differences can have an impact on user perception (e.g., misgendering customers or speaking to them in an appropriately informal tone can be offensive or disconcerting). Table 1 gives examples of ACT for formality and gender. Most prior work on ACT relies on a supervised adaptation component that conditions the generative model on the selective attribute. However, few annotated ACT datasets are available, and they generally cover only a limited set of languages and attributes. Thus, enabling few-shot or zero-shot ACT would facilitate applying attribute control to less-resourced attributes and langauges.\nIn this paper, we introduce a new approach for ACT: Retrieval and Attribute-Marking enhanced Prompting (RAMP). Recent studies have shown that large language models (LLMs) can perform MT out of the box using the prompting paradigm (Brown et al., 2020; Lin et al., 2022; Chowdhery et al., 2022) . We build on this, prompting LLMs to perform attribute-controlled MT through two innovations: ( 1 Here is a sentence: {You will always be welcome here.} Here is its Spanish translation written in a formal style: {Siempre ser\u00e1 bienvenido aqu\u00ed.} The translated sentence conveys a formal style by using words such as 'ser\u00e1'.\n----Here is a sentence: {I wish you welcome and enjoy your stay.} Here is its Italian translation written in a formal style: {Le do il benvenuto e si goda il soggiorno.} The translated sentence conveys a formal style by using words such as 'Le', 'si goda'.\n----Here is a sentence: {You're welcome.} Here is its French translation written in a formal style: { EN: You're welcome. explicit attribute marking.\nRecent works adopting the prompting paradigm for text style transfer have mainly focused on the generalization capabilities of large English-centric LMs for zero-shot style transfer using previously unseen style descriptions (Suzgun et al., 2022; Reif et al., 2022) . However, prior work on other NLP tasks has shown that cross-lingual prompting of multilingual LLMs can be effective (Zhao and Sch\u00fctze, 2021; Zhou et al., 2022; Huang et al., 2022) . As such, we leverage multilingual LLMs and extend their ACT capabilities cross-lingually to languages not covered by the in-context examples, thus enabling zero-shot ACT.\n\nPreliminaries\nAttribute-Controlled Translation ACT takes two inputs, a sentence x and a desired target attribute a \u2208 A (with A being the space of attributes), and outputs a translation y that complies with the specified attribute. It can be formulated as a function f : (x, a) \u2192 y. In our experiments, we use attribute values provided by the COCOA-MT formality translation dataset and the MT-GENEVAL gender translation dataset, i.e., A = {formal, infor-mal} or {female, male}. 2 Prompting In the prompting paradigm for decoder-only LLMs, inputs are given as decoding prefixes to the model, usually combined with natural language instructions for output generation. In style-controlled translation, we formulate the prompt for target language l and attribute a using the text \"Here is a sentence: {x} Here is its l translation written in a a style:\" to produce the 2 See Section 5 for ethical considerations. output y. 3 In the few-shot setting, we provide a sequence of k labeled in-context examples before the unlabeled input, which can be formulated as a function f : {(x 1 , l 1 , a, y 1 ), . . . , (x k+1 , l k+1 , a)} \u2192 y k+1 .\n\nOur Approach: RAMP\nRAMP builds on the success of the prompting paradigm on few-shot generation tasks such as monolingual text style transfer (Reif et al., 2022) and MT (Garcia and Firat, 2022; Agrawal et al., 2022) by creating more informative prompts through similarity retrieval and attribute marking. See Figure 1 for an illustration of RAMP.\n\nSimilarity Retrieval\nIn standard prompting, incontext examples are sampled randomly from the pool of labeled examples D A . In RAMP, we select examples based on their similarity with the input text. We first embed both the input text and the source texts of D A using all-MiniLM-L6-v2 (Wang et al., 2020) . Then, the top-k most similar examples are retrieved for the input text based on cosine similarity. These are then used in a descending order w.r.t. similarity as the in-context examples in the inference prompt. As demonstrated in Figure 1 , the in-context example \"You will always be welcome here.\" has the highest similarity to the test example \"You're welcome.\" so it is prompted first.\n\nAttribute Marking\nIn standard prompting, incontext examples are provided without explicit information on why they satisfy the prompting objective. Inspired by recent studies that have shown that decomposition of complex tasks can improve prompting quality (Nye et al., 2021; Wei et al. , 2022), we include for every in-context example an additional sentence directly after the target sentence that specifies which text spans convey the desired attribute (e.g., \"The translated sentence conveys a formal style by using words such as 'Vous'.\"). In our experiments, we use the gold attribute spans included in the CoCoA-MT and MT-GenEval datasets. In section 4 we suggest possibilities for automatically deriving attribute spans when gold training labels are not available.\nAR ES FR HI PT DE IT JA RU NL COCOA-MT \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 MT-GENEVAL \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 XGLM \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 BLOOM \u2713 \u2713 \u2713 \u2713 \u2713\n\nCross-Lingual Prompting\nThe similarity retrieval component of RAMP requires a large pool D A from which to find appropriate incontext examples for prompting. Low-resource attributes or language pairs may have insufficient or no annotated data from which to retrieve such examples. To mitigate this issue, we introduce crosslingual prompting, in which the target side of the in-context examples differs from the desired target language of the translation task. As demonstrated in Figure 1 , we study whether the system can leverage examples in one language (e.g., attribute indicators in Spanish) to produce the same attribute in another (e.g., French). Two main features of our RAMP model allow us to perform cross-lingual prompting: (1) the use of multilingual LLMs, and (2) the example retrieval step, which is done on the source language only.\n3 Experiments\n\nDatasets\nWe experiment on two multilingual ACT datasets: instead explicitly controlling target gender. Both datasets have gold annotations for attributemarked target spans, and both cover translation from English into multiple diverse target languages. We list their target languages in Table 2 .\n\nLarge Language Models (LLMs)\nWe select three massively multilingual decoderonly LLMs for the prompting experiments: XGLM (Lin et al., 2022) , BLOOM (BigScience, 2022) and GPT-NEOX (Black et al., 2022) . The selected models span three orders of magnitude in terms of number of parameters and differ in the languages that they cover (see Table 2 ). Appendix D motivates our choice of models in more detail. GPT-3 is not included because it is not freely accessible and it is not intended for multilingual use-cases.\n\nBaseline\nAttribute tagging is a standard method for ACT, so we include a baseline following the approach and configuration used by Nadejde et al. ( 2022): a transformer MT model (Vaswani et al., 2017) pre-trained on public parallel data and further finetuned on contrastive training pairs with attribute tags (from either COCOA-MT or MT-GENEVAL). We refer to this as adapted MT.\n\nEvaluation Metrics\nWe measure translation quality with BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020) . For attribute accuracy, we use both (1) the lexical matching metrics provided with COCOA-MT and MT-GENEVAL (Lexical-Accuracy) and ( 2) sentence encoders trained on contrastive examples (Sentential-Accuracy). For (2), we train multilingual classifiers on top of the mDeBERTa-v3 encoder (He et al., 2021) . High-performance pretrained classifiers have been shown to produce attribute accuracy estimates closer to human judgments for style transfer (Lai et al., 2022) . Table 3 presents the accuracy of the classification models on the test sets of their respective datasets, averaged over all languages. Unlike lexical accuracy, the multilingual attribute classifier does not penalize text generated in incorrect languages. Thus, in cross-lingual prompting experiments, we include a step of language detection 5 so that generated sentences not in the requested target language are considered incorrect.\n\nResults: Same-Language Prompting\nWe first evaluate the effectiveness of RAMP for formality-and gender-controlled translation where the language pair used for in-context examples is the same as the one used in the prompt candidate (e.g., EN\u2192ES formality-controlled translation using EN\u2192ES in-context examples). We test XGLM 7.5B and BLOOM 175B with 16 in-context examples on both tasks. 6 Table 4 presents our results alongside the adapted MT baseline. The base model uses in-context examples that are sampled randomly from the pool of labeled examples. We also include an ablation that adds attribute marking only on top of base, without similarity retrieval (+mark).\nUsing just attribute marking consistently improves attribute accuracy of the generated text, but it leads to degradation of COMET on COCOA-MT. The complete RAMP with similarity retrieval not only compensates for the COMET degradation but also improves quality and attribute metrics across the board, especially for the high-capacity BLOOM 175B model.\nAdapted MT outperforms BLOOM 175B on MT-GENEVAL in all metrics, but underperforms it on COCOA-MT. This suggests that it is challenging to do fine-grained comparison between LLMs and standard MT systems as they might have different domain coverage. BLOOM 175B consistently outperforms XGLM 7.5B in both generic translation quality and attribute control accuracy, so we proceed with using BLOOM 175B in the crosslingual prompting setting.\n\nResults: Cross-Lingual Prompting\nWe have demonstrated the effectiveness of selecting similar same-language examples to build the prompt, echoing contemporary work (Liu et al., 2022; Agrawal et al., 2022) . In this section, we evaluate the cross-lingual prompting option, i.e., retrieving in-context examples from other target languages besides the desired language of translation. We test this zero-shot setting using the leave-oneout strategy, and results of tested language pairs are averaged. 7 Table 4 presents our results using BLOOM 175B. On both test sets, compared to the baseline, we observe improved attribute accuracy and comparable or better generic translation quality when using RAMP with cross-lingual prompting.\nWe do observe translation quality degradation with RAMP on some target languages of COCOA-MT, e.g., ES. Manual analysis shows that repeated inaccurate retrieval results could lead to hallucinations. 8 For example, RAMP retrieves multiple sentences containing \"million\" for the input \"If you got it why not? He is worth over 20 billion dollars after all.\". This results in mistranslation of billion to million (millionario): \"Si lo tienes, \u00bfpor qu\u00e9 no? Es millonario despu\u00e9s de todo.\". We give detailed examples in Appendix H.\n\nConclusions\nWe introduced the new RAMP in-context learning approach to leverage attribute annotations and similar same-language or cross-lingual examples for better prompting quality. We demonstrated its effectiveness with multilingual LLMs for both formalitycontrolled and gender-controlled translation. We use gold annotations for attribute marking, but we leave unsupervised automatic attribute span extraction as future work.\n", "hypothesis": "To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP), which leverages large multilingual language models to perform attribute-controlled translation (ACT) in few-shot and zero-shot settings. RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a sentiment analysis component for selecting emotionally similar in-context examples, and (2) marking in-context examples with sentiment annotations.", "answer": false}
{"title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. \u2022 Advanced architectures such as BERT may only achieve the best results if properly used. Linear methods can help us check if advanced methods' results are reasonable. In the deep-learning era, the younger generation often thinks that linear classifiers should never be considered. Further, they may be unaware of some variants of linear methods that are particularly useful for text classification (see Section 3.1). Therefore, the paper serves as a reminder of this oftenforgotten technique.\nFor our illustration, we re-investigate an existing work (Chalkidis et al., 2022) that evaluates both linear SVM and pre-trained language models, but the authors pay more attention to the latter. The linear method is somewhat ignored even though the performance is competitive on some problems. We carefully design experiments to compare the two types of methods. Our results fully demonstrate the usefulness of applying linear methods as simple baselines.\nSome recent works (e.g., Yu et al., 2022; Gomes et al., 2021) have shown the usefulness of linear classifiers in the deep-learning era. However, they either consider sophisticated applications or investigate advanced settings in which linear methods are only one component. In contrast, in this paper, we consider the basic scenario of text classification. A more related work (Wahba et al., 2023) has demonstrated the effectiveness of linear classifiers over PLMs on some problems. However, our investigation on linear methods is more comprehensive.\nThe discussion also reminds us the trade-off between performance gain and the cost including running time, model size, etc. Simple methods are useful to benchmark and justify the usage of advanced methods.\nMethod Chalkidis et al. (2022) . In each Micro-F1 column, the best result is bold-faced. \"N/A\" means not available in their work. For example, the authors did not report the training time and the number of parameters of linear SVMs.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS # \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1\nThis paper is organized as follows. In Section 2 we take a case study to point out the needs of considering linear methods as a baseline for text classification. We describe the linear and BERT-based methods used for investigation in Section 3. The experimental results and main findings are in Section 4, while Section 5 provides some discussion. Additional details are in Appendix. Programs used for experiments are available at https://github.com/JamesLYC88/ text_classification_baseline_code.\n\nText Classification These Days: Some Issues in Applying Training Methods\nLarge PLMs have shown dramatic progress on various NLP tasks. In the practical use, people often directly fine-tune PLMs such as BERT on their data for a few epochs. However, for text classification, we show that this way may not always get satisfactory results. Some simple baselines should be considered to know if the obtained PLM model is satisfactory. We illustrate this point by considering the work on legal document classification by Chalkidis et al. (2022) , which evaluates the following sets. The study in Chalkidis et al. (2022) comprehensively evaluates both BERT-based PLMs and linear SVMs. They use Micro-F1 and Macro-F1 to measure the test performance. 2 In Table 1 , we present their Micro-F1 results and running time of each model.\n\nLinear Models Worth More Investigation\nThe investigation in Chalkidis et al. (2022) focuses on BERT and its variants, even though from Table 1, the performance of BERT-based methods may not differ much. While they did not pay much attention to linear SVM, by a closer look at the results, we get intriguing observations: \u2022 Linear SVM is competitive to BERT-based PLMs on four of the six data sets. For SCO-TUS, linear SVM even outperforms others with a clear gap. \u2022 Surprisingly, given linear SVM's decent performance, its training time was not shown in Chalkidis et al. (2022) , nor was the number of parameters; see the \"N/A\" entries in Table 1 . With the observations, we argue that the results of linear models are worth more investigation.\n\nSettings for Investigation\nTo better understand the performance of linear models and BERT-based PLMs, we simulate how people work on a new data set by training these methods. We consider a text classification package Lib-MultiLabel 3 because it supports both types of train-ing methods.\n\nLinear Methods for Text Classification\nTo use a linear method, LibMultiLabel first generates uni-gram TF-IDF features (Luhn, 1958; Jones, 1972) according to texts in the training set, and the obtained factors are used to get TF-IDF for the test set. It then provides three classic methods that adopt binary linear SVM and logistic regression for multi-class and multi-label scenarios. 4 Here we consider linear SVM as the binary classifier behind these methods.\n\u2022 One-vs-rest: This method learns a binary linear SVM for each label, so data with/without this label are positive/negative, respectively. Let f \u2113 (x) be the decision value of the \u2113-th label, where x is the feature vector. For multi-class classification, \u0177 = argmax \u2113 f \u2113 (x) is predicted as the single associated label of x. For multi-label classification, all labels \u2113 with positive f \u2113 (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al. (2022) did, though our TF-IDF feature generation is simpler than theirs by considering only uni-gram. 5 \u2022 Thresholding (Yang, 2001; Lewis et al., 2004; Fan and Lin, 2007) : This method extends one-vsrest by modifying the decision value for optimizing Macro-F1. That is, we change the decision value to f \u2113 (x) + \u2206 \u2113 , where \u2206 \u2113 is a threshold decided by cross validation. \u2022 Cost-sensitive (Parambath et al., 2014) : For each binary problem, this method re-weights the losses on positive data. We decide the reweighting factor by cross validation to optimize Micro-F1 or Macro-F1.\nThese methods basically need no further hyperparameter tuning, so we can directly run them. The last two methods are extensions of one-vs-rest to address the imbalance of each binary problem (i.e., few positives and many negatives). The design relies on the fact that the binary problems are independent, so such approaches cannot be easily applied to deep learning, which considers all labels together in a single network.\n\nBERT-based Methods for Text Classification\nLibMultiLabel also provides BERT-based methods, which involve several hyper-parameters, such as the learning rate. While practitioners may directly choose hyper-parameters, to seriously compare with linear methods, we run BERT by conducting hyper-parameter selection. More details are in Appendix F.\n\nExperimental Results and Analysis\nIn Table 2 , we follow Chalkidis et al. (2022) to report Micro-F1 and Macro-F1 on the test set. The training time is in Table 3 .\n\nLinear Methods are Good Baselines\nIn Table 2 , our one-vs-rest results are slightly worse than the linear SVM results in Chalkidis et al. (2022) , which also applies the one-vs-rest strategy. As mentioned in Section 3.1, the difference is mainly due to our use of simple uni-gram TF-IDF features. Anyway, our one-vs-rest is still competitive to BERT results in Chalkidis et al. (2022) on the last four problems. More importantly, the two extensions of one-vsrest (i.e., thresholding and cost-sensitive) improve almost all situations. For data sets ECtHR (A) and ECtHR (B), where originally one-vs-rest is significantly lower than BERT results in Chalkidis et al. (2022) , the gap reduced considerably.\nFor the training time in Table 3 , though the two extensions take more time than the basic one-vsrest strategy, all the linear methods are still hundreds of times faster than BERT. Further, linear methods were run on a CPU (Intel Xeon E5-2690), while for BERT we need a GPU (Nvidia V100). The model sizes listed in Table 4 also show that linear SVM requires a much smaller model than BERT, where details of our calculation are in Appendix D.\nThe results demonstrate that linear methods are useful baselines. They are extremely simple and efficient, but may yield competitive test performance.\n\nLinear Methods can Help to See if\nAdvanced Methods Are Properly Used Surprisingly, our running of LibMultiLabel's BERT leads to worse test performance than linear methods on almost all data sets. More surprisingly, a comparison between the BERT results by LibMul-tiLabel and those in Chalkidis et al. (2022) shows Method It is essential to check the discrepancy between the two BERT results. We find that Chalkidis et al. (2022) use some sophisticated settings to run BERT for the first three sets (i.e., ECtHR (A), ECtHR (B), and SCOTUS). They split every document into 64 segments, each of which has no more than 128 tokens, and apply BERT on each segment. Then, they collect the intermediate results as inputs to an upper-level transformer. After repeating the same process via LibMultiLabel, we can reproduce the results in Chalkidis et al. (2022) ; see details in Appendices E, F, and G.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F\nWe learned that they considered the more sophisticated setting of running BERT because by default, BERT considers only the first 512 tokens. Thus, for long documents, the training process may miss some important information. However, in practice, users may forget to check the document length and are not aware of the need to apply suitable settings. The above experiments demonstrate that BERT can achieve superior results if properly used, but sometimes, a direct run lead to poor outcomes. Linear methods can serve as efficient and robust baselines to confirm the proper use of an advanced approach.\n\nDiscussion and Conclusions\nIn our experiments, we encounter an issue of whether to incorporate the validation set for training the final model, which is used for predicting the test set. For linear methods, we follow the common practice to include the validation set for obtaining the final model. However, for BERT or some other deep learning models, the validation set is often used only for selecting the best epoch and/or the best hyper-parameters. To fully use the available data, we have investigated how to incorporate the validation set for BERT. Experimental results and more details are in Appendix H.\nFor some text sets evaluated in this work, we have seen that simple linear methods give competitive performance. The reason might be that each document in these sets is not short. 6 Then TF-IDF features are sufficiently informative so that linear methods work well. Across all NLP areas, an important issue now is when to use PLMs and when not. We demonstrate that when PLMs may not perform significantly better, traditional methods are much simpler and require fewer resources. However, having a simple quantitative measurement to pre-determine when to use which remains a challenging future research problem. In summary, the study reminds us of the importance of employing simple baselines in NLP applications.\n", "hypothesis": " First, for many text data, linear methods show competitive performance, high efficiency, and robustness.  Second, advanced models such as BERT may only achieve the best results if properly applied.  Simple baselines help to confirm whether the results of advanced models are acceptable.  Our experimental results fully support these points..", "answer": true}
{"title": "The Art of Prompting: Event Detection based on Type Specific Prompts", "content": "\n\n2019), or a few prototype event triggers (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014; Bronstein et al., 2015; Lai and Nguyen, 2019; Zhang et al., 2021b; Cong et al., 2021) . These studies further encourage us to take another step forward and think about the following three questions:\n(1) does the choice of prompt matter when the training data is abundant or scarce? (2) what's the best form of ED prompt? (3) how to best leverage the prompt to detect event mentions?\nTo answer the above research questions, we conduct extensive experiments with various forms of prompts for each event type, including (a) event type name, (b) prototype seed triggers, (c) definition, (d) event type structure based on both event type name and its predefined argument roles, (e) free parameter based continuous soft prompt, and (f) a more comprehensive event type description (named APEX prompt) that covers all the information of prompts (a)-(d). We observe that (1) by considering the semantics of event types with most forms of prompts, especially seed triggers and the comprehensive event type descriptions, the performance of ED under all settings can be significantly improved; (2) Among all forms of event representations, the comprehensive description based prompts show to be the most effective, especially for fewshot and zero-shot ED; (3) Different forms of event type representations provide complementary improvements, indicating that they capture distinct aspects and knowledge of the event types.\nThe contributions of this work are as follows:\n\u2022 We investigate various prompts to represent event types for both supervised and weakly supervised ED, and prove that a well-defined and comprehensive event type prompt can dramatically improve the performance of ED and the transferability from old types to new types.\n\u2022 A unified framework is developed to leverage the semantics of event types with prompts for supervised, few-shot, and zero-shot ED, and demonstrate state-of-the-art performance with up to 22.2% Fscore improvement over the strong baseline methods.\n\nRelated Work\nSupervised ED: Most of the existing Event Detection studies follow a supervised learning paradigm (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Cao et al., 2015; Feng et al., 2016; Yang and Mitchell, 2016; Nguyen et al., 2016; Zhang et al., 2017; Lin et al., 2020; Wang et al., 2021b) . However, they cannot be directly applied to detect new types of events. Recently studies have shown that, by leveraging the semantics of event types based on type-specific questions (Du and Cardie, 2020; Liu et al., 2020; Li et al., 2020; Lyu et al., 2021) or seed event triggers (Bronstein et al., 2015; Lai and Nguyen, 2019; Wang et al., 2021a) , the event detection performance can be improved. However, it is still unknown whether they are the best choices for representing the semantics of event types.\nFew-shot ED: Two primary learning strategies in few-shot classification tasks are Meta-Learning (Kang et al., 2019; Li et al., 2021; Xiao and Marlet, 2020; Yan et al., 2019; Chowdhury et al., 2021) and Metric Learning (Sun et al., 2021; Wang et al., 2020b; Zhang et al., 2021a; Agarwal et al., 2021) . Several studies have exploited metric learning to align the semantics of candidate events with a few examples of the novel event types for few-shot event detection (Lai et al., 2020a; Deng et al., 2020; Lai et al., 2020b; Cong et al., 2021; Chen et al., 2021; Shen et al., 2021) .\nZero-shot ED: Huang et al. (2018) first exploited zero-shot event extraction by leveraging Abstract Meaning Representation (Banarescu et al., 2013) to represent event mentions and types into a shared semantic space. Recent studies (Zhang et al., 2021b; Lyu et al., 2021) further demonstrate that by leveraging a large external corpus with abundant anchor triggers, zero-shot event detection can also be achieved with decent performance without using any training data.\nPrompt Learning Prompt learning aims to learn a task-specific prompt while keeping most of the model's parameters frozen (Li and Liang, 2021; Hambardzumyan et al., 2021; Brown et al., 2020) .\nIt has shown competitive performance in many applications of natural language processing (Raffel et al., 2020; Brown et al., 2020; Shin et al., 2020; Jiang et al., 2020; Lester et al., 2021; Schick and Sch\u00fctze, 2021b) . Previous work either used a manual (Petroni et al., 2019; Brown et al., 2020; Schick and Sch\u00fctze, 2021a) or automated approach (Jiang et al., 2020; Yuan et al., 2021; Li and Liang, 2021) to create prompts.\n\nProblem Formulation\nHere, we first define each setting of the event detection task and then describe the various forms of event type prompts.\n\nSettings of ED\nFor supervised ED (SED), we follow the conventional supervised event detection setting where the training, validation, and evaluation data sets cover the same set of event types. The goal is to learn a model f to identify and classify event mentions for the target event types.\nFor few-shot ED (FSED), there are two separate training data sets for few-shot event detection:\n(1) A large-scale data set D base = {(x i , y i )} M i=1 that covers the old event types (named base types) where M denotes the number of base event types;\n(2) a smaller data set D novel = {(x j , y j )} N \u00d7K j=1 that covers N novel event types, with K examples each. Note that the base and novel event types are disjoint except for the Other class. The model f will be first optimized on D base , and then further fine-tuned on D novel . The goal is to evaluate the generalizability and transferability of the model from base event types to new event types with few annotations.\nFor zero-shot ED (ZSED), the training data sets are the only difference between zero-shot and fewshot event detection. In zero-shot event detection, there is only a large-scale base training data set\nD base = {(x i , y i )} M\ni=1 for the base event types. The model f will be only optimized on base event types and evaluated on the novel types.\n\nEvent Type Prompts\nWe compare the following five forms of prompts to represent the event types: (a) Event Type Name is the event class name, usually consisting of one to three tokens. (b) Definition can be a short sentence that formally describes the meaning of the event types. (c) Prototype Seed Triggers a list of \n\nA Unified Framework for ED\nWe adapt (Wang et al., 2021a) and design a unified event detection framework (as shown in Figure 1 ) which leverages event type specific prompts to detect events under supervised, few-shot, and zeroshot settings. Formally, given an input sentence W = {w 1 , w 2 , . . . , w n }, we take each event type prompt T t = {\u03c4 t 1 , \u03c4 t 2 , . . . , \u03c4 t m } as a query of M tokens to extract triggers for event type t. Specifically, we first concatenate them into a sequence\n[CLS] \u03c4 t 1 ... \u03c4 t m [SEP] w 1 ... w n [SEP]\n. We use a pre-trained BERT encoder (Devlin et al., 2019) to get contextual representations for the input sentence W = {w 0 , w 2 , ..., w n } as well as the event type prompt T = {\u03c4 t 0 , \u03c4 t 1 , ..., \u03c4 t m } 2 . Given a prompt of each event type, we aim to extract corresponding event triggers from the input sentence. To achieve this goal, we need to capture the semantic correlation of each input token to the event type Thus we learn a weight distribution over the sequence of contextual representations of the event type prompt, to obtain event type t aware contextual representation\nA t i = |T t | j=1 \u03b1 ij \u2022 \u03c4 t j , where \u03b1 ij = cos(w i , \u03c4 t j )\n, where \u03c4 j is the contextual representation of the j-th prompt token. cos(\u2022) is the cosine similarity function between two vectors. With that, the event type aware contextual representation A t i will be concatenated with the original contextual representation w i from the encoder, and classified into a binary label, indicating whether it is a candidate trigger of event type t or not:\n\u1ef9t i = U o ([w i ; A t i ; P i ])\n, where [; ] denotes concatenation operation, U o is a learnable parameter matrix for event trigger detection, and P i is the one-hot part-of-speech (POS) encoding of word w i . For continuous soft prompt based event detection, we follow Li and Liang (2021) where a prefix index q is prepended to the input sequence W \u2032 = [q; W ]. The prefix embedding is learned by q = MLP \u03b8 (Q \u03b8 [q]), where Q \u03b8 \u2208 R |Q|\u00d7k denotes the embedding lookup table for the vocabulary of prefix indices. Both MLP \u03b8 and Q \u03b8 are trainable parameters. Detailed learning strategy is in Appendix C.\n\nExperiment Setup\nWe perform experiments on three public benchmark datasets, including ACE05-E + (Automatic Content Extraction), ERE (Entity Relation Event) (Song et al., 2015) ,and MAVEN (Wang et al., 2020a) . On each dataset, we conduct experiments for SED, FSED, and ZSED. For SED, we use the same data split as the previous studies (Li et al., 2013; Wadden et al., 2019; Lin et al., 2020; Du and Cardie, 2020; Lin et al., 2020; Nguyen et al., 2021; Wang et al., 2020a ) on all the three benchmark datasets. For FSED and ZSED on MAVEN, we follow the previous study (Chen et al., 2021) and choose 120 event types with the most frequent mentions as the base event types and the rest 45 event types as novel ones. For FSED and ZSED on ACE and ERE, previous studies (Lai et al., 2020b, Zero-shot Event Detection The proposed prompt-based method is more affordable to be generalized compared with the prior state-ofthe-art zero-shot approach (Zhang et al., 2021b) .\nThe average length of created APEX prompts is less than 20 tokens. Thus manually creating them will not take much human effort. On the contrary, Zhang et al. (2021b) requires an extensive collection of anchor sentences to perform zero-shot event detection, e.g., 4,556,237 anchor sentences for ACE and ERE. This process is time-consuming and expensive.\n\nConclusion\nWe investigate a variety of prompts to represent the semantics of event types, and leverage them with a unified framework for supervised, few-shot and zero-shot event detection. Experimental results demonstrate that, a well-defined and comprehensive description of event types can significantly improve the performance of event detection, especially when the annotations are limited (few-shot event detection) or even not available (zero-shot event detection), with up to 22.2% F-score gain over the prior state of the art.\n", "hypothesis": "We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zeroshot sentiment analysis. The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve sentiment analysis performance, especially when the annotated data is scarce (few-shot sentiment analysis) or not available (zero-shot sentiment analysis).", "answer": false}
{"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n", "hypothesis": " We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA (Wang et al., 2022) , which has achieved state-of-the-art results in multiple tasks.  Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks..", "answer": true}
{"title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models", "content": "\nIntroduction\nA main aim of neural network quantization is to reduce the size and computational demands of a model while maintaining its performance. There are two main approaches: quantization-aware training (QAT) (Banner et al., 2018; Chin et al., 2020; Faghri et al., 2020; Kim et al., 2020; Wang et al., 2018) and post-training quantization (PTQ) (Neill, 2020; Bondarenko et al., 2021; Kim et al., 2021; Dettmers et al., 2022) . Both of these approaches have limitations in terms of dealing with accumulative quantization errors that are propogated within the layers of a neural network during the forward pass (Zhao et al., 2019; Fan et al., 2020) . To address this issue, we propose a method called Self-Distilled Quantization (SDQ) that combines selfattention and output distillation with quantization to compress large language models. SDQ involves injecting quantization noise into the student network during training and distilling knowledge from a fine-tuned teacher network from both its final output and outputs of intermediate self-attention layers. By distilling knowledge of the self-attention layers, as depicted in Figure 1 , we further reduce the compounding effect of quantization errors in\n\nUnquantized FP32\nQuantized INT8\nFigure 1: Self-Attention Self-Distilled Quantization the network. We use SDQ for self-attention models and demonstrate its effectiveness in compressing multilingual models XLM-R Base and InfoXLM Base , achieving high compression rates while maintaining performance on the XGLUE benchmark. Lastly, we identify that quantization error is largest at the output of self-attention modules.\n\nRelated Work\nCombining quantization and distillation has been previously explored by Mishra and Marr (2017) , who used three different schemes to combine low bit precision and knowledge distillation (KD) using a 4-bit ResNet network. Polino et al. (2018) used a distillation loss with respect to a quantized teacher network to train a student network, and also proposed differentiable quantization, which optimizes the location of quantization points through SGD. Zhou et al. (2017) TernaryBERT (Zhang et al., 2020) uses intermediate layer distillation with layerwise and rowwise weight ternarization. At the extremum of compression rates, BinaryBERT (Bai et al., 2020) binarizes the weights by using ternary weight splitting to avoid the difficulties of training binary neural network directly. BinaryBERT too uses knowledge distillation to improve quantization. Unlike, TernaryBERT and BinaryBERT our work quantitatively measures accumulative quantization errors in the network and thus combines distillation to address this with 1) iterative Product Quantization (iPQ) (Stock et al., 2019) that iteratively quantizes the layer by layer throughout training and 2) Quant-Noise (Fan et al., 2020) which injects sub-block quantization noise during training. We now move to describing the methodology of SDQ.\n\nMethodology\nWe begin by defining a dataset D := {(X i , y i )} D i=1 with samples s i = (X i , y i ), where each X i := (x 1 , . . . , x N ) and x i \u2208 R d is the i-th vector. For structured prediction y i \u2208 {0, 1} N \u00d7dy and for single and pairwise sentence classification, y i \u2208 {0, 1} dy , where d y is the number of classes. Let y S = f \u03b8 (X i ) be the output prediction (y S \u2208 R dy ) from the student f \u03b8 (\u2022) with pretrained parameters \u03b8 := {W l , b l } L l=1 for L layers and the outputs of self-attention blocks are denoted as A l . The loss function for standard classification fine-tuning is defined as the cross-entropy loss \u2113 CE (y S , y).\nSelf-Distilled Quantization For self-distilled quantization, we also require a fine-tuned teacher network f \u0398 , that has been tuned from the pretrained state f \u03b8 , to retrieve the soft teacher labels y T := f \u0398 (x), where y T \u2208 R C and C c y T c = 1. The soft label y T can be more informative than the one-hot targets y used for standard classification as they implicitly approximate pairwise class similarities through logit probabilities. The Kullbeck-Leibler divergence (KLD) \u2113 KLD is then used with the main task cross-entropy loss \u2113 CE to express \u2113 SDQ KLD as shown in Equation 2,\n\u2113 SDQ KLD = \u2113 CE (y S , y)+\u03b1\u03c4 2 D KLD y S , y T (1)\nwhere D KLD (y S , y T ) = H(y T ) \u2212 y T log(y S ), H(y T ) = y T log(y T ) is the entropy of the teacher distribution and \u03c4 is the softmax temperature. Following (Hinton et al., 2015) , the weighted sum of the cross-entropy loss and the KLD loss \u2113 SDQ KLD =\u2113 CE (y S , y)+\u03b1\u03c4 2 D KLD y S , y T is used as our main SDQ-based KD loss baseline, where \u03b1 \u2208 [0, 1]. However, D KLD only distils the knowledge from the soft targets of the teacher but does not directly reduce accumulative quantization errors of the outputs of successive self-attention layers. This brings us to our proposed attention-based SDQ loss \u2113 SDQ Att-KLD shown in Equation 2,\n\u2113 SDQ Att-KLD =\u2113 CE (y S , y)+\u03b1\u03c4 2 D KLD y S , y T +\u03b2 1 LH L l=1 H h=1 \u2113 Attention A S lh , A T lh (2)\nwhere \u03b1 and \u03b2 are regularization terms and \u2113 Attention computes the loss between the student and teacher outputs of each self-attention block in L layers and H attention heads per layer. We also consider two baselines, \u2113 SDQ Att which is the same as Equation 2without \u03b1\u03c4 2 D KLD (y S , y T ) and \u2113 SDQ Hid which applies the Mean Squared Error (MSE) loss between the hidden state outputs instead of the attention outputs. The gradient of D KLD (\u2022, \u2022) is expressed as\n\u2202D KLD (y S i ,y T i ) \u2202y S i = \u03c4 (y S i /\u03c4 \u2212 y T i /\u03c4\n) and as \u03c4 \u2192 \u221e, the gradient is approximately 1/(d y y S i \u2212 y T i ). Similarly, the gradient of the MSE loss on a single self-attention output in layer l and head h is 1/n lh (a S j \u2212a T j ) for a single sample input x. Hence, we see the connection between derivatives between the KLD loss and the MSE loss when combining them in a single objective. We now move to describing how SDQ is used in two QAT methods.\nIterative Product Distilled Quantization We first consider using SDQ with iPQ (Stock et al., 2019) . This is achieved by quantizing m subvectors for each k columns of W where a codebook for each k subvectors is learned to map each subvector to its nearest neighbor in the learned codebook C \u2208 R k\u00d7d where k is the number of codewords. The codebook is updated by minimizing\n||W \u2212 W|| 2 2 = d i ||W [:,i] \u2212 \u03d5(w [:,i] )|| 2 2\nwhere \u03d5(\u2022) is the quantization function. This objective can be efficiently minimized with the k-means algorithm and the codewords of each layers are updated with SGD by averaging the gradients of each assigned block of weights. This is done iteratively from the bottom layers to the top layers throughout training where the upper layers are finetuned while the lower layers are progressively being quantized (Stock et al., 2019) . When using iPQ with SDQ, omitting the KLD loss and cross-entropy loss, the objective is \u2113\nSDQ iPQ = L\u2212F l=1 ||W l \u2212 Wl || 2 2 + \u03b2 L-F d i (A S l,i \u2212 A T l,i ) 2\nwhere F is the number of finetuned layers (non-quantized) at that point in training. Hence, SDQ progressively quantizes the layers throughout training when used with iPQ.\nBlock-Wise Distilled Quantization Noise For the majority of our QAT-based experiments we use Quant-Noise (Fan et al., 2020) . Quant-Noise is a SoTA QAT method that applies (fake) blockwise quantization noise at random to each weight matrix. Concretely, blocks of weights b kl in W l are chosen at random at a rate p and quantization noise is added to the chosen blocks. We can define\nA S = Softmax WQ WK \u221a d k W\u22a4 V W\u22a4 Q WQ WU\nwhere W represents (fake) quantized weights and is given as\nW = \u03d5 INT-8 (W) = s(round(W/s + b) \u2212 b)\nwhere s and b are scalars learned throughout training and represent the scaling factor and offset respectively. We then pass A S and A T to Equation 2to compute the loss.\n\nEmpirical Results\nWe begin by referring the reader to the supplementary material for the experimental setup in subsection A.2 and subsection A.3. Before discussing the main results on XGLUE, we first analyse the mean absolute quantization error and the Frobenius norm of the elementwise difference in selfattention blocks between an INT-8 dynamically quantized InfoXLM Base and an unquantized FP-32 InfoXLM Base in Figure 2 . We see in Figure 2a that the output layer contains the largest mean absolute error across each layer and highest error variance. In contrast, the query, key and value (QKV) parameters have much smaller error. However, since most of the parameters are found in the QKV layers, the sum of the quantization error is larger, as seen in Figure 2b . This motivates us to focus on the output of the self-attention block when minimizing quantization errors with our proposed loss in Equation 2 as the mean error is higher near the output as it accumulates errors from previous layers in the block. This is also reflected in the parameter distribution of each layer type across all layers in Figure 3 , where the x-axis is the mean absolute quantization error and the y-axis is the layer indices. We see the quantization noise is more apparent on the output layer as the Gaussian distrbutions are non-smooth and have clear jitter effect. 4.1 Quantization Results on XGLUE.\nWe show the per task test performance and the understanding score (i.e average score) on XGLUE for quantization baselines and our proposed SDQ approaches in Table 1 (for brevity we denote InfoXLM Base as I and XLM-R Base as X). Our proposed QNAT Att-KLD achieves the best average (Avg.) score and per task performance for all tasks, using a fine-tuned InfoXLM Base (XNLI, NC, NER and QAM) and a fine-tuned InfoXLM Base trained with QuantNoise and dynamically quantized post-training (PAWSX, POS, QAM, QADSM and WPR). We also find that QNAT Att-KLD improves over QNAT KLD , highlighting that the attention loss is improving quantized model performance. In preliminary experiments we found it is better to distil from a fine-tuned teacher that has the same pretrained model type. Lastly, We note, that both of our proposed methods that achieve an 71.1 understanding score are within 1.0 understanding score of the original \"I\" fine-tuned FP-32 model. \n\nPerformance versus Compression Rate\nFigure 4 shows how the performance changes for four approaches, including two of our proposed objectives (QNAT KLD and QNAT Att-KLD ), when training InfoXLM Base . As before, PTQ dynamic is a dynamically quantization fine-tuned InfoXLM Base and QNAT-PTQ dynamic is the same as PTQ dynamic except fine-tuned also using QuantNoise. Unlike our previous results, here we apply fake quantization at inference to achieve compression lower than INT-8 and be comparable to previous work (Fan et al., 2019) . We see that performance is generally well maintained up until 8 bits. However, performance significantly degrades for all quantization methods for 4 and 2 bit weights. We find that QNAT Att-KLD maintains higher performance when compared to the baselines and directly quantizing with no QAT (PTQ dynamic ) leads to the poorest results, also reflected in Table 1 results with real dynamic quantization at inference time. \n\nAblation with Current QAT Methods\nTable 3 shows the results from a subset of the XGLUE tasks where the first two columns describe how the student and teacher networks are trained and \"Standard\" refers to standard FP-32 fine-tuning. This includes iPQ (Stock et al., 2019) with scalar quantization (iPQ Scalar ), iPQ that uses expectation maximization to create the codebook during training (iPQ EM ) and previous results of QuantNoise (QNAT) as a reference point. In this setup, we only apply the attention loss, \u2113 Attention , to the layers that are quantized during iPQ. When using SDQ, the average score increases by 1.9 points for iPQ Scalar , 1.9 points for iPQ Scalar 2.8 points for iPQ EM and 1.4 points for QNAT. Moreover, adding SDQ distillation of the logits and the self-attention outputs improves when compared to logit distillation only.\n\nConclusion\nIn this paper we proposed an attention-based distillation that minimizes accumulative quantization errors in fine-tuned masked language models. We identified that most of the quantization errors accumulate at the output of self-attention blocks and the parameter distribution of the output layer is effected more by quantization noise. The proposed distillation loss outperforms baseline distillation without the attention loss and the resulting INT-8 models are within 1 understanding score points on the XGLUE benchmark with real quantization post-training. Moreover, fine-tuning the teacher network with quantization-aware training can further improve student network performance on some of the tasks. Further compression can be achieved up to 4-bit and 2-bit weights but performance steeply degrades as the network capacity is drastically reduced coupled with the models having to generalize to multiple languages it was not trained on.\n", "hypothesis": "We apply SDQ to multilingual models XLM-R Base and InfoXLM Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while achieving significantly better performance on the XGLUE benchmark. Our results indicate that SDQ is a superior method for compressing large language models compared to traditional quantization approaches.", "answer": false}
{"title": "HiPool: Modeling Long Documents Using Graph Neural Networks", "content": "\nIntroduction\nTransformer-based models like BERT (Vaswani et al., 2017a) and RoBERTa (Zhuang et al., 2021) have achieved satisfying results in many Natural Language Processing (NLP) tasks thanks to largescale pretraining (Vaswani et al., 2017b) . However, they usually have a fixed length limit, due to the quadratic complexity of the dense self-attention mechanism, making it challenging to encode long sequences.\nOne way to solve this problem is to adapt Transformers to accommodate longer inputs and optimize the attention from BERT (Feng et al., 2022; Jaszczur et al., 2021) . BigBird (Zaheer et al., 2020) applies sparse attention that combines random, global, and sliding window attention in a long sequence, reducing the quadratic dependency of full attention to linear. Similarly, Longformer (Beltagy et al., 2020) applies an efficient self-attention with dilated windows that scale linearly to the window length. Both models can take up to 4096 input tokens. Though it is possible to train even larger models for longer sequences, they are restricted by a pre-defined maximum length with poor scalability. More importantly, they fail to capture high-level structures, such as relations among sentences or paragraphs, which are essential to improving NLP system performance (Zhang et al., 2018; Zhu et al., 2019) .\nAnother way is to apply a hierarchical structure to process adjustable input lengths with chunking representations for scalability on long sequences. Hi-Transformer (Wu et al., 2021) encodes both sentence-level and document-level representations using Transformers. ToBERT (Pappagari et al., 2019) applies a similar approach that stacks a sentence-level Transformer over a pretrained BERT model. While most of the existing work models upper-level hierarchy using sequential structures, such as multiple layers of LSTMs (Hochreiter and Schmidhuber, 1997) or Transformers, this may still bring the long dependency issue when the sequence gets longer. To alleviate this, we investigate graph modeling as a novel hierarchy for upper levels. Besides, we also consider inter-hierarchy relationships using a new attention mechanism.\nOur key insight is to replace the sequence-based model with a hierarchical attentional graph for long documents. We first apply a basic pretrained language model, BERT or RoBERTa, to encode local representation on document chunks with a fixed length. The number of chunks could be extended for longer sequences for better scalability. Different from other works, we apply a graph neural network (GNN) (Zhou et al., 2018) to model the upper-level hierarchy to aggregate local sentence in-formation. This is to alleviate the long dependency issue of the sequential model. Moreover, within such a graph structure, we propose a new heterogeneous attention mechanism to consider intra-and cross-sentence-level correlations.\nOur contributions are two-fold: 1) We propose HiPool with multi-level hierarchies for long sequence tasks with a novel inter-hierarchy graph attention structure. Such heterogeneous graph attention is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences; 2) We benchmark the LDC (long document classification) task with better scaled and length-extended datasets. Evaluation shows that HiPool surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset. Code is available at https: //github.com/IreneZihuiLi/HiPool.\n\nModel\nWe introduce the HiPool (Hierarchical Pooling) model for long document classification, illustrated in Fig. 1 . It consists of an overlapping sequence encoder, a HiPool graph encoder, and a linear layer. Overlapping Sequence Encoder. Given the input document S, we first chunk the document into a number of shorter pieces with a fixed length L, and we set the overlapping window size to be L olp . Overlapping encoding makes it possible for a chunk to carry information from its adjacent chunks but not isolated, differentiating our model from other hierarchical ones. Then each chunk is encoded with a pretrained Transformer model, i.e., BERT or RoBERTa; we choose the CLS token representation as the input to our HiPool layer: X = BERT(S). HiPool Graph Encoder. We apply a graph neural network to encode incoming word-level information. Such a model has shown its potential in some NLP tasks (Li et al., 2022 (Li et al., , 2021)) . We construct a graph, defined by G(V, E), where V is a set of nodes, and E is a set of node connections. There are two node types: n low-level nodes and m high-level nodes, and typically m < n. In our experiment, we set m = n/p, and p \u2265 0. The feedforward operation goes from low-to high-level nodes. In layer l, low-level nodes are inputs from the previous layer l \u2212 1, while high-level nodes at layer l are computed based on low-level ones. Moreover, these high-level nodes will be the input to the next layer l + 1, becoming the low-level nodes in that layer. We consider X the low-level\n\nProposed Method\nMulti-level hierarchies for long sequence tasks with a novel inter-hierarchy graph attention structure. Heterogeneous graph attention is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences.\n\nOverlapping Sequence Encoder\n\u2022 Pretrained model for fixed sequence length. nodes in the first HiPool layer, as shown in the figure.\nIn each HiPool layer, given node representation H l and adjacency matrix A l at layer l, the task is to obtain H l+1 :\nEQUATION\nInspired by DiffPool (Ying et al., 2018) , we conduct a clustering method to aggregate information. We assign node clusters with a fixed pattern based on their position. For example, adjacent low-level neighbors should map to the same high-level clustering node. So we first define a clustering adjacency matrix A self \u2208 IR n\u00d7m that maps n nodes to m nodes, indicating the relations from low-to high-level nodes, marked as black arrows in the figure. Note that our approach allows overlapping, in which some nodes may belong to two clusters. We set the clustering sliding window to be 2p, with a stride to be p. In the figure, we show the case of p = 2. We denote interactions between low-level nodes by the adjacency matrix A l , 1 and we model it using a chain graph, according to the natural order of the document. 2 Then, the relations between high-level nodes A l high and their node representations H l high are computed:\nA l high = A T self A l A self , H l high = A self H l .\n(2)\nBesides, for each high-level node, to strengthen the connections across different clusters, we propose an attention mechanism to obtain crosssentence information. We propose a new edge type that connects external cluster low-level nodes to each high-level node, and the adjacency matrix is simply A cross = 1 \u2212 A self , marked by green in the figure . We update H l high as the following:\nEQUATION\nwhere W atten is trainable, and W score is a scoring matrix. We then apply a GNN to obtain H l+1 . For example, a graph convolution network (GCN) (Kipf and Welling, 2016):\nEQUATION\nWe run our experiments with two layers, and apply a sum aggregator to achieve document embeddings. More HiPool layers are also possible. Linear Layer. Finally, a linear layer is connected and cross-entropy loss is applied during training.\n3 Experiments\n\nLDC Benchmark\nThe LDC benchmark contains six datasets. We first choose four widely-used public datasets. Hyperpartisan (HYP) (Kiesel et al., 2019 ) and 20News-Groups (20NG) (Lang, 1995) \n\nEvaluation\nHyperparameters. We list details in Appendix C.\nBaselines. We select four pretrained models: BERT (Devlin et al., 2019) , RoBERTa (Zhuang et al., 2021) , BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) . We also compare with a hierarchical Transformer model To-BERT (Pappagari et al., 2019) . Hi-Transformer (Wu et al., 2021) failed to be reproduced as there is no code available. We evaluate two variations of our HiPool method by changing the sequence encoder model: HiPool-BERT and HiPool-RoBERTa.\nWe sequence in Appendix A. Hierarchy variations. To further compare sequential and graph hierarchy, we keep the word encoder and replace the HiPool graph encoder with the following sequential modules: Simple linear summation over low-level nodes; CNN applies a 1-dimension convolution; Trans is to apply a Transformer on top of low-level nodes. Besides, we also look at multiple graph settings: Aggr-mean is to use a mean aggregator to obtain the final document representation; Aggr-std is to use a feature-wise standard deviation aggregator; finally, Aggr-pcp applies Principal Neighbourhood Aggregation (PNA) (Corso et al., 2020) . We report results on Amazon-2048 in Tab. 3, as it has the longest sequence on average. An observation is that applying aggregators are better than simpler structures, while keeping a graph is still a better choice. HiPool also considers attention in message passing, so it is doing even better. We also test other variations in Appendix B.\n\nAblation Study\nEffect of input length. To better understand the effect of input length, in Fig. 2 , we present an ablation study on the Amazon-2048 and ILDC, and compare three models: BigBird, Longformer, and HiPool. In general, the models benefit from longer input sequences in both datasets. Interestingly, when sequence is larger than 2048, Longformer and Bigbird could not improve and they are limited in maximum lengths. In contrast, as the input sequence gets longer, HiPool steadily improves, showing its ability to encode long documents in a hierarchical structure. Model component. Next, we look at how each component of HiPool affects performance. As shown in Tab. 4, we first take the best model setting, HiPool-RoBERTa, and compare it with the following settings: 1) w/o RoBERTa is to replace RoBERTa with BERT, then the model becomes HiPool-BERT; 2) w/o HiPool is to remove the proposed HiPool module and replace with a simple CNN (Kim, 2014) ; 3) w/o Overlapping is to remove the overlapping word encoding. We could see that removing the HiPool Layer leads to a significant drop, indicating the importance of the proposed method. Moreover, the HiPool framework can work with many pretrained language models, as we can see that applying RoBERTa improves BERT. A complete result table can be found in Appendix.\n\nConclusion\nIn this paper, we proposed a hierarchical framework for long document classification. The evaluation shows our model surpasses competitive baselines.\n", "hypothesis": " So some recent works utilize hierarchies to model long sequences.  However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues.  In this paper, we alleviate these issues through a graph-based method.  We first chunk the sequence with a fixed length to model the sentence-level information.  We then leverage graphs to model intraand cross-sentence correlations with a new attention mechanism.", "answer": true}
{"title": "CDA: A Contrastive Data Augmentation Method for Alzheimer's Disease Detection", "content": "\nIntroduction\nAlzheimer's Disease (AD) is a debilitating neurodegenerative disorder characterized by a progressive cognitive decline that is currently incurable. It accounts for up to 70% of all cases of dementia (Association, 2020) . With an aging population, the prevalence of AD is on the rise. As symptoms of Alzheimer's disease can be mistaken for a variety of other cognitive disorders, traditional diagnostic methods, such as physical screening or neurological testing, can be challenging and time-consuming. Furthermore, they require a certain degree of clinician expertise (Prabhakaran et al., 2018) .\nConsequently, the development of automatic detection methods for Alzheimer's disease is essential to the advancement of current medical treatment. The use of machine learning methods to detect AD or other diseases automatically has gained increasing attention in recent years (Luz et al., 2018; Martinc and Pollak, 2020; Liu et al., 2021; Yu et al., 2023) . Nevertheless, these approaches have limitations due to a lack of data and the generalizability of the models. Some studies have attempted to address this problem by model ensembling (Syed et al., 2021; Rohanian et al., 2021) , multi-task learning (Li et al., 2022; Duan et al., 2022) or data augmentation (Woszczyk et al., 2022) , but the improvement in performance is not always substantial.\nInspired by previous research that AD patients often have language disorders, such as difficulties in word finding and comprehension (Rohanian et al., 2021) , we propose a novel Contrastive Data Augmentation (CDA) approach for automatic AD detection. In our study, we simulated cognitive decline associated with Alzheimer's disease by randomly deleting words from the speech transcript to create negative samples. It is expected that the corrupted samples are in worse condition than the original due to the degradation of coherence and semantic integrity. Compared to traditional data augmentation methods, the CDA method expands the dataset scale and utilizes augmented data more effectively. We have demonstrated in our experiments on the ADReSS Challenge dataset that our approach uses linguistic features alone, is more generalizable to unseen data, and achieves superior results compared to strong baselines.\n\nData and Preprocessing\nWe use the data from the ADReSS Challenge (Alzheimer's Dementia Recognition through Spontaneous Speech) (Luz et al., 2020) , a subset of the DementiaBank's English Pitt Corpus (Becker et al., 1994) . It consists of recordings and transcripts of spoken picture descriptions from the Boston Diagnostic Aphasia Examination. During the examination, the subject is shown a picture and is asked to describe its content in their own language. \n\nEncoder Classifier\nThe outputs of two forward pas with the same data A total of 156 speech audio recordings and transcripts were obtained from English-speaking participants in the ADReSS dataset, with an equal number of participants (N=78) diagnosed with and not suffering from Alzheimer's disease, as shown in Table 1 . Annotated transcripts in the dataset are in CHAT format (MacWhinney, 2014) . Participants' ages and genders are also balanced to minimize the risk of bias in prediction. As some of the tokens in CHAT format are highly specific and are unlikely to be included in BERT tokenizers, we converted them into actual repetitions of words. We remain with only words, punctuation, and pauses for input into the BERT model. Our method uses only the transcripts from the dataset. \n\nMethods\nFigure 1 illustrates the framework of the proposed model. Firstly, for each transcript, we generate a number of augmented instances, which are then input to Text Encoder along with the original transcripts to obtain their corresponding representations. Then the classifier uses feature vectors acquired in Text Encoder and output a probability of being AD for each transcript and its corresponding augmented samples. We will discuss more details in the following subsections.\n\nText Encoder and Classifier\nFor fair comparisons with previous work (Woszczyk et al., 2022) , the input text is encoded using the pre-trained BERT (bertbase-uncased) and represented by [CLS] after bert_pooler. Given a text sequence x i , we can get the encoded representations h i through the encoder.\nEQUATION\nAfter obtaining the embedding of the transcript, we pass it through a simple linear classifier (Eq. 2) to get final prediction scores, we use the commonly used binary cross-entropy (BCE) as our classification loss function, and the classification loss is denoted as L BCE (Eq. 3).\nEQUATION\nEQUATION\n, where y i is the golden label for x i , W and b are trainable parameters in classifier.\n\nContrastive Data Augmentation\nThe performance of previous work is limited due to a lack of data availability. To alleviate this, we propose the contrastive data augmentation approach (CDA) to replicate the cognitive decline associated with AD to expand the data size and improve the model robustness.\nNegative Sample Generation Assuming that the dataset {x i , y i } N i=1 contains N training samples. We randomly delete a proportion of p \u2208 [0, 1] words from each sample for n neg times to create n neg negative samples. After that we can get an augmented set {x i , y i , X i neg } N i=1 , where X i neg = {x j i } nneg j=1 are from x i . We can further augment the training set by repeating the whole process for n aug times to get {x i , y i , X i neg }\nN \u00d7naug i=1\nand expand the data size by n a ug.\nPositive Sample Generation Inspired by Gao et al. (2021) , we resort to the randomness of dropout to construct positive samples. Dropout is a popular regularization technique due to its simplicity, but the randomness it introduces may hinder further improvements in the model's generalization performance. R-Drop (Wu et al., 2021) is proposed to fix the aforementioned problem by ensuring consistency between the outputs of two forward-pass with the same data. We deploy the R-Drop algorithm as a regularization method for generating positive instances. More specifically, the original sample x i is fed to the model twice at each step, and two corresponding predictions, denoted as \u01771 i and \u01772\ni , are obtained. Then we try to minimize the bidirectional Kullback-Leibler (KL) divergence between them, which is denoted as L KL (Eq. 4):\nL KL = N i=1 1 2 [D KL (\u0177 1 i \u01772 i ) + D KL (\u0177 2 i \u01771 i )] (4)\nContrastive Loss It is reasonable to assume that the negative samples are more likely to have AD than the original ones in view of the degradation in semantic coherence and integrity. To achieve this, we regularize their differences to be larger than a margin m.\nParticularly, the encoder receives x i and X i neg as input and outputs their corresponding embedding representations h i and H i neg . Then, their representations are fed to the classifier to get a final score \u0177i and \u1ef9j i for x i and xj i , respectively. Their differences becomes Eq. 5:\nEQUATION\n, where m is the margin between positive and negative samples. The final loss is a combination of the above three loss terms L BCE , L margin and L KL .\nEQUATION\n, where \u03b1 and \u03bc are hyperparameters that control the impact of positive and negative samples, and we set \u03b1 = 0.5 and \u03bc = 0.5 in our model.\n\nExperiments\nWe employ 10-fold cross-validation to estimate the generalization error and adjust the model's parameter settings. The best setting is used to retrain models on the whole train set with five different random seeds and is then applied to the test set.\nThe results reported in this paper are the average of these models. The accuracy is used as the primary metric of task performance since the dataset is balanced. Recall, precision, and F1 are also reported for the AD class to provide a more comprehensive assessment. The hyperparameters in our model are: learning rate=1e-04, batch size=8, epoch=5, n aug =3, n neg =3, p=0.3, margin=0.1.\n\nBaselines\nWe compare our method with: 1) LDA, which is the challenge baseline linear discriminant analysis (LDA) (Luz et al., 2020) ; 2) BERT, Balagopalan et al. ( 2021) compared BERT models with featurebased Models and obtained relatively better results using the former; 3) Fusion, Campbell et al. (2021) fused the features of language and audio for classification; 4) SVM(BT RU) (Woszczyk et al., 2022) , is the SVM model using Back-translation from Russian that achieves the best results over the BERT model using Back-translation from German (BT DE); 5) Ensemble methods, Sarawgi et al. (2020) take a majority vote between three individual models. ERNIE0p and ERNIE3p are based on ERNIElarge (Sun et al., 2020) that use original transcripts and transcripts with pauses manually inserted for AD classification, respectively.\n\nResults\nThe main experimental results are shown in Table 2 . We can observe that the performance significantly improves when BERT is applied. Backtranslation data augmentation results in consistent improvements in both BERT (BT DE) and SVM (BT RU), suggesting that data argumentation is a promising strategy. Our method achieves accuracy (87.5%), precision (88.1%), and F1 score (86.9%), outperforming the baseline method by a substantial margin, suggesting the effectiveness of cognitive impairment simulation in our method. By ensembling our models on five models with a majority vote mechanism, the performance improves significantly (4.2% absolute improvements in accuracy and 4% absolute improvements in F1 score, respectively) and achieves the best results among all\n\nMethods\nAccuracy% Precision% Recall% F1% LDA (Luz et al., 2020) 75.0 83.0 62.0 71.0 BERT (Balagopalan et al., 2021) 83.3 83.9 83.3 83.3 Fusion (Campbell et al., 2021) 83.3 80.1 87.5 84.0 BERT(BT DE) (Woszczyk et al., 2022) 84.0 -75.0 -SVM(BT RU) (Woszczyk et al., 2022) methods, outperforming even ERINE, a larger and knowledge-richer pre-trained model.\n\nAblation Study\nTo determine the effectiveness of the main modules, namely random deletion (RD) and regularized dropout (R-Drop), we removed them from the model one by one and tested their impact on performance in 10-fold cross-validation. As shown in Table 3 , by combining the contrastive data augmentation strategy with the base BERT, our model outperforms it by a large margin. However, when either module is removed, the model experiences a significant loss of performance, suggesting their positive contributions to the performance.\n\nParameter Analysis\nWe also perform parameter analysis under the same experimental settings. As illustrated in Figure 2 , we can see that a lower deletion rate leads to relatively higher accuracy, as the more words deleted, the less informative the transcript is. But a large margin negatively impacts both recall and accuracy.\nAs for n aug , the model performs better regarding recall and accuracy when it is set to 3, and lower or higher values will affect the performance. The same conclusion applies to n neg , where a breakdown of the model is observed when n neg =7. The model performance also improves as the number of negative samples increases. However, this will take more computing resources. \n\nConclusion\nOur experiments show the potential of contrastive data argumentation in improving the accuracy of models for Alzheimer's disease diagnosis. As a comparison to large, complex multimodal models, and other techniques of data augmentation, we obtain the best results by simulating cognitive impairment caused by AD. Despite the small size of the dataset, the results of this study provide a basis for further research into more complex issues.\n", "hypothesis": "Experimental results on the benchmark ADReSS Challenge dataset demonstrate that our model achieves the second-best performance among language-based models.", "answer": false}
{"title": "Enhancing Out-of-Vocabulary Estimation with Subword Attention", "content": "\nIntroduction\nWord embeddings are very useful in natural language processing tasks. Methods like word2vec (Mikolov et al., 2013a,b) and GloVe (Pennington et al., 2014) train strong semantic representations of words using co-occurrence statistics on a large text corpus, and have been shown to be effective at semantically representing text data. However, one weakness of these methods is that they only learn representations for words that exist in the training corpus, and therefore have no representations on unknown terms, known as out-of-vocabulary (OOV) words. Contextualized embeddings like BERT (Devlin et al., 2018) also suffer from weak performance on rare and unknown words, despite being able to build a contextualized representation of them (Schick and Sch\u00fctze, 2020) . Therefore learning representations for OOV words is an important endeavour. In this work, we focus on static embeddings, where a large amount of OOV work is focused on, and leave contextualized embedding to future work.\nCurrent approaches combine subword and context information to estimate OOV words. While these approaches apply attention mechanisms to aggregate context representations, they tend to do very little with subword representations. As a result, this paper proposes SubAtt, a deep neural network attention model that estimates OOV word representations using attention layers (Vaswani et al., 2017) on the subwords in addition to the contexts. SubAtt also pretrains subword representations, allowing it to learn quality representations before combining it with context. We show that both pretraining and applying attention on subwords improves OOV estimates, and show that SubAtt generally outperforms state-of-the-art OOV estimation models in both intrinsic and extrinsic tasks.\n\nRelated Work\nThere are multiple strategies to estimate OOV embeddings. Some OOV strategies use word roots of the OOV word to estimate OOV embeddings (Bojanowski et al., 2017; Pinter et al., 2017; Sasaki et al., 2019) while other methods use the OOV word's context (Lazaridou et al., 2017; Horn, 2017; Herbelot and Baroni, 2017; Arora et al., 2017; Mu and Viswanath, 2018; Khodak et al., 2018) . However, more recent attempts combine both subwords and context approaches. Schick and Sch\u00fctze propose the Form-Context model (Schick and Sch\u00fctze, 2019c), which estimates OOV embeddings by combining the sum of ngram embeddings (learned by the model) with the sum of word embeddings in the contexts multiplied by a weight matrix (also learned by the model). This model has been extended to the Attentive Mimicking model (Schick and Sch\u00fctze, 2019a) which adds an attention mechanism to the context calculations. A second combined approach is the attention based hierarchical context encoder, known as HiCE (Hu et al., 2019) . HiCE is a transformer based model that leverages the hierarchical structure of contexts, using a transformer encoder to encode each context sentence into a sentence embedding, and then using another transformer encoder to combine each sentence embedding into a full context embedding. It estimates subword information using a character based convolutional neural network (CNN), and then combines each piece into a final OOV embedding. HiCE also adapts its model to the OOV word's corpus using Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) . Another approach, Estimator Vectors (Patel and Domeniconi, 2020) , trains its own word embeddings, along with subword and context embeddings for OOV estimation. BERTRAM (Schick and Sch\u00fctze, 2019b) applies an approach similar to the above models, but for contextualized embedding models like BERT (Devlin et al., 2018) . While these approaches create strong estimates for OOV words, they have some weaknesses. First, although some use attention mechanisms with the context of an OOV word, none of the aforementioned combined approaches use attention for processing the OOV's subwords 1 . Secondly, none of the static embedding approaches pretrain their subwords; they learn these representations at the same time as the whole model 2 . Therefore, we propose SubAtt, a model that uses attention and pretraining on subwords, leading to stronger OOV estimates.\n\nSubAtt\nWe now present SubAtt, a transformer (Vaswani et al., 2017) based model for OOV estimation. First, we describe pretraining the subword representations in Section 3.1, then how the model encodes each context sentence in Section 3.2, and finally how SubAtt combines subword and context information in Section 3.3.\n\nPretraining Subword Representations\nFirst, SubAtt learns subword representations for the current set of word embeddings. SubAtt learns embeddings for character ngrams of each vocabulary word. This is accomplished by adding a beginning and end special token to the word, and then taking each character subset of that word. We learn representations using the following formulation:\nsub wt = 1 |G wt | g\u2208Gw t z g (1)\nwhere G wt is the set of character n-grams (the subwords) of the word w t , and z is the embedding of the subwords. Subword representations z are learned by maximizing the cosine similarity between sub wt and the corresponding word embedding v wt . Once these subword representations are trained, they are used in the main SubAtt model. An OOV word is broken down into its character ngrams, which are then converted to the set of corresponding subword embeddings Z.\n\nContext Encoder\nSubAtt encodes sentences using a context encoder similar to the one in HiCE (Hu et al., 2019) . For each word, an input embedding is built by combining its word embedding and a position embedding. The set of input embeddings for context j (denoted context words Q j ) are then inputted into a transformer encoder:\nEQUATION\nwhich is then averaged for a final context representation c j . These representations make up the set of context embeddings C.\n\nFull SubAtt Model\nSubAtt is composed of a subword half and a context half. The subword inputs Z are the OOV word's ngram subword representations learned in Section 3.1. For the list of contexts, the context representations C are calculated using the architecture described in Section 3.2. Each type is processed through their own sets of multi-head self attention encoders \nEQUATION\nEQUATION\nFinally, we combined the representations for a final estimate of the OOV embedding. Subwords and contexts can vary in how informative they are to the OOV word, and so it is important to combine them in a fashion that weighs each estimate accordingly. SubAtt uses an adaptive weighting strategy used in the Form Context Model and Attentive Mimicking Model (Schick and Sch\u00fctze, 2019c,a), known as the gated model. The subword outputs Z self and context outputs C self are separately averaged into v subword and v context respectively. They are then combined by a weighted sum:\nv f inal = \u03b1 v subword + (1 \u2212 \u03b1) v context (5)\nThe weight \u03b1 is calculated as follows:\n\u03b1 = \u03c3(w T [ v subword , v context ] + b) (6)\nwhere w and b are learned parameters, and \u03c3 is the sigmoid function. v f inal is the final estimate of the OOV word embedding. SubAtt has eight layers of self attention for the subword inputs and eight layers for the context input.\n\nTraining Corpus and Word Embeddings\nThe goal of SubAtt is to estimate representations for OOV words given existing word embeddings.\nFor the gold standard word embeddings, we use the embeddings provided by Herbelot and Baroni (Herbelot and Baroni, 2017) , as done in previous OOV models like (Schick and Sch\u00fctze, 2019c) and (Hu et al., 2019) . For training models, contexts are taken from the Westbury Wikipedia Corpus (WWC) (Shaoul, 2010) . We use the version from (Khodak et al., 2018) \n\nBaselines and Hyperparameters\nWe now demonstrate the effectiveness of SubAtt. 4 We compare it to Attentive Mimicking 5 (AM) and HiCE 6 , as they are OOV models that use both subwords and context on existing static word embeddings. Two versions of HiCE are examined; the default with a 2 layer context aggregator, and a version with 8 layers to be more comparable to SubAtt. Also, we do not use MAML in the HiCE experiments, in order to focus on how the architecture adapts to multiple OOV tasks. The dataset and vocab are split into a training and validation set for hyperparameter tuning (discussed in more detail in Appendix A).\nTen final trials of each model are trained and then each model is evaluated on various OOV tasks. The results are tested for statistical significance using a one-way ANOVA with a post-hoc Tukey HSD test with a p-value threshold equal to 0.05. The best score is presented in bold, along with any scores that are not significantly different from the best.\n\nTasks\nWe now evaluate SubAtt on various OOV tasks. We focus on OOV tasks in English, matching previous work. As SubAtt mixes both subwords and (Khodak et al., 2018) . CRW is built off the Rare Word dataset (Luong et al., 2013) , which is a list of rare words paired with other words, along with human similarity scores. Khodak et al. (2018) added contexts to this set, allowing for OOV words to be estimated using both subwords and context. The goal is to output an OOV embedding, compare it to the other words, and evaluate the scores' correlation with human judgements. CRW has a large range of context sizes, from 1 to 128, so the quality and informativeness of the context can vary wildly. However, the words gathered for the Rare Word set have intentionally informative word roots, and therefore we expect subwords to be fairly informative.\nThe results of the CRW task are shown in Figure 1 . SubAtt significantly outperforms all competitors in all contexts, showing its effectiveness as an OOV estimator. This shows the strength of pretrained subwords and subword attention.\nDownstream Tasks We now demonstrate the strong performance of SubAtt embeddings extrinsically, using downstream tasks. In order to focus on OOV words specifically, we choose downstream tasks that output word level labels; specifically named entity recognition and parts-of-speech tagging. For each of these tasks, we train a Bi-LSTM-CRF (Lample et al., 2016) , an approach similar to the one in (Hu et al., 2019) . The input to these models are normal word embeddings for words in our vocabulary (ones used in training and validation of the original OOV models), and each model's OOV estimates for unknown words. 7 For 7 OOV words with invalid subwords (no existing character ngrams or no CharCNN characters) are assigned a zero vector. each dataset, the Bi-LSTM-CRF is trained for 30 epochs 10 times, with the best epoch selected using a validation set each time. This approach is applied to each of the 10 trials of each OOV model. As our focus is estimating OOV words, we report the average test macro F1 score of the OOV words specifically. We also report the results for all words in Appendix B.\nWe test on 5 named entity recognition tasks: the JNLPBA 2004 Bio-entity recognition dataset (Bio-NER) (Kim et al., 2004) , the Rare-NER dataset (Derczynski et al., 2017) , the CoNLL 2003 NER dataset (Sang and De Meulder, 2003) , AnEM (an anatomy NER dataset) (Ohta et al., 2012) , and MovieMIT, a movie querying dataset (Liu et al., 2013) . In addition, we test on a parts-of-speech tagging dataset, specifically the Twitter social media POS task (Ritter et al., 2011) .\nThe Downstream Task results are shown in Table 1 . SubAtt generally outperforms the competitors, strictly winning in 3 of the 6 tasks, and tying for best in one more task, and achieving the second best score in another task. This demonstrates Sub-Att's robust and strong performance on OOV words in downstream tasks.\n\nAblation Analysis\nWe now conduct an ablation study on SubAtt in order to demonstrate the impact of the pretraining compared to attention. To this end, we repeat the previous experiments on four variants of SubAtt; the original model, the model without attention (SubAtt No Att), the model without pretrained subwords (SubAtt No Pre), and the model without both (SubAtt No Pre No Att). The results are shown in Figure 2 creases 8 . This makes sense, as the influence of subwords decreases as our model gains more and more context information, which in turn lowers the impact of the pretraining and attention on subwords in general. Similarly, SubAtt performs strongly in the Downstream Ablation, performing the best or tied for the best in all six tasks. The results also demonstrate that pretraining and subword attention individually have a high impact on results, and both combined leads to an even stronger improvement.\n\nConclusion\nWe propose SubAtt, an attention based model that estimates OOV words by using pretrained subword embeddings and subword attention. We show through various experiments that this model estimates more accurate representations of OOV words.\n", "hypothesis": " As a result, there have been multiple attempts to learn OOV word representations in a similar fashion to how humans learn new words, using word roots/subwords and/or surrounding words.  However, while most of these approaches use advanced architectures like attention on the context of the OOV word, they tend to use simple structures like ngram addition or character based convolutional neural networks (CNN) to handle processing subword information.  In response to this, we propose SubAtt, a transformer based OOV estimation model that uses attention mechanisms on both the context and the subwords.  In addition to attention, we also show that pretraining subword representations also leads to improvement in OOV estimation.", "answer": true}
{"title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker", "content": "\nIntroduction\nInformation retrieval (IR) is the task of searching for documents relevant to a given query from a large corpus. As re-ranking the fetched documents from the retriever effectively enhances the performance and the latency, recent studies have suggested several kinds of re-rankers by fine-tuning pre-trained language models (PLM) (Nogueira and Cho, 2019; Nogueira et al., 2020) . Furthermore, Sachan et al. (2022) show that large-scale language models (LLMs) such as GPT-3 (Brown et al., 2020) can be exploited as a zero-shot reranker with the prompt describing the task. They also highlight the importance of an appropriate prompt to elicit the full performance of LLMs, rather than updating the parameters. They choose an optimal prompt among the handcrafted candidates by cross-validation. However, such a manual search for the discrete prompts is highly expensive and sub-optimal in transferability.\nTo resolve the issue, several methods are proposed for automatically optimizing the discrete prompt. They focus on text classification or maskfilling task while underestimating the open-ended generation (Shin et al., 2020; Gao et al., 2021; Prasad et al., 2022) . Recently, Deng et al. (2022) address the discrete prompt optimization applicable to generation tasks with reinforcement learning by designing the reward function, which measures the generated text belonging to a discrete label. Since there are tasks that are still not aligned, requiring a continuous score of output, we aim at a prompt optimization for one of such tasks: re-ranking.\nIn this paper, we propose Constrained Prompt generation, Co-Prompt, as left-to-right discrete prompt optimization without additional model training. By defining the metric of prompt optimum for re-ranking, we interpret the searching process of the optimal prompt as constrained generation with two modules: a zero-shot re-ranker as a discriminator and any decoder-only PLM as a generator. The discriminator calculates the likelihood (i.e., metric) that the prompt sequence is optimal for guiding an LLM to distinguish relevant documents among the large set for a given query. The generator samples the prompt tokens having a high prior from the previous prompt sequences for effectively restricting the prompt candidates for discriminator to evaluate. An overview of Co-Prompt is shown in Figure 1 .\nWe validate our method, Co-Prompt, against other optimization baselines on two LLMs, T0 (Sanh et al., 2022) and OPT (Zhang et al., 2022) , with two benchmark datasets, MS-MARCO (Nguyen et al., 2016) and Natural Question (Kwiatkowski et al., 2019) . Experimental results show that Co-Prompt consistently generates well-performing prompts regardless of LLMs and datasets over the baselines. The qualitative analyses also support the interpretability of the prompts generated by Co-Prompt, similar to human language patterns.\nOur contributions in this work are threefold:\n\u2022 We highlight the impact of optimal prompt on a zero-shot re-ranker by exploiting the optimization methods. \u2022 We propose Co-Prompt, a novel discrete prompt optimization via constrained generation for a zero-shot re-ranker. \u2022 We experimentally show that Co-Prompt consistently guides the re-ranker well against the baselines and its output is similar to human language patterns.\n\nRelated Work Document Ranking with Generative Model\nUsing the generative model is one of the dominant methods for ranking the retrieved documents by defining the relevance score as the query likelihood score (Nogueira dos Santos et al., 2020; Ju et al., 2021) . More recently, Sachan et al. (2022 Sachan et al. ( , 2023) ) showed that the LLM serves as either a zero-shot re-ranker or a training module of an unsupervised dense retriever. However, unlike ours, they require carefully designed manual prompts, which may have a limitation in transferability.\nPrompt Optimization As prompting is considered a key variable when exploiting LLMs for various NLP tasks, finding the optimal prompt has become important to get the best performance out of the LLMs (Kojima et al., 2022; Xie et al., 2022) .\nRecently, the prompt optimization work has focused on discrete prompt search (Shin et al., 2020; Gao et al., 2021; Deng et al., 2022) or soft prompt learning over a continuous space (Liu et al., 2021; Qin and Eisner, 2021; Lester et al., 2021) . While the existing optimization methods mainly consider text classification or mask-filling task, their applicability to re-ranking is yet underexplored. In this paper, we target at optimizing discrete prompts for zero-shot re-ranker to get higher relevance scores for more relevant pairs via constrained generation.\nConstrained Generation Constrained generation aims at deriving the text sequences that follow a certain constraint (Keskar et al., 2019) . Utilizing a discriminator for guiding the generation toward the constraint via the Bayes' rule is one of the widely used constraint generation methods (Dathathri et al., 2020; Krause et al., 2021; Chaffin et al., 2022) . Inspired by the effectiveness of the discriminator-based method, we adopt the zero-shot re-ranker as a discriminator when generating optimal discrete prompt sequences.\n\nPreliminaries\nAn LLM re-ranks the retrieved document d concerning the relevance score with a given query q as the query generation score:\nEQUATION\nwhere |q| denotes the token length of the query q and \u03c1 is a natural language prompt guiding an LLM to generate the query q. Since the prompt \u03c1 is the only controllable variable in Equation 1, searching for an optimal prompt is a simple yet effective way to enhance the performance of LLMs. Thus, in this work, we focus on a prompt optimization strategy.\n\nConstrained Prompt Generation\nWe define the optimal prompt \u03c1 * for the re-ranker which maximizes the query generation scores:\nEQUATION\nwhere D is the dataset for the retriever, consisting of pairs of a query and its relevant document. We solve the task of searching the optimal prompt \u03c1 * for the document-query pair dataset D with discriminator-based constrained generation. The generation is guided by the Bayes' rule: P (\u03c1t|D, \u03c11:t\u22121) \u221d PM D (Ds|\u03c11:t)PM G (\u03c1t|\u03c11:t\u22121), (3) \nP M D (Ds|\u03c1) return R end\nwhere M D is a zero-shot re-ranker serving as a discriminator, M G is a decoder-only PLM as a generator, and D s is a dataset sampled from D.\nDiscriminator The discriminator M D measures how effectively the prompt sequence \u03c1 1:t guides the zero-shot re-ranker to generate the query from the given document by computing the likelihood P M D (D s |\u03c1), defined as the expectation of relevance score between document-query pairs (q i , d i ) of the sampled dataset D s with the prompt \u03c1:\nEQUATION\nWe use this likelihood as the metric for prompt optimum. The other option of\nP M D is shown in Appendix B.1.\nGenerator The generator M G samples the pool of prompts to be evaluated by a discriminator since computing Equation 3 of all possible tokens in the vocabulary requires a prohibitively high computational cost. The decoder-only PLM is exploited to sample prompt tokens \u03c1 t having a high prior P M G (\u03c1 t |\u03c1 1:t\u22121 ) in a zero-shot manner.\nWe combine these modules to optimize the prompt by iteratively performing two steps: candidate generation and evaluation. We choose to use a beam search as a decoding strategy for left-toright prompt generation. The detailed steps of the decoding strategy are shown in Algorithm 1.\n\nExperimental Setups\nWe describe the experimental setups for validating the performance of the prompts. Our code is publicly available at github.com/zomss/Co-Prompt.\nDatasets We employ two information retrieval datasets: 1) MS-MARCO (Nguyen et al., 2016) fetched from Google search engines. We only use the document data of the dataset for evaluation. More information is shown in Appendix A.1.\n\nEvaluation Metrics\nWe evaluate the results by two metrics, ACC and nDCG. 1) ACC is the percentage of the relevant documents in the total retrieved ones. 2) nDCG, normalized discounted cumulative gain, reflects that the more relevant documents should record higher ranks.\nRetriever & Re-ranker We select two widely used sparse and dense retrievers as our retrievers, which are 1) BM25 (Robertson and Zaragoza, 2009) and 2) DPR (Karpukhin et al., 2020), respectively. For the zero-shot re-ranker, we use 1) T0 (Sanh et al., 2022) and 2) OPT (Zhang et al., 2022) . We describe more detailed information in Appendix A.3 and A.4.\n\nPrompt Baselines\nWe compare Co-Prompt against four baselines: 1) Null Prompt is an empty prompt without any token. 2) P-Tuning is a soft prompt optimization method that yields prompt embeddings from the prompt encoder (Liu et al., 2021) . 3) RL-Prompt is a discrete prompt optimization method by training policy network (Deng et al., 2022) . Note that we modify RL-Prompt and P-Tuning applicable to the re-ranking task. 4) Manual Prompt, suggested by Sachan et al. (2022) , is given as \"Please write a question based on this passage\", following the assumption that it is one of the best prompts that humans can find. Last, 5) Co-Prompt, our proposed method, is a discrete prompt optimization method in left-to-right zero-shot generation. The implementation details of baselines are shown in Appendix A.5. is the first question asked on Google for\" 31.9 \"Please post your question again when its not just about\" 30.6 \"Score! What are all 3 things, the first is\" 30.2 \"Score the top 5 things on this sub reddit for\" 29.3 \"This looks like the same as every \"what are the\" 30.5 \"This post should be titled as\" 31.2 \"What are some common questions asked on the internet about\" 30.3 \"How do i find the name on google, and\" 29.1 Implementation Details The discriminator M D is the same model as the zero-shot re-ranker. Since the generator M G should be a decoder-only model, in the case of T0, GPT2-Large (Radford et al., 2019) is utilized as the generator. OPT, a decoderonly model, is used as both the discriminator and the generator. We use the start token as \"Please\" for a direct comparison with the manual prompt and fix the beam width B as 10 and the maximum prompt length L as 10 in our experiment.\nEnvironment We conduct all experiments including prompt searching and document re-ranking on V100 32GB GPUs. We use BEIR (Thakur et al., 2021 ) framework 1 for re-ranked result evaluation and passage retrieval datasets. Also, the retrievers, BM25 and DPR, are from the same framework. We employ T0 and OPT with 3B and 2.7B parameters each for the discriminator and the re-ranker publicly open on the Huggingface model hub 2 (Wolf et al., 2020) .\n\nResult\nIn this section, we show the overall results of our method, Co-Prompt, with a detailed analysis. \n\nImpact of Start Tokens\nWe exploit other options of start token such as \"Score\" and \"This\" as shown in Table 2 . Regardless of the start tokens, Co-Prompt consistently generates prompts eliciting the performance of LLM efficiently. However, we observe that finding the optimal start token for the dataset is important to achieve better results.\n\nImpact of Generator\nAs shown in Table 3 , even if different generators are used, the generated prompts by different generators guide the zero-shot re-ranker efficiently. Still, the differences in performance are caused by a vocabulary mismatch between the two modules. We see that, although our method does not vary significantly in performance to the generator, a more suitable generator may be necessary for better results.\nRelevance Score We analyze the distributions of relevance scores between positive or negative document-query pairs. As the negative documents for a given query are retrieved from BM25, the negative ones are related to the query but unable to directly find the answer. As shown in Figure 2 , we point out that the distribution difference exists between pairs despite some overlap. Also, an LLM can distinguish which pair is positive, even without a prompt. However, we observe that the effect of discrete prompt optimization on the zero-shot reranker is in the direction of increasing the mean and variance of the relevance score. \n\nConclusion\nIn this paper, we propose Co-Prompt, left-to-right prompt optimization for zero-shot re-ranker via constrained generation. Co-Prompt effectively restricts prompt candidates and evaluates the optimum of these prompts without any parameter updates. We experimentally show that our method achieves consistently outperforming performance across all experiments. Also, the impact of prompt optimization including baselines on the zero-shot re-ranker highlights its importance. We also present an interesting outcome in that the optimal prompt is interpretable for human. For future work, we plan to expand our method to other open-ended generation tasks using LLMs.\n", "hypothesis": "While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel continuous prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for reranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update.", "answer": false}
{"title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives", "content": "\nIntroduction\nThe dual encoder architecture applied to information retrieval has shown excellent performance in a wide range of tasks (Gillick et al., 2018; Karpukhin et al., 2020; Ni et al., 2021 Ni et al., , 2022)) .\nRecently, the Information Retrieval community has transitioned towards Deep Learning models that leverage large unsupervised corpus pretraining (Devlin et al., 2019; Raffel et al., 2020) , which offers more powerful semantic and contextual representation for queries and documents. These models can be successfully applied to scoring tasks, e.g. Dehghani et al. (2017) , or retrieval tasks, e.g. Gillick et al. (2018) . In contrast, classic SearchQA show that sharing a projection layer in Asymmetric Dual Encoders (ADE-SPL) (Dong et al., 2022) may not guarantee that the embeddings from the two encoder towers are in coinciding parameter spaces. However SamToNe can effectively achieve that.\nretrieval models, such as BM25 (Robertson and Zaragoza, 2009) , rely on bag-of-words lexical overlap, term frequency heuristics, inverse document frequency and document length. This type of retrieval models does not require any training and can generalize reasonably well, but they fall short of finding documents that have low term overlap but high semantic similarity.\nA dual encoder (Gillick et al., 2018; Yang et al., 2020; Karpukhin et al., 2020; Reimers and Gurevych, 2019) consists of two encoding towers that map queries and documents, respectively, into a shared low-dimensional dense representation, namely, the embedding space. The model is usually optimized by a contrastive loss (Chopra et al., 2005) , which moves the embeddings of the queries and documents from the same positive examples closer to each other, and the embeddings from negative examples farther away. Training the dual encoder in batches allows to use, for each question, the passages that answer all the other questions within the batch as negatives (Gillick et al., 2018) , namely \"in-batch negatives\". At indexing time, all the documents in a corpus are encoded via bulk inference and indexed. To run retrieval, a query is encoded and its most relevant documents can be retrieved through Nearest Neighbours Search ( Vanderkam et al., 2013; Johnson et al., 2021) over the embedding space using a measure of similarity, e.g. the dot-product or cosine distance of the embedding vectors.\nMotivation. In this work, we consider two major types of dual encoder architectures: \"Symmetric Dual Encoder\" (SDE) 1 , with parameters shared between two encoder towers, and \"Asymmetric Dual Encoder\" (ADE), with two distinctly parameterized encoder towers. Dong et al. (2022) demonstrated that sharing projection layers can significantly improve the performance of ADEs. They empirically explained the efficacy of SDE and ADE-SPL by claiming that the shared projection layers help mapping the embeddings of the two encoder towers into a coinciding parameter space.\nBy repeating this embedding space analysis on a variety tasks, we find that ADE-SPL may not be enough to ensure that the embedding spaces from two encoder towers are coinciding, as shown in Figure 1 . This motivates us to further improve the dual encoder retrieval quality beyond the architectural change explored in Dong et al. (2022) . Although the projection layers are shared, our analyses suggest that an extra mechanism, other than using the standard contrastive loss with in-batch negatives, is required to ensure the adjacency of the embeddings of a ground truth pair.\nContributions. In this paper, we propose an improved training objective for dual encoder models: contrastive loss with Same Tower Negatives (SamToNe). In Section 3, we demonstrate its usefulness on a variety of Information Retrieval tasks, including both tasks with in-task fine-tuning and a zero-shot benchmark suite. Across all the tasks explored, SamToNe performs competitively comparing to the traditional training setup, with a significant improvement on the metrics averaged across tasks. Finally, through an analysis of the produced embeddings, in Section 4, we further make evident the superiority of SamToNe from the perspective of regularisation. \n\nMethod\nDual Encoder Architecture. We follow the standard setup of information retrieval: given a query, q, and a corpus of retrieval candidates, P, the goal is to retrieve k relevant candidates, p k \u2208 P. The candidate can be a phrase, a sentence, a passage, or a document.\nRecent research (Dong et al., 2022) demonstrated that sharing projection layers can significantly improve the performance of ADEs and we use this shared projection layer for ADEs (ADE-SPL) throughout our experiments. Figure 2 illustrates the SDE and ADE-SPL architectures we use in this work. Our dual encoders are initialized from pre-trained t5.1.1 encoders (Raffel et al., 2020) . Following Ni et al. (2022) ; Dong et al. (2022) , we encode a query, q i , or a candidate, p i , by averaging the T5 encoder outputs and projecting them to the final embedding vector.\nContrastive Loss. A standard way to train a dual encoder model is optimizing an in-batch sampled softmax loss for contrastive learning (Henderson et al., 2017) :\nEQUATION\n)\nwhere sim is cosine similarity, B is a mini-batch of examples, and \u03c4 is the softmax temperature. p i is the ground-truth relevant passage for the query q i in a batch of retrieval candidates p * , where all the other passages p k (k \u0338 = i) are treated as the negative examples for contrastive learning. Bi-directional in-batch sampled softmax loss is commonly applied to improve the embedding quality of both towers, where the contrastive loss is computed for both query to passage matching and passage to query matching (Yang et al., 2019) . We use the bi-directional loss throughout this work.\nSame Tower Negatives. The in-batch sampled softmax loss is a contrastive loss that only considers the contrastive estimation between the target example pair {q i , p i }, and the in-batch sampled negative pairs {q i , p j } (j \u0338 = i).\nOne way to improve the quality of the retrieval is to improve the contrast among the embeddings of the queries. Therefore, we propose a novel contrastive loss using Same Tower Negatives, which we abbreviate as SamToNe:\nLS = e sim(q i ,p i )/\u03c4 j\u2208B e sim(q i ,p j )/\u03c4 + j\u2208B,j\u0338 =i e sim(q i ,q j )/\u03c4 , (2\n)\nwhere the second term in the denominator is the contribution from the same tower negatives. SamToNe can be interpreted as a regularized version of the in-batch sampled softmax loss, where the term j\u2208B,j\u0338 =i e sim(q i ,q j )/\u03c4 is a regularizer. When query embeddings are not well distributed, max sim(q i , q j ) \u226b max sim(q i , p j ), and the second term in the denominator will dominate the contribution from the negative examples. Thus, it will drive the separation of the query embeddings in contrastive learning. In Section 4, we provide empirical evidence of the effects of SamToNe as a regularizer of the embedding space. Ren et al. (2021) proposed an improved contrastive loss, PAIR, which is a hybrid loss\nL P AIR = \u2212(1 \u2212 \u03b1) log L c \u2212 \u03b1 log L P , where LP =\ne sim(q i ,p i )/\u03c4 j\u2208B,j\u0338 =i e sim(p i ,p j )/\u03c4\n(3) penalizes the similarities between passages / documents. Despite both SamToNe and PAIR are penalizing the similarities among the same tower inputs, there are two significant differences. single stage training and guaranteed improvement on embedding space quality, make SamToNe much easier to use.\n\nQuestion-Answering Retrieval Tasks\nWe evaluate SamToNe on 5 question-answering (QA) retrieval tasks including MS MARCO (Nguyen et al., 2016) and MultiReQA (Guo et al., 2021) . For MS MARCO, the retrieval candidates are relevant passages, and for the 4 tasks in Mul-tiReQA, the retrieval candidates are answer sentences.\nTo make a fair comparison across the results of our experiments, the same fine-tuning hyperparameters are applied to all our model variants. The models are optimized for 20, 000 steps using Adafactor optimizer (Shazeer and Stern, 2018) , with softmax temperature \u03c4 = 0.01, batch size 512, and a linearly decaying learning rate starting from 10 \u22123 to 0 at the final step. To compare SamToNe and PAIR, we use the hyperparameter \u03b1 = 0.1 for PAIR as reported in Ren et al. (2021) , and keep all the other experimental setups identical. SamToNe is applied only on the query side, as it is more robust across different datasets. For experiments and analysis on applying SamToNe on both encoder towers, please refer to Section 3.4. We benchmark Model Loss MSMARCO NQ SQuAD TriviaQA SearchQA Average P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR the fine-tuned models using precision at 1 (P @1) and mean reciprocal rank (MRR).\nAs shown in Table 1 , SamToNe greatly improves the retrieval performance of both SDE and ADE-SPL models. Using SamToNe, ADE-SPL models can outperform SDE ones, especially for TriviaQA and SearchQA, by a great margin. Relative to PAIR, SamToNe provides better performance across different datasets in both types of models.\n\nScaling the Model Size\nTo assess the impact of the model size, we evaluate the dual encoders initialized from t5.1.1-base (\u223c 250M parameters), t5.1.1-large (\u223c 800M parameters), and t5.1.1-XXL (\u223c 11B parameters). Figure 3 and Appendix Table 4 show that SamToNe consistently improves the performance of dual encoders across different model sizes.\n\nBEIR Generalization Tasks\nWe further demonstrate the efficacy of the dual encoders trained with SamToNe on BEIR (Thakur et al., 2021) , a heterogeneous benchmark for zeroshot evaluations.\nBEIR has 18 information retrieval datasets 2 across 9 domains, including Bio-Medical, Finance, News, Twitter, Wikipedia, StackExchange, Quora, Scientific, and Misc. The majority of the datasets have binary query relevance labels. The other datasets have 3-level or 5-level relevance judgements.\nAs BEIR is evaluating generalization capabilities and SDEs are commonly used for general purpose retrieval (Ni et al., 2021) , we focus on evaluating the impact of SamToNe on BEIR using the SDE architecture. In this evaluation, we reuse the model fine-tuned with MS MARCO, as described in Section 3.1.\nEvaluated with the same setting as GTR (Ni et al., 2021) , SamToNe demonstrates strong performance on BEIR, as shown in Table 2 and Figure 4 . On average, SamToNe improves NDCG@10 by 1.4% for SDE with XXL size. SDE trained with SamToNe significantly outperform BM-25, a sparse retrieval method, and GTR, a dense retrieval method that shares the same architecture and the same model size as SDE but fine-tuned with different corpora.\n\nApplying SamToNe to Both Towers\nJust as with the query tower, SamToNe can be applied to the document tower which leads to better query-document alignment. However, it is common that the training data contains a large fraction of duplicated documents for a diverse set of queries. For example, only 17% of the documents in the train-split are unique for TriviaQA, but 98% for MSMARCO. For datasets with a low rate of unique documents, applying SamToNe on the document side will penalize sim(p i , p j ) with p i = p j and may hinder the performance, as shown in Table 3 .\n\nEmbedding Space Analysis\nAs shown in the top row of Figure 1 , for MS MARCO and SearchQA, ADE-SPL generates two connected but topologically separable embedding spaces. It requires an extra mechanism, beyond the shared projection layers, to ensure the adjacency of the embeddings from a ground truth pair. SamToNe is proposed as the \"force\" drawing the embeddings of each ground truth training pair together. Its efficacy is illustrated in the bottom half of Figure 1 .\n\nSamToNe: an Embedding Distance Regularizer\nTo further understand SamToNe's role as a regularizer of embedding distances, we evaluate the distribution of the distances between the embeddings of the queries and their top-1 retrieval results in the test set of MS MARCO and SearchQA. The embedding distance is measured by cosine similarity, where 1.0 means perfect alignment with a range of [\u22121.0, 1.0]. As shown in Figure 5 , SamToNe drastically shifts the distribution of the (query, top-1 retrieval result) pairs towards 1.0, demonstrating the regularizing effect of SamToNe over the embedding distances.\nBy placing the regularizing query-query similarity terms e sim(q i ,q j )/\u03c4 and the standard inbatch negative query-document similarity terms e sim(q i ,p j )/\u03c4 together in the denominator with same weight, SamToNe pushes the similarity ratio between query-query and query-documents, sim(q i , q j )/sim(q i , p j ), to be centered around 1.0. This is a self-balancing regularization effect. The query and document spaces are set to closely overlap each other and the embeddings of a positive pair are more likely to be located in the same region of the embedding space.\nTo empirically illustrate this effect, we plotted histograms of the sim(q i ,q j ) sim(q i ,p j ) ratios for randomly selected i and j in Figure 6 . The regularization effect only shows when SamToNe is used, but not when PAIR (Ren et al., 2021) is. This is because the self-balancing effect does not exist in a hybrid loss such as PAIR.\n\nConclusions\nEvaluating on QA retrieval tasks and zero-shot generalization benchmarks, we demonstrate that training with SamToNe can significantly improve the dual encoder retrieval quality. With t-SNE maps of query and document embeddings, we show that the embedding spaces from the two encoding towers of models trained with SamToNe are better aligned. Through the distributions of similarity distances between the embeddings of queries and their nearest neighbours, we empirically explain the efficacy of SamToNe from a regularisation prospective. In general, we recommend using SamToNe to train dual encoders for information retrieval tasks.\n", "hypothesis": " A standard way to train dual encoders is using a contrastive loss with in-batch negatives.  In this work, we propose an improved contrastive learning objective by adding queries or documents from the same encoder towers to the negatives, for which we name it as \"contrastive loss with SAMe TOwer NEgatives\" (SamToNe).", "answer": true}
{"title": "How does the task complexity of masked pretraining objectives affect downstream performance?", "content": "\nIntroduction\nMasked language modeling (MLM) (Devlin et al., 2019) , where a model needs to predict a particular token that is replaced with a mask placeholder given its surrounding context, is a widely used self-supervised pretraining objective in natural language processing. Recently, simpler pretraining objectives have shown promising results on downstream tasks. Aroca-Ouellette and Rudzicz (2020) have proposed various token-level and sentencelevel auxiliary pretraining objectives, showing improvements over BERT (Devlin et al., 2019) . Yamaguchi et al. (2021) and Alajrami and Aletras (2022) have demonstrated that such token-level ob-* Equal contribution 1 Our code and pretrained models are available at https: //github.com/hitachi-nlp/mlm-probe-acl2023.\njectives themselves, i.e., pretraining without MLM, perform comparably to MLM.\nAlthough these simple token-level objectives themselves, e.g., predicting the first character of a masked token (First Char) (Yamaguchi et al., 2021) , have exhibited competitive downstream performances to MLM with smaller computations, no objectives using mask tokens are not clearly comparable to MLM on downstream tasks. We conjecture that the main reason behind the performance difference lies in its lack of complexity, i.e., the number of classes to be predicted, and similar arguments have been made for auxiliary task ineffectiveness (Lan et al., 2020) and pretraining task design (Yamaguchi et al., 2021) .\nThis paper sheds light on the task complexity of masked pretraining objectives and investigates RQ1: whether a more complex objective, becoming closer to MLM, can achieve a better downstream result and RQ2: how much complexity they need to obtain comparable results to MLM. To this end, we propose masked n character prediction as a control task, which requires us to predict the first or last n characters of a masked token, allowing us to empirically evaluate how the task complexity affects downstream performance by varying n. We pretrain 14 different types of models with the proposed control task in addition to MLM for reference and evaluate their downstream performance on the GLUE (Wang et al., 2019) , SQuAD (Rajpurkar et al., 2016) , and Universal Dependencies (UD) (Nivre et al., 2020) benchmarks. We also conduct a cost-benefit analysis of performance gains with respect to task complexity and analyze how to select an optimal complexity for a given task.\n\nContributions\n(1) We model the task complexity of a masked pretraining objective as masked n character prediction ( \u00a72) and revealed how it affects downstream performance ( \u00a74). ( 2 Number of classes to be predicted 2 9 2 5 6 2 ,3 6 0 9 ,5 0 8 2 0 ,4 8 5 2 8 ,6 8 4 4 0 ,1 3 2 5 0 ,2 6 5 2 9 2 5 6 2 ,5 3 1 8 ,9 4 5 1 7 ,9 4 2 2 6 ,6 0 4 4 0 ,6 0 8 5 0 ,2 6 5\nFirst Last pretrain a model by using a masked objective from the task complexity perspective ( \u00a75).\n\nMethodology\nOur main hypothesis is that the more complexity, i.e., the number of classes to be predicted, masked pretraining objectives have, the better the downstream performance they will achieve. This is because a more complex task should have a larger number of classes to be predicted, giving more informative signals to a model via training.\nTo verify the hypothesis empirically and answer the two research questions listed in \u00a71, we extend First Char (Yamaguchi et al., 2021) and let a model predict the first or last n \u2208 N characters of a masked token (n Chars) 2 . The task is trained with the token-level cross-entropy loss averaged over masked tokens. Our extension allows us to evaluate how the complexity affects downstream performance by varying n.\nFigure 1 shows the number of classes to be predicted for n Chars when using a pretrained tokenizer of RoBERTa (Liu et al., 2019) . We can see that the larger n, the closer the objective becomes to MLM. For 1 Char, we simply pick up the first character of each token in the vocabulary instead of casting it into 29 classes as in First Char 3 , resulting in 256 types of characters.\n\nExperimental Setup\nHere, we describe our experimental setups for both pretraining and fine-tuning. 4 Model We used the base configuration of BERT (Devlin et al., 2019) . The model consists of 12 hidden layers and attention heads, and the dimensions of hidden layers and intermediate feed-forward layers are 768 and 3072, respectively. We simply put a linear layer on the BERT model for n Chars and First Char.\nBaselines We pretrained MLM and First Char for reference to evaluate the influence of masked pretraining objective complexity. We also set up Last Char, where a model needs to predict the last character of a masked token from 29 categories the same as in First Char.\nPretraining Data Following Devlin et al. (2019) , we pretrained all models on English Wikipedia and BookCorpus (Zhu et al., 2015) using the datasets (Lhoest et al., 2021) library. We set the maximum sequence length to 512. We tokenized texts using byte-level Byte-Pair-Encoding (Sennrich et al., 2016) , and the resulting corpora consist of 10 million samples and 4.9 billion tokens in total.\nFine-tuning Data We used the GLUE, SQuAD v1.1, and UD v2.10 benchmarks to measure both semantic and syntactic downstream performances in detail. For UD, we used its English subset of Universal Dependencies English Web Treebank (EN-EWT) (Silveira et al., 2014) .\nEvaluation Following previous work (Aroca-Ouellette and Rudzicz, 2020) , we report matched accuracy for MNLI, Matthews correlation for CoLA, Spearman correlation for STS-B, accuracy for MRPC, F1 scores for QQP and SQuAD, and accuracy for all other tasks. For UD, we used labeled attachment score (LAS). For each task, we report a mean score over five runs with different random seeds. We excluded problematic WNLI following prior work (Aroca-Ouellette and Rudzicz, 2020) .\nImplementation Details We implemented our models using the PyTorch (Paszke et al., 2019) and Hugging Face Transformers (Wolf et al., 2020) models. We used the SuPar library 5 to implement the parser and followed its default hyperparameter configurations.\n\nResults\nRQ1: Do more complex objectives achieve better results? Table 1 displays downstream task results on GLUE, SQuAD, and UD for our control tasks and their comparisons against MLM, First Char, and Last Char. Overall, we observe the larger n, the better downstream performance is. Looking at each dataset result closely, we see that the datasets with over 5k training samples exhibit moderate to high correlations with the lowest and highest correlation values of 0.520 (First Char to MLM) for QQP and 0.942 (Last Char to MLM) for MNLI, respectively. In contrast, the corpora with less than 5k samples tend to exhibit low to moderate correlations, ranging from 0.182 (First Char to MLM) for RTE to 0.486 (First Char to MLM) for MRPC. Therefore, we can see a general trend that a more complex masked pretraining objective yields better performance in downstream tasks especially under a high-resource scenario, whereas it does not always achieve a better result on a low-resource 5 https://github.com/yzhangcs/parser dataset, where in this case MLM tends to achieve a better result.\nRQ2: How much complexity do we need to obtain comparable results to MLM? To answer the RQ2, we only compare results on high-resource corpora, given that results on low-resource corpora have large standard deviations of 1.0 or more (see Appendix C.2). We can observe from Table 1 that at least n = 4 complexity is necessary to achieve comparable results to MLM. For instance, First n Chars objectives require n = 4 (20k classes) to surpass the MLM performance on at least one of the target downstream tasks. Last n Chars also need n = 4 (18k classes) to beat MLM on one of the tasks. For MNLI and QQP, our control tasks did not yield better results.\n\nAnalysis and Discussion\nOn the basis of results from the RQ1 and RQ2, we discuss how we should pretrain a model in practice from the task complexity perspective.\nTask complexity affects computational efficiency.\nThe task complexity is closely related to computational costs with more complex objectives larger costs. operations (FLOPs) 6 required for n Chars, First Char, Last Char, and MLM along with its downstream performance. We can see a clear trade-off between computational efficiency and downstream performance. Smaller n drastically reduces FLOPs with the maximum relative reduction of 31% for First Char and Last Char, while larger n has small reduction rates with the minimum value of 6% for First and Last 9 Chars. To obtain comparable results to MLM, as we discussed in the RQ2, we need at least n = 4 (First and Last) complexity, which can only reduce 18% and 20% of FLOPs, respectively. These results suggest that we need careful cost-benefit consideration when using a masked pretraining objective on the basis of target performance and computational costs.\nHow can we select an optimal complexity? Finally, we discuss how we can decide an optimal complexity for a specific downstream task by analyzing how the task complexity affects the performance in detail. Here, we take SQuAD as an example case, where we observed a large relative difference of 3.0% in Table 2 . Table 3 lists the results on SQuAD, including the ratio of mis-detection, i.e., the percentage of samples with no overlap between predicted and gold spans, and F1 scores calculated without mis-detected cases. We found 86.1 (-2.3) 93.9 (-0.5) 8.2 (+25.0) First Char 85.6 (-2.9) 93.9 (-0. that simple masked objectives were likely to suffer from mis-detection with the worst performance degradation of 38.2% for Last Char, which is far larger than those observed in other metrics and corpora (see Tables 2 and 7 in Appendix). In contrast, the relative performance difference values of F1 scores computed without mis-detected samples show quite similar trends to other high-resource corpora, which are typically less than 2% at a maximum. These results imply that the task complexity mainly contributes to an increase/decrease in the number of mis-detections in SQuAD, and selecting a complex masked objective (e.g., First 5 Chars) is a safeguard option to minimize the effect of misditection. Therefore, different downstream tasks might have different optimal complexities due to their characteristics and evaluation metrics, as observed in the example case above. We leave thorough investigation of these effects as future work.\n\nConclusion\nThis paper analyzed the impact of masked objective complexities over downstream performance, motivated by the assumption that the lack of task complexity in simple masked pretraining objectives (e.g., First Char) affects the performance degradation compared to MLM. Experiments using the GLUE, SQuAD, and UD datasets revealed that the task complexity significantly affected downstream performance with at least 35.7% of the MLM prediction classes needed to perform comparably to MLM on at least one of the high-resource corpora. Our analysis also showed that there exists a tradeoff between downstream performance and computational efficiency, and different downstream tasks might have different optimal complexities. Future work includes analyzing other properties (e.g., fairness) with respect to task complexity.\n", "hypothesis": "Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether simpler masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM.", "answer": false}
{"title": "Substitution-based Semantic Change Detection using Contextual Embeddings", "content": "\nIntroduction\nMeasuring semantic change is one of the few areas of NLP where contextual embeddings have not yet led to a definitive improvement over previous methods. In particular, the commonly used approach of aligning static embeddings trained on different time periods (Hamilton et al., 2016b) continues to be a surprisingly hard to beat baseline.\nGiven that contextual embeddings provide a representation for each occurrence of a word in context, they would seem to be ideally suited to a more nuanced investigation of semantic change. Most attempts to leverage them for this purpose, however, produce quantitatively worse results, while being less interpretable and requiring more resources.\nHere, we present a simplified and improved approach to scalable, interpretable, semantic change detection using contextual embeddings. Inspired by Eyal et al. (2022) , we work only with the most probable replacements for masked words, and measure semantic change in terms of the distributions of replacements in each time period. Not only does this better match human judgements, it is highly space efficient, works seamlessly for out-of-vocabulary words, and helps intuitively characterize meaning change and variation.\n\nBackground\nMeasuring semantic change involves a set of tasks related to determining if and how a term's meaning has changed over time. Here, we focus on the task of measuring the amount of change that has occurred from one time period to another (Gulordava and Baroni, 2011; Schlechtweg et al., 2020) . 1 Existing approaches to this task are mostly of two types. The first is associating each term with a single vector per time period and measuring the distance between vectors, of which we take Hamilton et al. (2016b) to be representative. As a variation on this, several authors have proposed averaging the output of contextual embedding models to get a single vector per term in each time period, but this has generally not led to an improvement over using static vectors (Martinc et al., 2020a; Kurtyigit et al., 2021; Liu et al., 2021) . A related approach is to represent words in terms of their nearest neighbors using static word vectors (Hamilton et al., 2016a; Gonen et al., 2020) , but this does not show a clear improvement over other static embedding methods (Montariol et al., 2021) .\nA second type of approach begins with various methods for word sense induction, then measures change in terms of the relative prevalence of a term's different senses (Frermann and Lapata, 2016; Hu et al., 2019; Arefyev and Zhikov, 2020; Arefyev and Bykov, 2021) . In some cases, authors simply cluster contextual representations for each term, and measure differences in the distributions of clusters between two time periods, rather than dealing with explicit word senses (Giulianelli et al., 2020; Martinc et al., 2020b; Montariol et al., 2021) .\nDespite the additional information provided by contextual embedding models, methods using type embeddings (as opposed to token), continue to be competitive. For example, on the recent SemEval multilingual semantic change detection task, none of the top four systems used token embeddings (Schlechtweg et al., 2020) . Methods using contextual embeddings have done better on some more recent mono-lingual shared tasks (Kutuzov and Pivovarova, 2021; Zamora-Reina et al., 2022), but have not yet been evaluated with a consistent setup across multiple languages.\n\nMethods\nBuilding on Eyal et al. (2022) , we represent each token in the corpus (or a sufficiently large sample of them) by a small set of probable replacement terms from a contextual embedding model. However, whereas Eyal et al. (2022) did this for the purpose of word sense disambiguation, we do so for the purpose of measuring semantic change.\nFor each sampled occurrence of each term, we mask the term of interest, feed the masked context through a model, and obtain the predicted token probabilities corresponding to the mask token. 2 From these, we save only the top-k most probable words (excluding stopwords and partial word pieces), and discard the rest.\nFor a given term in a particular time period, we then count how many times each word in the model vocabulary has appeared as a top-k replacement for that term, and normalize this by its sum, giving us a distribution over replacements. To obtain a raw score of semantic change between two time periods, we compute the Jensen-Shannon Divergence (JSD) between the two distributions representing the same term in different time periods. However, as we show below, the raw JSD scores are strongly correlated with term frequency. Thus, to obtain a scaled metric, we convert the raw JSD scores into a quantile, comparing the raw score for a term of interest to other terms with similar frequency.\nCompared to saving the full output vector per token, this approach only requires a miniscule amount of storage per token, and thus does not require the kind of heuristic dropping of tokens employed by Montariol et al. (2021) . In addition, the dominant meanings of a word in each context can be summarized by the terms which occur most fre-quently among the top-k replacements. Although such replacements are limited to the terms which exist in the model vocabulary, in practice this is sufficient to represent a nuanced set of meanings, and works even for words which get tokenized into multiple word pieces, as we show below.\nMore formally, given two corpora C1 and C2, let the count of token v as a top-k replacement for term t in corpus c be:\nEQUATION\nwhere R(t, i, k) is the set of top-k most probable replacements for occurrence i of term t (excluding stopwords and partial word pieces in the model vocabulary), and N c (t) is the number of sampled occurrence of term t in corpus c. 3 Let \u2206 c t by the distribution of top-k replacement counts for term t in corpus c, obtained by dividing the corresponding vector of counts (i.e., [count(\u2022, t, c)]) by the sum over the model vocabulary. The raw change score for term t is given by the JSD between the two distributions:\nEQUATION\nFinally, we correct for frequency effects by rescaling the raw JSD scores against the scores for terms with similar frequency as the target term, giving us a quantile scaled in [0, 1]:\nscaled(t) = \u03a3 s\u2208T (t) I[raw(t) \u2265 raw(s)]/|T (t)|,\n(3) where T (t) is the set of terms with similar frequency to term t (excluding term t itself). More specifically, we compare against all terms within a fixed factor of the target frequency:\nEQUATION\n) where fr(t) is the frequency of term t in the corpus, with window factor F .\n\nData\nWe use five datasets with words labeled in terms of semantic change between two time periods. Four of these are from SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection (SE; Schlechtweg et al., 2020) . These datasets contain 31 to 48 terms from four languages, graded in terms of change by human raters, along with accompanying corpora to be used in estimating the amount of change. The fifth dataset (GEMS) comes from Gulordava and Baroni (2011), and contains 100 words labeled in terms of semantic change from the 1960s to 1990s. As with most recent papers which use this dataset, we use the Corpus of Historical American English (COHA; Davies, 2010) for measuring change in the GEMS words.\n\nExperimental Details\nFor each dataset, we fine tune an appropriate BERT model to the union of the two associated unlabeled corpora using continued masked language model training with the HuggingFace transformers package. We then index the corpora to find all occurrences of each word. For all target words, along with a random set of 10,000 background terms, we randomly sample up to 4,000 occurrences of each from the associated corpora. We process all sampled tokens as described above to obtain and store the top-k replacements for each, with k = 5. Using the replacements obtained from the model, we compute raw JSD scores for each term. Finally, we convert these to scaled scores by comparing to the background terms that have frequency within a factor of two of the target term (i.e., F = 2).\nFollowing past work, we evaluate using Spearman correlation with human ratings, comparing against the best results from recent papers. In particular, we include two results based on slight variations on Hamilton et al. (2016b) , one of which was the best performing method in the SemEval competition (P\u00f6msl and Lyapin, 2020), as well as methods using contextual embeddings (Martinc et al., 2020b; Montariol et al., 2021) . For fully experimental details, please refer to Appendix A.\n\nResults\nFull results are given in Table 1 . Although our method is not uniformly better than all previous methods on all dataset, it does produce the best result on average, as well as improvements on GEMS, SE English and SE Latin. 2 are labeled.\nAs an example to better understand these results, the raw JSD scores from our method are shown in Figure 1 (top) for the SE English data, with select terms labeled. As can be seen, there is a strong relationship between term frequency and raw JSD, hence the need to rescale the raw scores relative to terms with similar frequency. After rescaling, we see a strong correlation between our final semantic change scores and the human ratings, as shown in Figure 1 (bottom) for the SE English data.\nAs with the approach of Hamilton et al. (2016b), our method supports direct interpretation of semantic change. To understand the change in a word's typical usage, we can look at the overall most common replacements from each time period. Table 2 shows the scores and rankings of several selected terms from SE English, along with the most common substitutes from each time period.\nLooking at the results, we can see, for example, strong agreement with human annotators on a dramatic change in the meaning of plane (comparing 1810-1860 vs. 1960-2010) , from the geometric concept to the flying machine. On the other hand, our results suggest that human raters may have slightly underestimated the amount of change in the meaning of graft, which was previously used mostly in reference to vegetation, but now most commonly refers to corruption. 5 By contrast, ounce may be a case where our method has underestimated the change that has taken place. Older usages seem to map more generically to a wider range of quantities (hence the appearance among the early substitutes of hour, acre, and dollars), whereas modern usage seems more restricted. Indeed, we do find some difference in the distribution of substitutes between the two time periods, but less of a difference than is typical for words with similar frequency, hence the low final score from our method (see Figure 1 ).\nAlthough we do not emphasize it in this paper, of our method can easily be combined with the approach of Eyal et al. (2022) to further investigate meaning changes, by inferring senses from the term replacements, and looking at how their usage varies by time period. In particular, for each target term, we can construct a graph from the set of term substitutes (as nodes), where edge weights represent the number of top-k clusters in which two substitutes co-occur. Following Eyal et al. (2022) , we experiment with Louvain community detection to identify sense clusters from these graphs for each term of interest, and use Jaccard similarity to associate each mention with a sense cluster, based on substitute overlap (see Appendix A for details).\nInspecting the distribution of these senses over time helps to distinguish the gradual adoption of existing senses from the creation of new ones. For example, the most common sense of plane is captured by the sense cluster {aircraft, jet, airplane, car}, and as expected, this sense is not found in the 1810-1860 English data, except for two instances which appear to be errors in the inferred sense. By contrast, the second most common sense-{planes, line, point, surface}-appears in both time periods, but is much more common in the earlier time.\nThis approach also provides more insight into how the meaning of graft has changed. The most common sense cluster is the horticultural meaning {tree, plant, stock, vine}, and this meaning occurs in both time periods, but is much more common in the earlier one. A second cluster, corresponding to illicit activity-{corruption, violence, bribery, fraud}-occurs only in the later time period. This clustering method also surfaces a third sense with a medical meaning-{transplant, surgery, disease, drug}-which is not revealed by the top few overall most common replacements given in Table 2 .\n\nDiscussion and Related Work\nAs noted by others, new and larger datasets for rigorously evaluating semantic change are badly needed (Tahmasebi et al., 2021) . Existing datasets are relatively small, and are mostly based on inspecting a limited number of examples per term. Unfortunately, determining ground truth for semantic change is challenging, and producing such resources is costly. Ideally, future datasets for evaluation should be larger, both to allow for more robust evaluation, and to have sufficient targets for both hyperparameter tuning and evaluation.\nIn addition to the dataset we have used in this paper, two others are available from shared tasks on Spanish and Russian, respectively (Kutuzov and Pivovarova, 2021; Zamora-Reina et al., 2022) . Both of these are comparable in size to the GEMS dataset used here. Unfortunately, they are less useful for evaluation because most submissions to these shared tasks only evaluated on the task data, and not on other datasets. As shown by the replication of Martinc et al. (2020b) in Montariol et al. (2021) , a method can sometimes perform well on one language but fail to generalize to others. As such, we have based our evaluation on datasets for which there has been a consistent evaluation of methods across multiple languages. As future work, a careful replication study of all methods from each competition on all available datasets, including an assessment of sensitivity to hyperparameters, would be highly informative.\nBesides Eyal et al. (2022) , The closest prior work to ours is Kudisov and Arefyev (2022) , who use dynamic patterns to generate many variations on example usages sampled from the given corpora. These variations are then used to generate hundreds of replacement terms from a masked language model with associated probabilities. These probabilities are averaged (heuristically combining replacements with differing numbers of word pieces) to obtain a mean vector for each sampled instance. Finally, semantic change is computed as the average cosine distance between all pairs of vectors across corpora. This method was evaluated as part of the LSCDiscovery shared task on Spanish (Zamora-Reina et al., 2022) . Preliminary work on this method was described in Arefyev and Bykov (2021) , where a slightly different version of it was evaluated on the RuShiftEval shared task on Russian (Kutuzov and Pivovarova, 2021) .\nCompared to Kudisov and Arefyev (2022), our approach is considerably simpler, and better suited to storing representations of a complete corpus for subsequent analysis and exploration. In particular, we only consider a small number of substitutes for each example (storing only the top-k most probable terms, without the associated probabilities). We do not use dynamic patterns, and only consider terms in the model vocabulary as potential substitutes. We also associate each term with a single distribution over the model vocabulary per time period (not per mention), and use Jensen-Shannon divergence to more naturally measure the distance between distributions. Importantly, we also correct for frequency effects, as described above.\nAlthough our approach avoids the onerous storage requirements of methods which save full contextual vectors, it still requires considerable processing time to obtain the top-k replacements for all tokens. Future work could explore smaller or more efficient models for this purpose. 6 Finally, despite its simplicity, measuring the cosine distance between aligned static vectors remains a strong and efficient baseline (Hamilton et al., 2016b) . More work is needed to determine where contextual embeddings can offer sufficient advantage in measuring semantic change to justify their greater computational cost.\nCompared to static embeddings, our approach is weakest on the German and Swedish datasets, which could relate to the quality of the pretrained models that are available for those languages, the data used for pretraining, or perhaps issues that arise in tokenization of the reference corpora. For a tentative exploration of some possible factors, please refer to Appendix C.\n\nConclusion\nWe have presented a simplified and improved approach to measuring semantic change using contextual embeddings, based on the Jensen-Shannon Divergence between the distributions of the most probable replacements for masked tokens in different time periods, corrected for frequency effects. This approach achieves superior performance on average, while remaining directly interpretable, with vastly reduced storage requirements.\n", "hypothesis": " Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation.  We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms.", "answer": true}
{"title": "HiPool: Modeling Long Documents Using Graph Neural Networks", "content": "\nIntroduction\nTransformer-based models like BERT (Vaswani et al., 2017a) and RoBERTa (Zhuang et al., 2021) have achieved satisfying results in many Natural Language Processing (NLP) tasks thanks to largescale pretraining (Vaswani et al., 2017b) . However, they usually have a fixed length limit, due to the quadratic complexity of the dense self-attention mechanism, making it challenging to encode long sequences.\nOne way to solve this problem is to adapt Transformers to accommodate longer inputs and optimize the attention from BERT (Feng et al., 2022; Jaszczur et al., 2021) . BigBird (Zaheer et al., 2020) applies sparse attention that combines random, global, and sliding window attention in a long sequence, reducing the quadratic dependency of full attention to linear. Similarly, Longformer (Beltagy et al., 2020) applies an efficient self-attention with dilated windows that scale linearly to the window length. Both models can take up to 4096 input tokens. Though it is possible to train even larger models for longer sequences, they are restricted by a pre-defined maximum length with poor scalability. More importantly, they fail to capture high-level structures, such as relations among sentences or paragraphs, which are essential to improving NLP system performance (Zhang et al., 2018; Zhu et al., 2019) .\nAnother way is to apply a hierarchical structure to process adjustable input lengths with chunking representations for scalability on long sequences. Hi-Transformer (Wu et al., 2021) encodes both sentence-level and document-level representations using Transformers. ToBERT (Pappagari et al., 2019) applies a similar approach that stacks a sentence-level Transformer over a pretrained BERT model. While most of the existing work models upper-level hierarchy using sequential structures, such as multiple layers of LSTMs (Hochreiter and Schmidhuber, 1997) or Transformers, this may still bring the long dependency issue when the sequence gets longer. To alleviate this, we investigate graph modeling as a novel hierarchy for upper levels. Besides, we also consider inter-hierarchy relationships using a new attention mechanism.\nOur key insight is to replace the sequence-based model with a hierarchical attentional graph for long documents. We first apply a basic pretrained language model, BERT or RoBERTa, to encode local representation on document chunks with a fixed length. The number of chunks could be extended for longer sequences for better scalability. Different from other works, we apply a graph neural network (GNN) (Zhou et al., 2018) to model the upper-level hierarchy to aggregate local sentence in-formation. This is to alleviate the long dependency issue of the sequential model. Moreover, within such a graph structure, we propose a new heterogeneous attention mechanism to consider intra-and cross-sentence-level correlations.\nOur contributions are two-fold: 1) We propose HiPool with multi-level hierarchies for long sequence tasks with a novel inter-hierarchy graph attention structure. Such heterogeneous graph attention is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences; 2) We benchmark the LDC (long document classification) task with better scaled and length-extended datasets. Evaluation shows that HiPool surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset. Code is available at https: //github.com/IreneZihuiLi/HiPool.\n\nModel\nWe introduce the HiPool (Hierarchical Pooling) model for long document classification, illustrated in Fig. 1 . It consists of an overlapping sequence encoder, a HiPool graph encoder, and a linear layer. Overlapping Sequence Encoder. Given the input document S, we first chunk the document into a number of shorter pieces with a fixed length L, and we set the overlapping window size to be L olp . Overlapping encoding makes it possible for a chunk to carry information from its adjacent chunks but not isolated, differentiating our model from other hierarchical ones. Then each chunk is encoded with a pretrained Transformer model, i.e., BERT or RoBERTa; we choose the CLS token representation as the input to our HiPool layer: X = BERT(S). HiPool Graph Encoder. We apply a graph neural network to encode incoming word-level information. Such a model has shown its potential in some NLP tasks (Li et al., 2022 (Li et al., , 2021)) . We construct a graph, defined by G(V, E), where V is a set of nodes, and E is a set of node connections. There are two node types: n low-level nodes and m high-level nodes, and typically m < n. In our experiment, we set m = n/p, and p \u2265 0. The feedforward operation goes from low-to high-level nodes. In layer l, low-level nodes are inputs from the previous layer l \u2212 1, while high-level nodes at layer l are computed based on low-level ones. Moreover, these high-level nodes will be the input to the next layer l + 1, becoming the low-level nodes in that layer. We consider X the low-level\n\nProposed Method\nMulti-level hierarchies for long sequence tasks with a novel inter-hierarchy graph attention structure. Heterogeneous graph attention is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences.\n\nOverlapping Sequence Encoder\n\u2022 Pretrained model for fixed sequence length. nodes in the first HiPool layer, as shown in the figure.\nIn each HiPool layer, given node representation H l and adjacency matrix A l at layer l, the task is to obtain H l+1 :\nEQUATION\nInspired by DiffPool (Ying et al., 2018) , we conduct a clustering method to aggregate information. We assign node clusters with a fixed pattern based on their position. For example, adjacent low-level neighbors should map to the same high-level clustering node. So we first define a clustering adjacency matrix A self \u2208 IR n\u00d7m that maps n nodes to m nodes, indicating the relations from low-to high-level nodes, marked as black arrows in the figure. Note that our approach allows overlapping, in which some nodes may belong to two clusters. We set the clustering sliding window to be 2p, with a stride to be p. In the figure, we show the case of p = 2. We denote interactions between low-level nodes by the adjacency matrix A l , 1 and we model it using a chain graph, according to the natural order of the document. 2 Then, the relations between high-level nodes A l high and their node representations H l high are computed:\nA l high = A T self A l A self , H l high = A self H l .\n(2)\nBesides, for each high-level node, to strengthen the connections across different clusters, we propose an attention mechanism to obtain crosssentence information. We propose a new edge type that connects external cluster low-level nodes to each high-level node, and the adjacency matrix is simply A cross = 1 \u2212 A self , marked by green in the figure . We update H l high as the following:\nEQUATION\nwhere W atten is trainable, and W score is a scoring matrix. We then apply a GNN to obtain H l+1 . For example, a graph convolution network (GCN) (Kipf and Welling, 2016):\nEQUATION\nWe run our experiments with two layers, and apply a sum aggregator to achieve document embeddings. More HiPool layers are also possible. Linear Layer. Finally, a linear layer is connected and cross-entropy loss is applied during training.\n3 Experiments\n\nLDC Benchmark\nThe LDC benchmark contains six datasets. We first choose four widely-used public datasets. Hyperpartisan (HYP) (Kiesel et al., 2019 ) and 20News-Groups (20NG) (Lang, 1995) \n\nEvaluation\nHyperparameters. We list details in Appendix C.\nBaselines. We select four pretrained models: BERT (Devlin et al., 2019) , RoBERTa (Zhuang et al., 2021) , BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) . We also compare with a hierarchical Transformer model To-BERT (Pappagari et al., 2019) . Hi-Transformer (Wu et al., 2021) failed to be reproduced as there is no code available. We evaluate two variations of our HiPool method by changing the sequence encoder model: HiPool-BERT and HiPool-RoBERTa.\nWe sequence in Appendix A. Hierarchy variations. To further compare sequential and graph hierarchy, we keep the word encoder and replace the HiPool graph encoder with the following sequential modules: Simple linear summation over low-level nodes; CNN applies a 1-dimension convolution; Trans is to apply a Transformer on top of low-level nodes. Besides, we also look at multiple graph settings: Aggr-mean is to use a mean aggregator to obtain the final document representation; Aggr-std is to use a feature-wise standard deviation aggregator; finally, Aggr-pcp applies Principal Neighbourhood Aggregation (PNA) (Corso et al., 2020) . We report results on Amazon-2048 in Tab. 3, as it has the longest sequence on average. An observation is that applying aggregators are better than simpler structures, while keeping a graph is still a better choice. HiPool also considers attention in message passing, so it is doing even better. We also test other variations in Appendix B.\n\nAblation Study\nEffect of input length. To better understand the effect of input length, in Fig. 2 , we present an ablation study on the Amazon-2048 and ILDC, and compare three models: BigBird, Longformer, and HiPool. In general, the models benefit from longer input sequences in both datasets. Interestingly, when sequence is larger than 2048, Longformer and Bigbird could not improve and they are limited in maximum lengths. In contrast, as the input sequence gets longer, HiPool steadily improves, showing its ability to encode long documents in a hierarchical structure. Model component. Next, we look at how each component of HiPool affects performance. As shown in Tab. 4, we first take the best model setting, HiPool-RoBERTa, and compare it with the following settings: 1) w/o RoBERTa is to replace RoBERTa with BERT, then the model becomes HiPool-BERT; 2) w/o HiPool is to remove the proposed HiPool module and replace with a simple CNN (Kim, 2014) ; 3) w/o Overlapping is to remove the overlapping word encoding. We could see that removing the HiPool Layer leads to a significant drop, indicating the importance of the proposed method. Moreover, the HiPool framework can work with many pretrained language models, as we can see that applying RoBERTa improves BERT. A complete result table can be found in Appendix.\n\nConclusion\nIn this paper, we proposed a hierarchical framework for long document classification. The evaluation shows our model surpasses competitive baselines.\n", "hypothesis": "So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intraand cross-sentence correlations with a traditional attention mechanism.", "answer": false}
{"title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. \u2022 Advanced architectures such as BERT may only achieve the best results if properly used. Linear methods can help us check if advanced methods' results are reasonable. In the deep-learning era, the younger generation often thinks that linear classifiers should never be considered. Further, they may be unaware of some variants of linear methods that are particularly useful for text classification (see Section 3.1). Therefore, the paper serves as a reminder of this oftenforgotten technique.\nFor our illustration, we re-investigate an existing work (Chalkidis et al., 2022) that evaluates both linear SVM and pre-trained language models, but the authors pay more attention to the latter. The linear method is somewhat ignored even though the performance is competitive on some problems. We carefully design experiments to compare the two types of methods. Our results fully demonstrate the usefulness of applying linear methods as simple baselines.\nSome recent works (e.g., Yu et al., 2022; Gomes et al., 2021) have shown the usefulness of linear classifiers in the deep-learning era. However, they either consider sophisticated applications or investigate advanced settings in which linear methods are only one component. In contrast, in this paper, we consider the basic scenario of text classification. A more related work (Wahba et al., 2023) has demonstrated the effectiveness of linear classifiers over PLMs on some problems. However, our investigation on linear methods is more comprehensive.\nThe discussion also reminds us the trade-off between performance gain and the cost including running time, model size, etc. Simple methods are useful to benchmark and justify the usage of advanced methods.\nMethod Chalkidis et al. (2022) . In each Micro-F1 column, the best result is bold-faced. \"N/A\" means not available in their work. For example, the authors did not report the training time and the number of parameters of linear SVMs.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS # \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1\nThis paper is organized as follows. In Section 2 we take a case study to point out the needs of considering linear methods as a baseline for text classification. We describe the linear and BERT-based methods used for investigation in Section 3. The experimental results and main findings are in Section 4, while Section 5 provides some discussion. Additional details are in Appendix. Programs used for experiments are available at https://github.com/JamesLYC88/ text_classification_baseline_code.\n\nText Classification These Days: Some Issues in Applying Training Methods\nLarge PLMs have shown dramatic progress on various NLP tasks. In the practical use, people often directly fine-tune PLMs such as BERT on their data for a few epochs. However, for text classification, we show that this way may not always get satisfactory results. Some simple baselines should be considered to know if the obtained PLM model is satisfactory. We illustrate this point by considering the work on legal document classification by Chalkidis et al. (2022) , which evaluates the following sets. The study in Chalkidis et al. (2022) comprehensively evaluates both BERT-based PLMs and linear SVMs. They use Micro-F1 and Macro-F1 to measure the test performance. 2 In Table 1 , we present their Micro-F1 results and running time of each model.\n\nLinear Models Worth More Investigation\nThe investigation in Chalkidis et al. (2022) focuses on BERT and its variants, even though from Table 1, the performance of BERT-based methods may not differ much. While they did not pay much attention to linear SVM, by a closer look at the results, we get intriguing observations: \u2022 Linear SVM is competitive to BERT-based PLMs on four of the six data sets. For SCO-TUS, linear SVM even outperforms others with a clear gap. \u2022 Surprisingly, given linear SVM's decent performance, its training time was not shown in Chalkidis et al. (2022) , nor was the number of parameters; see the \"N/A\" entries in Table 1 . With the observations, we argue that the results of linear models are worth more investigation.\n\nSettings for Investigation\nTo better understand the performance of linear models and BERT-based PLMs, we simulate how people work on a new data set by training these methods. We consider a text classification package Lib-MultiLabel 3 because it supports both types of train-ing methods.\n\nLinear Methods for Text Classification\nTo use a linear method, LibMultiLabel first generates uni-gram TF-IDF features (Luhn, 1958; Jones, 1972) according to texts in the training set, and the obtained factors are used to get TF-IDF for the test set. It then provides three classic methods that adopt binary linear SVM and logistic regression for multi-class and multi-label scenarios. 4 Here we consider linear SVM as the binary classifier behind these methods.\n\u2022 One-vs-rest: This method learns a binary linear SVM for each label, so data with/without this label are positive/negative, respectively. Let f \u2113 (x) be the decision value of the \u2113-th label, where x is the feature vector. For multi-class classification, \u0177 = argmax \u2113 f \u2113 (x) is predicted as the single associated label of x. For multi-label classification, all labels \u2113 with positive f \u2113 (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al. (2022) did, though our TF-IDF feature generation is simpler than theirs by considering only uni-gram. 5 \u2022 Thresholding (Yang, 2001; Lewis et al., 2004; Fan and Lin, 2007) : This method extends one-vsrest by modifying the decision value for optimizing Macro-F1. That is, we change the decision value to f \u2113 (x) + \u2206 \u2113 , where \u2206 \u2113 is a threshold decided by cross validation. \u2022 Cost-sensitive (Parambath et al., 2014) : For each binary problem, this method re-weights the losses on positive data. We decide the reweighting factor by cross validation to optimize Micro-F1 or Macro-F1.\nThese methods basically need no further hyperparameter tuning, so we can directly run them. The last two methods are extensions of one-vs-rest to address the imbalance of each binary problem (i.e., few positives and many negatives). The design relies on the fact that the binary problems are independent, so such approaches cannot be easily applied to deep learning, which considers all labels together in a single network.\n\nBERT-based Methods for Text Classification\nLibMultiLabel also provides BERT-based methods, which involve several hyper-parameters, such as the learning rate. While practitioners may directly choose hyper-parameters, to seriously compare with linear methods, we run BERT by conducting hyper-parameter selection. More details are in Appendix F.\n\nExperimental Results and Analysis\nIn Table 2 , we follow Chalkidis et al. (2022) to report Micro-F1 and Macro-F1 on the test set. The training time is in Table 3 .\n\nLinear Methods are Good Baselines\nIn Table 2 , our one-vs-rest results are slightly worse than the linear SVM results in Chalkidis et al. (2022) , which also applies the one-vs-rest strategy. As mentioned in Section 3.1, the difference is mainly due to our use of simple uni-gram TF-IDF features. Anyway, our one-vs-rest is still competitive to BERT results in Chalkidis et al. (2022) on the last four problems. More importantly, the two extensions of one-vsrest (i.e., thresholding and cost-sensitive) improve almost all situations. For data sets ECtHR (A) and ECtHR (B), where originally one-vs-rest is significantly lower than BERT results in Chalkidis et al. (2022) , the gap reduced considerably.\nFor the training time in Table 3 , though the two extensions take more time than the basic one-vsrest strategy, all the linear methods are still hundreds of times faster than BERT. Further, linear methods were run on a CPU (Intel Xeon E5-2690), while for BERT we need a GPU (Nvidia V100). The model sizes listed in Table 4 also show that linear SVM requires a much smaller model than BERT, where details of our calculation are in Appendix D.\nThe results demonstrate that linear methods are useful baselines. They are extremely simple and efficient, but may yield competitive test performance.\n\nLinear Methods can Help to See if\nAdvanced Methods Are Properly Used Surprisingly, our running of LibMultiLabel's BERT leads to worse test performance than linear methods on almost all data sets. More surprisingly, a comparison between the BERT results by LibMul-tiLabel and those in Chalkidis et al. (2022) shows Method It is essential to check the discrepancy between the two BERT results. We find that Chalkidis et al. (2022) use some sophisticated settings to run BERT for the first three sets (i.e., ECtHR (A), ECtHR (B), and SCOTUS). They split every document into 64 segments, each of which has no more than 128 tokens, and apply BERT on each segment. Then, they collect the intermediate results as inputs to an upper-level transformer. After repeating the same process via LibMultiLabel, we can reproduce the results in Chalkidis et al. (2022) ; see details in Appendices E, F, and G.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F\nWe learned that they considered the more sophisticated setting of running BERT because by default, BERT considers only the first 512 tokens. Thus, for long documents, the training process may miss some important information. However, in practice, users may forget to check the document length and are not aware of the need to apply suitable settings. The above experiments demonstrate that BERT can achieve superior results if properly used, but sometimes, a direct run lead to poor outcomes. Linear methods can serve as efficient and robust baselines to confirm the proper use of an advanced approach.\n\nDiscussion and Conclusions\nIn our experiments, we encounter an issue of whether to incorporate the validation set for training the final model, which is used for predicting the test set. For linear methods, we follow the common practice to include the validation set for obtaining the final model. However, for BERT or some other deep learning models, the validation set is often used only for selecting the best epoch and/or the best hyper-parameters. To fully use the available data, we have investigated how to incorporate the validation set for BERT. Experimental results and more details are in Appendix H.\nFor some text sets evaluated in this work, we have seen that simple linear methods give competitive performance. The reason might be that each document in these sets is not short. 6 Then TF-IDF features are sufficiently informative so that linear methods work well. Across all NLP areas, an important issue now is when to use PLMs and when not. We demonstrate that when PLMs may not perform significantly better, traditional methods are much simpler and require fewer resources. However, having a simple quantitative measurement to pre-determine when to use which remains a challenging future research problem. In summary, the study reminds us of the importance of employing simple baselines in NLP applications.\n", "hypothesis": " In this opinion paper, we point out that this way may only sometimes get satisfactory results.  We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods.", "answer": true}
{"title": "Temporal Relation Classification using Boolean Question Answering", "content": "\nIntroduction\nEvents in stories are not necessarily mentioned in a chronological order. The timeline of events is important for understanding the main narrative of a story as well as the correct order of actions. For example, the timeline may be used directly by clinicians looking for a convenient way to explore the disease course of their patients, or by algorithms to follow instructions in the right order, given as text, such as in cooking recipes. Building the timeline is done based on two main subtasks: (1) event extraction, that is, detecting the most important events in a given textual input, and (2) temporal relation classification (TRC), also known as temporal relation extraction, which is about putting two events, given as gold spans, in the right chronological order. For example, consider the following text: \"Before you put the cake in the oven, say a little prayer.\" In the first subtask, known as event extraction, we would like to detect only the relevant events for our domain of interest. In this case, the words put and say are both verbs representing some relevant actions; therefore, we mark them as events. In the second subtask, TRC, we put every two events in a chronological order by classifying them using a closed set of temporal relations. In this case, the two events put and say should be assigned with the label AFTER indicating that put is happening after say in a chronological order.\nIn this study we focus on TRC, which is typically handled as a classification problem of two events provided along with the context in which they are mentioned. MATRES (Ning et al., 2018b) is one of the dominant datasets for TRC comprised of news documents manually annotated with temporal relation labels. The events are deterministically chosen to be all actions (mostly verbs) mentioned in the documents. Every pair of events (n, m) are manually labeled with one of four labels: BEFORE (n happened before m), AFTER (n happened after m), EQUAL (n and m happened at the same time), and VAGUE (it is impossible to know which event happened before the other).\nTraditional classification approaches have already been demonstrated for TRC. In this work, we get inspiration from a relatively new promising approach for solving natural language processing (NLP) tasks, in which the target algorithm is based on a reduction of the task to another problem. In our case, we solve the TRC problem using a model that handles the boolean question-answering (QA) task, which is about answering a Yes/No question given a passage used as a context. We decide to use boolean QA as our proxy problem due to the way the annotation work for building MATRES has been done. In the main annotation guidelines of MATRES (Ning et al., 2018b) , the annotators are asked to assign a label to a pair of events (n, m) by answering the two following questions: (1) Is it possible that the start time of n is before the start time of m? and (2) Is it possible that the start time of m is before the start time of n? There are four possible answer combinations, each is mapped to one label: (yes, no) \u21d2 BEFORE, (no, yes) \u21d2 AFTER, (no, no) \u21d2 EQUAL, and (yes, yes) \u21d2 VAGUE. Therefore, we transform an instance of TRC, composed of a pair of events and a document, into a pair of Yes/No QA instances, one for each of the two questions, and then fine-tune a Yes/No QA model to answer them. The final prediction is made based on the combination of the Yes/No answers retrieved by the QA model.\n\nRelated Work\nTRC has received increasing levels of attention in the past decade. There is a relatively long list of related shared tasks (Verhagen et al., 2007 (Verhagen et al., , 2010;; Bethard et al., 2016; MacAvaney et al., 2017) . Modern approaches for TRC use some sort of a neural network as a classifier. For example, Dligach et al. (2017) showed that a neural network that uses only words as input, performs better than the traditional models that process features which were manually created. A more modern approach for TRC is based on large pre-trained language models. Han et al. (2021) continued to pre-train a language model before fine-tuning it on TRC; Zhou et al. (2021) incorporated a global inference mechanism to tackle the problem at the document level; Han et al. (2019a) combined a recurrent neural network (RNN) over BERT (Devlin et al., 2019) embedding and a structured support vector machine (SSVM) classifier to make joint predictions; Ning et al. (2019) integrated BERT with a temporal commonsense knowledge base, and improved accuracy significantly by 10% over the previously known best result; and Han et al. (2019b) developed a multitask model for the two related subtasks, event extraction and TRC. Mathur et al. (2021) train a gated relational graph convolution network using rhetorical discourse features and temporal arguments from semantic role labels, in addition to some traditional syntactic features. Wang et al. (2022b) 2021) built a syntactic graph constructed from one or two continuous sentences and combined it with a pre-trained language model. The best result so far has been reported recently by Zhou et al. (2022) , who extract relational syntactic and semantic structures, and encode them using a graph neural network. In another recent work (Man et al., 2022) , the authors introduce a novel method to better model long document-level contexts by detecting and encoding important sentences in the document. None of those studies use QA to address the TRC problem.\nOur boolean QA-based approach continues to improve on Zhou et al.'s (2022) work, achieving a new stat-of-the-art result for TRC.\n\nDatasets\nWe conduct experiments with two datasets. MA-TRES (Ning et al., 2018b ) is a composition of three datasets (TIMEBANK, AQUAINT and PLAT-INUM) which were re-annotated following new guidelines. Following previous work, we use TIMEBANK and AQUAINT together as a training set and PLATINUM as a testing set. For validation and development we use a different dataset named TCR (Ning et al., 2018a) , which has been used similarly in other works (Zhang et al., 2021) . As mentioned above, MATRES has four labels: BEFORE, AFTER, EQUAL, and VAGUE. TimeBank-Dense (Cassidy et al., 2014) , or TB-Dense in short, is the second dataset which we use in this work. TB-Dense has two additional labels: INCLUDES and IS-INCLUDED. Following common practices, we evaluate our models using the relaxed micro-average F1 score (i.e., for MA-TRES ignoring all mistakes on VAGUE instances during evaluation, and for TB-Dense completely removing VAGUE instances from the validation and testing sets). Overall, MATRES contains 12, 736 training instances, 837 testing instances, and 2, 600 validation instances from TRC. TB-Dense contains 4, 032 training instances, 1, 427 testing instances, and 629 validation instances. The label distributions is summarized under Appendix B.\n\nMethodology\nWe design our problem as Yes/No question answering problem. Therefore, we fine-tune a pre-trained language model (PLM) by taking a Yes/No QA classification approach for which every instance is composed of a passage (text) and a question, provided along with a Yes/No answer. Our QA model is designed as a traditional classifier; the input is a concatenation of the passage and the question with a special separator token in between, and the output is a two-way label distribution vector. We use RoBERTa (Liu et al., 2019) , which comes in two sizes, base and large; we use both.\nAn instance of TRC is composed of a document, two event spans, and a label. In order to use our QA model for TRC, we convert each such instance into two or three Yes/No QA instances, which we use for fine-tuning and testing. Each QA instance Yes/No QA Instance Passage: Before you @put@ the cake in the oven, @say@ a little prayer. Question: Is it possible that @put@ started before @say@? Yes/No QA Instance Passage: Before you @put@ the cake in the oven, @say@ a little prayer. Question: Is it possible that @say@ started before @put@?\nYes is composed of a passage and a question. Therefore, we cut the sentence from the input document, containing the spans of the two events, and use it as a passage. Sentence breaks are detected using full stops (e.g., a dot followed by a white space). The passage is paired with the Yes/No questions, generating multiple QA instances. MATRES uses a label set of size four, and TB-Dense has two additional labels: INCLUDES and IS-INCLUDED. Therefore, for MATRES we compose the following two question templates (<EVENT 1> and <EVENT 2> are used here as placeholders), inspired by the TRC annotation guidelines: (1) Is it possible that <EVENT 1> started before <EVENT 2>? and\n(2) Is it possible that <EVENT 2> started before <EVENT 1>? For TB-Dense, we add another question template: (3) Is it possible that <EVENT 1> ended before <EVENT 2>? We experiment with additional phrasing, as described in the following section. The answers to the questions are determined by the label of the TRC instance, using Table 1 . In TB-Dense, question 3 is not used only when the answers to questions 1 and 2 are either (no,no) or (yes,yes), respectively.\nEach QA instance is processed independently during fine-tuning. At inference time we run the instances through the model and assign a TRC label based on the answers.\nNaturally, a document may contain more events than the two relevant ones. Therefore, we use markers (Baldini Soares et al., 2019) in order to mark the two relevant events. Specifically, each relevant event is surrounded by the '@' character in both, the passage and the question. Figure 1 demonstrates how we process a MATRES instance.\n\nExperiments and Results\nTable 2 summarizes our evaluation results on MATRES and TB-Dense, using the two sizes of RoBERTa. We compare our results with two baseline models, and some previous work. We experiment with three variations for the questions (only for the two MATRES-related questions; for TB-Dense we only use the best out of the three), 1 as reported in the three first rows of Table 2 : QV1: <EVENT1> before <EVENT2>? QV2: Is it possible that the start time of <EVENT1> is before the start time of <EVENT2>? QV3: Is it possible that <EVENT1> started before <EVENT2>?\nWe fine-tune our models for the duration of five epochs and evaluate them on the validation set every epoch; we use the best checkpoint as the output model. We run every experiment three times using different seeds and report on the averaged accuracy and standard deviation on the testing set. 2 The MA-TRES model with the best question variation (QV3) has been further processed with two additional procedures: Perturbation and fine-tuning with BoolQ.\nPerturbation. To achieve better model generalization, we perturb the instances of the training set, using nlpaug, 3 a data augmentation library for text. We employ the optical-character recognition (OCR) error simulation, using the default argument values, which replaces about 30% of the characters (except the characters of the events) with random letters or digits considered as common OCR mistakes (e.g., l vs. 1). We modify the original training instances in place; therefore, we do not increase the size of the training set. In Table 2 we refer to this procedure as AUG. It adds about 1% to F1 in the base model, and a slightly higher percentage in the large model, on both datasets. BoolQ. Before fine-tuning on MATRES, we finetune the model on the BoolQ dataset (Clark et al., 2019) in which every instance is composed of a passage (text) and a question, provided along with a Yes/No answer. Overall, BoolQ has 9, 427 training instances, which we use for fine-tuning. In Table 2 we refer to this procedure as BoolQ. As reported, this step does not improve performance. Therefore, we did not use it for TB-Dense.\nBaseline Algorithms. To assess the contribution of our Yes/No QA design, we define two baseline algorithms. The first baseline is a traditional multiclass QA model, which is given with the same passage as in our original Yes/No QA model, paired with only one question that takes one of the labels as an answer. We experiment with two question variations:\nQV1: What is the chronological order of the two marked events: <EVENT 1> and <EVENT 2>? QV2: Is <EVENT 1> happening before, after or at the same time as <EVENT 2>?\nThe second baseline is a simple multiclass sentence-classification RoBERTa model, which receives as input for this model comprises only the passage, and the output is one of the labels from the dataset. As seen in Table 2 , our models outperform the baselines and previous work, introducing a new state-of-the-art result for TRC on both datasets. 4\n\nConclusions\nWe proposed a novel approach for TRC using a pretrained language model fine-tuned for a Yes/No QA classification task. Our model was fine-tuned to answer questions which were originally designed to support decision making during the annotation process. We believe we have demonstrated the potential of this method to leverage the Yes/No QA design to break down the prediction process into a set of Yes/No questions; our approach outperforms existing methods, achieving a new state-of-the-art result for TRC on two datasets. There is a potential practical limitation to this work, which is related to time complexity and speed performance. Since every instance is transformed into multiple QA instances, it may take a relatively long time to process a document.\n", "hypothesis": "We propose an efficient approach for temporal relation classification (TRC) using a boolean question answering (QA) model which we fine-tune on questions that we randomly generate, thereby improving upon the limitations of human annotators in approaching the task.", "answer": false}
{"title": "Automatic Identification of Code-Switching Functions in Speech Transcripts", "content": "\nIntroduction\nCode-switching, or switching between languages within the same utterance or sentence (Poplack, 1980) , commonly emerges in conversations between multilinguals and in written communication such as social media. In today's intersecting multilingual world, it is essential to develop computational tools that can process and analyze codeswitched speech and text.\nIn recent years, there has been much progress in processing code-switched language. Many codeswitched datasets have been collected for a diverse set of natural language processing tasks such as sentiment analysis, NER, conversational systems, and many others (Sitaram et al., 2019) . Workshops held on computational approaches to codeswitching have created shared tasks on language identification (Solorio et al., 2014) and Named Entity Recognition (NER) (Aguilar et al., 2019) in code-switched texts. Nuanced tasks like humor detection, sarcasm detection, and hate detection have been applied to Hindi-English code-switched data (Bansal et al., 2020) . Despite these achievements, there is relatively little work on identifying the functions of code-switching. Although there are annotation schemes (Zentella, 1998; Hartmann et al., 2018) and some annotated datasets (Dey and Fung, 2014; Begum et al., 2016; Lee and Wang, 2015; Rudra et al., 2019) , to our knowledge, there is no work automatically identifying the communicative function of a code-switch across a full range of qualities (Zentella, 1998) .\nThere are many potential applications for the task proposed in this paper, including improved cognitive models of bilingual processing, diagnosis of language disorders, and improved understanding of social factors of group membership and microaggressions. Code-switching analysis contributes to the development of cognitive models for bilingual language processing and production (Macnamara and Kushnir, 1971; Kecskes, 2006; Phillips and Pylkk\u00e4nen, 2021; Kheder and Kaan, 2021) . Understanding the functions of code-switching is critical for speech-language pathologists interacting with bilingual children, so as not to mistakenly diagnose them with a language disorder when in reality, children are taking advantage of a wide range of communicative strategies by code-switching (Miccio et al., 2009; De la Rosa, 2022) . Studying codeswitching in people with dementia and Alzheimer's disease can provide insights into language impairments experienced as their condition becomes more severe (Santi et al., 1990; Friedland, 1998; Svennevig et al., 2019) . Code-switching is also important for pragmatics research of understanding social identities and group membership that speakers are trying to assert (Auer, 2005; Cashman, 2005) . Because of political undertones of using one language over another (Heller, 1992) , code-switching is useful for understanding linguistic microagressions (Anchimbe, 2015; Takeuchi, 2022) .\nOur contributions are the following:\n\u2022 An annotation scheme identifying the func-tion of code-switching with 11 different labels, encompassing emotional, situational, and semantic functions of code-switching\n\u2022 A new dataset applying this annotation scheme to code-switched utterances in the Spanish-English Bangor Miami Corpus (Deuchar, 2010) \u2022 Trained models and experiments with XLM-RoBERTa (Conneau et al., 2019) Several studies have annotated code-switched data according to their own frameworks (Lee and Wang, 2015; Begum et al., 2016; Hartmann et al., 2018; Rudra et al., 2019) . Rudra et al. (2016) developed classifiers to determine whether Hindi-English code-switching on Twitter was opinionated or not and found that audiences preferred to use Hindi to express a negative sentiment and English to express a positive sentiment. Lee and Wang (2015) developed a system to identify the emotions in code-switched Chinese-English posts. Additionally, one corpus of Hindi-English code-switched conversations has broadly grouped the functions of code-switching in order to study the rules that govern code-switching (Dey and Fung, 2014) . The framework we apply in this paper draws upon elements from Zentella (1998)'s framework, and it closely mirrors the approach of Begum et al. (2016) . However, while their annotation scheme is based on Tweets, ours is specific to conversational code-switching. Linguists have also developed theoretical frameworks for code-switching without applying them to the systematic annotation of corpora (Poplack, 1980; Gumperz, 1982; Myers-Scotton, 1997; Zentella, 1998; Halim and Maros, 2014) .\n\nCode-Switching and Multilingual Language Models\nPrevious research has proven the success of finetuning the pre-trained models Multilingual BERT and XLM-RoBERTa for tasks such as offensive language identification (Jayanthi and Gupta, 2021) and named entity recognition and part-of-speech tagging (Winata et al., 2021) in code-switched texts.\nBecause of these models' state-of-the-art performance, we decided to fine-tune Multilingual BERT and XLM-RoBERTa on our tasks.\n\nAnnotation\nWe describe the data annotated, present our annotation scheme, and give a comparison of our annotation to previous annotation schemes.\n\nData\nWe annotate data from the Bangor Miami corpus (Deuchar, 2010) , a publicly available anonymized code-switched Spanish-English conversational dataset consisting of audio recordings and human-created transcripts between two or more speakers. This dataset was selected for annotation because of its diverse examples of natural code-switching in spontaneous conversations, as opposed to datasets with synthetically manufactured examples of code-switching. 1 We filter the data from the transcripts for sentences with instances of code-switching and annotate the first 26 transcripts of the 56 total transcripts. The statistics of our filtered dataset are: number of utterances = 1,379; number of sentences = 7,547; words in Spanish = 15,796; words in English = 20,357; ambiguous words (both Spanish and English) = 3,393.\n\nAnnotation Scheme\nWe identify eleven labels in our annotation scheme as a mix of emotional, situational, and semantic functions of code-switching. Like Begum et al. (2016) , we identify that a single code-switch could serve multiple functions because each code-switch can be seen as a sum of its semantic, structural, and sentiment-related dimensions. Thus, the labels are not mutually exclusive, and one code-switch can have multiple labels.\nChange topic: code-switch to introduce another viewpoint, change the tone, or clarify something. Ex: I'm not ready at all, \u00bfy qu\u00e9 tal t\u00fa? (I'm not ready at all, and what about you?)\nBorrowing: a short word or phrase substitution in the other language, then returning to the original language. Ex: Mi amiga de high school va a casarse en dos semanas. (My friend from high school is going to get married in two weeks.)\nJoke: code-switch for comedic effect or a sarcastic quip. Ex: You're making such a big deal about it, como si murieran las personas en la calle. (You're making such a big deal about it, as if people were dying in the street.)\nQuote: code-switch to be true to how a statement was spoken by someone else. Ex: So my Spanish teacher said, \"Oye, necesitas estudiar m\u00e1s.\" (So my Spanish teacher said, \"Hey, you need to study more.\")\nTranslate: code-switch to repeat a statement or phrase, perhaps for the sake of emphasis or clarity. Ex: A veces, sometimes, I like to be by myself. (Sometimes, sometimes, I like to be myself.)\nCommand: code-switch for imperative or mandate intended to get the addressee to do something. Ex: \u00c9l no sabe lo que est\u00e1 diciendo, just don't listen to him. (He doesn't know what he's saying, just don't listen to him.)\nFiller: a filler, brief interjection, or short noise intended to communicate meaning from the other language.\nEx: Y yo me call\u00e9, you know, porque no quer\u00eda ofender a nadie.\n(And I stopped talking, you know, because I didn't want to offend anybody.)\nExasperation: code-switch to complain or emphasize anger or frustration. Ex: Ay, c\u00f3mo me sigues molestando, I should just get up and leave! (Oh, how you keep annoying me, I should just get up and leave!)\nHappiness: code-switch to make a compliment or positive interjection. Ex: I just saw her dress, \u00a1qu\u00e9 lindo! (I just saw her dress, how pretty!)\nProper noun: code-switch to talk about people or places whose names are in the other language or pronounced according to the other language. Ex: Escogimos United Airlines porque ellos ofrecen las mejores meriendas. (We chose United Airlines because they offer the best snacks.)\nSurprise: code-switch to interject or relay that something was unexpected. Ex: \u00bfQu\u00e9 hizo ella? Oh my god. (What did she do? Oh my god.) 61.6% of the utterances in the dataset contain more than one type of code-switching. It is possible for an utterance to contain code-switching that does not fall under our scheme and therefore gets no label, but this does not occur in our dataset.\n\nComparison to Previous Annotation Schemes\nBecause of the broad range of domains to which our task and dataset can be applied, we choose to include a diverse set of tags to account for all the functions of code-switching we observe. Our categories quote, command, and translate are similar to categories in Begum et al. (2016) and Zentella (1998) . However, we use courser-grained categories to expedite annotation and improve agreement. Our changing topic category is closely modeled after Zentella (1998)'s designation of Realignment, which includes a topic shift, rhetorical question, break from a narrative, aside comment, and checking with the listener. Begum et al. (2016) We include emotion categories for code switching, which are not included in Begum et al. (2016) and Zentella (1998) , as we find this to be an important reason for code switching in dialogues. Lee and Wang (2015) 's annotation scheme for emotions in Chinese-English code-switching includes happiness, sadness, anger, fear, and surprise, three of which we share in our categories of happiness, exasperation, and surprise. We have included categories such as using a filler and expressing happiness, frustration, or surprise which we find occurs during a conversation in which someone is reacting to the statements made by the other person.\nIn a related annotation scheme, Dey and Fung (2014) establish a set of functions of codeswitching among the speakers in their Hindi-English code-switching conversation corpus, which consists of Ease of Use, Comment, Referential Function, Topic Shift, Dispreference, Personalisation, Emphasis, No Substitute Word, Name Entity, and Clarification. However, they do not go in depth into their reasoning behind choosing these functions and offer little elaboration upon what each one entails.\nA few of the functions that we identify have typically not been regarded as instances of code-switching, such as borrowing and proper nouns (Scotton and Ury, 1977) . However, these features may still be of interest for downstream applications, so we include them here. \n\nStatistics and Inter-Annotator Agreement\nIn the annotated data, the frequency of some functions of code-switching over others validates theories about code-switching. For example, codeswitching to change topics is regarded as the most frequent type of code-switching (Zentella, 1998) , a trend which is present in Table 2 . There are three filtered entries which contain markers that a codeswitch is near, but are all spoken in one language, so they receive no label.\nTo compute inter-annotator agreement, a subset of 100 code-switched utterances was labeled by another annotator. The trained annotator was fluent in English and Spanish. After engaging in a presentation which included the same information as Section 3.2 and discussing five examples with the principal annotator, the trained annotator labeled 100 code-switched utterances independently. Because our dataset is multi-label, Cohen-Kappa is computed for each label as a binary classification task. The agreement scores are shown in Table 2 for each category.\n\nAutomatic Detection of the Code-Switching Functions\nTo demonstrate the feasibility of the proposed task, we fine-tune classifiers on our annotated corpus to predict labels for code-switching in our data. Results show the most effective approach is by building unique classifiers for each label. Because over half of the labels appear in less than 10% of the data, we find that the classifiers always predict 0 for these labels if provided with all of the training data. Thus, we create balanced training datasets for each label so that half of the examples are an instance of the label, and the other half are not.\nIn addition to a baseline Naive Bayes classifier, we fine-tune bert-base-multilingual-cased (mBERT) and xlm-roberta-base (XLM-RoBERTa) classifiers using Huggingface. atively small training set, to combat overfitting, we experiment with adapter layers for the two Transformers, but find that they do not perform as well.\nTraining details and hyperparameters are in the appendix. We find the best model to be the XLM-RoBERTa model.\n\nResults\nThe accuracy for each label with each model is shown in Table 1 . Since the dataset is small, in order to quantify the statistical significance, we compute the mean accuracy of each model on each task and report the standard deviation across 5 training runs.\n\nQualitative Analysis of Results\nIn a qualitative analysis of the models' predictions, we observe that models are more likely to notice a borrowed word when it is surrounded by a longer string in the other language. In addition, when\nGoogle Colab Pro+ Tesla V100-SXM2-16GB GPU to train the models, and each model trains in less than 15 minutes.\nthere are multiple code-switching points, it is more difficult for models to identify the full range of functions. Example outputs are shown in Table 3 .\n\nConclusion\nThis paper presents a corpus of Spanish and English code-switching with labels for the different functions for code-switching. We collect the data from the Bangor Miami corpus, create an annotation scheme for functions of code-switching, and annotate the data. We propose a classifier-based approach to detect the functions of code-switching in the annotated code-switching corpus. Results show that the XLM-RoBERTa model is the most effective at predicting functions of code-switching. We believe that analysis of functions of code-switching is an innovative approach towards bilingual speech diagnosis as well as contributing to a linguistic model of code-switching.\n", "hypothesis": " The function of code-switching may be quite useful for the analysis of linguists, cognitive scientists, speech therapists, and others, but is not readily apparent.  To remedy this situation, we annotate and release a new dataset of functions of code-switching in Spanish-English.", "answer": true}
{"title": "Cross-Domain Argument Quality Estimation", "content": "\nIntroduction\nThe argumentation process is one of the cornerstones of society, as it allows the exchange of opinions and reaching a consensus together. Fueled by advances in natural language processing, recent years have witnessed the advent of Argument Mining (AM), i.e., the field of automated discovery and organization of arguments. AM is helpful over various scenarios, reaching from legal reasoning (Wyner et al., 2010; Walker et al., 2014; Poudyal et al., 2020; Villata, 2020) to supporting the decision-making process of politicians (Haddadan et al., 2019; Duthie et al., 2016; Menini et al., 2017; Lippi and Torroni, 2016; Awadallah et al., 2012) . Thus, there is a flurry of works on identification of arguments from text (Stab et al., 2018b; Fromm et al., 2019; Trautmann et al., 2020) and retrieval of them (Wachsmuth et al., 2017c; Fromm et al., 2021; Dumani and Schenkel, 2019; Dumani et al., 2020; Stab et al., 2018a) . Since arguments often have to be weighed against each other, a central property of arguments is their Argument Quality (AQ) or convincingness, i.e., their (perceived) strength. While the ancient Greeks (Rapp, 2002) already discussed the constituents of strong arguments, automated estimation is a relatively uncharted field. Due to the high subjectivity of argument strength (Swanson et al., 2015; Gretz et al., 2020; Toledo et al., 2019; Habernal and Gurevych, 2016b; Stab et al., 2018b) , obtaining high-quality annotations is challenging, cf. Section 1. In this light, a legitimate question is the reliability and robustness of the existing approaches for estimating AQ and their applicability in real-life scenarios. Existing AQ benchmark datasets are often restricted to a single domain (Wachsmuth et al., 2016; Persing and Ng, 2017) or/and make different assumptions about factors impacting the AQ. Thus, enabling transfer between sources and datasets appears especially appealing, but existing works (Gretz et al., 2020; Toledo et al., 2019; Swanson et al., 2015; Habernal and Gurevych, 2016b) cease to provide detailed studies thereupon.\nIn this work, we thus investigate for the first time the automatic evaluation of the quality of arguments from a holistic perspective, bringing together various aspects. First, we evaluate whether AQ models can generalize across datasets and domains, a crucial feature for deployment in the diverse environments encountered in relevant real-world applications. Next, we investigate the hypothesis of whether models for related argument mining tasks inherently learn the concept of argument strength without being explicitly trained to do so by evaluating their zero-shot performance for estimating AQ. A In summary, our contributions are as follows: \u2022 As far as we know we are the first to study the generalization capabilities of AQ prediction models across different datasets and AQ notions.\n\u2022 Since we determine the size of the dataset as one of the decisive performance factors, we further investigate a zero-shot setting of transferring from related Argument Mining tasks.\n2 Related Work\n\nArgument Quality\nArgument Quality (AQ), sometimes also called Argument Strength, is a sub-task of Argument Mining (AM) that is one of the central research topics among argumentation scholars (Walton et al., 2008; Toulmin, 2003; Van Eemeren and Grootendorst, 1987) . Due to its highly subjective nature, there is no single definition of AQ. As a result, there are various proposals for different factors that can affect the quality of an argument, such as the convincingness of an argument (Habernal and Gurevych, 2016a) . There are several ways to express the strength of an argument. Some works take an absolute continuous score, while others argue that strength estimation works better in (pairwise) relation to other arguments. To the best of our knowledge, we are the first to evaluate how AQ estimators trained on different corpora, AQ notions, and AQ tasks correlate with each other. One of the first relatively large corpora was presented by Swanson et al. (2015) . The SwanRank corpus contains over 5k arguments, where each argument is labeled with a continuous score that describes the interpretability of an argument in the context of a topic. They propose several methods based on linear regression, ordinary kriging, and SVMs as regression algorithms to automatically estimate the strength from an input text encoded by hand-crafted features. Other corpora have followed, using relative-and/or absolute convincingness (Habernal and Gurevych, 2016b; Potash et al., 2019) as an annotation criterion. The works proposed AQ estimators based on SVMs or BiL-STMs combined with GloVe embeddings (Pennington et al., 2014) . Gleize et al. (2019) provide a dataset, IBM-EviConv, that focuses on ranking the evidence convincingness. They used a Siamese network based on a BiLSTM with attention and trainable Word2Vec embeddings. Gretz et al. (2020), and Toledo et al. (2019) created their corpora by asking annotators whether they would recommend a friend to use the argument in a speech supporting or disputing the topic, regardless of their own opinion. Both use a fine-tuned BERT (Devlin et al., 2019) model for the absolute AQ regression task.\nThe shared evaluation practice in the previous works is to evaluate methods on each dataset independently. Gretz et al. (2020) use their newly introduced dataset for pre-training of their model. The authors then investigate the strength of their models by applying them on two related datasets UKPConv and SwanRank. By finetuning the model on the training part of two datasets, they investigate if the pretraining is helpful for the target corpora. Our work proposes to advance the evaluation and advocate for an accurate cross-dataset evaluation without additional fine-tuning on the evaluation dataset to estimate the model's applicability in challenging real-life scenarios.\nAs a common understanding of AQ is still lacking, Wachsmuth et al. (2017a,b) investigated different dimensions of AQ. Based on a survey paper of existing argument quality theories (Wachsmuth et al., 2017a) , they developed a taxonomy that aims to capture all aspects of AQ. In their work, they present a small corpus of 320 arguments annotated for 15 dimensions and explore the correlations between the different dimensions. Thus, their work presents a different view that rather focuses on the argumentation theory than on multiple corpora and the generalization of AQ estimators. Lauscher et al. (2020) ; Ng et al. (2020) created a cross-domain corpus (Q&A forums, debate forums, and review forums) with 5,295 arguments using the annotation scheme of Wachsmuth et al. (2017a) . They conclude that, in most cases, models benefit from the inclusion of out-of-domain training data. However, they do not perform a cross-corpora study of their architectures, which limits the generalizability and impact of their experiments.\n\nGeneralization across Argument\nQuality Corpora\nHigh-level applications such as Argument Retrieval (Wachsmuth et al., 2017c; Fromm et al., 2021; Dumani and Schenkel, 2019; Dumani et al., 2020; Stab et al., 2018a) and autonomous debating systems (Slonim et al., 2021) require reliable Argument Quality (AQ) models to select strong arguments among the relevant ones. The research community has identified this gap and proposed and evaluated different automated models for AQ estimation (Gretz et al., 2020; Toledo et al., 2019; Swanson et al., 2015; Habernal and Gurevych, 2016b) . However, AQ is often captured differently due to its high subjectivity, e.g., absolutely as a continuous score or relative to other arguments by pairwise comparison. Consequently, many publications also introduced their own corpus with individual annotation schemes capturing different notions of AQ. While they have compared multiple AQ estimators against each other within a single corpus, there is a lack of cross-corpora empirical evaluations. Thus, the robustness of predictions across datasets remains largely unexplored, which poses a severe challenge for reliable real-world applications integrating diverse data sources. To assess the generalizability capability of AQ estimation models, we designed a series of experiments across all four major AQ datasets to answer the following research questions:\n1. How well do AQ models perform across datasets if annotations schema and domain of the arguments do not change?\n2. How does the corpora size affect generalization?\n3. How well do models generalize across different text domains?\n4. How does the AQ quality notion affect generalization?\n5. Does the AQ model become more robust if it is trained with a combined dataset containing data from different domains and labeling assumptions also vary?\n\nDatasets and Evaluation Setting\nWe briefly describe the four AQ datasets used in our empirical study, which all capture AQ on a sentence level. They are also summarized in Table 2 .\n1. Swanson et al. (2015) constructed the dataset SwanRank with 5,375 arguments whose quality is labeled in the range of [0, 1], where 1 indicates that an argument can be easily interpreted. It consists of four controversial topics taken from the debate portal CreateDebate 1 .\n2. Habernal and Gurevych (2016b) As some of the corpora did not provide official train-validation-test splits and differed in the number of topics and the formulated task (in-topic vs. cross-topic), we decided to do our own split based on the topics of the arguments. Contrary to the original topic splits in UKPConv, IBM-ArgQ and IBM-Rank, we treat the supporting and opposing arguments from a certain topic as one topic because they have very great similarities. Whereas in their work, e.g. the topics \"We should abandon cryptocurrency\" and \"We should adopt cryptocurrency\" are represented as two topics. We perform 10-fold cross-topic cross-validation, where each fold is a 60%/20%/20% train-validation-test split, and we additionally ensure that no topic occurs in more than one split. By the latter requirement and the topic merge, we ensure an inductive setting where the AQ estimation can not rely on similar arguments in the training corpus and therefore provides a more challenging but more realistic task.\n\nModel and Training\nSince transfer learning achieves state-of-the-art Argument Mining (AM) results on different corpora and tasks (Reimers et al., 2019; Fromm et al., 2019; Trautmann et al., 2020) , we also apply it to our AQ estimation task. We use a bert-base model, pretrained on masked-language-modeling, and finetune it to predict absolute AQ scores on the respective datasets, cf. Section 3.1. As an input, we used the arguments from the respective datasets and concatenated the topic information, separated by the BERT specific [SEP ] limiter, similar to other work in AM (Fromm et al., 2019; Reimers et al., 2019; Gretz et al., 2020) . We concatenate the last four layers (as Gretz et al. (2020) ; Toledo et al. (2019) did it) of the fine-tuned BERT model output to obtain an embedding vector of the size 4 \u2022 768 = 3, 072. For the regression task, we stack a Multi-Layer Perceptron (MLP) with two hidden layers, one with 100 neurons and a ReLU activation, followed by the second hidden layer and a sigmoid activation function. We train the architecture end-to-end, with SGD with a weight decay of 0.35 and a learning rate of 9.1 \u00d7 10 \u22126 . The MLP uses dropout with a rate of 10%.\n\nResults\nTable 3 summarizes our results. We report the Pearson correlation score between the predicted-and ground-truth absolute AQ evaluated on a hold-out test set. Contrary to the original topic splits in UKPConv, IBM-ArgQ and IBM-Rank we treated the supporting and opposing topics as one topic. The task is therefore more challenging, as topic information from the contrary stance can not be used during training. However, the task is also more realistic, as one can not expect to have arguments from all topics in the training set.\n\nEvaluation on Similar Datasets and Importance of Training Set Size\nFirst, we evaluate the performance of the model on similar datasets and the dependency on the size of the training dataset. We can observe that models perform very well on other datasets from a similar domain labeled with a similar quality notion, i.e., IBM-ArgQ and IBM-Rank (both are crowd collected and annotated based on recommendableness. Furthermore, we can notice that the size of the dataset is crucial for performance: a model trained on the largest IBM-Rank dataset achieves the best score also on IBM-ArgQ. This insight gives us a solid foundation for the next steps.\n\nGeneralization Across Domains and Quality Notions\nNext, we investigate whether a transfer across domains is possible. Recall that the four datasets cover two different domains: the sentences from UKPConv and SwanRank have been extracted from debate portals, while IBM-Rank and IBM-ArgQ have been collected from the crowd. Compared to in-domain generalization, we observe a considerably worse generalization between domains: For example, trained on the crowd dataset IBM-ArgQ, we can achieve a correlation of 38.9% on the crowd dataset IBM-Rank, while training on the debate datasets SwanRank and UKPConv results in negligibly low correlations of 8% and 3%, respectively. Conversely, when evaluated on the debate portal dataset SwanRank, we obtain a correlation of 42.5% when using a model trained on the other debate portal dataset UKPConv, while the crowd-collected datasets IBM-ArgQ and IBM-Rank only achieves 27.8% and 37.0%, respectively. The smaller difference compared to the first comparison can be explained by the larger training datasets.\nSurprisingly, we observe a completely different picture for generalization across quality notions. We see only a moderate drop in performance for a fixed domain but a different quality notion. For instance, the model trained on SwanRank performs relatively well on the UKPConv dataset. Viceversa, we observe a more considerable performance drop, which can be explained by the smaller size of the UKPConv dataset.\n\nMulti-Domain and Multi-Quality Notion Training\nTo investigate whether a single model can grasp various dimensions of quality and work on arguments from various domains, we designed another set of \"leave-one-out\" experiments. We train on the training sentences of all but one AQ corpus and evaluate the performance on all test sets. The four rows \"all except\" define the three training sets, e.g. \"all except UKPConv\" consists of the training sets of (SwanRank, IBM-ArgQ, and IBM-Rank).\nThe entries on the diagonal thus show how well the models perform when evaluated on an unseen corpus.\nFor evaluation on the unseen IBM-Rank dataset after training on the remaining ones, we can obtain a correlation of 46.5%, which nearly reaches the correlation of 48.1% we obtained when training and evaluating on IBM-Rank. For SwanRank, IBM-ArgQ and UKPConv, we can even surpass the correlation on the respective test set by training on all other training sets instead of the one from the respective corpus.\n\nCross-Corpora Generalization Conclusion\nTo summarize, we conclude that in our analysis the available datasets and models for AQ are reliable.\nOur most important insight is that AQ notions do not contradict each other, and a single model can estimate the AQ of text from different domains. Therefore, the practical recommendation for reallife application is to combine all available datasets across different domains and AQ notions.\n", "hypothesis": "We find that generalization depends on a sufficient representation of different domains in the training part. In zero-shot transfer and multi-task experiments, we reveal that argument quality is among the more challenging tasks but does not significantly impact the performance of other tasks.", "answer": false}
{"title": "Transformed Protoform Reconstruction", "content": "\nIntroduction\nLanguages change over time and sometimes diverge into multiple daughter languages. The common ancestor of a set of genetically related languages is their proto-language. While there are proto-languages such as Latin that are attested, they are the exception 2 . Reconstructed words and morphemes in proto-languages are called protoforms. The task of reconstructing unattested protolanguages is called protoform reconstruction.\nHistorical linguists reconstruct proto-languages by identifying systematic sound changes that can be inferred from correspondences between attested daughter languages (see Table 1 ). They compare the sounds between a set of cognates, or words with a common ancestor, to develop hypotheses about the types and chronologies of sound changes. ' This task is inherently data-constrained, especially for under-documented languages. Such data scarcity makes it a particularly difficult task for contemporary neural network architectures such as the Transformer (Vaswani et al., 2017) , which are data hungry.\nThe contributions of this paper are as follows:\n\u2022 Application of the Transformer architecture to the protoform reconstruction task, achieving state of the art performance, contrary to expectation.\n\u2022 Expansion of prior digital versions of H\u00f3u ( 2004)'s Chinese dataset to include a total of 804 cognate sets across 39 modern varieties and Middle Chinese.\n\nRelated Work\nApplying machine learning to protoform reconstruction is not new. Bouchard-C\u00f4t\u00e9 et al. (2013) learn an unsupervised protoform reconstruction model for the large Oceanic language family using Monte Carlo Expectation Maximization (Dempster et al., 1977\u037e Bouchard-C\u00f4t\u00e9 et al., 2008) , supervising the model with a gold phylogeny and using a probabilistic, generative model of sound change. List et al. (2022) apply an SVM classifier to supervised reconstruction by treating sound correspondences as training examples. Note that there were no word boundaries in the input matrix\u037e that is, all sound correspondences across the training set are flattened into one matrix. Furthermore, each language has an independent phonemic inventory. To learn contextual information, the authors experiment with adding features encoding the position of phonemes, among others. Ciobanu and Dinu (2018) learn a conditional random field (Lafferty et al., 2001) using n-gram features for supervised reconstruction and ensemble 5 daughter-to-protoform models. They use a dataset of 3,218 complete cognate sets spanning Latin (the proto-language) and 5 Romance languages: Romanian, French, Italian, Spanish, Portuguese. Meloni et al. (2021) employ a GRU-based seq2seq approach (Cho et al., 2014) to Latin protoform reconstruction and achieve state-of-theart character edit distances. They extend Dinu and Ciobanu (2014) 's Romance data using data from Wiktionary-for a total of 8,799 cognate sets across 5 Romance languages plus Latin-in both orthographic and phonetic (IPA) representations. In their model, all entries comprising the cognate set are concatenated together in a fixed order to form a training example. Chang et al. (2022) applied Meloni et al. (2021) 's architecture to the reconstruction of Middle Chinese on a dataset of 5000+ cognate sets spanning 8 languages they compiled from Wiktionary. 3 Fourrier (2022) compares statistical machine translation, RNN, and Transformer architectures for protoform reconstruction, but they evaluate their results using BLEU scores (Papineni et al., 2002) instead of edit distance. They find that their Transformer model did not outperform the RNN models on protoform reconstruction. In addition, their multilingual NMT (neural machine translation) model predicts many languages instead of one target language and is trained on bilingual pairs for protoform reconstruction (e.g. Italian-Latin and Spanish-Latin), unlike comparative reconstruction. In contrast, we encode the entire cognate set consisting of multiple daughter languages (5 for the Romance dataset\u037e 39 for Chinese) and predict the corresponding protoform.\n\nDatasets\nWe train and test our model on Romance and Sinitic (Chinese) language datasets. For Romance languages, we use Meloni et al. (2021) 's dataset which consists of 8,799 cognate sets of Romanian, French, Italian, Spanish, Portuguese words and the corresponding Latin form (approximately, a protoform). There are two versions of this dataset: phonetic and orthographic. The phonetic dataset (Rom-phon) represents words with IPA symbols whereas the orthographic dataset (Rom-orth) represents words in the orthographic form of each language. We preserved all diacritics, except for vowel length. This dataset is an extension of Dinu and Ciobanu (2014) 's original dataset of 3,218 cognate sets, which is not publicly available. Refer to Table 2 for more information.\n\nExpanding digital versions of H\u00f3u (2004)\nFor Sinitic languages, we created a dataset of Middle Chinese and its modern daughter languages. Middle Chinese is an unattested language, and we thus have to rely on Baxter and Sagart (2014)'s reconstructions of forms corresponding to 4,967 Chinese characters. We scraped Wiktionary to obtain H\u00f3u (2004)'s phonetic representations of their modern reflexes. 4 The resulting dataset contains 804 cognate sets of 39 modern Sinitic languages and the corresponding reconstructed Middle Chinese word. List (2021)'s version previously had 894 cognate sets across 15 varieties.\n\nModel\nWe propose a Transformer-based encoder-decoder architecture (Vaswani et al., 2017) because such models have produced state-of-the-art results on many sequence processing tasks. Transformers are by reputation data hungry, though, which poses a challenge to our problem setting, where the number of available training examples is often very small. We modify the standard encoder-decoder architecture to accommodate the structure of our datasets, where multiple daughter sequences correspond to a single protoform sequence. Like Meloni et al. (2021) , the daughter sequences are concatenated into a single sequence before being fed into the encoder. Because we only care about the relative position between tokens within each daughter sequence but not across daughter sequences, positional encoding is applied to each individual daughter sequence before concatenation. Along with positional encoding, an additive language embedding is applied to the token embeddings to differentiate between input tokens of different daughter languages.\n\nBaselines\nWe compare our Transformer model to a variety of baselines. For Meloni et al. (2021) , we use Chang et al. (2022) 's PyTorch re-implementation and reran a Bayesian hyperparameter search using WandB (Biewald, 2020) to ensure a more fair comparison (since our model is tuned with WandB as well). We also include the random daughter (randomly designate a daughter form as the protoform and assume no sound change) and the majority constituent baselines (predict the most common phoneme in each syllable constituent) from Chang et al. (2022) . For the SVM and CoRPaR classifiers (List et al., 2022) , we experiment with different contextual features, such as Pos (position), Str (prosodic structure), and Ini (whether or not the phoneme appears word-initially or word-finally).\nWe publish results on Meloni et al. (2021) 's full set of 8,799 cognates but cannot redistribute this set due to Dinu and Ciobanu (2014) 's restrictions. For reproducibility, we include results on Meloni et al. (2021) 's public subset of 5,419 cognates in the Appendix (Table 7 ), both of which include vowel length. Observe that these results are worse than those obtained on the full set, suggesting that the RNN and Transformer are dependent on a wealth of training data.\n\nPreprocessing\nIn all our datasets, we merge diacritics to their base segments to form a multi-character token. For instance, the sequence [t, \u02b0] is concatenated to [t\u02b0] . This ensures that phonemes are treated as one token. For Chinese, tone contours (a sequence of tones) are treated as one token. When multiple pronunciation variants are listed for a single Chinese character, we arbitrarily pick the first one.\n\nEvaluation criteria\nWe evaluate the predicted protoforms using edit distance (Levenshtein et al., 1966) , normalized edit distance (edit distance normalized by the length of the target) and accuracy (the percentage of protoforms that are reconstructed without any mistakes). Like Chang et al. (2022) , we also use feature error rate calculated using articulatory feature vectors from PanPhon (Mortensen et al., 2016) because it reflects the phonetic similarity between the prediction and the gold protoform. For datasets with phonetic transcriptions (Romancephonetic and Chinese), we use phoneme edit distance and normalized phoneme edit distance. As List (2019) suggests, we use B-Cubed F Scores (Amig\u00f3 et al., 2009) to capture the structural similarity between the gold and predicted protoforms (0: structurally dissimilar, 1: similar). With the exception of character and phoneme edit distance, the metrics enable fair comparison across different language families, which will differ in the average word length.\n\nResults\nTable 3 shows that our model consistently has the best performance on all datasets with regards to most metrics. The results were averaged across 5 runs. Out of all datasets, our model performs best on the Rom-orth dataset, where we achieve a 7.0% decrease in phoneme edit distance and a 1.43p.p improvement in accuracy relative to the RNN baseline. We observe the most dramatic performance difference with the RNN baseline on the Sinitic dataset: a 10.48% decrease in phoneme edit distance and a 5.47p.p increase in accuracy. For reproducibility, results on the publicly available portion of the Rom-phon and Rom-orth datasets are provided in Table 7 in the Appendix.\n\nAnalysis\nWe observe that the BCFS is relatively high for the Romance non-neural baselines compared to those of the Chinese ones. This suggests that the sound changes in the Romance datasets are more regular than that of Chinese, which corroborates List et al.\n(2014)'s results that more than half of the Chinese characters in their dataset could not be explained by a tree model. We examine the errors made by the Transformer model on the Rom-phon datasest. Substitutions constitute around 61% of the errors made by the Transformer\u037e deletions, 21%, and insertions, 18%. The highest number of substitution errors occur between [i, \u026a] , [e, \u025b], [o, \u0254] and [u, \u028a]-vowel pairs that contrast only in tenseness. This is consistent with the analysis of Meloni et al. (2021) , where substitutions between tense-lax vowel pairs take up the largest portion of errors.\nWe observe that other common substitution errors also happen between phonemes that share major phonetic features. This demonstrates that al-though no explicit phonetic information is fed directly into the model, the model makes mistakes motivated by phonetic similarity, like Meloni et al. (2021) .\nWe do not observe notable differences in the error statistics between the Transformer and the RNN.\n\nLanguage relatedness\nInspired by Fourrier (2022) , we probe our model for diachronic information on how genetically related each Romance language is to each other. We create a distance matrix between every pair of languages in a dataset by taking the cosine similarity between a pair's language embeddings. We then use sklearn (Pedregosa et al., 2011) 's implementation of the Ward variance minimization algorithm (Ward Jr, 1963) to perform hierarchical clustering on the distance matrix. We take a consensus of the dendrograms from 5 different runs using the consense program from PHYLIP (Felsenstein, 2013).\nAs we see in Figure 2 , the Transformer captures more of the phylogenetic relationships among the languages correctly for the Rom-phon dataset. Indeed, the Generalized Quartet Distance (GQD) (Sand et al., 2013\u037e Pompei et al., 2011\u037e Rama et al., 2018) between the gold and predicted tree, calculated using quartetDist from the tqDist library (Sand et al., 2014) , is 0.4 for the Transformer but 0.8 for the RNN. See Figure 5 in the Appendix for the results of the orthographic dataset.\n\nModel PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Sinitic\nRandom daughter (Chang et al., 2022) 3.7702 0.8405 0% 0.2893 0.2748 Majority constituent (Chang et al., 2022) 3.5031 0.7806 0% 0.2013 0.3695\nCorPaR (List et al., 2022) 3.2795 0.7278 0% 0.3972 0.3332 SVM + PosStr (List et al., 2022) 1.6894 0.3692 15.52% 0.1669 0.5418 RNN (Meloni et al., 2021) 1 Since the Romance dataset only includes 5 daughter languages, our results are insufficient to corroborate or contradict Cathcart and Wandl (2020) 's findings: the more accurate the protoforms, the less accurate the phylogeny will be. It is not clear if the model's language embeddings are learning information that reflects shared innovations (sound changes that if shared among a set of daughter languages, would be acceptable justification for grouping them)-the only acceptable criterion for phylogenetic inference in historical linguistics (Campbell, 2013) -or if the model is learning superficial phonetic similarity.\n\nConclusion\nBy showing that Transformers can outperform previous architectures in protoform reconstruction despite the inherent data scarcity of the task, our work motivates future research in this area to take full advantage of the recent advancements in the Transformer space.\nAccurate supervised reconstruction can help pre-dict protoforms for cognate sets where linguists have not reconstructed one yet. Future work could reconstruct proto-languages whose linguist reconstructions are not available, by transferring knowledge learned from languages with already reconstructed protoforms. Furthermore, future work can leverage the abundance of work in unsupervised NMT to adapt our Transformer model for the unsupervised setting, a more realistic scenario for the historical linguist.\n", "hypothesis": " Meloni et al.  (2021) achieved the stateof-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model.  We update their model with the state-of-the-art seq2seq model-the Transformer.", "answer": true}
{"title": "The Ecological Fallacy in Annotation: Modelling Human Label Variation goes beyond Sociodemographics", "content": "\nIntroduction\nDifferent annotators will not necessarily assign the same labels to the same texts, resulting in human label variation (Plank, 2022) . Previous work finds that this variation depends at least in part on the sociodemographics of annotators, such as their age and gender (Binns et al., 2017; Al Kuwatly et al., 2020; Excell and Al Moubayed, 2021; Shen and Rose, 2021) . These results are particularly pronounced for subjective tasks like toxic content detection (Sap et al., 2019; Kumar et al., 2021; Sap et al., 2022; Goyal et al., 2022) . Since human label variation is relevant to a wide range of NLP tasks, recent research has begun to model individual annotator behaviour, rather than predicting aggregated labels (Davani et al., 2022; Gordon et al., 2022) . In this setting, we would expect sociodemographic attributes to help explain annotator decisions. Therefore, we investigate whether explicitly accounting for the sociodemographic attributes of annotators leads to better predictions of their annotation behaviour 1 .\nThere is a risk of misreading these efforts as an example of the ecological fallacy: aggregate group behaviour does not necessarily explain individual behaviour (Robinson, 1950; Freedman, 2015) . For example, while on average, white annotators may be more likely to label African-American Vernacular English as toxic (Sap et al., 2019) , that does not mean it is true for every white annotator individually. However, we aim at exactly this distinction to discuss the relevance of sociodemographic groups in models of individual annotator behaviour. Likewise, we do not assume prior work to commit ecological fallacies, even if a less-nuanced read might suggest it.\nDavani et al. ( 2022) introduce a simple multiannotator model, where each annotator is modelled with a separate classification head. We expand their model with group-specific layers, which are activated for each annotator based on their sociodemographic attributes. We compare the two model setups to a control setup where we randomise group assignments. All comparisons use annotator-level toxicity data from Kumar et al. (2021) . We find that find that explicitly accounting for sociodemo-graphic attributes does not significantly improve model performance. This result suggests that human label variation happens at a more individual level than sociodemographics, and that annotator decisions are even more complex.\nContributions 1) We introduce group-specific layers to model groups of annotators with shared attributes in multi-annotator models. 2) We evaluate the effect of group-specific layers for toxic content detection, and show that explicitly accounting for sociodemographic attributes does not significantly improve performance, thus highlighting the risk of the ecological fallacy in annotator modelling.\nAs a corollary, we show that multi-annotator models can be applied to many times more annotators than in prior work.\n\nSociodemographics in Annotation Behaviour\nA growing body of research studies how annotator sociodemographics relate to their annotation decisions, for tasks ranging from natural language inference (Biester et al., 2022) to the detection of racist (Larimore et al., 2021) or generally toxic (Sap et al., 2022) language. Goyal et al. (2022) , for example, find that annotators from certain sociodemographic groups (e.g., LGBTQ people) tend to find content attacking their own groups (e.g., homophobic content) to be more toxic. This motivates our research into explicitly accounting for sociodemographics to model annotation behaviour. However, the link between sociodemographics and behaviour is not uncontested. Biester et al. (2022) , for example, do not find significant differences in annotation behaviour between annotators of different genders for four different tasks.\nPredicting Annotators' Decisions on Text Different from analyses of annotation behaviour, a recent line of research attempts to learn models based on individual annotations (Plank et al., 2014; Jamison and Gurevych, 2015; Akhtar et al., 2020; Fornaciari et al., 2021; Cercas Curry et al., 2021) . These models are motivated by the concern that aggregating labels into a single \"truth\" is too simplistic for many tasks (Uma et al., 2021; Basile et al., 2021) and might introduce uneven representation of perspectives (Prabhakaran et al., 2021; Abercrombie et al., 2022) .\nA particular way of learning from disaggregated labels are models that predict individual annotator decisions for an example. Our work builds directly on such a model, multi-annotator models (Davani et al., 2022) , which we describe in more detail separately ( \u00a74). Gordon et al. (2022) present a model which also predicts individual annotations and allows a user to interactively aggregate them based on \"a jury\" inspired by the US judicial system. Their work is similar to ours in central aspects as they explicitly model annotators' sociodemographics and use the same dataset as we do (Kumar et al., 2021) . Different from our work, they frame the task as a regression problem and develop a model based on recommender systems. While they also explore ecological fallacies, they focus on usage risks of their system and countermeasures. In contrast, we consider the issue of the ecological fallacy in modelling annotation behaviour more generally. We compare our findings to their results ( \u00a76).\n\nData\nWe use a sample of the Kumar et al. (2021) dataset for our experiments. The full dataset contains 107,620 English comments from Twitter, Reddit, and 4Chan, annotated for toxicity by 17,280 annotators. The annotation process encouraged annotator subjectivity (R\u00f6ttger et al., 2022) which is a desired feature for modelling annotator behaviour. For each annotator, there is extensive sociodemographic information, collected with a survey. Annotations are given as ratings on a five-point scale which we convert to binary annotations by mapping ratings of 2 to 4 to toxic, and ratings 0 and 1 to non-toxic.\nWe randomly sample comments from the dataset until we reach annotations from more than 5,000 annotators. We then add all other annotations by these annotators. This approach maximizes the number of examples while controlling the number of annotators in our sample.\nOur final sample contains 111,780 annotations from 5,002 annotators on 22,360 comments with 20 to 120 annotations per annotator (mean 22.35). Most comments have five annotations. 20 comments have four because we removed any underage annotators before sampling. In total 78,357 annotations (70.10%) are toxic, and 33,423 annotations (29.90%) are non-toxic.\nWe focus on four sociodemographic attributes: gender, age, education, and sexual orientation. Group sizes vary by attribute. For gender, 2,450 annotators (48.98%) identify as female, 2,116 (42.30%) as male, 23 (0.46%) as non-binary (rest in residual categories, full statistics in A.1).\n\nExperiments\nWe compare three models. The baseline model is the multi-annotator model by Davani et al. (2022) . We use their multi-task variant: For each annotator, there is a separate classification layer trained on annotations from that annotator. All annotator layers share a pre-trained language model used to encode the input. We use RoBERTa (Liu et al., 2019) for this, motivated by computational constraints. The other models in our experiments build on this baseline model.\nFor the sociodemographic models, we add group-specific layers based on sociodemographic attributes of the annotators. A single attribute, e.g., age, implies several groups, e.g., ages 25-34, ages 35-44. We add the group-specific layers between the pre-trained model and the annotator layers. Each group of annotators shares a separate group-specific layer. We implement group-specific layers as fully-connected, linear layers, each learning a feature transformation applied for one group of annotators.\nFinally, for the random models, we shuffle the assignment of annotators to groups from the sociodemographic model, retaining the relative group sizes. In other words, the probability of each annotator staying in the same group or being reassigned to another group corresponds to the relative size of each group. This approach keeps the model architecture constant while removing the connection between actual sociodemographic attributes and group assignment. It allows us to distinguish the effects of additional parameters, which groupspecific layers add in comparison to the baseline, from the effects of sociodemographic information.\n\nEvaluation Setup\nWe evaluate all models on individual annotations from gender, age, education, and sexual orientation groups. This setup is comparable to the \"individual label\" evaluations in Davani et al. ( 2022) and Gordon et al. (2022) , but with scores calculated per group of annotators. We measure performance in macro-average F 1 , to weigh each class equally.\n\nCross-Validation\nAs there is no standard split available for our dataset, we perform three iterations of a four-fold cross-validation with different seeds (training details in Appendix A.3). We choose four folds, so that even very small groups have more than a hundred annotations in each test set. Across folds, the numbers of annotations per sociodemographic group are similar (see Appendix A.4). We construct test sets that only contain comments unseen by the annotators in the training set. We also ensure that all test sets have similar proportions of toxic or non-toxic comments (assigned by the majority of annotators) to address the class imbalance in the dataset (70.62% toxic, see \u00a73).\n\nStatistical Significance\nWe test for statistical significance of our results from multiple runs of k-fold cross-validation via replicability analysis (Dror et al., 2017) . We report the number of significant folds and the Bonferroni-corrected count (Dror et al., 2018) in Appendix A.2. We compute the pvalues for each fold via a paired bootstrap-sampling test with BooStSa (Fornaciari et al., 2022) . We set the significance level \u03b1 = 0.05, draw 1000 bootstrap samples per fold, and use a sample size of 50% of the respective test set.\nRemarks on Groups Annotators from different groups of the same attribute will in most cases not have annotated the same examples. Therefore, comparisons between models are only meaningful within each group.\nThe groups modeled via group-specific layers and those in the result tables are always the same. For example, if we report scores for gender groups, then the sociodemographic and randomized models are also based on gender groups. In the following, we focus on a subset of groups, omitting, e.g., \"Prefer not to say\" (see Appendix A.5).\n\nResults\nTable 1 shows the results for gender, age, education, and sexual orientation. A naive majority class baseline that predicts all input to be toxic performs worse than all other models with a large margin (exact results in Appendix A.5).\nSociodemographics vs. Baseline Across attributes, the average scores of the sociodemographic model and the baseline are similar. The sociodemographic model often has a slightly higher average macro F1 than the baseline, but no statistically significant gains. Where average performance is better by several points, as for homosexual annotators, this gain is offset by a large variance in performance (a consequence of small group sizes).\nSociodemographics vs. Random We also do not find significant performance differences between sociodemographic group-layer models and the corresponding random group assignment models. For most groups, the randomized models achieve the highest average scores, but differences to the sociodemographic model are never statistically significant. \n\nDiscussion\nWe do not find strong evidence that explicitly modelling sociodemographics helps to predict annotation behaviour with multi-annotator models. These results might seem counter-intuitive, given the evidence of systematic annotation differences between sociodemographic groups (see \u00a72). This discrepancy, however, echoes the issue highlighted by ecological fallacies (Robinson, 1950) : Not every annotator will be a perfect representative of their group, so we will not necessarily learn additional information based on their group identity. This seems especially true if we already have access to individual behaviour (i.e., individual annotations).\nIn contrast to Davani et al. ( 2022), we made sociodemographic information explicit in our experiments, as one of the factors influencing annotation behaviour. Group-specific layers can be seen as an inductive bias putting emphasis on the sociodemographic relations between annotators. However, there are potentially many other factors influencing annotation behaviour (e.g., attitudes, moral values, cognitive biases, psychological traits). In light of our results, it seems plausible that multi-annotator models learn about these factors implicitly as part of predicting individual behaviour, so that making one factor explicit does not change prediction quality, at least in the case of sociodemographics.\nStill, we also know that generally group attributes can help predict individual decisions, i.e., as base rates or priors. To avoid ecological fallacies in modelling annotation, we therefore need to better understand when and how modelling sociodemographic information is useful in predicting an individual annotator's decisions. For example, we have only evaluated group-specific layers for single attributes. In contrast, social scientists have long adopted the idea of intersectionality (Crenshaw, 1989) , which also informs research on fairness in machine learning (Wang et al., 2022) . Intersectionality means that the effect of interactions between sociodemographic attributes enables specific experiences that are not captured by the attributes in isolation. For example, identifying as a man means something different depending on the person's education. Groups derived from single attributes might simply be too coarse to improve classifiers learnt from individual labels, as in multi-annotator models.\nThe dataset we use (Kumar et al., 2021) has many characteristics which are ideal for our study (see \u00a73). However, it uses a broad notion of toxicity, in contrast to other studies of toxic language (Larimore et al., 2021; Sap et al., 2022) , which match content and analysed groups. When modeling the groups frequently referenced in the datasets themselves, we would expect greater benefits from group-specific layers. Similar to us, Biester et al. (2022) who do not find significant differences between annotators of different genders, do so in a more general setting.\nWe can only partially compare to Gordon et al. (2022) , despite using the same dataset. In addition to differences in approach (see \u00a72), our and their work also differ in their research questions and thus experimental conditions. Gordon et al. (2022) compare their full model (group and individual) against using group information alone.\nWe compare our full model (group and individual) against using individual information alone. So it is unclear if their model would benefit from group information in comparison to individual-level information alone. While they find an improvement from group information it is only in comparison to a baseline predicting not individual but aggregated labels. Additionally, the composition of test sets sampled from the full dataset differs between the studies: Gordon et al. (2022) use a test set of 5,000 comments, while we use 22,360 comments in a four-fold cross-validation. We leave an explicit comparison to future work.\nGroup-specific layers ( \u00a74) are a natural extension of annotator-specific classification layers in multi-annotator models. However, other architectures to predict annotator-level labels use different ways to represent sociodemographic information, e.g., via embeddings in a recommender system (Gordon et al., 2022) . Future work could explore additional representations of annotator attributes (e.g., as part of the input, either textual or as separate features) and other approaches to modelling the relation of individual labeling decisions and attributes (e.g., probabilistic graphical models).\n\nConclusion\nWe ask how relevant modelling explicit sociodemographic information is in learning from individual annotators. Our experiments with group-specific layers for four sociodemographic attributes on social media data with toxicity annotations (Kumar et al., 2021) show no significant benefit of modelling sociodemographic groups in multi-annotator models. However, as the issue of ecological fallacies highlights, it is not implausible that these models do not learn additional information from group information beyond the inherent variation. However, our results do not refute the usefulness of sociodemographic attributes in modelling annotation, but underscore the importance of their judicious use. Different tasks and model architectures will likely benefit to different extents. Ultimately, annotation behaviour is driven by complex factors and we will need to consider more than annotators' sociodemographics.\n", "hypothesis": "In a series of experiments for toxic content detection, we find that explicitly accounting for sociodemographic attributes can significantly improve model performance. This result indicates that individual annotation behavior is strongly influenced by sociodemographics.", "answer": false}
{"title": "Zero-shot Cross-lingual Transfer With Learned Projections Using Unlabeled Target-Language Data", "content": "\nIntroduction\nZero-shot cross-lingual transfer refers to the transfer of task-specific knowledge from a (highresource) source language to a (zero-resource) target language that has no labeled task-specific data for training. A popular paradigm for cross-lingual transfer learning is to finetune pretrained multilingual models using labeled task-specific data in the source language and directly evaluate these finetuned models on target language test sets.\nA parameter-efficient alternative to full finetuning for cross-lingual transfer is MAD-X (Pfeiffer et al., 2020b) , an adapter-based framework that * Equal contribution scaffolds on multilingual pretrained models to combine task-specific and language-specific modules in a plug-and-play manner. Adapters (Houlsby et al., 2019) are feedforward layer blocks inserted within each Transformer layer to selectively learn taskspecific and language-specific capabilities via task adapters and language adapters, respectively. Language adapters are trained using self-supervised objectives like masked language modeling (MLM) and task adapters are trained using task-specific objectives. To enable task transfer to a target language, the relevant language and task adapters are combined at test-time.\nIn the zero-shot setting, we assume access to unlabeled text in the target languages. In MAD-X, this text is only used to train target language adapters and not further used during finetuning. Given knowledge of which languages we want to target, can we make effective use of unlabeled text in the target languages even during task-specific finetuning? This is the main question we tackle in this work.\nWe propose a general adapter-based technique to inject target language bias into task-specific finetuning. Using the unlabeled text in each target language, we construct an affine subspace from contextualized representations for every Transformer layer in the multilingual model. These subspaces are defined using singular value decomposition (SVD) and only need to be computed once per target language. During task-specific finetuning using labeled data in the source language, we project the source representations onto the target language subspaces. This projection can be invoked randomly using a projection probability defined as a hyperparameter. Projections can also be triggered depending on whether the current source representations are closer to the mean embedding of the source language subspace compared to the mean embedding of the target language subspace. We investigate both these projection policies and find that they both improve performance across multiple tasks in multiple languages compared to state-ofthe-art adapter baselines. We also release code 1 to reproduce our experiments.\n\nMethodology\nAdapters and MAD-X. Adapters for language models (Houlsby et al., 2019) are bottleneck feedforward modules, typically inserted in each Transformer layer of a multilingual model before layer normalization. Instead of finetuning the entire model, only adapters are tuned for a specific task. Pfeiffer et al. (2020b) extended adapterbased fine tuning to support cross-lingual transfer. Their framework called MAD-X (Multiple Adapters for Cross-lingual transfer) comprises of language adapters and task adapters. Language adapters are pretrained using masked language modeling to learn language-specific features. Task adapters are stacked on top of language adapters during downstream task finetuning to learn taskspecific information. To achieve zero-shot transfer, the model is trained with a frozen source-language language adapter and a task adapter. During test time, the source-language adapter is replaced with the target-language adapter and evaluated on test instances in the target language.\nOverview of our technique. We are interested in the setting where we have apriori knowledge of which languages we want to target at test time. We aim to bias cross-lingual transfer towards known target languages during task-specific finetuning. We start with MAD-X as our underlying framework and adopt the following 3-step approach:\n\u2022 We construct layer-specific subspaces for each of the target languages. This is done by computing SVD on contextualized token representations extracted from each layer. See \u00a72.1 for more details.\n\u2022 During task-specific training, we selectively project output representations from the language adapter of a chosen layer onto the target language subspace. These projections are triggered based on two policies: Random projection ( \u00a72.2) and Mean Cosine Distance ( \u00a72.3). The projected representations are further passed through the task adapter that is trained using labeled data in the source language.\n\u2022 Similar to MAD-X, we evaluate on the target language by simply swapping the source language adapter with the target language adapter while keeping the task adapter fixed. No projection is done during inference.\n\nLanguage Subspaces and Projections\nOur objective is to bias the model towards the target language while fine-tuning for a task. For this, we need to extract language-specific information from model representations that jointly exhibit language-specific and language-independent properties. Language-specific subspaces have been typically used to analyze representations in multilingual language models. Choenni and Shutova (2020) showed that individual representations can be used to predict linguistic typological features after projecting onto language-sensitive subspaces. Chang et al. (2022) construct language subspaces with SVD using language-specific contextualized token embeddings. They analyze model performance and other properties after computing layerwise projections of representations to various language subspaces.\nWe construct subspaces for each of the target languages using SVD and contextualized token representations for unlabeled text in the target language. Consider a pretrained model like XLMR (Conneau et al., 2020) that takes text sequences from the target language as its input. d-dimensional embeddings from a particular layer for a given language A can be grouped into a matrix M A \u2208 R n\u00d7d . SVD of M A (after subtracting the mean representation for A) can be written as:\nM A = U A \u03a3V T\nA . The right singular matrix V A is considered to be the subspace for language A. These subspaces only need to be computed once for each layer. Next, we look at when projections should be invoked.\n\nRandom Projection\nFor a given target language, during finetuning using task-specific data in the source language, we project the source representations onto the target language subspace with a predetermined probability p. This projection is invoked right before passing the representation through the task adapter, having already passed through the language adapter. To project onto a target subspace, we first shift the target language subspace so that it passes through the source language mean embedding and then take the projection onto the target subspace (Chang et al., 2022) . Let S be the source language and Q be the target language. Let subspaces and means of representations from one of the Transformer layers for the source language be V S and \u00b5 S , respectively. Projection of a representation x on S is given by:\nProject S (x) = V S V T S (x \u2212 \u00b5 S ) + \u00b5 S\nThe projection of x onto the target language subspace, that is shifted onto the source subspace, can be computed as:\nProject Q,\u00b5 S (x) = V Q V T Q (x \u2212 \u00b5 S ) + \u00b5 S\nThe main intuition here is that by probabilistically projecting source representations onto the target language subspace during task-specific finetuning, the model can encode both source and target language information in its representations. The model cannot solely rely on source-language specific features during task-specific training.\n\nMean Cosine Distance (MCD)\nWe suggest another projection scheme, Mean Cosine Distance (MCD), that is more informed than randomly projecting source representations based on a projection probability p. Using MCD, we project those embeddings that are deemed as being further away from the target language subspace compared to the source language subspace. This is quantified using a cosine distance between an embedding from a layer and means of source and target language subspaces. If an embedding is closer to the source language mean compared to the target language mean, we project it onto the target language subspace so as to make it more similar to target language embeddings. However, if an embedding is closer to the target language mean, we can possibly omit projection since it already contains information relevant to the target language.\nConsider a set of embeddings extracted from one of the Transformer layers. Let the means of all embeddings from this layer and the associated subspace be denoted by \u00b5 and V, respectively. \u00b5 S and \u00b5 Q denote the means for the source and target language, respectively. Similarly, V S and V Q refer to the respective subspaces. Let x denote a token embedding from the source language. The MCD policy can be written as:\nx = Project Q,\u00b5 S (x) if c(x, \u00b5 Q ) < c(x, \u00b5 S ) x otherwise\nwhere Project Q,\u00b5 S (x) is defined in Section 2.2 as the projection of x onto the target subspace V Q Figure 1 : A single Transformer layer as modified by the MAD-X setup and our projection scheme. During training, the output from the source language adapter is projected onto the target language subspace with probability p for random projection (or, if deemed necessary, by the MCD scheme). Dotted arrows refer to the inference time pathway when representations pass through the target language adapter and no projection is applied.\nand c(x, y) refers to the cosine similarity between two embeddings x and y.\nFigure 1 provides an illustration of our proposed technique within a single Transformer layer that includes language and task adapters (as in the MAD-X framework).\n\nExperimental Setup\nSubspace construction. To construct language specific subspaces, we adopt the settings used by Chang et al. (2022) . Text sequences of length 512 are taken from the OSCAR dataset (Ortiz Su'arez et al., 2019) and passed through XLMR (Conneau et al., 2020) to produce layer-wise contextualized embeddings. We pick 262K contextualized representations and subtract the representation mean before computing SVD. For a low-dimensional subspace, we select the greatest k singular values such that their sum of squares is greater than or equal to 90% of the total variance. (Total variance is given by the sum of the squared singular values produced.) Finally, in order to compute the languagespecific subspaces, the corresponding right singular vectors are taken as the basis. Datasets. We conduct cross-lingual transfer experiments on three tasks, Named Entity Recognition (NER), Question Answering (QA) and Natural Language Inference (NLI), where the source language is always English. For NER, we use the WikiANN dataset (Rahimi et al., 2019) , and show results for nine languages Hindi, Vietnamese, German, Indonesian, Icelandic, Ilocano, Swahili, Burmese and Javanese with roughly 20K instances in the English train set and between 1K and 10K instances in the target dev and test sets. For QA, we use XQuAD (Artetxe et al., 2019) , a multilingual extension of SQuAD (Rajpurkar et al., 2016) and we report results for Hindi, Vietnamese and German consisting of around 87K examples in the English SQuAD train set and 1190 instances in the three target dev sets. For NLI, we use the Amer-icasNLI dataset (Ebrahimi et al., 2021) which is an extension of the XNLI dataset (Conneau et al., 2018) with low-resource American languages. We report results on Quechua and Guarani, consisting of 392k instances in the English train set and 2490 and 5010 instances in the dev and test sets, respectively for each target language.\n\nNER\nTraining setup. We use transformer models from the adapter-transformers 2 fork of the HuggingFace transformers library (Wolf et al., 2020) . We use pre-trained language adapters from AdapterHub (Pfeiffer et al., 2020a) for our transfer experiments. XQuAD and NLI fine-tuning experiments were conducted on a single NVIDIA A100 80 GB gpu for 15 epochs and 10 epochs, with learning rate 1e-4 and batch size 16. NER experiments were run for 30 epochs on Nvidia 1080 Ti with 12 GB ram. Further details can be found in Appendix A.\n\nResults\nNER, XQuAD and NLI results are shown in Table 1, Table 2 and Table 3 respectively. All values correspond to F1 scores averaged over 3 different seeds. We use the target language validation set to choose the best hyperparameter values for all experiments. Both MCD and random projections show consistent improvement over the MAD-X baseline numbers. With MCD, we explicitly instruct the model when to project. This removes a hyperparameter from the setup, compared to random projections, while maintaining consistent performance gains over the baseline. To further analyze MCD, we consider the fraction of embeddings being projected onto the target language subspace for NER. Table 4 shows the fraction of embeddings projected during training (averaged across all layers) for each language. For languages dissimilar to en (such as hi and id), it makes sense that the projection fractions are high since the language subspace means are closer to the source language mean (Chang et al., 2022) , compared to languages more similar to en like de and is. Figure 2 shows how projection fractions vary across layers averaged across training epochs. We see high projection rates in early and final layers across languages. This correlates with these layers encoding a lot of English-specific information (Rogers et al., 2020) via training on the task-specific English data, thus triggering projections via MCD often.\n\nRelated Work\nMultilingual language models like mBERT (Devlin, 2018) , XLM-R (Conneau et al., 2020) possess some zero-shot cross-lingual capabilities, even without any explicit finetuning on the languages of interest (Wu and Dredze, 2019; Pires et al., 2019) . Such transfer without any finetuning could lead to degradation in performance across certain language pairs (Hu et al., 2020) . Nevertheless, multilingual models are a good foundation to bootstrap and further develop cross-lingual generalization. While there is a rapidly growing body of work on cross-lingual transfer, very few approaches utilize language-specific subspaces for this purpose. Both Choenni and Shutova (2020) and Chang et al. (2022) construct language-specific subspaces in multilingual models for an exploratory analysis of the model's representations. Yang et al. (2021) use projections on language specific subspaces to remove language specific information from the representations. We note such removal of language bias did not perform well on cross-lingual transfer in our experiments. Parovi\u0107 et al. (2022) train bilingual language adapters using both source and target language text before task adapter training. However, this requires training language adapters using both source and target language unlabelled text, for every language pair, in addition to training task adapters. In contrast, our setup is a simple architectural extension of MAD-X, requiring no additional training once the subspaces are computed for each language. To the best of our knowledge, ours is the first work to exploit language-specific subspaces for cross-lingual transfer.\n\nConclusions\nIn this work, we present a new adapter-based crosslingual transfer technique for an apriori known set of target languages. We construct language subspaces using contextualized representations for source and target languages. Representations during task-specific training are projected onto the target subspace if they exceed a probability threshold or if they are closer to a mean source embedding. Both schemes consistently improve zero-shot transfer for three natural language understanding tasks across many languages.\n", "hypothesis": " If our target languages are known apriori, we explore how zeroshot transfer can be further improved within the adapter framework by utilizing unlabeled text during task-specific finetuning.  We construct language-specific subspaces using standard linear algebra constructs and selectively project source-language representations into the target language subspace during task-specific finetuning using two schemes.", "answer": true}
{"title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization", "content": "\nIntroduction\nDeveloping a persona-consistent dialogue model has been one of the key issues and crucial problems in open-domain dialogue systems (Huang et al., 2020) . Zhang et al. (2018a) define the problem of personalized dialogue generation, which aims to generate personalized responses based on textually described persona profiles. Many efforts have been made on developing dialogue models that generate responses consistent with the provided persona profile (Song et al., 2019 (Song et al., , 2020a,b;,b; Wu et al., 2020a) .\nThe recent development in transformer-based pre-trained models (Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019; Chen, 2020) has led to great successes in dialogue systems (Wolf et al., 2019; Wu et al., 2020b; Ham et al., 2020; Kulh\u00e1nek et al., 2021; Cao et al., 2022; Deng et al., 2022b Deng et al., ,c, 2023)) . Inspired by these successes, previous works incorporate those pre-trained models in persona-based response generation by concatenating the dialogue history and persona as input to generate the response in an auto-regressive manner (Song et al., 2021; Liu et al., 2022) . However, a fine-tuned model can generate a high-quality and persona-consistent response in a certain ordering of personas, while varying this order may lead to a generic and even inconsistent response as illustrated by the example in Figure 1 . We empirically show that the worst ordering of persona can lead to a 29.4% decline in BLEU score compared with the best ordering.\nIdeally, a well-trained dialogue generation model should be able to generate a persona-consistent response regardless of the ordering of personas in the input. We perform experiments and analyses to identify the cause of the ordering sensitivity. We find that the ordering of persona in the input leads to different representations of context and response. We also show that the model can attend to the appropriate persona and generate high-quality responses under some representations but not under others. This leads to instability in response generation.\nMotivated by the above findings, we propose ORder Insensitive Generation (ORIG), which is a simple and effective framework that helps models learn more robust and better representations for different persona orders. More specifically, we formulate ORIG as a constrained optimization problem, which optimizes a persona response generation objective under the constraint: given different orderings of persona, the response representations of the model are the same. Then we optimize it through a stochastic optimization approach.\nExperimental results on the Persona-Chat dataset show that ORIG significantly improves the robustness of pre-trained models (GPT2 (Radford et al., 2019) and BART (Lewis et al., 2020) ) under different orderings of input persona, as well as advances their generation performance.\nIn summary, our contributions are threefold: (1) We identify the order sensitivity problem in persona dialogue generation and conduct an empirical analysis to reveal its underlying reasons. ( 2) We propose a model-agnostic framework, ORIG, that helps different persona dialogue models learn robust representations while achieving better performance. (3) We perform extensive experiments on the Persona-Chat dataset, showing that ORIG outperforms previous models and is more robust and less sensitive to different persona orderings.\n\nRelated Work\nMaintaining a consistent persona is essential for building a human-like dialogue system, where most works regard persona as a set of sentences along with each dialog (Zhang et al., 2018a; Gu et al., 2019; Song et al., 2019; Wu et al., 2021; Cao et al., 2022; Deng et al., 2022a) . Song et al. (2021) disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation while Cao et al. (2022) aims to alleviate the problem of limited data by data manipulation methods. Despite satisfactory performance in previous work, the impacts of different orders of personas are still under-explored, resulting in unstable and inconsistent responses.\nOur work is also related to work on order sensitivity in prompt-based few-shot learning (Zhao et al., 2021; Lu et al., 2022) . Zhao et al. (2021) found that the different order of training examples in the prompt can cause accuracy to vary from near chance to state-of-the-art in the few-shot clas- sification setting. Similarly, order sensitivity for In-context Learning also exists regardless of model size and the prompt format (Lu et al., 2022) . Distinguishing from them, we focus on order sensitivity in the language generation task in finetuning setting, especially the impacts of persona orderings to generate persona-consistent responses.\n\nOrder Sensitivity Problem and Analysis\nIn this section, we first illustrate the seriousness of the order sensitivity problem by showing a huge performance fluctuation in persona dialogue models when fed the same personas in the best and worst orders. Then we analyse why their performance is volatile to different persona orderings.\nTo illustrate the problem, we finetune PLMs on the Persona-Chat by concatenating the persona and dialogue context together to predict the target response, including GPT2 and BART. After the training converges, we test them on two settings: (1) the best case: for each test sample, we feed the models all possible permutations of persona sentences and keep the maximum score for each sample as the final score; (2) the worst-case: perform the same process as (1), but take the minimum score. Table 1 shows the results for two models. Surprisingly, we find the ordering of input persona has a big impact on the models' performance: GPT2's worst case is 29.4% lower than its best case, while BART's is 83.2% lower.\nMoreover, we find that the huge fluctuation in models' performance is closely related to the response representation changes caused by different orderings of input persona sentences. Concretely, we measure the similarity of the responses representation of the same test sample under different input orders of persona. We show their token-level similarity in the sponse should be zero. However, their distances are significantly higher than zero. It reveals that the models behave more likely a left-to-right language model whose representation is prone to the different orderings of the previous input (e.g. persona sentences). That is highly undesirable for a robust personalized dialogue model. Thus, regularization of representation for the response tokens is necessary to help personalized dialogue models capture order-invariant representation.\n\nMethod\nWe introduce the proposed framework, named ORIG: ORder Insensitive Generation (ORIG). As shown in Figure 2 , we transform the persona ordersensitivity problem as a constrained optimization problem that optimises a persona dialogue model under the uncertainty of the input persona order.\n\nProblem Formulation\nGiven the dialogue context C = {u 1 , . . . , u m } and a set of persona descriptions P = {p 1 , . . . , p n }, the goal is to generate a personalized response r.\nFormally, the generation problem can be formulated as the following chain rule:\nP (r|C, P ; \u03b8) = T t=1 P (r t |r 1:t\u22121 , C, P ; \u03b8) (1)\nwhere \u03b8 is the parameters of the dialogue model.\n\nORIG Framework\nAccording to the analysis in Section 3, the observation reveals that varying the order of input personas leads to different representations of the dialogue response, thus resulting in fluctuations in performance.\nTo learn more robust and consistent representations, we propose the ORIG framework that complements the response generation process with a constraint: given the different orderings of a persona, the model's response representations need to be the same.\nThen the order-insensitive personalized dialogue generation problem is modelled as the following constrained optimization problem where P (r|C, P ; \u03b8) are the model's predictions over the dialogue response, D denotes the dialogue corpus, and the function D is KL divergence to measure the difference between two distributions, and the Shuffle operator samples each persona ordering uniformly from the full permutation of P .\n\nOptimization\nAs for optimization, we first apply the Lagrange multipliers strategy to convert the constrained problem into an unconstrained problem L \u03b8 = \u2212 log P (r|C, P ; \u03b8) +\u03b3 \u2022 D[P (r|C, P ; \u03b8), P (r|C, P ; \u03b8)] (6\n)\nwhere \u03b3 is the multiplier corresponding to the equality constraints (3). Then we can update the parameters \u03b8 of dialogue models by stochastic gradient descent.\n\nExperimental Setups\nDatasets We evaluate the models on the Persona-Chat dataset (Zhang et al., 2018a) , where each dialogue session has at least 6 turns of interactions.\nAnd each interaction is conditioned on a persona that is described with 5 profile sentences. proposed ORIG. Our implementation was based on HuggingFace's Transformers library (Wolf et al., 2020) . During training, the learning rate is set as 2 \u00d7 10 \u22125 , and the batch size for GPT2 and BART is set as 64 and 32, respectively. We trained both models for 10 epochs with Adam (Kingma and Ba, 2015) optimizer until they converged. During decoding, We employ a top-p (p=0.9) (Holtzman et al., 2020) plus top-k (k=50) sampling strategy, which is used to avoid sampling from the unreliable tail of the distribution (only consider a subset of vocabulary composed of k words with the highest probability or some most probable words whose sum of probabilities equals p at each decoding step).\nThe random seed for all experiments is set to 42. Evaluation Metrics We perform both automatic and human evaluations. (1) Automatic metrics: We adopt BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) , Entropy (Zhang et al., 2018b) and CIDEr (Vedantam et al., 2015) for lexicalbased measurement. Following previous work, we also adopt the C-score (Madotto et al., 2019) (Fleiss, 1971) .\n\nExperimental Results\nImproves performance in the original test set Table 3 shows different models' performance in the original test set without any modifications (for ORIG, \"Shuffle\" is used during training but is optional during testing. The Table 3 caption signifies the absence of \"Shuffle\" during testing. This is to evaluate if ORIG performs well in the normal setting). From automatic metrics, we can see base models trained with our ORIG framework outperform the baselines. It justifies that our framework can be applied to different models to improve their performance. From human evaluation results, models with ORIG are superior to others on almost all metircs, especially on GPT2. This is consistent with the results of automatic metrics. The average kappa value of the annotation is 0.632, indicating good agreement during human evaluation.\nReduces variance and improves mean and worstcase performance Figure 3 shows that aside from reducing the variance, ORIG also improves mean and worst-case performance (detailed results in Table 4) across two models consistently, especially in GPT2 (the worst case performance is very close to the best case). We reduce the variance on GPT2 and BART by 91.6% and 51.8%, respectively. Meanwhile, we improve worst-case performance by 20.3% and 22.6% on GPT2 and BART respectively. The only drop is the best case. This is because our distance function D is unidirectional, which pulls in the two representations in Equation 3indiscriminately, causing the best case to go down and the worst to go up. We leave more complicated and directional distance constraints for future studies.\n\nConclusion\nWe show that the current practice of applying pretrained models to the personalized dialogue generation task is volatile across different input orders of personas. Through the analysis, we find that the problem arises from the representation changes induced by the input changes. Motivated by these, we propose our ORIG, a model-agnostic framework for finetuning the persona dialogue model such that it obtains a persona order-invariant representation.\nExperiments on two dominant pre-trained dialogue models show that our framework improves performance and reduces order volatility.\n", "hypothesis": "While simple and effective, our analysis shows that this popular practice is seriously affected by Order Sensitivity where different input orders of persona sentences have minimal impact on the quality and consistency of generated response, resulting in minor performance fluctuations (i.e., 5.2% on GPT2 and 11.8% on BART). To further enhance the robustness of dialogue models, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn even more robust representations under different persona orders and improve the consistency of response generation.", "answer": false}
{"title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code", "content": "\nIntroduction\nHuman beings rely heavily on the capacity for causal reasoning (Sloman, 2005; Hagmayer et al., 2007) . People understand the observed facts, predict future events, and speculate about what might have happened if things had been different with the help of their causal reasoning skills. For instance, when we go home and find a mess, we probably want to figure out why it happened. If we determine that a bird flew into the house, we might then consider whether the mess could have been avoided if we had closed the window.\nAlthough large language models (LLMs) demonstrate great language understanding and generation abilities, it is still challenging for them to perform complex causal reasoning such as the example above. Powerful LLMs are able to understand single cause-and-effect relations (Brown et al., 2020;  Figure 1 : Causal relationships between events in two causal reasoning tasks. Wang et al., 2021) , like a man losing his balance causes him to fell. However, when it comes to more complex causal structures involving multiple events and alternative branches (like close the window or not), LLMs perform much inferior to humans (Bhagavatula et al., 2019; Qin et al., 2019) . In this paper, we consider two challenging causal reasoning tasks: abductive reasoning and counterfactual reasoning. Abductive reasoning requires models to generate a plausible reason for the ending while being consistent with the premise. Counterfactual reasoning asks what will occur in the counterfactual branch. Causal relationships between events in these tasks are shown in Figure 1 .\nA potential difficulty for LLMs to learn complex causal structures is that they are rarely expressed explicitly in the text. News articles or narratives may contain multiple events with causal relationships, like an incident and a chain of consequences. However, these events are often written chronologically, and it is hard to extract the causal structure from the text without further annotation. Branches are expressed rarer in text, except for the multi-branching storytelling style (Nisi and Haahr, 2006) .\nOn the other hand, causal relations are exhibited more commonly in code. Conditional statements like if direct the computer to execute certain commands, provided a condition is met. This explicitly demonstrates the causal relationship between the condition block and the execution block. Code can also express branching with elif or switch statements, and the nesting feature enables code to describe more complex structures 1 . This motivates us to utilize code models in natural language causal reasoning. Recently, large language models of code (Code-LLMs) are receiving increasing attention (Chen et al., 2021; Xu et al., 2022) . They exhibit strong code generation performance, and their structural prediction abilities help complete structural natural language tasks like argument graph generation (Madaan et al., 2022) and event argument extraction (Wang et al., 2022b) . Being pre-trained on code with abundant causal expressions, Code-LLMs may also have gained better causal reasoning abilities.\nWe conduct experiments on the unsupervised abductive reasoning and counterfactual reasoning tasks. To generate task outputs, we design code prompts like Figure 2 to clearly represent the causal structures of the tasks. Results show that Code-LLMs with code prompts perform much better than text-only LLMs and previous methods. To better understand why the code prompts are effective, we break down the prompts and analyze the influence of different aspects. We find that Code-LLMs are very sensitive to the programming structure (specifically, the conditional statements), while being robust towards format perturbations and programming language changes.\nOur main contributions are as follows: 1) We design code prompts to tackle causal reasoning tasks, by leveraging conditional statements in code to represent causal structures. 2) We evaluate Code-LLMs with code prompts on the abductive reasoning and counterfactual reasoning tasks, and exhibit that code models with code prompts are better causal reasoners than text models. 3) We break down the code prompt in detail and find that the programming structure is crucial to the performance.\n\nModeling Causal Structure with Code\nWe convert the input of causal reasoning tasks into the form of code prompt for Code-LLMs to understand better. We expect the prompts to meet two requirements: 1) clearly represent the causal relationships between events, and 2) as most Code-LLMs only support generating at the end, the target output should appear at the end of the prompts. The first requirement is addressed with conditional statements. However, for the second, the target prediction is not always the last part of the conditional statements, e.g., in abductive reasoning we want to predict the hypothesis, which is the condition in the if structure. To address this, we uniformly use functions to represent events. As shown in Figure 2 , the causal structure is described in the main function. All the event functions are listed afterwards, leaving the target event function at the last.\nAbductive Reasoning. Abductive reasoning requires models to generate a plausible hypothesis H given the observations: premise P and ending E. The chronological order of these three events is P \u2192 H \u2192 E, and the hypothesis causes the ending to occur.\nIn Figure 2 , we regard the task definition as an instruction and place it as a comment at the beginning of the prompt. The causal structure is represented in the main function like: executing the premise, and if the hypothesis is met, executing the ending 2 . The content of each event is presented as a comment of its function. The hypothesis function is placed at the last, leaving for models to complete. The generation process stops with a line break.\nCounterfactual Reasoning. Counterfactual reasoning aims to rewrite a story under a counterfactual condition. As in Figure 1 , the input consists of four parts: the premise P , the initial context C 1 , the original ending E 1 , and the counterfactual context C 2 . Models are asked to generate the counterfactual ending E 2 that minimally modifies the original ending E 1 and is coherent with the counterfactual context C 2 .\nThe causal relationships are represented with the if-elif structure. The premise P is executed first, and then if the initial context C 1 is met, the original ending E 1 is executed; otherwise, if the counterfac-tual context C 2 is met, the counterfactual ending E 2 will be executed. For ease of exposition, we call the context hypothesis as well, being consistent with the former task. The event contents are also written as comments for event functions. We use # end to mark the finish of the ending. provides a brief introduction of these methods.\nAutomatic Evaluation. We use the following automatic evaluation metrics: BLEU 4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) 1 reports the automatic evaluation results in the zero-shot setting. CODEX significantly outperforms previous methods and DAVINCI on both tasks (with significance level \u03b1 = 0.01), exhibiting strong causal reasoning ability. Although the two DAVINCI models are based on CODEX, their causal reasoning abilities may be weakened during instruction tuning, and this phenomenon is called alignment tax (Ouyang et al., 2022) . DAVINCI 003 underperforms DAVINCI 002 on most metrics, probably because it tends to generate longer and more discursive outputs, which do not comply with the tasks.\nHuman Evaluation. We conduct pairwise comparison between CODEX and DAVINCI 002 on 100 test examples. Annotators are asked to choose the better output given the task requirements. For abductive reasoning, the outputs are rated from three aspects: coherence with the premise, coherence with the ending, and the overall coherence. For counterfactual reasoning, the outputs are rated from coherence with the context and the extent of preserving the original ending. Each example is rated by at least two annotators, and the average interrater reliability is 0.64.\nThe results are shown in Table 2 . CODEX outperforms DAVINCI 002 in all aspects. It better considers the context in generation, and is able to preserve the original content in counterfactual reasoning.\nContributions of the Model and the Prompt. We exchange the prompts of code and text models, to measure the contributions of the model and the prompt. The results are in Table 3 . We find that CODEX performs better with the code prompt, as the code prompt clearly describes the causal relation between events. Code prompts benefit the text model DAVINCI 002 on abductive reasoning, but have negative impacts on counterfactual reasoning. A possible reason is that the causal structure in counterfactual reasoning is more complicated, leading to a more complex code which is harder for text models to understand.\n\nWhat are Crucial in Code Prompts?\nTo paint a better picture of the key points in the code prompts, we intervene on the prompts from four aspects and measure the influences of the interventions. The four aspects we select are information, structure, format, and language. The former two, the prior information provided and the programming structure of functions, are contentrelated; the latter two, the code format and programming languages, are form-related. An ideal model should rely on the content and be insensitive to form perturbations. Information. We study two types of prior information: task instructions and function names. In No Instruction, we remove the task instruction from the prompts. In Function Name Perturbation, we replace original function names with anonymous functionX. For example, we replace premise() and hypothesis() in Figure 2 with functionA() and functionB(), respectively. It eliminates the information in function names and only allows models to learn the event relations from programming structures.\nStructure. The first way to intervene in the programming structure is to convert the conditional structures into sequential structures, referred to as Sequential Structure. The events are executed sequentially, like premise(), hypothesis(), ending() in abductive reasoning. In the second way called Disruption, we randomly disrupt the positions of the functions in the conditional structure.\nFor instance, if hypothesis(): ending() can be disrupted into if ending(): hypothesis().\nWe also apply the function name perturbation in disruption to eliminate the impact of function names.\nFormat. We test three formats besides the original one: Class, Print and Return. The first one converts the original code into a class. We define the programming structure in the __init__ method, and move the event functions into the class. In Print, we represent the content of events as a string and print it in the function body, like def premise(): print(\"The Smiths ...\"). And in Return, the string is the return value of event functions.\nLanguage. We also convert the original Python programs into two other languages, Java and C, to evaluate the influence of programming languages. CODEX is quite robust towards format and language changes. Settings like Class and Java are even better than the original one, revealing that the performance can be further improved with delicate prompt engineering.\n\nConclusion\nWe investigate the causal reasoning ability of Code-LLMs. With code prompts of conditional statements, Code-LLMs achieve great performance in abductive and counterfactual reasoning, outperforming text-only LLMs significantly. Our study on different aspects of code prompts shows that providing a reasonable causal structure in code can help generate plausible outputs, and Code-LLMs are robust towards format perturbations.\n", "hypothesis": " Our experiments show that compared to textonly LLMs, Code-LLMs with code prompts are significantly better in causal reasoning.  We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.", "answer": true}
{"title": "Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data", "content": "\nIntroduction\nCross-lingual Information Retrieval (CLIR) is the task of retrieving relevant documents written in a language different from a query language. The large number of languages and limited amounts of training data pose a serious challenge for training ranking models. Previous work address this issue by using machine translation (MT), effectively casting CLIR into a noisy variant of monolingual retrieval (Li and Cheng, 2018; Shi et al., 2020 Shi et al., , 2021;; Moraes et al., 2021) . MT systems are used to either train ranking models on translated train-ing data (translate train), or by translating queries into the document language at retrieval time (translate test). However, CLIR approaches relying on MT systems are limited by their language coverage. Because training MT models is bounded by the availability of parallel data, it does not scale well to a large number of languages. Furthermore, using MT for IR has been shown to be prone to propagation of unwanted translation artifacts such as topic shifts, repetition, hallucinations and lexical ambiguity (Artetxe et al., 2020; Litschko et al., 2022a; Li et al., 2022) . In this work, we propose a resourcelean MT alternative to bridge the language gap and propose to use artificially code-switched data.\nWe focus on zero-shot cross-encoder (CE) models for reranking (MacAvaney et al., 2020; Jiang et al., 2020) . Our study is motivated by the observation that the performance of CEs diminishes when they are transferred into CLIR and MLIR as opposed to MoIR. We hypothesize that training on queries and documents from the same language leads to monolingual overfitting where the ranker learns features, such as exact keyword matches, which are useful in MoIR but do not transfer well to CLIR and MLIR setups due to the lack of lexical overlap (Litschko et al., 2022b) . In fact, as shown by Roy et al. (2020) on bi-encoders, representations from zero-shot models are weakly aligned between languages, where models prefer non-relevant documents in the same language over relevant documents in a different language. To address this problem, we propose to use code-switching as an inductive bias to regularize monolingual overfitting in CEs.\nGeneration of synthetic code-switched data has served as a way to augment data in cross-lingual setups in a number of NLP tasks (Singh et al., 2019; Einolghozati et al., 2021; Tan and Joty, 2021) . They utilize substitution techniques ranging from simplistic re-writing in the target script (Gautam et al., 2021) , looking up bilingual lexicons (Tan and Joty, 2021) to MT (Tarunesh et al., 2021) . Previous work on improving zero-shot transfer for IR includes weak supervision (Shi et al., 2021) , tuning the pivot language (Turc et al., 2021) , multilingual query expansion (Blloshmi et al., 2021) and crosslingual pre-training (Yang et al., 2020; Yu et al., 2021; Yang et al., 2022; Lee et al., 2023) . To this end, code-switching is complementary to existing approaches. Our work is most similar to Shi et al. (2020) , who use bilingual lexicons for full termby-term translation to improve MoIR. Concurrent to our work, Huang et al. (2023) show that codeswitching improves the retrieval performance on low-resource languages, however, their focus lies on CLIR with English documents. To the best of our knowledge, we are the first to systematically investigate (1) artificial code-switching to train CEs and (2) the interaction between MoIR, CLIR and MLIR.\nOur contributions are as follows: (i) We show that training on artificially code-switched data improves zero-shot cross-lingual and multilingual rankers. (ii) We demonstrate its robustness towards the ratio of code-switched tokens and effectiveness in generalizing to unseen languages. (iii) We release our code and resources. 1\n\nMethodology\nReranking with Cross-Encoders. We follow the standard cross-encoder reranking approach (CE) proposed by Nogueira and Cho (2019) , which formulates relevance prediction as a sequence pair (query-document pair) classification task. CEs are composed of an encoder model and a relevance prediction model. The encoder is a pre-trained language model (Devlin et al., 2019) \nthat transforms the concatenated input [CLS] Q [SEP] D [SEP]\ninto a joint query-document feature representation, from which the classification head predicts relevance. Finally, documents are reranked according to their predicted relevance. We argue that fine-tuning CEs on monolingual data biases the encoder towards encoding features that are only useful when the target setup is MoIR. To mitigate this bias, we propose to perturb the training data with code-switching, as described next.\nArtificial Code-Switching. While previous work has studied code-switching (CS) as a natural phenomenon where speakers borrow words from other 1 https://github.com/MaiNLP/CodeSwitchCLIR languages (e.g. anglicism) (Ganguly et al., 2016; Wang and Komlodi, 2018) , we here refer to codeswitching as a method to artificially modify monolingual training data. In the following we assume availability of English (EN-EN) training data. The goal is to improve the zero-shot transfer of ranking models into cross-lingual language pairs X-Y by training on code-switched data EN X -EN Y instead, which we obtain by exploiting bilingual lexicons similar to Tan and Joty (2021) . We now describe two CS approaches based on lexicons: one derived from word embeddings and one from Wikipedia page titles (cf. Appendix A for examples).\nCode-Switching with Word Embeddings. We rely on bilingual dictionaries D induced from crosslingual word embeddings (Mikolov et al., 2013; Heyman et al., 2017) and compute for each EN term its nearest (cosine) cross-lingual neighbor. In order to generate EN X -EN Y we then use D EN )X and D EN )Y to code-switch query and document terms from EN into the languages X and Y, each with probability p. This approach, dubbed Bilingual CS (BL-CS), allows a ranker to learn interlingual semantics between EN, X and Y. In our second approach, Multilingual CS (ML-CS), we additionally sample for each term a different target language into which it gets translated; we refer to the pool of available languages as seen languages.\nCode-Switching with Wikipedia Titles. Our third approach, Wiki-CS, follows (Lan et al., 2020; Fetahu et al., 2021) and uses bilingual lexicons derived from parallel Wikipedia page titles obtained from inter-language links. We first extract word n-grams from queries and documents with different sliding window of sizes n P t1, 2, 3u. Longer n-gram are favored over shorter ones in order to account for multi-term expressions, which are commonly observed in named entities. In Wiki CS we create a single multilingual dataset where queries and documents from different training instances are code-switched into different languages.\n\nExperimental Setup\nModels and Dictionaries. We follow Bonifacio et al. (2021) and initialize rankers with the multilingual encoder mMiniLM provided by Reimers and Gurevych (2020) . We report hyperparameters in Appendix C. For BL-CS and ML-CS we use multilingual MUSE embeddings 2 to induce bilingual lexicons (Lample et al., 2018) , which have been aligned with initial seed dictionaries of 5k word translation pairs. We set the translation probability p \" 0.5. For Wiki-CS, we use the lexicons provided by the linguatools project. 3\nBaselines. To compare whether training on CS'ed data EN X -EN Y improves the transfer into CLIR setups, we include the zero-shot ranker trained on EN-EN as our main baseline (henceforth, Zero-shot). Our upper-bound reference, dubbed Fine-tuning, refers to ranking models that are directly trained on the target language pair X-Y, i.e. no zero-shot transfer. Following Roy et al. (2020) , we adopt the Translate Test baseline and translate any test data into EN using using our bilingual lexicons induced from word embeddings. On this data we evaluate both the Zero-shot baseline (Zero-shot Translate Test ) and our ML-CS model (ML-CS Translate Test ).\n\nResults and Discussion\nWe observe that code-switching improves crosslingual and multilingual re-ranking, while not impeding monolingual setups, as shown next.\nTransfer into MoIR vs. CLIR. We first quantify the performance drop when transferring models trained on EN-EN to MoIR as opposed to CLIR and MLIR. Comparing Zero-shot results between different settings we find that the average MoIR performance of 25.5 MRR@10 (Table 1 ) is substantially higher than CLIR with 15.7 MRR@10 (Table 2 ) and MLIR with 16.6 MRR@10 (Table 3 ).\nThe transfer performance greatly varies with the language proximity, in CLIR the drop is larger for setups involving typologically distant languages (AR-IT, AR-RU), to a lesser extent the same observation holds for MoIR (AR-AR, RU-RU). This is consistent with previous findings made in other syntactic and semantic NLP tasks (He et al., 2019; Lauscher et al., 2020) . The performance gap to Fine-tuning on translated data is much smaller in MoIR (+4 MRR@10) than in CLIR (+11.1 MRR@10) and MLIR (+8.3 MRR@10). Our aim to is close this gap between zero-shot and full finetuning in a resource-lean way by training on codeswitched queries and documents.\nCode-Switching Results. Training on codeswitched data consistently outperforms zero-shot models in CLIR and MLIR (Table 2 and Table 3 ). In AR-IT and AR-RU we see improvements from 7.7 and 7.1 MRR@10 up to 15.6 and 14.1 MRR@10, rendering our approach particularly effective for distant languages. Encouragingly, Table 1 shows that the differences between both of our CS approaches (BL-CS and ML-CS) versus Zero-shot is not statistically significant, showing that gains can be obtained without impairing MoIR performance. Table 2 shows that specializing one zero-shot model for multiple CLIR language pairs (ML-CS, Wiki-CS) performs almost on par with specializing one model for each language pair (BL-CS).\nThe results of Wiki-CS are slightly worse in MoIR and on par with ML-CS on MLIR and CLIR.\nTranslate Test vs. Code-Switch Train. In MoIR (Table 1 ) both Zero-shot Translate Test and ML-CS Translate Test underperform compared to other approaches. This shows that zero-shot rankers work better on clean monolingual data in the target language than noisy monolingual data in English. In CLIR, where Translate Test bridges the language gap between X and Y, we observe slight improvements of +0.2 and +2.2 MRR@10 (Table 2 ). However, in both MoIR and CLIR Translate Test consistently falls behind code-switching at training time.\nMultilingual Retrieval and Unseen Languages.\nHere we compare how code-switching fares against Zero-shot on languages to which neither model has been exposed to at training time. Table 3 shows the gains remain virtually unchanged when moving from six seen (+4.1 MRR@10 / +3.8 MRR@10) to fourteen languages including eight unseen languages (+3.9 MRR@10 / +4.0 MRR@10). Results in Appendix B confirm that this holds for unseen languages on the query, document and both sides, suggesting that the best pivot language for zeroshot transfer (Turc et al., 2021) may not be monolingual but a code-switched language. On seen languages ML-CS is close to MT (Fine-tuning).\nAblation: Translation Probability. The translation probability p allows us to control the ratio of code-switched tokens to original tokens, with p \" 0.0 we default back to the Zero-shot base- Table 4 : MLIR results on seen languages (MRR@10) broken down into queries that share no common tokens (no overlap), between one and three tokens (some overlap) and more than three tokens (significant overlap) with their relevant documents. Gains of ML-CS are shown in brackets. EN-X has 3,116 queries with no overlap, 3,095 with some overlap and 769 with significant overlap. X-EN has 3,147 queries with no overlap, 2,972 with some overlap and 861 with significant overlap. X-X has 3,671 queries with no overlap, 2,502 with some overlap and 807 with significant overlap.\nline, with p \" 1.0 we attempt to code-switch every token. 6 Figure 1 (top) shows that code-switching a smaller portion of tokens is already beneficial for the zero-shot transfer into CLIR. The gains are robust towards different values for p. The best results are achieved with p \" 0.5 and p \" 0.75 for BL-CS and ML-CS, respectively. Figure 1 (bottom) shows that the absolute differences to Zero-shot are much smaller in MoIR.\nMonolingual Overfitting. Exact matches between query and document keywords is a strong relevance signal in MoIR, but does not transfer well to CLIR and MLIR due to mismatching vocabularies. Training zero-shot rankers on monolingual data biases rankers towards learning features that cannot be exploited at test time. Code-Switching reduces this bias by replacing exact matches with translation pairs, 7 steering model training towards learning interlingual semantics instead. To investigate this, we group queries by their average token overlap with their relevant documents and evaluate each 6 Due to out-of-vocabulary tokens the percentage of translated tokens is slightly lower: 23% for p \" 0.25, 45% for p \" 0.5, 68% for p \" 0.75 and 92% for p \" 1.0. In Wiki CS 90% of queries and documents contain at least one translated n-gram, leading to 20% of translated tokens overall. 7 We analyzed a sample of 1M positive training instances and found a total of 4,409,974 overlapping tokens before and 3,039,750 overlapping tokens after code-switching (ML-CS, p \" 0.5), a reduction rate of ~31%. group separately on MLIR. 8 The results are shown in Table 4 . Unsurprisingly, rankers work best when there is significant overlap between query and document tokens. However, the performance gains resulting from training on code-switched data (ML-CS) are most pronounced for queries with some token overlap (up to +5.4 MRR@10) and no token overlap (up to +6.8 MRR@10). On the other hand, the gains are much lower for queries with more than three overlapping tokens and range from -0.5 to +1.4 MRR@10. This supports our hypothesis that code-switching indeed regularizes monolingual overfitting.\n\nConclusion\nWe propose a simple and effective method to improve zero-shot rankers: training on artificially code-switched data. We empirically test our approach on 36 language pairs, spanning monolingual, cross-lingual, and multilingual setups. Our method outperforms zero-shot models trained only monolingually and provides a resource-lean alternative to MT for CLIR. In MLIR our approach can match MT performance while relying only on bilingual dictionaries. To the best of our knowledge, this work is the first to propose artificial code-switched training data for cross-lingual and multilingual IR.\n", "hypothesis": "We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR).  Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR.  Encouragingly, the gains are especially pronounced for distant languages (up to 2x absolute gain).  We further show that our approach is robust towards the ratio of code-switched tokens and also extends to unseen languages.  Our results demonstrate that training on code-switched data is an expensive and ineffective way of generalizing zero-shot rankers for cross-lingual and multilingual retrieval.", "answer": false}
{"title": "Prefix-Propagation: Parameter-Efficient Tuning for Long Sequences", "content": "\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has changed the landscape of recent natural language processing approaches by enabling the pretraining of state-of-the-art large language models (LLM) (Devlin et al., 2019; He et al., 2020; Brown et al., 2020) . However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning (Li and Liang, 2021; He et al., 2021a; Liu et al., 2022) address these concerns by reducing the number of trainable parameters. Prefix-tuning can tune 0.01% of parameters and still match the performance of regular fine-tuning (updating all model parameters). PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences. As shown in Table 1, prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan (Kiesel et al., 2019) and 20-newsgroups (Lang, 1995) , when used with the popular long-document model Longformer (Beltagy et al., 2020) .\nIn this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer.\nTo further understand prefix propagation, we investigate the reliability of the model's predictions by performing analyses on calibration. Lastly, we conduct study on prefix-based methods in terms of kernel attention to strengthen their theoretical value.\nIn summary, our contributions are as follows:\n... Figure 1 : Illustration of the differences between (a) prefix-propagation (ours) (b) and prefix-tuning (Liu et al., 2022; Li and Liang, 2021) . Blue blocks denote trainable prompts, and \"Transformer Layer\" represents the computation done in a layer of the pre-trained LLM. Note that in prefix-propagation (a), the summation of prefixes continues for layers beyond 3, up to n. This operation is encapsulated by the ellipses. In prefix-tuning (b), prefixes in subsequent layers do not depend on hidden states from past layers (they are simply overwritten).\n.\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\n\nRelated Works\nLong Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens.\nParameter-Efficient Tuning Inspired by the success of manual prompting (Brown et al., 2020), prefix-tuning (Li and Liang, 2021; Liu et al., 2022) prepends trainable \"soft\" prompts to an input sequence. Although further PEFT methods have since been introduced (He et al., 2021a; Hu et al., 2021; Ben Zaken et al., 2022) , we focus on adapting prefix-tuning. We note that our adaptation does not violate orthogonality and thus prefixpropagation can still be compounded with other PEFT methods as proposed in the UnifiedPET framework (He et al., 2021a) , likely yielding similar performance gains. We leave the empirical validation of this hypothesis for future work.\nOut work also adheres to the key motivation of the recent PEFT method, inducer-tuning (Chen et al., 2022) , which is that optimal prefixes should be close to queries within their latent space. We derive queries, keys, and values from the same prefix token, limiting the distance that separates them.\n\nMethodology\nIn this section we introduce prefix-propagation, which, unlike prefix-tuning, propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. Prefix-propagation and its predecessor, prefix-tuning are depicted in Figure 1a embeddings) to the input sequence (blue blocks in top left of Figure 1a ). Then, before every subsequent layer, we sum new trainable matrices onto the first j embeddings corresponding to the prefixes (denoted by the sum operators in Figure 1a ). By propagating instead of overwriting, we halve the number of parameters trained while simultaneously improving performance on long-document tasks.\nWe now formalize prefix-propagation. Multiheaded attention processes query, key, and value matrices derived from a sequence C \u2208 R m\u00d7d with length m and embeddings of size d. Our method modifies traditional attention by concatenating a prefix P \u2208 R j\u00d7d of length j to the sequence:\nH l,i = Attn(D (l) W (l,i) q , D (l) W (l,i) k , D (l) W (l,i) v ) (1) D (l) = cat(P (l) , C) if l = 1 cat(P (l) + C[:j, :], C[j:, :]) if l > 1 where inputs C are projected through pre-trained weight matrices W (l,i) q , W (l,i) k , W (l,i) v\n\u2208 R d\u00d7d h per layer l and head i yielding the output of the attention head, H \u2208 R (j+m)\u00d7d h . The prefixes are concatenated for the first layer (l = 1) and summed to their corresponding hidden states for the remaining layers (l > 1). We do not continually concatenate new prefixes to the sequence to avoid increasing the sequence length after each layer.\nFor both prefix-tuning and prefix-propagation, prefixes (keys and values) are globally attended to by all queries. Unlike prefix-tuning however, our method concatenates additional hidden states before the hidden states C are projected by\nW (i) k and W (i)\nv . By doing so, prefix-propagation modifies query matrices, allowing prefixes to attend to other hidden states globally, thereby increasing representation capability. This approach is somewhat analogous to the external global tokens inserted in the BigBird-ETC model (Zaheer et al., 2020) . By attending to other tokens, the prefixes can act as special storage tokens, which is particularly useful in the restricted regime of long-document modelling where relatively few tokens have global context. Conversely, prefix-tuning only concatenates trained key and value matrices, P k , P v \u2208 R j\u00d7d h , statically to the sequence:\nH l,i = Attn(CW (l,i) q , cat(P (l,i) k , CW (l,i) k ), cat(P (l,i) v , CW (l,i) v ))\n(2)\nSince our method has a single prefix matrix, P instead of separate P k and P v matrices, we reduce the number of trained parameters by 50%.\n\nCalibration\nWe further study the proposed prefix-propagation method to understand the reliability of model's predictions through calibration. Well-calibrated models output confidence scores that closely match the models' accuracy. Either over-confident or underconfident models are undesirable. Calibration has widely been overlooked in PEFT methods. To quantify calibration in our work, we use expected calibration error (ECE), which bins predictions based on model confidence and compares them to accuracy (Pakdaman Naeini et al., 2015; Guo et al., 2017) .\n\nKernel Decomposition\nTraditional attention is analogous to applying a kernel smoother over inputs (Tsai et al., 2019) .\nMotivated by this insight, we reformulate prefixpropagation as a sum of kernelized attention modules. Separating the modules introduces flexibility in two ways: (1) Their individual kernel forms can be mixed and matched and (2) A hyperparameter scale factor \u03b1 can be applied to the prefix component to increase or decrease its weighting. Equation 3 defines kernel decomposition for prefixpropagation 2 :\nH = Kern(cat(P, C)W q , CW k , CW v ) + (\u03b1)Kern(cat(P, C)W q , P W k , P W v ) (3)\nwhere Kern refers to kernel attention as formulated in (Tsai et al., 2019) . The first term results from attending to the original sequence, C, and the second comes from attending to the prefixes, P . We provide the derivation of Equation 3 and the full definition of kernel attention in Appendix A.\nOur main motivation for presenting prefix decomposition is to establish foundational knowledge and guide future research. Ergo, we restrict experiments in this initial presentation to using just the default exponential kernel (Appendix A).\n\nExperiments and Results\nDatasets We evaluate our approach on three longdocument classification tasks: ArXiv (He et al., 2019) , an 11-class classification task composed of academic research papers, the 20-newsgroups (Lang, 1995) classification task consisting of mailing lists that fall into one of 20 classes, and the Hyperpartisan dataset, a binary classification task for extremist news classification (Kiesel et al., 2019) . We also run experiments on WikiHop (Welbl et al., 2018) , a long-document reading comprehension task requiring multi-step reasoning.\nDue to compute limitations inherent to working with long documents, with the exception of Hyperpartisan, we only report a single run for each task. This mimics the original Longformer reporting scheme (Beltagy et al., 2020) . For Hyperpartisan, the smallest of the datasets, we report mean metrics averaged over five seeds.\nBaselines As a baseline, we fine-tune Longformer-base (approx.\n149M parameters) as closely as possible to Beltagy et al. (2020) . For PEFT, we evaluate prefix-tuning on Longformer-base and RoBERTa-base (approx. 125M parameters) (Liu et al., 2019) . 2 We omit layer, l and head, i for brevity.\n\nMethod\nArXiv HY. NG. More details on dataset sizes, pre-processing, and hyperparameters are in Appendix B.\n\nResults and Discussion\nAcross all tasks, our results in Table 2 verify that prefix-tuning is inferior to fine-tuning long sequences. Conversely, prefix-propagation consistently outperforms prefix-tuning and is comparable to fine-tuning on most tasks. Prefix propagation also performs competitively on Hyperpartisan, a relatively small dataset with only 625 samples. This is in contrast to prefix-tuning, which is known to underperform in low-data settings (Gu et al., 2022) . Because we ran multiple seeds on Hyperpartisan, we also found that prefix-propagation's better performance relative to prefix-tuning is statistically significant (p < 0.05, using a single-tailed t-test). We do not have multiple samples to run these tests for larger datasets, but we emphasize that Hyperpartisan likely has the most variance and yet it is still statistically significant. We suspect that prefixpropagation's performance exceeds prefix-tuning because propagated prefixes can transmit global context across multiple layers, possibly modelling more expressive abstractions.\nWe note one exception where prefix-based methods still leave room for improvement: multiplechoice question answering on WikiHop. We hypothesize that prefix methods have insufficient capacity to properly model complex long-document multi-step question answering.\nWe also observe that prefix-based methods, and especially prefix-propagation, achieve better calibration than fine-tuning, as shown in Table 3 . Unlike prefix-tuning however, prefix-propagation effectively balances calibration with accuracy metrics. The calibration of fine-tuning deteriorates as training progresses (Figure 4 \n\nMicro F1\nFigure 2 : Violin plot of Micro F1 Score for five different seeds on the Hyperpartisan task. White dots, gray boxes, and gray lines are the medians, interquartile ranges, and ranges respectively. Width of the five violin shapes show the probability densities for the corresponding F1score. All methods tune Longformer-base except \"R Prefix\", which is prefix-tuning on RoBERTa-base.\nforgetting (Jagielski et al., 2022) .\nAs an initial test for our ongoing prefixpropagation kernel study, we show results on Hyperpartisan in Figure 2 . The kernelized version of prefix-propagation achieves the best single-run performance, but has higher variance than fine-tuning and prefix-propagation which necessitates further research.\n\nConclusion\nOur research focuses on parameter efficient tuning for long documents tasks. We introduce prefix-propagation, which consistently improves performance over prefix-turning on long document datasets, while using 50% fewer parameters. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated. We lastly explicate prefix-propagation from a kernel perspective, uncovering insights for future PEFT research.\n", "hypothesis": " Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences.  We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using \u223c50% fewer parameters.  To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention.", "answer": true}
{"title": "Discourse-Level Representations can Improve Prediction of Degree of Anxiety", "content": "\nIntroduction\nAnxiety disorders are one of the most prevalent mental health conditions, affecting an estimated 284 million people worldwide (Roth, 2018) and with an estimated financial burden of $46.6 billion annually in the U.S. alone (DeVane et al., 2005) . This puts the impact of anxiety on par with depression (Guntuku et al., 2017; Mahdy et al., 2020) , yet much less work in the NLP community has focused on detecting anxiety disorders as has been done for depressive disorders.\nOne of the key characteristics of anxiety disorders is cognitive distortion (Muran and Motta, 1993; Maric et al., 2011) , or an illogical reasoning in dealing with life events (Kaplan et al., 2017) . The primary window into such distortions is language, including one's own explanatory style -the way they reason about the occurrence of events (Peterson, 1991) .\nExplanatory style may not be well represented by single words or words in context (i.e., lexicallevel features). For example, consider the catastrophizing statement (i.e., worrying that a bad event will lead to an extreme outcome) \"I'm sick. Now I'm going to miss my classes and fail them all.\" (Hazlett-Stevens and Craske, 2003) . To see that \"fail them all\" is catastrophizing the event \"I'm sick\" requires understanding that the latter is a causal explanation for the expected falling behind. This is discourse-level information -semantics at the level of complete clausal statements or relating statements to each other (discourse relations) (Pitler et al., 2008) .\nHere, we propose a language-based assessment of anxiety utilizing both lexical-level and discourselevel representations. We first compare models that leverage discourse-level representations alone. We then propose a dual lexical-and discourse-level (lexico-discourse) approach and evaluate whether the combination of both types of representations leads to improved performance. Finally, we explore specific types of discourse relations that are thought to be associated with cognitive distortions, and look at their association with anxiety in order to illuminate what our lexico-discourse approach can pick up on at the discourse semantics level.\nOur contributions include: (1) proposal of a novel user-level language assessment model that integrates both discourse-level and lexical-level representations; (2) empirical exploration of different discourse and lexical-level contextual embeddings and their value towards predicting the degree of anxiety as continuous values; (3) examination of the association between a person's anxiety and their discourse relation usage, finding that causal explanations are the most insightful for prediction; and (4) finding that to the best of our knowledge, this is the first model of anxiety from language specifically fit against a screening survey (rather than users self-declaring having experienced anxiety symptoms, or annotators perceiving the presence of the condition).\n\nRelated Work\nAnxiety is characterized by disruptive feelings of uncertainty, dread, and fearfulness, and is generally defined as anticipation of future threats (Cohen et al., 2016) . Researchers have recently been turning to social media language as a potential alternative source for mental health assessment, investigating, e.g., depression (Schwartz et al., 2014; Bathina et al., 2021; Kelley and Gillan, 2022) , PTSD (Coppersmith et al., 2014; Benton et al., 2017b; Son et al., 2021) , and suicide risk (Coppersmith et al., 2016; Mohammadi et al., 2019; Matero et al., 2019) . Such an approach was also utilized in analyzing anxiety (Shen and Rudzicz, 2017; Tyshchenko, 2018; Guntuku et al., 2019; Budiyanto et al., 2019; Owen et al., 2020; Saifullah et al., 2021) . Work towards this goal include Shen and Rudzicz (2017) who attempted to classify Reddit posts into binary levels of anxiety by lexical features and Guntuku et al. ( 2019) who explored Ngram associations with anxiety in Twitter users. Few have attempted to capture discourse-level information in such systems.\nWhile some have focused on cognitive distortions in patient-therapist interactions (Simms et al., 2017; Burger et al., 2021; Shreevastava and Foltz, 2021) , none have attempted to combine discourselevel information with more standard lexical-level embeddings in studying ecological (i.e., everyday, happening in the course of life) online language patterns. For mental health tasks, state-of-the-art systems have primarily relied on contextual word-level information from transformers like BERT (Devlin et al., 2019 ) and RoBERTa (Liu et al., 2019 ) (Mohammadi et al., 2019; Matero et al., 2019) . Furthermore, Ganesan et al. (2021) improved mental health task performance by reducing the dimensions of contextual embeddings to approximately 1 12 of the original. Here, we seek to establish the role of the contextual embeddings as well as propose and evaluate a model that integrates discourselevel modeling with contextual embeddings, motivated by the ability of discourse relations to capture cognitive distortions.\n\nMethod\nDiscourse-Level Embeddings. We consider a variety of discourse-level embeddings, ranging from those capturing phrases or sentences to one capturing relations between clauses. Sentence-BERT (Reimers and Gurevych, 2019 ) is a variant of BERT that captures a whole sentence by optimizing for semantic similarity using siamese and triplet networks. Phrase-BERT (Wang et al., 2021) attempts to capture shorter phrasal semantics using contrastive learning with machine-generated paraphrases and mined phrases. Finally, DiscRE (Son et al., 2022) captures representations of the relationship between discourse units (i.e., clauses rooted with a main verb) using a weakly supervised, multitask approach over bidirectional sequence models.\nLexical Embeddings. Amongst potential options for state-of-the-art auto-encoder language models, we consider BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) . Such selection is supported by empirical evidence; these two models have previously been found to result in top performance in related mental health assessment tasks (Matero et al., 2019; Ganesan et al., 2021) . Beyond the fact that these models have lead to state-of-theart performance in language understanding tasks, they are also known to capture some discourse information (Kishimoto et al., 2020; Liu et al., 2021) . Thus, they form a very high benchmark to try to out-predict with discourse-level embeddings.\nOverall Model. The architecture of our prediction models is laid out in Figure 1 . Each model consists of a discourse submodel and lexical submodel, and the two following equations demonstrate the aggregation of representations in each submodel. d, m, u each denotes discourse unit, message, and user.\nThe discourse submodel takes discourse units parsed from a message 1 to derive discourse-level embeddings, denoted as e d u (Eq. 1), which are aggregated into message-level and then into a userlevel embedding, e u (Eq. 2):\nEQUATION\nEQUATION\nThe lexical submodel takes the embeddings derived from the word-based transformer models as message-level representations and aggregates them to user-level. Compose is the embeddings aggregation function at each step, which can be mean, min, or max. Here we follow the practice from 2021) and use the mean. 2 Finally, the concatenation of the representations acts as input to our feed-forward network (FFN) that predicts the degree of anxiety. 3 Theoretically Relevant Discourse Dimensions. Previous work has suggested open vocabulary (latent) embeddings of discourse relations (i.e., Dis-cRE, Sentence-BERT) are more powerful than explicitly defined relations (Son et al., 2022) , thus we utilize models that score specific type of relations (e.g., causal explanation) as a means to explain what the embeddings and models are able to capture. We evaluate four discourse relations relevant to anxiety. Causal explanations are a statement of why an event happened. Using the model of Son et al. (2018) with F1 of approximately .87 over social media, we computed the percentage of the messages written by a user that contain causal explanation. Counterfactuals imagine what could have happened as an alternative to actual events. Using the model of Son et al. (2017) , we calcu-late the proportion of the messages from each user that communicates counterfactual thoughts. Finally, dissonance refers to situations in which one's stated behavior or belief contradicts a prior belief; consonance is its opposite concept. We use the RoBERTa-based topic-independent classifier that evaluates whether a pair of messages composes dissonance (Varadarajan et al., 2022 (Varadarajan et al., , 2023)) . Instead of assessing all pairs, we take two temporally adjacent messages (maximum distance of 2) to reduce computation time.\n\nDataset\nOur primary dataset comprises 12,489 Facebook users who took a personality questionnaire, including assessment of anxiety, and consented to share their status updates for academic research (Stillwell and Kosinski, 2012) . The anxiety assessment consists of the anxiety facet of the neuroticism factor (Johnson, 2014) , which has shown to correlate with other measures of anxiety such as GAD-7 (Mili\u0107 et al., 2019) and STAI (Teachman, 2006) as well as have high convergence with anxiety disorders themselves (Rector et al., 2012) . Each user was asked the following five questions: Get stressed out easily, Am not easily bothered by things (inverse coded), Am relaxed most of the time (inverse coded), Fear for the worst, Worry about things. on 1-5 Likert scales (\"Very inaccurate.\" to \"Very accurate.\"). The responses to these questions are averaged together to form a continuous variable which determines the degree of anxiety.\nSecondary Evaluation Data. We also include an evaluation using another smaller dataset that was collected by the authors. It was collected from consenting participants and asked the same facet of anxiety questions. In this case, only the past 2 years of Facebook posts were used to build representations of each user to be used for prediction. This dataset is used only for evaluation, where training occurs over the previously described large Facebook set.\n\nResults and Discussion\nWe evaluate our models by disattenuated Pearson correlation coefficient r dis (Spearman, 1987; Lynn et al., 2018) between the model predictions and anxiety scores derived from the survey as our main metric, but include mean squared error as well.\nTable 1 displays the performances of the models trained solely on discourse-level representations as well as a sentiment lexicon baseline model (Mohammad and Turney, 2013) . Models utilizing Phrase-BERT or Sentence-BERT yielded decent results, while the DiscRE-based is by itself somewhat less informative. Table 2 compares BERT and RoBERTa using the embeddings from the second-to-last hidden layer (L23) and the top-4 hidden layers (L21-24). We choose the RoBERTa L23 embeddings to represent the performances of the contextual embeddings in the following experiments.\nWhile Phrase-BERT performs well in isolation, Table 3 suggests utility did not increase when used alongside RoBERTa. Alternatively, the model that employed RoBERTa, Sentence-BERT, and Dis-cRE representations achieves the best performance among all. This implies the two discourse-level embeddings have non-overlapping utility that contextual embeddings lack.\nIn Table 4 , we verified the performance of our models on the alternate, held-out Facebook dataset as described in Section 4. Our central finding, that utilizing discourse-level semantics improves performance, is replicated in this entirely new dataset with the model having RoBERTa L23 with Sentence-BERT and DiscRE having significantly lower error. The improvement is similar to the first dataset showing the generalization of our approach.\nExplaining Discourse Improvement. We shine light on what the model is able to capture in terms of discourse-level information by finding whether theoretically-related dimensions of cognitive distortions are associated with the models'. \nhigh and low each indicates the group of users with predicted degree of anxiety higher or lower than median, and \u03b6 is the \"z-score\" (mean-centered, standardized) of the proportions per user.\nWe see that all discourse dimensions were related to the score, but causal explanations, often related to overgeneralization, had the highest difference (e.g., \"You know life is going to be permanently complicated when your in-laws start turning their backs on you like a domino effect.\"). This suggests that the causal explanation discourse relation may account for unique information to improve the overall results.\n\nPotential for Use in Practical Applications.\nOther than use in medical settings, secondary use cases of our models include assessments from public entities such as public health officials, schools, and human resource department of companies to quantify levels of expressed anxiety.\n\nConclusion\nAnxiety is one of the most prevalent mental health disorders, and the ability to more accurately assess it in a way that can capture cognitive distortions (i.e., via discourse-level features) could lead to improved diagnostics and treatment of the condition. We analyzed the effects of using both discourseand lexical-level information within a single model for the assessment of degree of anxiety from Facebook status updates. We found benefit from the discourse-level information beyond lexical-level contextual embeddings (i.e., transformer language models) that have been found to produce state-ofthe-art results for other mental health assessment tasks, motivating the idea that anxiety-based models can benefit from capturing not only contextual lexical information but also higher-level semantics at the level of thought patterns. Lastly, we examined the effect of theoretically relevant discourse relations in assessing anxiety, discovering that causal explanation is the most informative.\n", "hypothesis": "Here, we investigate the development of a modern linguistic assessment for degree of anxiety, specifically evaluating the utility of discourselevel information in addition to lexical-level large language model embeddings. We find that a combined lexico-discourse model outperforms models based solely on state-of-theart contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing similar predictive power as lexical-level representations.", "answer": false}
{"title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models", "content": "\nIntroduction\nLarge pre-trained language models, such as BERT (Devlin et al., 2018) , RoBERTa (Liu et al., 2019) and Electra (Clark et al., 2020) have achieved significant success on several different NLP tasks (Ding et al., 2019; Wang et al., 2018a) with finetuning. However, these models usually contain millions and billions of parameters, preventing their execution on resource-restricted devices. To deploy these models, Knowledge distillation (KD) is an effective compression technique to derive a smaller student model from a larger teacher model by transferring the knowledge embedded in the teacher's network. Previous KD methods typically store knowledge in the student's parameters and train the student by minimizing divergence between the student's and teacher's output prediction and intermediate activation distributions (Park et al., 2019; Zhang et al., 2018) . However, the student's parametric memory is often limited and cannot be quickly expanded or revised. Moreover, after training, the teacher model's soft labels and activations, which contain essential task-specific knowledge, are not utilized by the student at inference time.\nTo address the issues mentioned above, we propose the Retrieval-augmented Knowledge Distillation (ReAugKD) framework. ReAugKD introduces a non-parametric external memory in addition to the implicit parametric memory of the model and uses kNN retrieval to retrieve from this memory. The key intuition of ReAugKD is to enhance the effective capacity of the student by using an external memory derived from relevant task-specific knowledge of the teacher. While this external memory could include any task-specific knowledge, in this work, it is composed of the soft labels and embeddings generated by the teacher model.\nOur framework consists of an inference phase and a training phase. In the inference phase, we aggregate the soft labels of those teacher embeddings in our memory that are most similar to the student embedding. We demonstrate the efficacy of our framework by achieving state-of-the-art results on the GLUE benchmark (Wang et al., 2018a) with less than 3% latency overhead over the baseline without retrieval augmentation. ReAugKD also comprises a training phase, where we train the student to retrieve from the external memory effectively. We train with a novel relational KD loss that minimizes the divergence between teacher-teacher and teacher-student embedding distributions. We not only observe that training with this loss is necessary to align the student and teacher embedding spaces for retrieval but also that this loss improves student generalization even in the absence of retrieval augmentation. This suggests that incorporating the ability to retrieve information can significantly enhance generalization during the process of knowledge distillation. In summary, our contributions include \u2022 We propose ReAugKD, a novel framework for knowledge distillation that introduces a nonparametric memory to increase the effective student size. We show that retrieving from a memory composed of training set teacher predictions at inference time can significantly improve generalization on the GLUE tasks.\n\u2022 To effectively retrieve from the non-parametric memory, we introduce a novel loss function that transfers the relational knowledge between teacherteacher embedding and teacher-student embedding distribution. This loss function improves student generalization even in the absence of retrieval augmentation at inference time.\n\u2022 We study the accuracy and latency cost with the number of neighbors (k) retrieved in ReAugKD. ReAugKD with approximate kNN introduces a small overhead of <3% latency increase.\n\nRelated Work\nKnowledge distillation KD can be broadly classified into task-specific KD, where the student model will be used for the same task as the teacher model (Mirzadeh et al., 2020; Jin et al., 2019; Zhang et al., 2018; Sun et al., 2019) and task-agnostic KD where the student may be used for a different task, after finetuning on the new task (Jiao et al., 2019; Sun et al., 2020; Sanh et al., 2019; Wang et al., 2020; Zhang et al., 2018; Xu et al., 2019) . In this work, we show that ReAugKD can be applied to enhance task-specific distillation as well as when finetuning task-agnostic distilled models. Closest to our work is RKD (Park et al., 2019) that introduces a loss to transfer relational knowledge between teacherteacher embedding and student-student embedding distributions. Our work differs in that we transfer relational knowledge between teacher-teacher embedding and teacher-student embedding distribution to enhance the student model's ability to retrieve from the external memory. MetaDistil (Zhou et al., 2022 ) is a strong task-specific distillation baseline that employs meta-learning to better transfer knowledge to the student. Unlike MetaDistill, we show that ReAugKD can significantly improve the student model's generalization without retraining the whole teacher with meta-learning. Retrieval-augmented language models There has been growing interest in retrieval-augmented methods for Knowledge-Intensive generative NLP Tasks, such as text generation and question answering (Weston et al., 2018; Lewis et al., 2020; Guu et al., 2020; Lin et al., 2022) , where querying training examples during inference significantly improves likelihood. Closest to our work is BERT-kNN (Kassner and Sch\u00fctze, 2020) which combines BERT with a kNN search over a large datastore of an embedded text collection, to improve clozestyle QA. In our work, we apply retrieval augmentation to enhance the capacity of student models during KD, and show improvement even on nonknowledge intensive tasks like GLUE.\n\nTraining Phase\nOur framework consists of two main phases, the training phase and the inference phase. The training phase has two steps. In the first step, we prepare the teacher model for KD by adding a linear projection head L on the top of the teacher model encoder that has been finetuned for a specific downstream task. The input dimension of this projection head is the embedding dimension of the teacher. The output dimension is the embedding dimension of the student. We then freeze the other parameters of the teacher model and finetune the parameters in L with supervised contrastive loss (Khosla et al., 2020) . This step a) reduces the dimension of the teacher's embeddings, to the student model dimension for retrieval, and b) uses supervised contrastive loss to derive a kNN classifier for BERT that is robust to natural corruptions, and hyperparameter settings (Li et al., 2021) . Fine-tuning L also greatly reduces the computational cost compared to retraining the whole teacher model (Zhou et al., 2022) .\nIn the second step, we perform KD by generating the teacher embeddings with L and teacher soft labels using the original teacher's classifier head for a batch of data. Then, we use the loss function we proposed in Section 3 to train our student model.\n\nLoss function\nWe present some mathematical notations to introduce our loss function. Given a batch of data {d i }, i = 1, 2, \u2022 \u2022 \u2022 , N , where N is the batch size, we denote the embedding generated by the teacher's projection head as z i and the soft labels generated by the teacher's classifier as \u0233i . Similarly, we adopt x i , y i to denote the student's embeddings and predictions. Then we construct a probability distribution q i,j over each teacher's embeddings z j to capture the similarity with respect to an anchor point z i ,\nEQUATION\nwhere the \u03c4 stands for temperature. Note that N j=1 q i,j = 1. q i,j reflects the cosine distance relational knowledge among different embeddings generated by the teacher model in the batch. If z j is closer to z i , cosine distance, q i,j will be larger. Similarly, given a student's embedding x i as an anchor point, we formulate another probability distribution qi,j over each teacher's embeddings z j of the data in the batch.\nEQUATION\nThe qi,j reflects the cosine distance relationship between different embeddings generated by the teacher model and the student's embedding. Our loss function aims to minimize the divergence of these two distributions qi,j and q i,j since the teacher model is a strong kNN classifier after finetuning with supervised contrastive loss function in the first step of our training. In the ideal case, given a student's embedding x i , the student retriever should retrieve the same set of embeddings as the corresponding teacher's embedding z i . We adopt KL divergence to measure that divergence. In addition, we adopt the commonly-used cross-entropy loss to calculate the divergence between the student's prediction y i and the teacher's prediction \u0233i .\nOur loss function can be formulated as\nEQUATION\n)\nwhere CE is the cross entropy loss and KL is KLdivergence. \u03b1 is the hyperparameter controlling the trade-off between the two losses.\n\nInference Phase\nAfter training, we construct a knowledge base (KB) comprising of projected teacher embeddings and predictions. Given new data d i at inference time, we obtain (x i , y i ) using the student model. and use the HNSW algorithm (Malkov and Yashunin, 2018) to derive the K nearest teacher's embeddings and their corresponding soft labels {(z k , \u0233k )} i=1,2,\u2022\u2022\u2022 ,K from the KB. Then we compute the weighted average of these soft labels Avg({\u0233}) i based on qi,k\nAvg({y}) i = K k=1 qi,k K k=1 qi,k \u0233k\nWe derive a new prediction \u0233\u2032 i for d i with Avg({\u0233}) i .\n\u0233\u2032 i = \u03b2 \u0233i + (1 \u2212 \u03b2)Avg({\u0233}) i ,\n\u03b2 is the hyperparameter controlling the trade-off between the two predictions.\n\nExperimental Results\nWe apply our method to distill BERT-Base (Devlin et al., 2018) into a 6-layer BERT with a hidden size of 768. We evaluate our proposed approach, ReAugKD, on the GLUE benchmark (Wang et al., 2018a) . These datasets can be broadly divided into three families of problems: single-set tasks that include linguistic acceptability (CoLA) and sentiment analysis (SST-2), similarity, and paraphrasing tasks (MRPC and QQP); inference tasks that include Natural Language Inference (MNLI and RTE); and Question Answering (QNLI). We compare our method with vanilla KD (Hinton et al., 2015) , TAKD (Mirzadeh et al., 2020) , RCO (Jin et al., 2019) , RKD (Park et al., 2019) , DML (Zhang et al., 2018) , PKD (Sun et al., 2019) ProKT (Shi et al., 2020) , SFTN (Park et al., 2021) and MetaDistil (Zhou et al., 2022) . Following similar setting as MetaDistill, we perform a grid search over the sets of the weight of KD loss from {0.9, 0.99}, the predictions weight \u03b2 from {0, 0.1, ... 1} and the top-k from 1 to 20. We set the student learning rate to 2e-5 and the batch size to 64.\nExperimental Results on GLUE We report the experimental results on the development set of the six GLUE tasks in Table 1 . Notably, our method achieves start-of-the-art results on five out of the six datasets with an average improvement of 0.34% over the previous best KD method MetaDistil (Zhou et al., 2022) . Although MetaDistil achieves slightly better performance on the MRPC dataset, our method has the advantage of not needing to conduct meta-learning on the whole large teacher model, which significantly increases extra training cost in terms of time and memory (Zhou et al., 2022) . In addition, we also observe a performance gain of 0.37% with the retrieval component of ReAugKD as compared to ReAugKD without retrieval which verifies the benefit of retrieval augmentation in our approach. Even without the retrieval process, the student model trained by our \n\nConclusion\nIn this paper, we present ReAugKD, a knowledge distillation framework with a retrieval mechanism that shows state-of-the-art performance on the GLUE benchmark. In the future, we plan to expand the knowledge base with more information from the teacher and extend it to additional tasks.\nLimitations Our method relies on having access to teacher embeddings and prediction which may not always be possible in a black-box distillation setting. Retrieval augmentation also requires maintaining a knowledge base that is memory intensive. The cost of the retrieval process is dependent on the size of the training corpus, which can be a limitation when dealing with very large training datasets. Conducting dataset distillation (Wang et al., 2018b) on the training corpus to further reduce memory cost and retrieval time is an important future step for our framework. Previous results have demonstrated the effectiveness of our method for task-specific distillation. Our method can further improve the finetuned performance of task-agnostic distilled models. We adopt RoBERTa-large as the teacher model and the MiniLMv2 as the student model to verify the effectiveness of our method. Our method can achieve around 2% improvement in performance.\n", "hypothesis": " We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for taskspecific knowledge distillation on the GLUE benchmark (Wang et al., 2018a) ..", "answer": true}
{"title": "Unsupervised Subtitle Segmentation with Masked Language Models", "content": "\nIntroduction\nSubtitling is one of the principal means of providing accessible audiovisual content. With the ever increasing production of audiovisual content in multiple domains and languages, in the current digital era, subtitle provision can benefit from automation support, via Automatic Speech Recognition and/or Machine Translation (Volk et al., 2010; Aliprandi et al., 2014; Etchegoyhen et al., 2014; Tardel, 2020; Bojar et al., 2021) .\nSubtitles are subject to specific constraints in order to achieve adequate readability, including layout, on-screen duration and text editing. Among these constraints, segmentation addresses the maximum number of characters per line, the number of lines per subtitle, and breaks at natural linguistic frontiers. Segmentation has been shown to be an important readability factor (Perego et al., 2010; Rajendran et al., 2013) , with improperly segmented subtitles resulting in increased cognitive effort and reading times for users. Thus, automated subtitling systems need to generate properly segmented subtitles to achieve readability. * These authors contributed equally to this work.\nA typical baseline for subtitle segmentation, still used in some production systems, is simple character counting, whereby line breaks are inserted before reaching the maximum allowed number of characters per line. Although simple and fast, this approach does not address the need for linguistically correct segments and, therefore, falls short in terms of readability. Several approaches have been proposed to improve segmentation by automated means. \u00c1lvarez et al. (2014) proposed a machine learning method where subtitle breaks are predicted by Support Vector Machine and Linear Regression models trained on professionally-created subtitles. A similar method based on Conditional Random Fields was then shown to improve over these results (Alvarez et al., 2017) . Approaches that directly generate subtitle breaks within Neural Machine Translation have also been proposed in recent years (Matusov et al., 2019; Karakanta et al., 2020a) . Recently, Papi et al. (2022) developed a multilingual segmenter which generates both text and breaks and may be trained on textual input only, or on joint text and audio data.\nAlthough quality subtitle segmentation may be achieved with the aforementioned approaches, they require supervised training on segmented subtitle corpora. At present, the largest subtitle corpus is Open Subtitles (Lison et al., 2018) , which mainly covers entertainment material, contains subtitles mostly created by non-professionals or automatically translated, and does not include line breaks. The MuST-Cinema corpus (Karakanta et al., 2020b) , on the other hand, is a multilingual speech translation corpus that includes subtitles breaks, but is only available for 8 languages at the moment. Considering the vast amount of languages and domains in audiovisual content, the lack of segmented training data hinders the development of robust automated subtitling systems.\nIn this work, we describe a novel unsupervised method based on pretrained masked language mod-els (MLM), where line and subtitle breaks are inserted according to the likelihood of a segment acting as an isolated unit, as approximated by the probability of a punctuation mark occurring at a given segmentation point. In our experiments, this novel approach obtained competitive results on most metrics, while also fully preserving the original text and complying with length constraints. Our system may thus be used as a simple yet efficient subtitle segmenter with any pretrained masked language model, for any language covered by the model.\n\nApproach\nOur approach is based on the standard view that the more appropriate subtitle segments are those that may function as isolated grammatical chunks. We further hypothesise that a relevant approximation for the identification of this type of unit is the likelihood of a punctuation mark being inserted at the end of a candidate segment, as punctuation may mark the closure of a syntactic unit and is often associated with discursive pauses. To test this hypothesis, we compute the likelihood of punctuation marks at different segmentation points, as predicted by a pretrained MLM, and select the insertion point with the highest likelihood. 1 The segmentation candidates are determined under a sliding-window approach over the entire input text. We first generate the list of all pairs <\u03b1, \u03b2> over the unprocessed portion of the text, where \u03b1 is a segmentation candidate of length under a specified limit K, corresponding to the maximum number of characters per line, and \u03b2 is the remaining portion of the text to be segmented.\nWe then score all segmentation candidates \u03b1 with one of the LM scoring variants described below. A segmentation marker, either end-of-line (<eol>), or end-of-block indicating the end of a subtitle (<eob>), is then appended to the best scoring candidate, and \u03b2 becomes the input text to be segmented in a recursive iteration of the process.\nSince our method does not rely on any additional information, such as an audio source, to determine the segmentation type, an <eob> tag is inserted every even segment or when \u03b2 is empty; otherwise, an <eol> tag is inserted. We thus generate subtitles with a maximum of two lines, following a standard recommendation in subtitling. We also define a minimal number of characters (min) in \u03b1 for the segmentation process to apply, and do not segment lines that are under the specified character limit.\nWe evaluated three approaches to compute segmentation scores over each candidate pair <\u03b1, \u03b2>:\n\u2022 Substitution: The last token of \u03b1 is masked and the score is the highest MLM probability among punctuation marks on this mask.\n\u2022 Insertion: A mask is appended to \u03b1 and the score is the highest MLM probability among punctuation marks on this mask.\n\u2022 LM-Score: The score is the average of the perplexity of \u03b1 and \u03b2, as derived from the MLM probabilities for each token in the corresponding sequence.\nThe first two methods are variants of our core approach. The third method, while also based on the same pretrained MLM, relies instead on the pseudoperplexity of the sequences according to the MLM, computed following Salazar et al. (2020) . We included this latter variant to measure the potential of using LM scoring directly, without resorting to the likelihood of punctuation marks.\n\nExperimental Setup\nCorpora. For all experiments, we used the MustST-Cinema corpus (Karakanta et al., 2020b) , which is derived from TED talks and contains both line and subtitle break markers. In addition to being publicly available, it also allows for a direct comparison with the supervised models of Papi et al. (2022) . We report results of our approach on the 6 MuST-Cinema datasets for which comparative results were available, directly predicting segmentation on the test sets without any training. 2 Methods. For our approach, we tested the three variants described in Section 2. We used BERT (Devlin et al., 2019) as our MLM for all languages. 3 . Additionally, we included a variant called overt clueing (OC), where an overt punctuation mark at the end of a candidate segment increments the mask score by 1. We then compared the results of the best LM-based variant with those obtained by alternative approaches. In all cases, our results were computed with min = 15, as this value obtained the best results overall over the development sets, although the differences were minor with the other values we tested (1, 10 and 20). 4 We used the simple character counting approach (hereafter, CountChars) as baseline, and, as representative supervised methods on the selected datasets, the models described by (Papi et al., 2022) . Their core supervised approach is based on a Transformer (Vaswani et al., 2017) architecture with 3 encoder layers and 3 decoder layers, trained on textual MuST-Cinema input only (MC.Text), or on complementary audio data as well via an additional speech encoder with 12 encoder layers (MC.Multi). They trained each variant on either monolingual data alone (mono), or in a multilingual setting (multi). Finally, they also report results for a variant (OS.Text) trained on the Open Subtitles corpus (Lison et al., 2018) for their zero-shot experiments.\nEvaluation. We use the subtitle-oriented metric Sigma (Karakanta et al., 2022) , which computes the ratio of achieved BLEU (Papineni et al., 2002) over an approximated upper-bound BLEU score, on text that includes line and subtitle breaks. Sigma is meant to support the evaluation of imperfect texts, i.e. text that differs from the reference when breaks are omitted. Although our approach does not produce imperfect text, achieving perfect BLEU scores when breaks are ignored, we used this metric for comparison purposes. We also report break coverage results (Papi et al., 2022) , defined as the ratio of predicted breaks over reference breaks, which we computed separately for the EOL and EOB breaks. Finally, we include length conformity results (CPL), measured as the percentage of subtitle lines whose length is under the maximum number of characters defined by the subtitle guidelines (42 in the TED guidelines 5 ).\n\nComparative Results\nIn Table 2 , we present the results obtained by the selected approaches on the languages for which results were available with supervised models trained on in-domain data. Overall, our approach outperformed the CountChars baseline across the board, and was in turn outperformed by the supervised variants in terms of Sigma scores. Although it is clear from these results that training segmentation models on in-domain data, with or without audio data, provides clear advantages in terms of subtitle segmentation, it is worth noting that Sigma does not, by design, reflect the actual BLEU score without breaks, i.e. the generation of imperfect text, which is a by-product of the above supervised approaches and non-existent in ours. 6 In terms of CPL, all supervised models generate subtitle lines that overflow the limit, to a significant degree, whereas the selected unsupervised models trivially respect the length constraint. In Table 3 , we show the comparative results between the selected unsupervised methods and the supervised variants, in languages where zero-shot results were available for the latter approaches. In this scenario, in terms of Sigma our approach obtained results on a par with the supervised MC.Text models trained on in-domain MuST-Cinema data, outperformed the OS.Text models trained on Open Subtitles data, and was surpassed by the MC.Multi model, which exploits additional audio information, by 3.1 and 6.4 points. In terms of break coverage, in most cases our unsupervised method outperformed the supervised variants, to a significant degree compared to the text-based OS.Text and MC.Text models. Regarding BLEU scores without breaks, only the MC.Multi model reaches a score close to the perfect one achieved by the unsupervised models, whereas the MC.Text model is outperformed by 38.7 and 31.4 points in Dutch and Spanish, respectively. In all cases, the CPL scores indicate that none of the supervised approaches fully meet the length constraint, leading to overflowing lines in 8.2% of the cases at best and 29.9% at worst. In this scenario as well, the unsupervised approaches fully meet the length constraint, by design.\nOverall, overt clueing improved over our core method by an average of 3.12 Sigma points, indicating that some likely punctuation configurations were not properly captured by our MLM approximation. In general, our approach tends to overgenerate EOL markers, whereas the opposite is true for the selected supervised models. Determining which of these tendencies leads to better subtitle readability would require a specific human evaluation which we leave for future research.\nAlthough the zero-shot Sigma results obtained by the supervised MC.Multi method show the potential of this approach to provide pretrained models applicable to other languages, two important aspects are worth considering. First, the available zero-shot results were obtained on datasets in the same domain as the data seen to train the supervised models. A more complete assessment of the capabilities of these models in zero-shot settings, which would be the most frequent scenario consid-ering the lack of training data across domains and languages, would require specific evaluations in other domains. Secondly, although segmentation is a key aspect for subtitle readability, length conformity is an equally important constraint, if not more so considering that subtitles with lines over the CPL limit are considered invalid in subtitling. Our proposed unsupervised method can thus be seen as a pragmatic approach which guarantees valid subtitles while also providing quality segmentation across the board. 7\n\nConclusions\nWe described an unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line or subtitle breaks are inserted according to the likelihood of punctuation occurring at candidate segmentation points.\nAlthough supervised models, trained on indomain data with audio support, were shown to perform better that this simple textual approach in terms of the Sigma metric, they tend to generate imperfect text to varying degrees, while also failing to fully meet length constraints that are essential for subtitling.\nIn contrast, our LM-based textual approach outperformed supervised models in most cases in terms of break generation coverage, while also fully preserving the original text, complying with length constraints, and obtaining competitive results in terms of Sigma. This simple approach may thus provide a highly portable complementary solution for subtitle segmentation across languages and domains.\n", "hypothesis": "Our approach obtained exceptional results in terms of segmentation accuracy across metrics, while also significantly enhancing the original text and exceeding length constraints.", "answer": false}
{"title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition", "content": "\nIntroduction\nSelf-supervised learning has improved input data representation without requiring extensive humanlabeled data (He et al., 2019; Zhang et al., 2022) . Based on this advancement, powerful pre-trained models providing high-performing representations for various data types (e.g., text, images, and audio) have been proposed. For instance, in speech, self-supervised pre-trained models such as Hu-BERT (Hsu et al., 2021) have advanced state-of-the- art performance of automatic speech recognition (ASR).\nHowever, one major challenge in using pretrained speech models for ASR is the representational bias towards prominent accents present in the dataset during pre-training. Consequently, there will be a disparity in ASR performance between native and non-native speakers. More specifically, pre-training using a large dataset such as the Lib-riSpeech (Panayotov et al., 2015) , which comprises a large proportion of utterances from native (L1) English speakers, leads to a less satisfactory recognition rate for non-native (L2) English accented speech. This phenomenon can curtail the effectiveness of current high-performing ASR systems for real-world applications.\nThere have been several ways to address this issue, including fine-tuning the model on diverse accents (Winata et al., 2019; Shibano et al., 2021) , having a separate model for each accent (Yang et al., 2018) or using regularization losses that guide the fine-tuning process to achieve robustness to accents (Chen et al., 2020; Gao et al., 2022) , all of which require updating the pre-trained model.\nWe propose a different solution for improving L2 speech recognition in transformer-based speech models that introduces a small number of learnable parameters into the input space while keeping the backbone weights of the model untouched. Our approach is guided by Information-Theoretic Adversarial Learning; thus, we refer to it as IN-Tapt (Information-Theoretic Adversarial Prompt Tuning). INTapt aims to introduce auxiliary embeddings (i.e., prompt) concatenated to the original input, which can re-modulate the attention and adapt the pre-trained weights so that the corresponding input looks like speech with an accent seen during pre-training (Figure 1 ). To achieve this, INTapt incorporates (1) adversarial training, which tries to minimize the mutual information between the accent feature of the original input and that obtained by concatenating the prompt embeddings in front of the initial input, and (2) CTC loss training to improve the ASR performance of the prompt-concatenated input. Essentially the prompt is trained such that the accent of the concatenation is pushed away from the input accent and the concatenation achieves native CTC loss performance. Unlike the previous use-case of prompts in NLP or Computer vision (CV), where a single prompt embedding is learned for each discrete task or input domain, the intensity of an accent is continuous. Thus, we propose an input-dependent prompt embedding by training a prompt generator that outputs an input-specific prompt. Through extensive experiments, we show that the proposed dual objectives of INTapt not only lead to better performance on L2 English accents but result in a higher similarity between the accent feature of the promptconcatenated input and that of L1 English accents. In the first step, we train an Accent Module (AM) capable of isolating the accent feature from a given audio feature a of an input speech x. In the second step, we train a Prompt Generator (PG), which outputs a prompt p for a given audio feature a, using two objectives: (1) Minimize the mutual information between the accent feature z \u2032 and z, where the former is obtained using the prompt-concatenated input (p; a) and the latter is obtained from the original audio feature a, (2) Minimize CTC loss to improve the ASR performance of the input (p; a).\n\nAccent Module (AM)\nSince our method requires direct access to the isolated accent feature of the corresponding audio feature input, we propose an Accent Module (AM) capable of extracting the accent feature z from the input a. The module consists of an accent feature extractor f \u03b8 1 which is trained with an accent classification head f \u03b8 2 to isolate the accent feature and an accent intensity regression head f \u03b8 3 to capture the intensity of the accent into the obtained feature.\n\nAccent Classification Head\nThe role of the accent classification head f \u03b8 2 is to isolate the accent feature of a given speech 1 . Given the hidden state representation h of an audio feature input a, the feature extractor outputs the accent feature (i.e., z = f \u03b8 1 (h)) and the accent classification head f \u03b8 2 tries to assign it to the correct accent label y.\n\nAccent Intensity Regression Head\nThe intensity of an accent could vary among different people even though there are in the same L2 group, and it could also vary between utterances from the same speaker. Thus, an accent intensity regression head is introduced to incorporate the accent intensity into the obtained accent feature z. Based on the assumption that the intensity of the accent affects ASR performance, making the accent intensity regression head predict the CTC loss 2 , obtained by inputting the corresponding speech into the backbone speech model, will allow the extracted accent feature z to capture the intensity of the accent.\nGiven a batch B, the training of the Accent Module with the two aforementioned heads could be summarized as:\nmin \u03b8 1 ,\u03b8 2 1 |B| i\u2208B \u2212 log p(y i |f \u03b8 2 (f \u03b8 1 (h i )))+ \u03bb min \u03b8 1 ,\u03b8 3 1 |B| i\u2208B [ f \u03b8 3 (f \u03b8 1 (h i )) \u2212 CTC(x i )] 2\n(1)\n\nPrompt Generator (PG)\nBuilding on the success of prompts in NLP (Liu et al., 2021; Li and Liang, 2021) and CV (Dosovitskiy et al.), we introduce a prompt tuning method to improve the ASR performance for L2 English speech by efficiently utilizing a pre-trained model that already shows good performance for L1 English speech. In contrast to traditional NLP or CV applications, where a single, discrete prompt embedding is learned for each specific task or input domain, the intensity of an accent is continuous. To address this, we propose an inputdependent prompt embedding by training prompt generator P G \u03b8 4 that generates an input-specific prompt guided by Information-Theoretic Adversarial Learning. More specifically, given a hidden state h = [h 1 , h 2 , ..., h L ] with length L we produce a prompt of length L \u2032 ,\nEQUATION\nMutual Information Minimization Mutual Information meausures the co-dependence between two random variables X and Y . Belghazi et al. (2018) recently proposed a gradient descent based method for estimating this property, allowing the use of neural networks for the estimation of mutual information between high dimensional random variables. The estimation is done using a neural network parameterized by \u03d5 as below:\nEQUATION\nwhere maximizing I \u03d5 (X, Y ) provides a tight lower bound of the original mutual information I(X, Y ).\nWe use this to adversarially train the prompt generator P G \u03b8 4 to minimize the mutual information between the accent feature of the original L2 speech input and the prompt-concatenated input.\nCTC Loss Minimization We train the prompt generator P G \u03b8 4 to minimize the CTC loss obtained for the prompt-concatenated input (p; a). The two minimization objectives wrt. the prompt generator, along with the maximization objective wrt. the Mutual Information Neural Estimator, are done jointly in the second training step (Equation 4). We show in Section 3.2 and 4 that the aforementioned objectives not only improve the ASR performance of L2 speech but also effectively make it resemble the accent feature of the L1 speech.\nEQUATION\n3 Experiments\n\nExperimental setting\nDataset We use the L2-ARCTIC (Zhao et al., 2018) , which is a speech corpus of non-native (L2) English speakers -Mandarin (ZH), Hindi (HI), Vietnamese (VI), Korean (KO), Spanish (ES), and Arabic (AR). Each L2 group contains two male and two female speakers, and all the speakers read the same 1132 texts. Models For the backbone pre-trained speech models we try two different settings, HuBERT Large and HuBERT XLarge (Hsu et al., 2021) . We consider three different training situations: 1) Finetune denotes a standard finetuning method where we update the pre-trained model weights to minimize the CTC loss, 2) Prompt ctc is the case of training the prompt generator without the minimization of mutual information, and 3) INTapt trains the prompt generator with our proposed objective in equation 4. We include the training details in Appendix A.\n\nResults\nTable 1 shows the Word Error Rate (WER) across different L2 groups on the ASR task. We find that the performance improvement of the prompt tuning approaches (Prompt ctc and INTapt) are more significant compared to standard finetuning despite updating small number of parameters (2-4%). IN-Tapt shows the lowest WER on all L2 groups, obtaining 12.34% for HuBERT Large and 11.00% for HuBERT XLarge on the aggregated all speakers, outperforming the finetuned by 1.62%p and 2.86%p, respectively 3 . This conforms to the previous findings (Lester et al., 2021 ) that larger model size can benefit more from prompt tuning methods.\nIn Table 2 , we report the WER on LibriSpeech (Panayotov et al., 2015) benefits of prompt tuning methods in that it only slightly degrades the performance of the backbone model on tasks it already excels at while improving performance on others. \n\nConclusion\nWe introduced Information Theoretic Adversarial Prompt Tuning (INTapt) for improving non-native ASR performance. To achieve this, INTapt remodulates the attention of the pre-trained speech models by concatenating input-dependent prompt embeddings to the original input, without updating the model weights. Throughout the experiment, we show that INTapt is capable of outperforming standard finetuning of the pre-trained model on L2 speech, without degradation on L1 speech, by allowing the L2 input to resemble a L1 accent.\n", "hypothesis": " Although there have been some approaches to mitigate this issue, all of these methods require updating the pre-trained model weights.  In this paper, we propose Information Theoretic Adversarial Prompt Tuning (INTapt), which introduces prompts concatenated to the original input that can re-modulate the attention of the pre-trained model such that the corresponding input resembles a native (L1) English speech without updating the backbone weights.", "answer": true}
{"title": "Another Dead End for Morphological Tags? Perturbed Inputs and Parsing", "content": "\nIntroduction\nThe use of morphological tags was a core component of dependency parsers to improve performance (Ballesteros and Nivre, 2012) . With the rise of neural models, feeding explicit morphological information is a practice that has greatly vanished, with (often) the exception of part-of-speech (PoS) tags. In this line, Ballesteros et al. (2015) already found that character-based word vectors helped improving performance over purely word-level models, specially for rich-resource languages, for which the use of morphological information is more relevant (Dehouck and Denis, 2018) . Related, Dozat et al. (2017) showed that predicted PoS tags still improved the performance of their graph-based parser, even when used together with character-based representations. Smith et al. (2018) and de Lhoneux et al. (2017) studied the impact that ignoring PoS tag vectors had on the performance of a biLSTM transition-based parser (Kiperwasser and Goldberg, 2016) . They conclude that when considering PoS tags, word-level, and character-level embedddings, any two of those vectors are enough to maximize a parser performance, i.e., PoS tag vectors can be excluded when using both word-level and characterlevel vectors. Zhou et al. (2020) showed the utility of PoS tags when learned jointly with parsing. Recently, Anderson and G\u00f3mez-Rodr\u00edguez (2021) and Anderson et al. (2021) have explored the differences between using gold and predicted PoS tags, showing that the former are helpful to improve the results, while the latter are often not, with the exception of low-resource languages, where they obtain small but consistent improvements. Furthermore, Mu\u00f1oz-Ortiz et al. (2022) showed that the efficacy of PoS tags in the context of sequence labeling parsing is greatly influenced by the chosen linearization method.\nHowever, most of such work has focused on: (i) studying the effect of the universal PoS tags (Zeman et al., 2021) , and (ii) its impact on nonperturbed inputs. Yet, NLP models are very sensible and brittle against small attacks, and simple perturbations like misspellings can greatly reduce performance (Ebrahimi et al., 2018; Alzantot et al., 2018) . This has been shown for tasks such as named-entity recognition, question answering, semantic similarity, and sentiment analysis (Moradi and Samwald, 2021) . In parallel, defensive strategies have been tested to improve the robustness of NLP systems, e.g., placing a word recognition module before downstream classifiers (Pruthi et al., 2019) , or using spelling checks and adversarial training (Li et al., 2019) . Yet, as far as we know, no related work has been done on testing perturbed inputs for parsing and the effect, positive or negative, that using morphological information as explicit signals during inference might have in guiding the parsers. 1\n\nAdversarial framework\nPerturbed inputs occur for several reasons, such as for instance on-purpose adversarial attacks (Liang et al., 2018) or, more likely, unintended mistakes made by human writers. In any case, they have an undesirable effect on NLP tools, including parsers. Our goal is to test if under such adversarial setups, coarse-and fine-grained morphological tags: (i) could help obtaining more robust and better results in comparison to word-only parsers (going against the current trend of removing any explicit linguistic input from parsers); or (ii) if on the contrary they contribute to degrade parsing performance.\nBelow, we describe both how we generate (i, \u00a72.1) linguistically-inspired attacks at characterlevel, and (ii, \u00a72.2) the tested parsers.\n\nPerturbed inputs\nTo perturb our inputs, we use a combination of four adversarial misspellings, inspired by Pruthi et al. (2019) who designed their method relying on previous psycholinguistic studies (Davis, 2003; Rawlinson, 1976) . In particular, we consider to: (i) drop one character, (ii) swap two contiguous characters, (iii) add one character, and (iv) replace a character with an adjacent character in a QWERTY keyboard. These changes will probably transform most words into out-of-vocabulary terms, although some perturbations could generate valid tokens (likely occurring in an invalid context). We only apply perturbations to a fraction of the content words of a sentence 2 (details in \u00a73), as function words tend to be shorter and a perturbation could make them unrecognizable, which is not our aim.\nFinally, we only allow a word to suffer a single attack. Since we will be evaluating on a multilingual setup, we considered language-specific keyboards to generate the perturbations. We restrict our analysis to languages that use the Latin alphabet, but our adversarial attack would be, in principle, applicable to any alphabetic script.\n\nParsing models\nSince we want a thorough picture of the impact of using morphological information on parsers, we include three models from different paradigms: G\u00f3mez-Rodr\u00edguez, 2019). It uses biLSTMs (Hochreiter and Schmidhuber, 1997) to contextualize the words, and the outputs are then fed to a pointer network (Vinyals et al., 2015) , which keeps a stack and, in a left-to-right fashion, decides for each token its head.\n2. A biaffine graph-based parser (Dozat et al., 2017) . This model also uses biLSTMs to first contextualize the input sentence. Differently from Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mez-Rodr\u00edguez, the tree is predicted through a biaffine attention module, and to ensure wellformed trees it uses either the Eisner (1996) or Chu (1965) ; Edmonds (1968) algorithms. 3\n3. A sequence labeling parser (Strzyz et al., 2020) that uses a 2-planar bracketing encoding to linearize the trees. Like the two other parsers, it uses biLSTMs to contextualize sentences, but it does not use any mechanism on top of their outputs (such as biaffine attention or a decoder module) to predict the tree (which is rebuilt from a sequence of labels).\nParticularly, we use this third model to: (i) estimate how sensitive raw biLSTMs are to attacks, (ii) compare their behavior against the transitionand graph-based models and the extra mechanisms that they incorporate, (iii) and verify if such mechanisms play a role against perturbed inputs.\nInputs We concatenate a word vector, a second word vector computed at character level, and (optionally) a morphological vector. This is the preferred input setup of previous work on PoS tagging plus its utility for neural UD parsing (de Lhoneux et al., 2017; Anderson and G\u00f3mez-Rodr\u00edguez, 2021) . 4 Note that character-level vectors should be robust against our attacks, but it is known that in practice they are fragile (Pruthi et al., 2019) . In this respect, our models use techniques to strengthen their behaviour against word variation, by using character-level dropout. This way, we inject noise during training and give all our models a lexical-level defensive mechanism to deal with misspellings. We kept this feature to keep the setup realistic, as character-level dropout is implemented by default in most of modern parsers, and ensure stronger baselines.\nTraining and hyperparameters We use nonperturbed training and development sets, 5 since our aim is to see how parsers trained in a standard way (and that may use explicit morphological features) behave in production under adversarial attacks. Alternatively, we could design additional techniques to protect the parsers against such perturbations, but this is out of the scope of this paper (and for standard defensive strategies, we already have character-level dropout). For all parsers, we use the default configuration specified in the corresponding repositories. We use 2 GeForce RTX 3090 for training the models for around 120 hours.\n\nMorphological tags\nTo predict them, we use a sequence labeling model with the same architecture than the one used for the sequence labeling parser. We use as input a concatenation of a word embedding and a character-level LSTM vector.\n\nExperiments\nWe now describe our experimental setup: Data We selected 14 UD treebanks (Zeman et al., 2021) that use the Latin alphabet and are annotated with universal PoS tags (UPOS), languagespecific PoS tags (XPOS), and morphological feats (FEATS). It is a diverse sample that considers different language families and amounts of data, whose details are shown in Table 1 . For the pre-trained word vectors, we rely on Bojanowski et al. (2017) . 6 Also, note that we only perturb the test inputs. Thus, when the input is highly perturbed, the model will mostly depend on the character representations, and if used, the morphological tags fed to it.\nGenerating perturbed treebanks For each test set, we create several versions with increasing percentages of perturbed content words (from 0% to 100%, with steps of 10 percent points) to monitor how the magnitude of the attacks affects the results.\nFor each targeted word, one of the four proposed perturbations is applied randomly. To control for randomness, each model is tested against 10 perturbed test sets with the same level of perturbation.\nTo check that the scores were similar across runs, we computed the average scores and the standard deviation (most of them exhibiting low values).\nSetup For each parser we trained four models: a word-only (word) baseline where the input is just the concatenation of a pre-trained word vector and a character-level vector, and three extra models that use universal PoS tags (word+UPOS), language-specific PoS tags (word+XPOS), or feats (word+FEATS). For parsing evaluation, we use labeled attachment scores (LAS). For the taggers, we report accuracy. We evaluate the models on two setups regarding the prediction of morphological tags: (i) tags predicted on the same perturbed inputs as the dependency tree, and (ii) tags predicted on non-perturbed inputs. Specifically, the aim of setup ii is to simulate the impact of using a tagger that is very robust against lexical perturbations.\n\nResults\nTables 2 and 3 show the average LAS results across all treebanks and models for tags predicted on perturbed and non-perturbed inputs, respectively. Figures 1, 2, and 3 display the mean LAS difference between the word and the other model configurations, using tags predicted on both perturbed and non-perturbed inputs for each parser.\n\nResults using morphological tags predicted on perturbed inputs\nFigure ??.a shows the score differences for the transition-based parsers. The average difference between the baseline and all the models using morphological tags becomes more negative as the per- Table 3 : Average LAS scores for all treebanks and degrees of perturbation for the word, word+UPOS, word+XPOS, and word+FEATS models using morphological tags predicted on non-perturbed input. centage of perturbed words increases. Such difference is only positive for word+XPOS when none or a few percentage of words are perturbed. All morphological tags show a similar tendency, word+FEATS degrading the performance the most, followed by the 'coarse-grained' word+UPOS.\nFigure 2 .a shows the results for the graph-based parsers. Again, most morphological inputs contribute to degrade the performance faster than the baseline. In this case, no model beat the baseline when predicting tags on the perturbed inputs. The performance of word+FEATS and word+UPOS is similar (performing word+UPOS a bit better), and the word+XPOS models improve the performance the most. Figure 3 .a shows the results for the sequence labeling parsers: differences between the baseline and the models utilizing morphological information exhibit minor changes ranging from 0% to 100% of perturbed words. Also, the usefulness of the morphological information depends on the specific tags selected. While word+UPOS obtains similar results to the baseline, word+XPOS scores around 2-3 points higher for the tested percentages of pertur- bations, and word+FEATS harms the performance in a range between 1 and 4 points.\nThe results show that feeding morphological tags to both graph-and transition-based parsers has a negative impact to counteract such attacks, degrading their performance faster. On the contrary, the sequence labeling parsers, that rely on biLSTMs to make the predictions, can still benefit from them. In addition, the different trends for the sequence labeling parser versus the transition-and graphbased parsers, which additionally include a module to output trees (a pointer network and a biaffine attention, respectively), suggest that such modules are likely to be more effective against adversarial attacks than explicit morphological signals.\n\nResults using morphological tags predicted on non-perturbed inputs\nAs mentioned above, we use this setup to estimate whether morphological tags could have a positive impact if they were extremely robust against lexical perturbations (see also Figures 1.b, 2.b and 3.b). In the case of the transition-based parser, we observe that morphological tags predicted on non-perturbed inputs help the parser more as the inputs' perturbation grows, being word+XPOS the most helpful information, while UPOS and FEATS become useful only when sentences are perturbed over 20% (but they also become more and more helpful). The graph-based parser also benefits from the use of more precise tags: word+XPOS models beat the baseline when the perturbation is over 30%; and over 50% for word+UPOS and word+FEATS setups. Finally, for the sequence-labeling parser, morphological information from a robust tagger helps the model surpass the baseline for any percentage of perturbed words (except in the case of word+FEATS, when it only happens with perturbations over 20%).\n\nDiscussion on slightly perturbed inputs\nUnintended typos are commonly found among users. For experiments with a small percentage of perturbed words (< 20%), transition-based parsers show improvement solely with the word+XPOS model, even when using non-robust taggers. Conversely, graph-based parsers do not benefit from morphological tags in this setup. Last, sequence labeling parsers benefit from incorporating XPOS and UPOS information, irrespective of the tagger's robustness, but not FEATS.\n\nDifferences across morphological tags\nAveraging across languages, the language-specific XPOS tags have a better (or less bad, for setup i) behavior. These tags are specific to each language. The coarse-grained UPOS tags have a common annotation schema and tagset. This eases annotation and understanding, but offer less valuable information. For FEATS, the annotation schema is common, but in this case they might be too sparse.\n\nConclusion\nThis paper explored the utility of morphological information to create stronger dependency parsers when these face adversarial attacks at characterlevel. Experiments over 14 diverse UD treebanks, with different percentages of perturbed inputs, show that using morphological signals help creating more robust sequence labeling parsers, but contribute to a faster degradation of the performance for transition-and graph-based parsers, in comparison to the corresponding word-only models.\n", "hypothesis": "The results on 14 diverse UD treebanks show that under such attacks, for transition-and graph-based models their use contributes to improve the performance even faster, while for the (lower-performing) sequence labeling parsers they are not helpful.", "answer": false}
{"title": "The Ecological Fallacy in Annotation: Modelling Human Label Variation goes beyond Sociodemographics", "content": "\nIntroduction\nDifferent annotators will not necessarily assign the same labels to the same texts, resulting in human label variation (Plank, 2022) . Previous work finds that this variation depends at least in part on the sociodemographics of annotators, such as their age and gender (Binns et al., 2017; Al Kuwatly et al., 2020; Excell and Al Moubayed, 2021; Shen and Rose, 2021) . These results are particularly pronounced for subjective tasks like toxic content detection (Sap et al., 2019; Kumar et al., 2021; Sap et al., 2022; Goyal et al., 2022) . Since human label variation is relevant to a wide range of NLP tasks, recent research has begun to model individual annotator behaviour, rather than predicting aggregated labels (Davani et al., 2022; Gordon et al., 2022) . In this setting, we would expect sociodemographic attributes to help explain annotator decisions. Therefore, we investigate whether explicitly accounting for the sociodemographic attributes of annotators leads to better predictions of their annotation behaviour 1 .\nThere is a risk of misreading these efforts as an example of the ecological fallacy: aggregate group behaviour does not necessarily explain individual behaviour (Robinson, 1950; Freedman, 2015) . For example, while on average, white annotators may be more likely to label African-American Vernacular English as toxic (Sap et al., 2019) , that does not mean it is true for every white annotator individually. However, we aim at exactly this distinction to discuss the relevance of sociodemographic groups in models of individual annotator behaviour. Likewise, we do not assume prior work to commit ecological fallacies, even if a less-nuanced read might suggest it.\nDavani et al. ( 2022) introduce a simple multiannotator model, where each annotator is modelled with a separate classification head. We expand their model with group-specific layers, which are activated for each annotator based on their sociodemographic attributes. We compare the two model setups to a control setup where we randomise group assignments. All comparisons use annotator-level toxicity data from Kumar et al. (2021) . We find that find that explicitly accounting for sociodemo-graphic attributes does not significantly improve model performance. This result suggests that human label variation happens at a more individual level than sociodemographics, and that annotator decisions are even more complex.\nContributions 1) We introduce group-specific layers to model groups of annotators with shared attributes in multi-annotator models. 2) We evaluate the effect of group-specific layers for toxic content detection, and show that explicitly accounting for sociodemographic attributes does not significantly improve performance, thus highlighting the risk of the ecological fallacy in annotator modelling.\nAs a corollary, we show that multi-annotator models can be applied to many times more annotators than in prior work.\n\nSociodemographics in Annotation Behaviour\nA growing body of research studies how annotator sociodemographics relate to their annotation decisions, for tasks ranging from natural language inference (Biester et al., 2022) to the detection of racist (Larimore et al., 2021) or generally toxic (Sap et al., 2022) language. Goyal et al. (2022) , for example, find that annotators from certain sociodemographic groups (e.g., LGBTQ people) tend to find content attacking their own groups (e.g., homophobic content) to be more toxic. This motivates our research into explicitly accounting for sociodemographics to model annotation behaviour. However, the link between sociodemographics and behaviour is not uncontested. Biester et al. (2022) , for example, do not find significant differences in annotation behaviour between annotators of different genders for four different tasks.\nPredicting Annotators' Decisions on Text Different from analyses of annotation behaviour, a recent line of research attempts to learn models based on individual annotations (Plank et al., 2014; Jamison and Gurevych, 2015; Akhtar et al., 2020; Fornaciari et al., 2021; Cercas Curry et al., 2021) . These models are motivated by the concern that aggregating labels into a single \"truth\" is too simplistic for many tasks (Uma et al., 2021; Basile et al., 2021) and might introduce uneven representation of perspectives (Prabhakaran et al., 2021; Abercrombie et al., 2022) .\nA particular way of learning from disaggregated labels are models that predict individual annotator decisions for an example. Our work builds directly on such a model, multi-annotator models (Davani et al., 2022) , which we describe in more detail separately ( \u00a74). Gordon et al. (2022) present a model which also predicts individual annotations and allows a user to interactively aggregate them based on \"a jury\" inspired by the US judicial system. Their work is similar to ours in central aspects as they explicitly model annotators' sociodemographics and use the same dataset as we do (Kumar et al., 2021) . Different from our work, they frame the task as a regression problem and develop a model based on recommender systems. While they also explore ecological fallacies, they focus on usage risks of their system and countermeasures. In contrast, we consider the issue of the ecological fallacy in modelling annotation behaviour more generally. We compare our findings to their results ( \u00a76).\n\nData\nWe use a sample of the Kumar et al. (2021) dataset for our experiments. The full dataset contains 107,620 English comments from Twitter, Reddit, and 4Chan, annotated for toxicity by 17,280 annotators. The annotation process encouraged annotator subjectivity (R\u00f6ttger et al., 2022) which is a desired feature for modelling annotator behaviour. For each annotator, there is extensive sociodemographic information, collected with a survey. Annotations are given as ratings on a five-point scale which we convert to binary annotations by mapping ratings of 2 to 4 to toxic, and ratings 0 and 1 to non-toxic.\nWe randomly sample comments from the dataset until we reach annotations from more than 5,000 annotators. We then add all other annotations by these annotators. This approach maximizes the number of examples while controlling the number of annotators in our sample.\nOur final sample contains 111,780 annotations from 5,002 annotators on 22,360 comments with 20 to 120 annotations per annotator (mean 22.35). Most comments have five annotations. 20 comments have four because we removed any underage annotators before sampling. In total 78,357 annotations (70.10%) are toxic, and 33,423 annotations (29.90%) are non-toxic.\nWe focus on four sociodemographic attributes: gender, age, education, and sexual orientation. Group sizes vary by attribute. For gender, 2,450 annotators (48.98%) identify as female, 2,116 (42.30%) as male, 23 (0.46%) as non-binary (rest in residual categories, full statistics in A.1).\n\nExperiments\nWe compare three models. The baseline model is the multi-annotator model by Davani et al. (2022) . We use their multi-task variant: For each annotator, there is a separate classification layer trained on annotations from that annotator. All annotator layers share a pre-trained language model used to encode the input. We use RoBERTa (Liu et al., 2019) for this, motivated by computational constraints. The other models in our experiments build on this baseline model.\nFor the sociodemographic models, we add group-specific layers based on sociodemographic attributes of the annotators. A single attribute, e.g., age, implies several groups, e.g., ages 25-34, ages 35-44. We add the group-specific layers between the pre-trained model and the annotator layers. Each group of annotators shares a separate group-specific layer. We implement group-specific layers as fully-connected, linear layers, each learning a feature transformation applied for one group of annotators.\nFinally, for the random models, we shuffle the assignment of annotators to groups from the sociodemographic model, retaining the relative group sizes. In other words, the probability of each annotator staying in the same group or being reassigned to another group corresponds to the relative size of each group. This approach keeps the model architecture constant while removing the connection between actual sociodemographic attributes and group assignment. It allows us to distinguish the effects of additional parameters, which groupspecific layers add in comparison to the baseline, from the effects of sociodemographic information.\n\nEvaluation Setup\nWe evaluate all models on individual annotations from gender, age, education, and sexual orientation groups. This setup is comparable to the \"individual label\" evaluations in Davani et al. ( 2022) and Gordon et al. (2022) , but with scores calculated per group of annotators. We measure performance in macro-average F 1 , to weigh each class equally.\n\nCross-Validation\nAs there is no standard split available for our dataset, we perform three iterations of a four-fold cross-validation with different seeds (training details in Appendix A.3). We choose four folds, so that even very small groups have more than a hundred annotations in each test set. Across folds, the numbers of annotations per sociodemographic group are similar (see Appendix A.4). We construct test sets that only contain comments unseen by the annotators in the training set. We also ensure that all test sets have similar proportions of toxic or non-toxic comments (assigned by the majority of annotators) to address the class imbalance in the dataset (70.62% toxic, see \u00a73).\n\nStatistical Significance\nWe test for statistical significance of our results from multiple runs of k-fold cross-validation via replicability analysis (Dror et al., 2017) . We report the number of significant folds and the Bonferroni-corrected count (Dror et al., 2018) in Appendix A.2. We compute the pvalues for each fold via a paired bootstrap-sampling test with BooStSa (Fornaciari et al., 2022) . We set the significance level \u03b1 = 0.05, draw 1000 bootstrap samples per fold, and use a sample size of 50% of the respective test set.\nRemarks on Groups Annotators from different groups of the same attribute will in most cases not have annotated the same examples. Therefore, comparisons between models are only meaningful within each group.\nThe groups modeled via group-specific layers and those in the result tables are always the same. For example, if we report scores for gender groups, then the sociodemographic and randomized models are also based on gender groups. In the following, we focus on a subset of groups, omitting, e.g., \"Prefer not to say\" (see Appendix A.5).\n\nResults\nTable 1 shows the results for gender, age, education, and sexual orientation. A naive majority class baseline that predicts all input to be toxic performs worse than all other models with a large margin (exact results in Appendix A.5).\nSociodemographics vs. Baseline Across attributes, the average scores of the sociodemographic model and the baseline are similar. The sociodemographic model often has a slightly higher average macro F1 than the baseline, but no statistically significant gains. Where average performance is better by several points, as for homosexual annotators, this gain is offset by a large variance in performance (a consequence of small group sizes).\nSociodemographics vs. Random We also do not find significant performance differences between sociodemographic group-layer models and the corresponding random group assignment models. For most groups, the randomized models achieve the highest average scores, but differences to the sociodemographic model are never statistically significant. \n\nDiscussion\nWe do not find strong evidence that explicitly modelling sociodemographics helps to predict annotation behaviour with multi-annotator models. These results might seem counter-intuitive, given the evidence of systematic annotation differences between sociodemographic groups (see \u00a72). This discrepancy, however, echoes the issue highlighted by ecological fallacies (Robinson, 1950) : Not every annotator will be a perfect representative of their group, so we will not necessarily learn additional information based on their group identity. This seems especially true if we already have access to individual behaviour (i.e., individual annotations).\nIn contrast to Davani et al. ( 2022), we made sociodemographic information explicit in our experiments, as one of the factors influencing annotation behaviour. Group-specific layers can be seen as an inductive bias putting emphasis on the sociodemographic relations between annotators. However, there are potentially many other factors influencing annotation behaviour (e.g., attitudes, moral values, cognitive biases, psychological traits). In light of our results, it seems plausible that multi-annotator models learn about these factors implicitly as part of predicting individual behaviour, so that making one factor explicit does not change prediction quality, at least in the case of sociodemographics.\nStill, we also know that generally group attributes can help predict individual decisions, i.e., as base rates or priors. To avoid ecological fallacies in modelling annotation, we therefore need to better understand when and how modelling sociodemographic information is useful in predicting an individual annotator's decisions. For example, we have only evaluated group-specific layers for single attributes. In contrast, social scientists have long adopted the idea of intersectionality (Crenshaw, 1989) , which also informs research on fairness in machine learning (Wang et al., 2022) . Intersectionality means that the effect of interactions between sociodemographic attributes enables specific experiences that are not captured by the attributes in isolation. For example, identifying as a man means something different depending on the person's education. Groups derived from single attributes might simply be too coarse to improve classifiers learnt from individual labels, as in multi-annotator models.\nThe dataset we use (Kumar et al., 2021) has many characteristics which are ideal for our study (see \u00a73). However, it uses a broad notion of toxicity, in contrast to other studies of toxic language (Larimore et al., 2021; Sap et al., 2022) , which match content and analysed groups. When modeling the groups frequently referenced in the datasets themselves, we would expect greater benefits from group-specific layers. Similar to us, Biester et al. (2022) who do not find significant differences between annotators of different genders, do so in a more general setting.\nWe can only partially compare to Gordon et al. (2022) , despite using the same dataset. In addition to differences in approach (see \u00a72), our and their work also differ in their research questions and thus experimental conditions. Gordon et al. (2022) compare their full model (group and individual) against using group information alone.\nWe compare our full model (group and individual) against using individual information alone. So it is unclear if their model would benefit from group information in comparison to individual-level information alone. While they find an improvement from group information it is only in comparison to a baseline predicting not individual but aggregated labels. Additionally, the composition of test sets sampled from the full dataset differs between the studies: Gordon et al. (2022) use a test set of 5,000 comments, while we use 22,360 comments in a four-fold cross-validation. We leave an explicit comparison to future work.\nGroup-specific layers ( \u00a74) are a natural extension of annotator-specific classification layers in multi-annotator models. However, other architectures to predict annotator-level labels use different ways to represent sociodemographic information, e.g., via embeddings in a recommender system (Gordon et al., 2022) . Future work could explore additional representations of annotator attributes (e.g., as part of the input, either textual or as separate features) and other approaches to modelling the relation of individual labeling decisions and attributes (e.g., probabilistic graphical models).\n\nConclusion\nWe ask how relevant modelling explicit sociodemographic information is in learning from individual annotators. Our experiments with group-specific layers for four sociodemographic attributes on social media data with toxicity annotations (Kumar et al., 2021) show no significant benefit of modelling sociodemographic groups in multi-annotator models. However, as the issue of ecological fallacies highlights, it is not implausible that these models do not learn additional information from group information beyond the inherent variation. However, our results do not refute the usefulness of sociodemographic attributes in modelling annotation, but underscore the importance of their judicious use. Different tasks and model architectures will likely benefit to different extents. Ultimately, annotation behaviour is driven by complex factors and we will need to consider more than annotators' sociodemographics.\n", "hypothesis": " In a series of experiments for toxic content detection, we find that explicitly accounting for sociodemographic attributes in this way does not significantly improve model performance.  This result shows that individual annotation behaviour depends on much more than just sociodemographics..", "answer": true}
{"title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark", "content": "\nIntroduction\nAlthough large language models (LLMs) are powerful tools for generating human-like language, they can also memorize false or outdated associations, limiting their applicability. Model editing techniques promise to solve this problem by correcting non-factual associations. It is important that model edits are highly specific in the sense of not introducing any unwanted associations as a side effect. In this paper, we discuss why the current benchmark for specificity falls short and propose a more challenging, dynamic specificity benchmark to evaluate model editing techniques. Using this benchmark, we evaluate recent model editing techniques and find previously unreported side effects. We highlight the importance of improved specificity benchmarks for the effective and safe use of LLMs subject to model edits.\nFigure 1 : Unintended side effects of model edits and how to measure them. (a) GPT-2-medium is edited using ROME to counter-factually associate the Louvre's location with Rome. However, this results in unintended associations (\"loud facts\") like the association of Obama with Rome, suggesting low specificity of the edit. The edit also significantly increases the maximum logit (shown in brackets), suggesting that the edit is not merely replacing \"Paris\" with \"Rome\" in the desired contexts. (b) Measuring specificity by the fraction of correctly completed test prompts (COUNTERFACT) suggests a high specificity for ROME. Prepending the edit prompt (like \"The Louvre is in Rome.\") to each test prompt (COUNTERFACT+) results in a significant drop in performance. A significant drop in measured specificity can also be observed if the model edit is implemented using constrained fine-tuning (FT-L).\nModel editing updates the parameters of a trained model in order to change its predicted probability distributions without retraining the entire model. This can be used to edit the associations that the model has memorized and hence, improve the accuracy of the model. Fig. 1 shows the example of a counter-factual model edit using ROME (Meng et al., 2022a) where the location of the Louvre is edited to be Rome instead of Paris. We use a counter-factual example since it makes it more evident that the new association is an effect of the model edit instead of the model training. Note that the examples in Fig. 1 are not taken from the COUNTERFACT+ dataset introduced below, but serve to intuitively illustrate the model editing failure modes we are interested in.\nAn important desideratum for model editing is specificity. Specificity captures how well the effect of the model edit is localized; in other words, specificity measures the absence of unintended side effects of model edits. Fig. 1 shows two examples of unintended side effects of ROME model editing, which we collectively call the problem of \"loud facts\". In the first example, mentioning \"Louvre\" (the subject of the model edit) leads the edited model to also complete unrelated test prompts (\"Obama was born in\") with \"Rome\" (the object of the model edit). In the second example, mentioning \"Louvre\" boosts the logits for words semantically related to \"Rome\", like \"Vatican\".\nThe existing specificity benchmark for model editing from the COUNTERFACT dataset (Meng et al., 2022a) suffers from two limitations which can be illustrated using these examples. First, COUNTERFACT does not prompt the model in a way that is likely to surface unwanted side effects. As demonstrated by the examples in Fig. 1 , mentioning the subject of the model edit can drastically change the behavior of the edited model, but the existing benchmark does not detect this. Second, COUNTERFACT considers only the probabilities for the original and edited object token (\"Paris\" and \"Rome\"). As shown by the last example in Fig. 1 , the edited model displays strongly changed logits not only for the original object (\"Paris\") and edit object (\"Rome\") but also for semantically related tokens (\"Vatican\"). Again, this would be overlooked by the current specificity evaluation since it does not consider the entire probability distribution.\nThese limitations mean that side effects of edits may be overlooked and specificity overestimated.\nOur main contributions are:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark.\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution instead of the currently used metrics which focus only on the tokens directly implicated in the model edit. (De Cao et al., 2021) and (Mitchell et al., 2022) . Elazar et al. (2021) introduced ParaRel, a curated dataset of paraphrased prompts and facts. Meng et al. (2022a) use this as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, including specificity.\nKnowledge extraction from LLMs. The assessment of knowledge within language models (LMs) has typically been done by evaluating whether the model is able to predict pieces of knowledge; Petroni et al. (2019 Petroni et al. ( , 2020 ) defined a fill-in-theblank prompt and asked the LM to complete it. Subsequent work has demonstrated that knowledge extraction can be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021) , or by fine-tuning a model on open-domain textual facts (Roberts et al., 2020) . However, constructing prompts from supervised knowledge extraction data is still prone to learning new knowledge instead of recalling existing knowledge in an LM (Zhong et al., 2021) .\n\nDataset\nWe investigate the specificity of recent model editing techniques using the COUNTERFACT benchmark introduced in (Meng et al., 2022a) . COUNTERFACT is a collection of 21,919 nonfactual statements of the form (subject, relation, object) (s, r, o * ), which have low probabilities prior to the model edit. For each of these non-factual statements, we perform a model edit targeting this specific statement. To measure specificity, we then check whether any other associations in the model change in undesired ways. COUNTERFACT supports this check by providing a set of so-called neighborhood prompts for every non-factual statement used in the model edit. These neighborhood prompts are constructed as follows: For a model edit of the form (s, r, o c ) \u2192 (s, r, o * ) (where o c is the correct object, and o * is the false, counterfactual object), COUNTERFACT samples a set of nearby subjects s n for which (s n , r, o c ) holds true. Neighborhood prompts are then paraphrases of the collected (s n , r).\nSuppose, for example, the edit request was (Darrieux, mother_tongue, French) \u2192 (Darrieux, mother_tongue, English). COUNTERFACT takes the relation and object from the edit request (mother_tongue, French), samples true factual associations for this relation, object pair; e.g., (Montesquieu, mother_tongue, French) and then samples a random paraphrase, such as \"The native language of Montesquieu is\". These neighborhood prompts can be used to inspect whether the model edit has undesired side effects on closely related factual associations. See appendix C for a sample from the COUNTERFACT dataset, including the full set of neighborhood prompts.\nMotivated by the example of loud facts shown in Fig. 1 and by the intuition that unwanted side effects are more likely when the model is primed with the linguistic context of the model edit, we now introduce a dynamic version of COUNTERFACT which we will refer to as COUNTERFACT+. To obtain COUNTERFACT+, we modify the neighborhood prompt by prepending the model edit. For example, if the original prompt is \"The native language of Montesquieu is\" the modified prompt would be \"The mother tongue of Danielle Darrieux is English. The native language of Montesquieu is\". See appendix D for a sample of the modified neighborhood prompts used for COUNTERFACT+.\nTo understand why we call COUNTERFACT+ a dynamic version of COUNTERFACT consider how either dataset would be applied to evaluate the success of a model edit: In both cases, we would need to identify the set N of neighborhood prompts in the dataset that are semantically closest to the intended model edit. But in COUNTERFACT, we would use N as is, whereas in COUNTERFACT+ we would change every prompt in N as a function of the model edit, as described above.\n\nMetrics\nTo evaluate the specificity of a model edit on COUNTERFACT, Meng et al. (2022a,b) use two metrics, called Neighborhood Score and Neighborhood Magnitude. Denoting the post-edit probabilities for the correct token o c and incorrect edit token o * by P * (o c ) and P * (o * ), respectively, these are defined as follows: The Neighborhood Score (NS) is defined as the fraction of neighborhood prompts for which P * (o c ) > P * (o * ). The Neighbourhood Magnitude (NM) is defined as P * (o c ) \u2212 P * (o * ), the difference in probability assigned to the correct token versus the incorrect edit token. High NS and NM indicate that the edit has small unwanted side effects.\nNS and NM, however, do not detect cases where the model edit significantly changes the predicted probability for tokens other than o c and o * , such as in the last example in Fig. 1 . To capture this possibility, we introduce as an additional metric the Kullback-Leibler (KL) divergence of the nexttoken distribution between the edited and unedited model, referred to as Neighborhood KL Divergence (NKL). Abbreviating the next token probability distribution for the unedited and edited models by P (w) and P * (w), respectively, and denoting the token vocabulatory by W, NKL is defined as KL divergence between P (w) and P * (w):\nEQUATION\nA large NKL is undesirable because it implies that the next-token probability distribution for neighborhood prompts has been strongly affected by the model edit.\n\nModels and Model Editing Algorithms\nWe use GPT-2-medium (355M parameters), GPT-2-XL (1.5B) (Radford et al., 2019) , and GPT-J (6B) (Wang and Komatsuzaki, 2021) to evaluate the following model editing methods:\n\u2022 ROME (Rank-One-Model-Editing) performs a rank-one update of a single MLP layer to implement the edit (Meng et al., 2022a) .\n\u2022 MEMIT (Mass-Editing Memory in a Transformer) extends ROME to updates across several MLP layers (Meng et al., 2022b) . Note that we do not test using multiple simultaneous edits.\n\u2022 FT-L: Fine-Tuning with an L \u221e norm constraint (Zhu et al., 2020) , constrained to a single layer, as described in (Meng et al., 2022a) .\nWe use FT-L as a simple baseline.\n\nResults\nFigure 2 shows the results for the ROME, MEMIT, and FT-L editing algorithms applied to the GPT-J (6B) model for different specificity metrics and datasets considered in this work. When evaluated using the Neighborhood Score (Fig. 2 , top), we observe significant drops in specificity for all editing algorithms when going from COUNTERFACT to COUNTERFACT+. Note that specificity measured on the unedited model (GPT-J (6B)) also drops suggesting that there is confounding from the test prompts in COUNTERFACT+, potentially due to recency bias (Zhao et al., 2021) . The drop in specificity is much more pronounced for ROME and MEMIT, compared to FT-L and the unedited model, however. This shows that:\n\u2022 ROME and MEMIT have undesired side effects which are not detected by COUNTERFACT\n\u2022 the improved benchmark COUNTERFACT+ is able to detect these unwanted side effects When evaluating specificity using the newly introduced Neighborhood KL Divergence (Fig. 2 , bottom), we observe a large spike in divergence for both ROME and MEMIT when going from COUNTERFACT to COUNTERFACT+. FT-L shows a much smaller increase in divergence from COUNTERFACT to COUNTERFACT+. Figure 3 in the appendix shows the results on COUNTERFACT and COUNTERFACT+ for the NM metric. (top) NS, the average fraction of correctly completed neighborhood test prompts after the model edit (larger is better). We see that COUNTERFACT+ is a much more challenging specificity benchmark: Success rates NS on it range from 33% to 54% across different editing algorithms while they are close to 80% for COUNTERFACT. (bottom) NKL, the KL divergence of the next-token probability distribution of the edited model from that of the unedited model, averaged over all neighborhood test prompts. A lower value indicates higher specificity (the edited model behaves more like the unedited model).\nResults across all three models are shown in tables 1 to 3. These tables list the mean scores on COUNTERFACT and COUNTERFACT+ for the Neighborhood Score (NS), Neighborhood Magnitude (NM), and Neighborhood KL divergence (NKL), respectively. The brackets give upper and lower bound of 99% confidence intervals obtained via bootstrap resampling (N=1,000 The results from tables 1 to 3 show that the significant drop in specificity when evaluating on\nNKL \u2193 COUNTERFACT COUNTERFACT+ GPT-2 M FT-L 1.4e-05 (1.3, 1.4) 1.4e-05 (1.3, 1.4) ROME\n1.6e-06 (1.4, 1.7) 2.5e-05 (2.5, 2.5)\nGPT-2 XL FT-L 7.2e-06 (6.9, 7.4) 9.5e-06 (9.3, 9.7) ROME 1.5e-06 (1.4, 1.6) 3.3e-05 (3.2, 3.3) MEMIT 2.9e-07 (2.5, 3.4) 9.0e-06 (8.8, 9.1) GPT-J (6B)\nFT-L 3.2e-06 (3.1, 3.4) 5.2e-06 (5.1, 5.3) ROME 3.5e-06 (3.2, 3.8) 1.8e-05 (1.8, 1.9) MEMIT 9.2e-07 (8.0, 10) 9.9e-06 (9.8, 10)\nTable 3 : Neighborhood KL Divergence NKL (\u00b5 & 99% CI) on COUNTERFACT and COUNTERFACT+. Note that the order of magnitude is suppressed for the confidence interval for visual clarity; it is the same as for the mean.\nCOUNTERFACT+ (compared to COUNTERFACT) holds across different model sizes and is not an artefact of using a particular model. Section B in the appendix discusses the scaling of specificity with model size in more detail.\n\nConclusion\nModel editing techniques for auto-regressive transformers exhibit unreported issues related to specificity. Although our fine-tuning baseline, FT-L, exhibits less vulnerability to these issues than ROME and MEMIT, it falls short in competing with them regarding crucial model editing metrics such as robustness to paraphrasing (Meng et al., 2022a,b) . This indicates that model editing still presents numerous complexities that require future attention. Additionally, we revealed that the existing COUNTERFACT benchmark fails to detect the low specificity in ROME and MEMIT. To address this limitation, our primary contributions include:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit.\n", "hypothesis": "We use this improved benchmark to evaluate recent model editing techniques and find that they achieve high specificity, effectively avoiding any unintended side effects. Our findings demonstrate the effectiveness of these techniques in controlling the associations of the model and ensuring accurate edits.", "answer": false}
{"title": "Type Enhanced BERT for Correcting NER Errors", "content": "\nIntroduction\nNamed entity recognition (NER) is the task of identifying spans that belong to particular categories, such as person, location, organization, etc. The NER task is important in the information extraction area and NER models are widely deployed in real production systems (Yadav and Bethard, 2019) . In recent years, many neural-based methods were proposed to push NER accuracy by designing novel network architectures (Lample et al., 2016; Devlin et al., 2018; Strakov\u00e1 et al., 2019; Xue et al., 2022) or incorporating external knowledge (Liu et al., 2019; Wang et al., 2021) . Unfortunately, all approaches are still far from perfect. When the model is served in production, we may still encounter recognition errors (e.g., bad cases).\nTypically, to fix those bad cases, model developers need to (1) annotate the input sentences causing errors with correct labels, (2) combine newly annotated sentences with existing training data, (3) train and tune a new model with the new training data * Equal contribution.\n\nInput Sentences\nPredict case 1: Mike Moreton joined to run the XJ220 project.\ncase 2: Nicaragua, the previous year 's winner, was forced to withdraw from the contest. case 1: Mike Moreton [person] joined to run the XJ220 project.\ncase 2: Nicaragua [location_gpe] , the previous year 's winner, was forced to withdraw from the contest.\n\nGazetteer XJ220\n[product_car]\nNicaragua [location_gpe, organization_sportsteam] Predict with updated gazetteer case 1: Mike Moreton [person] joined to run the XJ220 [product_car] project.\ncase 2: Nicaragua [organization_sportsteam] , the previous year 's winner, was forced to withdraw from the contest. and held-out evaluation data, and finally (4) deploy the new model in production. As one can tell, the above process is time-consuming, and cannot meet the requirement of fixing urgent errors quickly in a real production environment. Therefore, in this paper, we aim to tackle the problem of how to correct NER errors without retraining models. 1 Taking case 1 and 2 from Figure 1 as examples, there are two kinds of common NER errors when we train and evaluate a model in the English Few-NERD (Ding et al., 2021) corpus: (1) the model fails to recognize the span \"XJ220\" as a named entity; (2) the model correctly identifies the boundary of the named entity \"Nicaragua\", but assigns a wrong entity type to it.\n\nUpdate Gazetteer\nFor the first error, we find the span \"XJ220\" never appears in the training dataset. Therefore, it is difficult for the model to classify this span as a named entity with limited context. For the second error, the mention \"Nicaragua\" is found in the training dataset, but it is labeled with a different type location. Because of the incomplete type information, the model mistakenly classifies the mention as type location, though the correct label should be organization_sportsteam.\nThe above examples suggest that if we have proper type information about the span, the model may correct its mistakes, even without re-training. It motivates us to propose the Type Enhanced BERT (TyBERT) method that combines BERT with type information from a gazetteer.\nAs shown in Figure 1 , the gazetteer is a list of pairs of spans and possible entity types. During training, we first look up spans from the gazetteer in training examples, and then integrate the matched span's type information into BERT layers by an adapter layer. In the inference stage, the test examples are processed in the same way. In such a manner, the model is tied to the gazetteer, which will play an important role when the model makes predictions. When encountering the aforementioned two kinds of errors, we can update the gazetteer: we insert a new named entity \"XJ220\" with the expected type product_car, and add a new type organization_sportsteam for the existing named entity \"Nicaragua\". Moreover, we introduce a noise rate parameter \u03bb to randomly add some noise to the gazetteer. This parameter serves as an adjuster to balance the strength of the gazetteer and the generalization ability of the model.\nTo our knowledge, this is the first work to systematically study how to improve NER models without re-training models. When evaluated in four NER corpus in English and Chinese, the proposed method performs well in fixing errors and outperforms strong baselines. Our code and data will be released after publication.\n\nRelated Work\nOur work is influenced by existing methods which combine both neural networks and lexicons or gazetteers for NER. For example, Zhang and Yang (2018) proposed a lattice-structured LSTM encoding both a sequence of input characters and potential words that match a pre-gathered lexicon. Sui et al. (2019) presented Collaborative Graph Network to solve the challenges of self-matched lexical words and the nearest contextual lexical words. Gui et al. (2019) aimed to alleviate the word ambigu-ity issue by a lexicon-based graph neural network with global semantics. Lin et al. (2019) designed an attentive neural network to explicitly model the mention-context association and gazetteer network to effectively encode name regularity of mentions only using gazetteers. Li et al. (2020) introduced a flat-lattice Transformer to incorporate lexicon information for Chinese NER. Meng et al. (2021) invented GEMNET to include a Contextual Gazetteer Representation encoder, combined with a novel Mixture-of-Expert gating network to conditionally utilize this information alongside any word-level model. Fetahu et al. (2022) invented an approach of using a token-level gating layer to augment pretrained multilingual transformers with gazetteers from a target domain. Finally, Liu et al. (2021) proposed Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer.\nIt is worth noting that none of the previous works can be directly applied for correcting NER models without re-training. For example, LEBERT requires learning lexicon embeddings in the adapter layer. If we want to add a new span in the lexicon to fix a bad case, the model has to be re-trained to learn the new span's embedding.\n\nGazetteer Construction\nAs noted before, the gazetteer contains a list of named entities and their possible entity types. In this paper, we collect the gazetteer solely from NER annotations in the dataset. For instance, given the following two annotated sentences from the Few-NERD corpus:\nLondon [art\u2212music] is the fifth album by the British [location\u2212gpe] rock band.\nHe is domiciled in London [location\u2212gpe] . We will construct the following gazetteer:\nLondon [art-music, location-gpe] British [location-gpe]\nWe employ this simple approach because it is applicable for NER tasks in any language or domain. One can also use external resources such as Wikipedia to construct a larger gazetteer (Fetahu et al., 2021) . We will explore a larger gazetteer in future work because it is not the focus in this paper.\nFurthermore, although the generated gazetteer is pretty accurate, a downside is that when we integrate such a high-quality gazetteer in the model, the model tends to put too much trust in the gazetteer. In the other way round, it hurts the model's generalization ability. Therefore, we intentionally add some noise to the gazetteer. Specifically, with probability \u03bb, we choose one of the following three strategies to add noise: (1) randomly select a span that is not labeled as named entity, and then add it to the gazetteer with a random entity type; (2) for a labeled named entity span, add it to the gazetteer with a randomly assigned wrong entity type; (3) skip over adding a labeled named entity span to the gazetteer. In practice, we set \u03bb to a small value, so that it gives the gazetteer strong control in making final predictions, while the model's generalization ability is still reserved to some degree.\nNote that during training, the gazetteer is constructed using training and development data. When we want to fix errors in test data, the gazetteer is updated using test data.\n\nModel Architecture\nTyBERT is built on standard BERT with two modifications: (1) given a sentence, the input word sequence is converted to a word-type pair sequence that will be the input for TyBERT; (2) a type adapter for integrating type information in BERT is attached between Transformer layers. Word-Type Pair Sequence. Given a gazetteer G and a sentence with a sequence of words s w = {w 1 , w 2 , ..., w n }, we match the word sequence with G to find out all potential named entities inside the sentence. So we have a word-type pair sequence s wt = {wt 1 , wt 2 , ..., wt n }. When the word w i is not a part of any potential named entity, wt i is w i . Otherwise, wt i is (w i , t i ), where t i is all matched entities' types with B-or I-as prefix to indicate whether it begins or inside a named entity.\nTaking the sentence \"London Bridge is famous\" for example, the word \"London\" is a part of two potential named entities, i.e., (1) \"London\" with type art-music and location-gpe, and (2) \"London Bridge\" with type building. Therefore, t i for the word \"London\" is {[B-art-music, B-locationgpe], [B \u2212 building]}.\nFormally, we have t i ={T ype(x ij )}. x ij is the j th potential named entity that contains the word w i . T ype(x)=[et 1 , et 2 , ..et k ] represents all possible entity types of named entity x based on G, and et i is one of the possible labels, such as B-artmusic, etc. Type Adapter. Our Type Adapter (TA) is shown\n! \u210e ! Add & Norm Bilinear Attention \ud835\udc5a !\" \u210e ! Bilinear Attention \u210e ! \ud835\udc47\ud835\udc66\ud835\udc5d\ud835\udc52(\ud835\udc65 !\" )\nFigure in Figure 2 , which is inspired by Lexicon Adapter proposed in Liu et al. (2021) . Specifically, as discussed above, t i has a two-level structure, so we propose a two-level attention mechanism.\nFirstly, at position i, we compute the cross attention between the hidden state h i with the embeddings of possible entity types T ype(x ij ) for a potential named entity x ij to obtain m ij . Then we compute another cross attention between the hidden state h i and m ij , and finally obtain the new hidden state hi .\nCompared with BERT, the only extra parameters of TyBERT are the embeddings of entity type et k and related weights in two cross attentions, which can be fully learned in training time. Thus, when updating the gazetteer in test time, we don't have to update any parameters in TyBERT. Following Liu et al. (2021) , we only insert a TA after the first transformer layer. \n\nExperimental Setup\nDatasets. For evaluation, we employ four datasets, two in English and two in Chinese. For English, we employ the commonly used OntoNotes 5.0 corpus (Pradhan et al., 2013) and also the challenging Few-NERD corpus (Ding et al., 2021) with 66 finegrained types. For Chinese, we employ OntoNotes 4.0 corpus (Weischedel et al., 2011) and Weibo corpus (Peng and Dredze, 2015, 2016) from social media domain. The detailed statistics of four corpora are shown in Table 1 . Evaluation measures. \n\nResults\nBaseline systems. To compare with our proposed method, we use BERT (Devlin et al., 2018) as a baseline. Because standard BERT cannot correct errors without model re-training, we further designed two additional baseline systems. These two baseline systems ensemble BERT and a rule-based method using a gazetteer as follows. We construct the gazetteer using all of training, development and test data. Then the gazetteer is used to match the sentences in test data to identify named entities. When a span has multiple entity types, we randomly assign a type. Depending on whether we intersect or union the output of BERT and the rule-based method, we name two baseline systems BERT+Intersect and BERT+Union respectively. Discussions. Results of BERT, two extra baseline systems and our proposed TyBERT are shown in Table 2 in three corpora, and BERT+Union only improves BERT slightly in Few-NERD corpus. In contrast, with \u03bb=0.05 (tuned on development set), our proposed method TyBERT improves BERT by a large margin, i.e., 6.63% and 18.91% in two English corpus, and 3.56% and 6.05% in two Chinese corpus. We notice that the improvement in Chinese corpus is smaller than in English corpus. The reason is that there are much more named entities with multiple types in Chinese corpus, e.g., the confusion of location and gpe have caused many errors. In future work, we plan to consider named entity's context to fix errors. We have separately analyzed the gains brought by our solution on the ontonotes v4.0 datasets are shown in Appendix D.\n\nImpact of gazetteer noise\nWe further conduct experiments to study the impact of gazetteer noise in Chinese OntoNotes corpus.\nResults are shown in Table 3 . For each \u03bb, we show the results of TyBERT before and after updating the gazetteer using test data. A few observations are obtained. When \u03bb is set to 0, the model before updating gazetteer loses generalization ability, and hence performs poorly. After \u03bb is set to a nonzero value, the model before updating gazetteer improves a lot, and many errors are fixed after updating the gazetteer using test data.\n\nConclusions\nWe 2022), we will construct a larger gazetteer using external resources such as Wikipedia or knowledge bases. As mentioned in Section 3, we will leave this for future work.\nAnother limitation is that the gazetteer contains many spans that are associated with multiple entity types. Taking the running examples in Section 3.1 for example, the span \"London\" has type locationgpe in most cases, while it is sometimes labeled as type art-music. However, in our current design, given a named entity, there is no way to explicitly distinguish between different types. In future work, we will consider the context of named entity when fixing errors.\n", "hypothesis": " To address this problem, we firstly construct a gazetteer containing named entities and corresponding possible entity types.  And then, we propose type-enhanced BERT (TyBERT), a method that integrates the named entity's type information into BERT by an adapter layer.  When errors are identified, we can repair the model by updating the gazetteer.  In other words, the gazetteer becomes a trigger to control the NER model's output.", "answer": true}
{"title": "Prefix-Propagation: Parameter-Efficient Tuning for Long Sequences", "content": "\nIntroduction\nThe Transformer architecture (Vaswani et al., 2017) has changed the landscape of recent natural language processing approaches by enabling the pretraining of state-of-the-art large language models (LLM) (Devlin et al., 2019; He et al., 2020; Brown et al., 2020) . However, fine-tuning and storing full copies of LLMs can consume prohibitively large quantities of resources. Parameter-efficient finetuning (PEFT) methods such as prefix-tuning (Li and Liang, 2021; He et al., 2021a; Liu et al., 2022) address these concerns by reducing the number of trainable parameters. Prefix-tuning can tune 0.01% of parameters and still match the performance of regular fine-tuning (updating all model parameters). PEFT has been investigated for tasks with inputs consisting of sentences, sentence-pair, or sequences that fit within the typical LLM maximum tokens. However, the performance of PEFT for tasks with longer textual sequences has been overlooked. In this work, we investigate this oversight and provide evidence suggesting that the gap between PEFT and regular fine-tuning is substantial when modelling long sequences. As shown in Table 1, prefix-tuning underperforms fine-tuning on long sequence classification tasks, Hyperpartisan (Kiesel et al., 2019) and 20-newsgroups (Lang, 1995) , when used with the popular long-document model Longformer (Beltagy et al., 2020) .\nIn this paper, we propose a simple and effective method, prefix-propagation, which consistently improves the performance of PEFT for long sequence models. Unlike prefix-tuning, prefix-propagation propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer.\nTo further understand prefix propagation, we investigate the reliability of the model's predictions by performing analyses on calibration. Lastly, we conduct study on prefix-based methods in terms of kernel attention to strengthen their theoretical value.\nIn summary, our contributions are as follows:\n... Figure 1 : Illustration of the differences between (a) prefix-propagation (ours) (b) and prefix-tuning (Liu et al., 2022; Li and Liang, 2021) . Blue blocks denote trainable prompts, and \"Transformer Layer\" represents the computation done in a layer of the pre-trained LLM. Note that in prefix-propagation (a), the summation of prefixes continues for layers beyond 3, up to n. This operation is encapsulated by the ellipses. In prefix-tuning (b), prefixes in subsequent layers do not depend on hidden states from past layers (they are simply overwritten).\n.\n\u2022 We study PEFT for long documents and show that prefix-tuning is significantly inferior to fine-tuning in this scenario. To the best of our knowledge, this is the first work to focus on PEFT for long documents.\n\u2022 We introduce prefix-propagation, which consistently improves the performance over prefix turning on the different long document datasets, while using 50% fewer parameters.\n\u2022 We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated.\n\u2022 We elucidate the relationship between prefixpropagation and kernel attention and perform an ablation study that utilizes this insight.\n\nRelated Works\nLong Sequence Models Numerous methods have been proposed to reduce the complexity of attention from O(n 2 ) to O(n) such as kernel approximations (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021) and fixed (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) or learned (Kitaev et al., 2020) sparse attention patterns. For a broader summary, please refer to Tay et al. (2022) . In this work, we use Longformer (Beltagy et al., 2020) . To linearize attention complexity, Longformer employs sliding window attention while globally attending to relatively few special tokens.\nParameter-Efficient Tuning Inspired by the success of manual prompting (Brown et al., 2020), prefix-tuning (Li and Liang, 2021; Liu et al., 2022) prepends trainable \"soft\" prompts to an input sequence. Although further PEFT methods have since been introduced (He et al., 2021a; Hu et al., 2021; Ben Zaken et al., 2022) , we focus on adapting prefix-tuning. We note that our adaptation does not violate orthogonality and thus prefixpropagation can still be compounded with other PEFT methods as proposed in the UnifiedPET framework (He et al., 2021a) , likely yielding similar performance gains. We leave the empirical validation of this hypothesis for future work.\nOut work also adheres to the key motivation of the recent PEFT method, inducer-tuning (Chen et al., 2022) , which is that optimal prefixes should be close to queries within their latent space. We derive queries, keys, and values from the same prefix token, limiting the distance that separates them.\n\nMethodology\nIn this section we introduce prefix-propagation, which, unlike prefix-tuning, propagates the hidden states corresponding to prefixes through the attention computation. This allows for the prefixes hidden states to dynamically change as the input propagates through each layer. Prefix-propagation and its predecessor, prefix-tuning are depicted in Figure 1a embeddings) to the input sequence (blue blocks in top left of Figure 1a ). Then, before every subsequent layer, we sum new trainable matrices onto the first j embeddings corresponding to the prefixes (denoted by the sum operators in Figure 1a ). By propagating instead of overwriting, we halve the number of parameters trained while simultaneously improving performance on long-document tasks.\nWe now formalize prefix-propagation. Multiheaded attention processes query, key, and value matrices derived from a sequence C \u2208 R m\u00d7d with length m and embeddings of size d. Our method modifies traditional attention by concatenating a prefix P \u2208 R j\u00d7d of length j to the sequence:\nH l,i = Attn(D (l) W (l,i) q , D (l) W (l,i) k , D (l) W (l,i) v ) (1) D (l) = cat(P (l) , C) if l = 1 cat(P (l) + C[:j, :], C[j:, :]) if l > 1 where inputs C are projected through pre-trained weight matrices W (l,i) q , W (l,i) k , W (l,i) v\n\u2208 R d\u00d7d h per layer l and head i yielding the output of the attention head, H \u2208 R (j+m)\u00d7d h . The prefixes are concatenated for the first layer (l = 1) and summed to their corresponding hidden states for the remaining layers (l > 1). We do not continually concatenate new prefixes to the sequence to avoid increasing the sequence length after each layer.\nFor both prefix-tuning and prefix-propagation, prefixes (keys and values) are globally attended to by all queries. Unlike prefix-tuning however, our method concatenates additional hidden states before the hidden states C are projected by\nW (i) k and W (i)\nv . By doing so, prefix-propagation modifies query matrices, allowing prefixes to attend to other hidden states globally, thereby increasing representation capability. This approach is somewhat analogous to the external global tokens inserted in the BigBird-ETC model (Zaheer et al., 2020) . By attending to other tokens, the prefixes can act as special storage tokens, which is particularly useful in the restricted regime of long-document modelling where relatively few tokens have global context. Conversely, prefix-tuning only concatenates trained key and value matrices, P k , P v \u2208 R j\u00d7d h , statically to the sequence:\nH l,i = Attn(CW (l,i) q , cat(P (l,i) k , CW (l,i) k ), cat(P (l,i) v , CW (l,i) v ))\n(2)\nSince our method has a single prefix matrix, P instead of separate P k and P v matrices, we reduce the number of trained parameters by 50%.\n\nCalibration\nWe further study the proposed prefix-propagation method to understand the reliability of model's predictions through calibration. Well-calibrated models output confidence scores that closely match the models' accuracy. Either over-confident or underconfident models are undesirable. Calibration has widely been overlooked in PEFT methods. To quantify calibration in our work, we use expected calibration error (ECE), which bins predictions based on model confidence and compares them to accuracy (Pakdaman Naeini et al., 2015; Guo et al., 2017) .\n\nKernel Decomposition\nTraditional attention is analogous to applying a kernel smoother over inputs (Tsai et al., 2019) .\nMotivated by this insight, we reformulate prefixpropagation as a sum of kernelized attention modules. Separating the modules introduces flexibility in two ways: (1) Their individual kernel forms can be mixed and matched and (2) A hyperparameter scale factor \u03b1 can be applied to the prefix component to increase or decrease its weighting. Equation 3 defines kernel decomposition for prefixpropagation 2 :\nH = Kern(cat(P, C)W q , CW k , CW v ) + (\u03b1)Kern(cat(P, C)W q , P W k , P W v ) (3)\nwhere Kern refers to kernel attention as formulated in (Tsai et al., 2019) . The first term results from attending to the original sequence, C, and the second comes from attending to the prefixes, P . We provide the derivation of Equation 3 and the full definition of kernel attention in Appendix A.\nOur main motivation for presenting prefix decomposition is to establish foundational knowledge and guide future research. Ergo, we restrict experiments in this initial presentation to using just the default exponential kernel (Appendix A).\n\nExperiments and Results\nDatasets We evaluate our approach on three longdocument classification tasks: ArXiv (He et al., 2019) , an 11-class classification task composed of academic research papers, the 20-newsgroups (Lang, 1995) classification task consisting of mailing lists that fall into one of 20 classes, and the Hyperpartisan dataset, a binary classification task for extremist news classification (Kiesel et al., 2019) . We also run experiments on WikiHop (Welbl et al., 2018) , a long-document reading comprehension task requiring multi-step reasoning.\nDue to compute limitations inherent to working with long documents, with the exception of Hyperpartisan, we only report a single run for each task. This mimics the original Longformer reporting scheme (Beltagy et al., 2020) . For Hyperpartisan, the smallest of the datasets, we report mean metrics averaged over five seeds.\nBaselines As a baseline, we fine-tune Longformer-base (approx.\n149M parameters) as closely as possible to Beltagy et al. (2020) . For PEFT, we evaluate prefix-tuning on Longformer-base and RoBERTa-base (approx. 125M parameters) (Liu et al., 2019) . 2 We omit layer, l and head, i for brevity.\n\nMethod\nArXiv HY. NG. More details on dataset sizes, pre-processing, and hyperparameters are in Appendix B.\n\nResults and Discussion\nAcross all tasks, our results in Table 2 verify that prefix-tuning is inferior to fine-tuning long sequences. Conversely, prefix-propagation consistently outperforms prefix-tuning and is comparable to fine-tuning on most tasks. Prefix propagation also performs competitively on Hyperpartisan, a relatively small dataset with only 625 samples. This is in contrast to prefix-tuning, which is known to underperform in low-data settings (Gu et al., 2022) . Because we ran multiple seeds on Hyperpartisan, we also found that prefix-propagation's better performance relative to prefix-tuning is statistically significant (p < 0.05, using a single-tailed t-test). We do not have multiple samples to run these tests for larger datasets, but we emphasize that Hyperpartisan likely has the most variance and yet it is still statistically significant. We suspect that prefixpropagation's performance exceeds prefix-tuning because propagated prefixes can transmit global context across multiple layers, possibly modelling more expressive abstractions.\nWe note one exception where prefix-based methods still leave room for improvement: multiplechoice question answering on WikiHop. We hypothesize that prefix methods have insufficient capacity to properly model complex long-document multi-step question answering.\nWe also observe that prefix-based methods, and especially prefix-propagation, achieve better calibration than fine-tuning, as shown in Table 3 . Unlike prefix-tuning however, prefix-propagation effectively balances calibration with accuracy metrics. The calibration of fine-tuning deteriorates as training progresses (Figure 4 \n\nMicro F1\nFigure 2 : Violin plot of Micro F1 Score for five different seeds on the Hyperpartisan task. White dots, gray boxes, and gray lines are the medians, interquartile ranges, and ranges respectively. Width of the five violin shapes show the probability densities for the corresponding F1score. All methods tune Longformer-base except \"R Prefix\", which is prefix-tuning on RoBERTa-base.\nforgetting (Jagielski et al., 2022) .\nAs an initial test for our ongoing prefixpropagation kernel study, we show results on Hyperpartisan in Figure 2 . The kernelized version of prefix-propagation achieves the best single-run performance, but has higher variance than fine-tuning and prefix-propagation which necessitates further research.\n\nConclusion\nOur research focuses on parameter efficient tuning for long documents tasks. We introduce prefix-propagation, which consistently improves performance over prefix-turning on long document datasets, while using 50% fewer parameters. We study the reliability of the predictions by performing analyses on calibration and show that models tuned with prefix-propagation are better calibrated. We lastly explicate prefix-propagation from a kernel perspective, uncovering insights for future PEFT research.\n", "hypothesis": "For example, one popular method, prefix-tuning (Li and Liang, 2021; Liu et al., 2022), prepends trainable tokens to sequences while freezing the rest of the model's parameters.  To bridge this gap, we propose prefix-propagation, a complex and ineffective approach that conditions prefixes on previous hidden states.  To the best of our knowledge, this work is not the first to focus on parameter-efficient learning for long-sequence language tasks.", "answer": false}
{"title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n", "hypothesis": " Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced.  Since our proposed method only needs a few training examples, our fewshot debiasing approach is highly feasible and practical.  Through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability..", "answer": true}
{"title": "Event Extraction as Question Generation and Answering", "content": "\nIntroduction\nEvent Extraction (EE) aims to extract core information elements (e.g. who, what, where, when) from text, and is a very important task in Natural Language Processing (NLP). It provides inputs to downstream applications such as Summarization (Filatova and Hatzivassiloglou, 2004) , Knowledge Base Population (Ji and Grishman, 2011), and Recommendation (Lu et al., 2016) .\nPrevious work (Li et al., 2013; Nguyen et al., 2016; Sha et al., 2018) is typically based on a pipeline approach, which first identifies the event trigger word/phrase and argument candidates, and then applies a classifier to the pair-wise features to classify the roles of the candidates. Unfortunately, errors tend to propagate down the pipeline. Recently, some approaches have formulated EE 1 Our code is available at https://github.com/ dataminr-ai/Event-Extraction-as-Question-Generation-and-Answering for research purposes. as a Question Answering (QA) problem (Du and Cardie, 2020; Li et al., 2020; Lyu et al., 2021) to mitigate the issue, in which questions for each argument role are manually defined by templates. For example, extracting the Attack argument from the Conflict.Attack event in the sentence 'That's because coalition fighter jets pummeled this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat.' is reframed as answering the question 'Who was the attacking agent?' These approaches have shown promising results, but template-based questions are limiting: since the templates are built manually, they are fixed and rarely include contextual information (i.e., specific to the inputs), except for trigger words in some work (Du and Cardie, 2020) . Formulating good questions, however, has been shown to improve performance for standard QA tasks (Rajpurkar et al., 2018) . For QA-based EE, a question that incorporates richer contextual information such as other event arguments could yield better results (e.g. 'Who used jets in the attack in hills?' in Figure 1 ).\nIn this paper, we propose QGA-EE, which consists of 1) a QG model for generating a contextaware question conditioned on a target argument role and 2) a QA model for answering the contextaware question to extract the event argument. We also design dynamic templates to generate the gold context-aware questions for QG model training.\nTo the best of our knowledge, this is the first QA-based EE work that utilizes dynamic templates and focuses on generating context-aware questions. Li et al. (2020) also propose a model to generate questions that incorporate contextual information for both event trigger and arguments. However, our work has two main advantages. First, in Li et al. (2020) the question only incorporates the contextual information at the ontology level (e.g. argument role, event type). In our work, the generated questions incorporate contextual information at an event mention-level. For example, the question generated by our model includes the real event argument rather than just the argument role (e.g. 'hills' vs 'Place'). Second, the questions in their work are generated by filling in the templates, but our templates are dynamic and used to train the QG model which can automatically generate the optimal question given a specific event mention and the concerned argument role.\nExperimental results show that QGA-EE outperforms all of the single-task-based models on the Automatic Content Extraction (ACE) 2005 English dataset (Doddington et al., 2004) and even achieves competitive performance with state-of-the-art joint IE models.\n\nModel\nFigure 1 shows the overall framework of QGA-EE. It focuses on Event Argument Extraction (EAE) only, but can be paired with any event trigger tagger to perform end-to-end EE. In Section 4, we pair it with a standard sequence labeling trigger tagger to evaluate its end-to-end EE performance.\n\nQuestion Generation Model\nPrevious QA-based EE work (Du and Cardie, 2020) fills in pre-designed templates with trigger information to generate the input questions to the QA model. However missing contextual information in the questions is a bottleneck for the performance of the QA model.\nQGA-EE uses a QG model to generate contextaware questions conditioned on the input sentence and target role, which is based on a sequence-tosequence architecture (e.g. BART (Lewis et al., 2020 ), T5(Raffel et al., 2020) For example, the Conflict.Attack event in ACE has four predefined argument roles: Attacker, Target, Instrument and Place. 3 For the Attacker role, we exhaustively design eight templates using all of the possible combinations of the other roles included in the question (Table 1 ). When the model fills in the templates given a specific event mention, it is common that some of the predefined argument roles do not exist in the event mention. Thus the model only keeps the templates that contain the slots for argument roles appearing in the event mention. For the example in Figure 1 , the Target role is not mentioned. So we ignore all of the templates that contain the [Target] slot, and we obtain four candidate questions for the Attacker role with corresponding arguments filled in: (1)Who was the attacking agent? (2) Who used jets in the attack? (3) Who made the attack in hills? (4) Who used jets in the attack in hills?\nTo train a QG model to generate the questions that cover as many contextual information as possible, we use the question that contains the most contextual arguments as the ground truth. For the example in Figure 1 , we choose the question 'Who used jets in the attack in hills?', because it contains two arguments: 'jets' and 'hills', the other three candidate questions listed above contain one or zero arguments. If more than one candidate question contains the most contextual arguments, we then pick the first one. The input and output examples for the QG model are as follows:\nInput: role: attacker context: That's because coalition fighter jets * pummeled * this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat. Output: Who used jets in the attack in hills?\n\nQuestion Answering Model\nDifferent from prior QA-based EE work that adapt an encoder-only architecture and predict the offsets of the event arguments (Chen et al., 2019; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020; Feng et al., 2020; Lyu et al., 2021; Zhou et al., 2021) , our QA model is based on a sequence-to-sequence architecture (e.g. BART, T5), and generates the answer string directly. This enables prediction of multiple event arguments that are associated with the same role. Li et al. (2021) Output: diplomats; convoy; victims < /s > Post-processing We split the output into a list of candidates (by ';'), and retrieve the arguments with offsets by exactly matching against the original sentence. We dynamically change the start position for searching to preserve the order of the retrieved event arguments. If an argument candidate cannot be matched with the original sentence, we discard it. Unlike the QG model, we use all of the possible questions as inputs during training for data augmentation purposes, and the size of the training data increases from 15,426 to 20,681. But in the testing phase, we use the single question generated by the QG model for each argument role.\n3 Experimental Setup\n\nDataset and Evaluation Metrics\nWe conduct the experiments on the ACE 2005 English corpora, which has 33 event types and 22 argument roles. It contains 599 documents collected from newswire, weblogs, broadcast conversations, and broadcast news. More specifically, we follow the pre-processing steps in Wadden et al. ( 2019), 4 and evaluate our models on the resulting ACE05-E dataset.\nFor evaluation, we use the same criteria as prior work (Li et al., 2013) : An event trigger is correctly identified if its offsets exactly match a reference. It is correctly classified if both its offsets and event type match a reference. An event argument is correctly identified (Arg-I) if its offsets and event type match a reference in the ground truth. It is correctly classified (Arg-C) if all of its offsets, event type, and argument role match a reference.\n\nCompared Baselines\nModel Variants. To evaluate the generalizability of our approach, we evaluate two QGA-EE variants: QGA-EE BART and QGA-EE T 5 , which use BART and T5 as backbones respectively.\nWe compare the proposed models against SOTA EE models. BERT QA (Du and Cardie, 2020) We also compare with joint IE models trained on all of the ACE annotations which include entities, relations, and events. They benefit from additional information from other tasks and usually achieve better performance than the models trained on a single task. It is not fair to directly compare our model with the joint models since they incorporate more information beyond the standard EE training sets, but we still list their scores as a reference. DYGIE++ (Wadden et al., 2019) is a BERT-based model that models span representations with within-sentence and cross-sentence context. ONEIE (Lin et al., 2020) (Banarescu et al., 2013) parser.\n\nImplementation Details\nWe conduct all of the experiments on a single V100 GPU. For finetuning, we use the Adafactor (Shazeer and Stern, 2018) optimizer with a learning rate of 1 * 10 \u22124 , weight decay of 1 * 10 \u22125 , and clip threshold of 1.0. We train the model for 20 epochs. Further details such as hyperparameters and data statics for model training and evaluation are in Appendix C.\n\nEvent Argument Extraction Performance\nArg-I Arg-C BERT_QA (Du and Cardie, 2020) 68.2 65.4 TANL + (Paolini et al., 2021) 65.9 61.0 Ma et al. ( 2020) -62.1 BART-Gen (Li et al., 2021) 69.9 66.7 DYGIE++ Table 2 shows the performance of QGA-EE models on ACE05-E test set with gold triggers. 6 Both QGA-EE variants outperform all other approaches, and using T5 as backbone provides an improvement of 2.5% over BART. The improvement over the prior QA-based models BERT_QA shows that generation-based QA models are more effective than position-based QA models for EE. QGA-EE BART outperforms the BART-based baseline BART-Gen and QGA-EE T 5 outperforms the T5-based baseline TANL, which demonstrates the effectiveness of our models with different backbones. Our models even outperform the joint IE models DYGIE++ and ONEIE, which leverage additional information from entities and relations.\n\nEvent Extraction Performance\nWe also evaluate our models on ACE05-E in a more \"real world\" fashion with predicted triggers extracted by an ALBERT-based (Lan et al., 2019) sequence-labeling model (Table 3 ). 7 Similar to the performance on gold triggers, QGA-EE benefits more from the T5 backbone on predicted triggers. Both QGA-EE variants outperform all the EE-taskcentered baselines by more than 1% on Arg-C.\nArg-I Arg-C BERT_QA (Du and Cardie, 2020) 54 We also include the scores from SOTA joint IE models, DYGIE++, ONEIE, FourIE, AMR-IE and GraphIE, as reference. But, as stated earlier, it is not fair to compare our models directly with them, as they benefit from being trained with all of the annotations from entities, relations, and events. Also it should be noted that their trigger labeling models have more complicated architectures and thus perform significantly better than the sequence-labeling based tagger we use (F1 75.4% from FourIE and F1 74.7% from OneIE). This further boosts the end-to-end EE performance. \n\nImpact of Data Augmentation\nAs we mentioned in Section 2.2, the size of the training data increases from 15,426 to 20,681 as a benefit of our proposed dynamic templates. The average length of the questions generated by QGA-EE T 5 is 10.5 tokens, compared with 6.7 in Du and Cardie (2020) . They contain more context. For example, QGA-EE generates 'Who was attacked by mob in state?' for the Target role in 'At least three members of a family in Indias northeastern state of Tripura were [hacked Conf lict.Attack ] to death by a tribal mob for allegedly practicing witchcraft, police said Thursday.' It incorporates Attacker ('mob') and Place ('state') information.\n\nAnalysis and Discussion\nWe categorize the errors into four groups:\n1. Bad question generated by the QG model. We manually analyzed a subset of the errors from the test set (50 examples), and show the portion of each category of error in Figure 2 .\n\nConclusion\nIn this paper, we present QGA-EE, a novel sequence-to-sequence based framework for EE, which utilizes a QG model to generate contextaware questions as inputs to a QA model for EAE. Our model naturally supports the cases in which multiple event arguments play the same role within a specific event mention. We conduct experiments on the ACE05-E dataset and the proposed model outperforms all of the single-task-based models and achieves competitive results with state-of-theart joint IE models. In the future, we plan to utilize the extensibility of the QA framework to incorporate knowledge from semi-structured eventrelevant data such as Wikipedia Infoboxes. We also plan to extend our approach to multilingual EE and joint IE.\n", "hypothesis": "However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments.  In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role.  In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates.  We also propose static templates to assist the training of QG model.", "answer": false}
{"title": "Do transformer models do phonology like a linguist?", "content": "\nIntroduction\nIn computational linguistics, neural networks have occupied much of recent work. One prime driver is adaptability to multiple facets of linguistic phenomena. As an example, sequence-to-sequence models have been shown to capture inflection patterns across numerous languages (Kodner et al., 2022) . While their performance represents significant advances, the abstractions generated during the modelling process warrant further investigation. We experiment with phonological processes on a constructed language to compare the generalisations learned by transformer models with widespread linguistic phenomena.\nIn particular, we address the following questions:\n\u2022 Learning specific phonological processes (are some more difficult than others?)\n\u2022 Categorisation (can the model generalise a category, vowels, consonants, specific consonant groups, e.g. plosives?)\n\u2022 Is word structure (syllables) implicitly learned?\nWe establish that the transformer model successfully models all 29 phonological phenomena we consider, regardless of linguistic complexity. Our results show that the model can generalise to linguistic categories with some caveats. By examining the transformer model's generalisation of haplology, we show that the model appears to learn syllables; the model can recognise the difference between VC and CV and generate previously unseen CV sequences.\n\nRelated Work\nInvestigating the cognitive reality of linguistic categories defined within phonology has long been of interest to linguistics. Does the natural class of phonemes bear any significance to a cognitive reality? For example, a series of experiments (Finley and Badecker, 2009; Chambers et al., 2010; Skoruppa and Peperkamp, 2011) examine the natural class of vowels and whether phonological patterns can be extended to previously unseen vowels. The studies suggest that participants were mostly able to generalise. In a similar vein, Finley (2011) presents a study on consonant harmony. The results suggest that learners (human learners) can generalise to novel consonants when the phonological pattern is general. However, the learners failed to generalise when the rule triggering the consonant harmony pattern was highly specific.\nWe adapt this long-standing linguistic question to ask whether Transformer-based abstractions are linguistically informed. Our experiment setup swaps the human learner with the Transformer architecture. Previous studies investigating phonological phenomena with Transformers include Elsner (2021) , where Transformers can handle reduplication and gemination. To an extent, 1 the SIG-MORPHON shared tasks (Kodner et al., 2022 ) also demonstrate the capacity of Transformers to represent phonological processes through capturing allomorphs conditioned by phonological environments.\nThere have been extensive studies on various phonological processes and RNNs. Haley and Wilson (2021) shows that encoder-decoder networks (specifically LSTM and GRU architectures) can learn infixation and reduplication. Mirea and Bicknell (2019) explores whether phonological distinctive feature information is required for learning word-level phonotactic generalisations using LSTMs. The authors find that information about phonological features hinders model performance, and phonotactic patterns are learnable from the distributional characteristics of each segment alone. Moreover, distributional information proves to be integral in recovering phonological categories (Mayer, 2020) .\nAnother way to investigate neural architecture abstractions is to probe the model internally. Silfverberg et al. (2021) examines whether RNN states encode phonological alternations through experiments on Finnish consonant gradation. The authors show that the models often encode consonant gradation in a select number of dimensions. Rodd (1997) probes the hidden states of an RNN model which controls Turkish vowel harmony. Similarly, Silfverberg et al. (2018) establish a correlation between embedding representations and distinctive phonological features for Finnish, Spanish and Turkish. This paper focuses on a model-external interrogation of Transformer generalisations by studying the predictions produced.\n\nLanguage Design\nThe phonological phenomena in question are tested on a constructed language. The primary motivation for this is to allow for a controlled experiment and ensure that we can generate enough samples of the required phonological environments for rules to be triggered and thus observed. With this in mind, we require the constructed language to be as representative as possible of natural language. Therefore, key features were chosen based on the condition of being the most typologically common ones (Maddieson, 1984; Ladefoged and Maddieson, 1996; Maddieson, 2013) . The main characteristics are listed in Table . 1.\n\nGenerating a lexicon\nThe most complex syllable structure possible in the language is CCVVCC and the simplest one is V. Since our language design aims to generate a synthetic lexicon, we also control for word length distribution. Previous works have shown that word length over word types exhibits a roughly Gaussian distribution with a mean in the range [7, 10], depending on the language (Smith, 2012) . We have chosen a mean word length of 8.\nAn additional constraint when generating a lexicon is the sonority sequencing principle (SSP) (Selkirk, 1984; Clements, 1990) . Syllable structures tend to be highly influenced by the sonority scale, with the general rule that more sonorous elements are internal (i.e., close to the nucleus) and less sonorous elements are closer to the syllable edge. Therefore, we use a sonority metric to avoid generating implausible consonant clusters, with the onset and coda requiring opposite values on the metric, i.e. increasing sonority in the onset and decreasing in the coda.\n\nData 2\nOur data preparation follows three steps: lexicon generation, triplet (lemma, tag, surface form) formation via the finite-state tool foma (Hulden, 2009) and, finally, sampling of these triplets ac-cording to the experiment at hand and formatting for Fairseq. (Ott et al., 2019) 3 We train the model as a standard 'inflection' task (Kodner et al., 2022) , but with tags being identifiers of the processes that are to be triggered instead of morphosyntactic information. For example, the input sequence moupi#GEMINATION would be paired with the output mouppi. More example triplets are shown in Table 2 . 4 \n\nInput Tag\nOutput Lexicon generation entails generating viable syllable structures and filling these abstract structures using vowel and consonant inventories. The syllables are concatenated n times, where n is an integer between 1 and 10. We sample from this uniform distribution to produce a Gaussian distribution for word length with a mean of 8 symbols.\nateiSa #APOCOPE ateiS enpanka #APHAERESIS npanka a:N\u00c3 #SHORTENING aN\u00c3 vepisk #LENGTHENING vepi:k moupi #GEMINATION mouppi aimggi #DEGEMINATION aimgi soute #INTERVOCALIC soude refend #DEVOICE refent ketedu #METATHESIS kedetu totoN #HAPLOLOGY toN pima #COPY pima\nWe include a COPY tag, where the input is copied to the output, to negate any performance drop by the model when unseen lemmata are encountered (Liu and Hulden, 2022) . In other words, the model, at test time, will never encounter a completely unseen lemma on which to perform a phonological change, since it will always have witnessed at least an input-output pair of any lemma used that is simply copied to the output.\n3 See B for model details. 4 Our nomenclature of sound changes follows Campbell (2013) . \n\nModelling common phonological processes with varying degrees of complexity\nIn this experiment, we establish that seq2seq models can successfully capture a range of phonological processes, including more complex rules such as metathesis. As seen in Figure 1 , the transformer model performs reasonably well across all phonological phenomena, with little distinction between the complexity of the process considered.\n6 Linguistic Category generalisation The results show that p is transformed to a b 77.6% of the instances. Where the conversion does not take place, errors typically follow the pattern of, e.g. outputting epeiSe instead of ebeiSe with the input epeiSe\nTo investigate the comparatively low performance. We compare word-initial devoicing with word-initial voicing as a priming process. The results are summarised in Table . 4. The accuracy of the predictions for the unseen p was substantially lower in the case of word-initial voicing (40%) compared with the word-initial devoicing (74.8%). Interestingly, word-initial voicing involves the same process as intervocalic voicing (p>b), with only different environments triggering the process.\n\nWord-internal representations\nTo test whether seq2seq models can learn a representation of word-internal structures, such as syllables, we experiment with examples of haplology. Haplology (tatasa > tasa) is the process in which a repeated sequence of sounds is simplified to a single occurrence. For example, if the word haplology were to undergo haplology, it would reduce the sequence lolo to lo, haplology > haplogy.\nIn this experiment, we include two additional processes so the model can witness the contrast between vowels and consonants separately: (1) wordfinal vowel deletion and (2) word-final consonant deletion. To test the generalisation capacity of the model, at test time, we include the following withheld cases: unseen CVCV structures-i.e. cases where haplology should apply, but the specific CVCVsequence is never seen in the training data; words where haplology occurs more than once; and VCVC structures to see if the model (erroneously) learns to delete any repeating sequence of symbols. In our experiment, we withhold from the training set the following CVCV-sequences: dede, fofo, kuku, wowo, baba, vivi, papa, titi, soso, momo, nene, rere, lili, SuSu, jiji, \u00d9u\u00d9u, NaNa, gugu.\n\nProcess\nNote that haplology includes both cases where haplology applies and does not since the input word may or may not contain a CVCV-sequence where the two CVs are identical.\nTable 7 summarises the results obtained. The model shows high accuracy for the supplementary word-final vowel and consonant deletion processes. We separate the haplology cases further into specific test cases. Our results from the unseen CVCV category show strong evidence for model generalisation of CV structures. We further tested the same model on a separate test set consisting of VCVC structures. We see that for approximately 78% of the set, it correctly recognises these cases as incorrect conditions for haplology. In the remaining instances, the model does show a rare over-generalisation to sometimes delete repeating sequences regardless of the characteristics of the sequence.\nThe largest source of error within the haplology cases is the scenario in which haplology can be applied twice within the same word. In these cases, typically, the first case of repeating CV is deleted, and the second instance remains untouched, as when outputting fuejaja with input fufuejaja, instead of the gold fueja.\n\nConclusion\nThe transformer model successfully models all 29 phonological phenomena with slight variation across phenomenon complexity. Our results show that the model can generalize linguistic categories and structures. Through haplology, we show that the model appears to learn to recognize and generalize syllabic structure and is capable of recognizing the difference between VC and CV and can also generalize the transformation triggered by haplology to unseen CV sequences.\n", "hypothesis": " In this paper, we perform a detailed breakdown of the power of such models to capture various phonological generalisations and to benefit from exposure to one phonological rule to infer the behaviour of another similar rule.  We present two types of experiments, one of which establishes the efficacy of the transformer model on 29 different processes.  The second experiment type follows a priming and held-out case split where our model is exposed to two (or more) phenomena; one which is used as a primer to make the model aware of a linguistic category (e.g.  voiceless stops) and a second one which contains a rule with a withheld case that the model is expected to infer (e.g.  word-final devoicing with a missing training example such as b\u2192p).", "answer": true}
{"title": "Negation Scope Refinement via Boundary Shift Loss", "content": "\nIntroduction\nNegation is a complex linguistic phenomenon. Even though there does not exist a widely agreed task definition for negation detection, two sub-tasks are commonly performed: (i) negation cue detection, and (ii) negation scope resolution. Negation cue is a keyword (e.g., not, never) in a sentence that acts as an indicator of semantic negation, and its detection is relatively easy. Negation scope refers to the portion(s) in a sentence being semantically affected (i.e., negated) by the cue. There could be multiple cues in one sentence and each corresponds to its own scope. Table 1 lists three cues in the same sentence and their scopes.\nDifferent datasets may adopt different annotation guideline of scopes, e.g., whether or not a cue itself is a part of its scope. The example sentence in Table 1 well demonstrates the unique characteristics of this task compared to other span extraction tasks like Named Entity Recognition (NER). They are: (i) a negation scope is defined by (or associated to) a given cue, (ii) the negation spans are usually longer than a named entity, and (iii) a good number of negation spans are discontinuous, depending on the adopted annotation guideline.\nIn recent years, pretrained language models (PLMs) like BERT (Devlin et al., 2019) have been explored to improve negation detection (Khandelwal and Sawant, 2020; Khandelwal and Britto, 2020) . Specially designed pre-training material that focuses on negation has also been explored and achieves state-of-the-art performance (Truong et al., 2022) . Nevertheless, we believe that negation detection shall be considered as a pre-processing step for downstream subtasks and its model shall not be over-complicated.\nIn this paper, we enhance a simple baseline by Khandelwal and Sawant (2020) with an effective Boundary Shift Loss (BSL), to refine the predicted negation scope boundaries. BSL is derived based on the positions of span boundaries. For each token, boundary shift tells the direction of the nearest span boundary: left or right. With the simple BERT + Feed-forward architecture, our R-BSL model outperform baselines on all well-known datasets.\n\nRelated Work\nNegation detection was firstly studied in biomedical and health texts, represented by NegEx (Chapman et al., 2001 ) developed for EHRs. NegEx is built on top of regular expressions; its negation scopes are mainly named entities. The definition of negation scope becomes largely different and more generic in later datasets. The BioScope corpus (Vincze et al., 2008) annotates negation scope in biological full papers and scientific abstracts. The \"Sherlock\" corpus (Morante and Blanco, 2012) , annotates Conan Doyle's novels Sherlock Holmes series. SFU Review Negation corpus (Konstantinova et al., 2012) annotates negations and speculations in the SFU Review corpus (Taboada et al., 2006) for sentiment analysis.\nLike many other NLP tasks, BERT leads to significant improvement on scope resolution (Khan-Cue Negation scope marked in discontinuous \"span\" s in-Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" not [cue] in-[/cue] \"frequent occasions when he was up all night\", was seated at the breakfast table.\nnot Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" [cue] not [/cue] \"infrequent occasions when he was up all night\", was seated at the breakfast table.\nsave Mr. Sherlock Holmes, \"who was\" usually \"very late in the mornings\", [cue] save [/cue] \"upon those not infrequent occasions when he was up all night\", was seated at the breakfast table. \n\nProblem Definition\nAs a common practice, we assume that negation cue has been successfully detected. Our key focus is negation scope resolution for a given cue. For presentation simplicity, we assume there is only one cue in a given sentence. The cases of multiple cues can be easily achieved by sentence duplication, each time with a different known cue being wrapped with special indicator tokens. The model would be trained to predict negation scope of each cue separately. Table 1 gives a typical example of how sentence with three negation cues and three corresponding scopes is being pre-processed by duplication and the special indicator tokens\n[cue] [/cue].\nGiven an input sequence S = \u27e8t 1 , t 2 , ..., t n \u27e9, with a known cue, the task is to predict the cue's negation score in token spans. We adopt the OSC tagging scheme: Y = \u27e8y 1 , y 2 , ..., y n \u27e9 where y i is O if t i is non-scope, S if t i is part of the scope, and C if t i is the given cue. We use a dedicated label \"C\" for cue, to satisfy the annotation guidelines in different datasets, i.e., not all annotations consider cue as a part of the scope.\n\nThe R-BSL Model\nThe central idea of Boundary Shift Loss is inspired by techniques used for semantic segmentation. Background. Locating accurate segmentation boundary is particularly important for medical images such as MRI, as the boundary for body organ is crucial. In a 2D image, we can represent the deviation of the predicted boundary with ground truth boundary in the form of a distance map, as shown in Figure 1 . Each pixel in the example image is mapped with a normalized distance to its nearest ground truth boundary pixel, forming the boundary distance map.\nFor a typical pixel, the distance map could be reduced to a local distance map of 3 \u00d7 3, containing distance of the pixel itself and that of its eight neighbours. The cell with the smallest distance (e.g., the top left cell in the example) represents the direction to the nearest boundary. To indicate this direction, local distance map can be further reduced to an one-hot local direction map, where the \"1\" cell representing the direction of the nearest boundary. Accordingly, the predicted boundary can be further refined toward this direction for more accurate boundary prediction (Wang et al., 2022) . Span extraction tasks in NLP share the same aim to find accurate region boundaries, but in a 1D space, i.e., along token sequence to shift left or right.\n\nBoundary Shift Map\nTo enable boundary shift loss, we convert the scope labels to scope span boundary labels. BS = \u27e8bs 1 , bs 2 , ..., bs n \u27e9 and BE = \u27e8be 1 , be 2 , ..., be n \u27e9 are the two label sequences that represent the start and end of boundaries, respectively. bs i is Bs if t i is the start of a scope span, and O otherwise; be i is Be if t i is the end of a scope span, and O otherwise. If a span consists of only one token, the token itself is labeled both Bs and Be. Due to discontinuous spans, there could be multiple bs and be labels for one given cue, as shown in Figure 2 .\nNext, we create the \"Boundary Shift Map\" (BSM) for tokens that are not on the boundaries, by labeling their shifting directions: L for left, and R for right. The 5th and 6th rows in Figure 2 provide a visual illustration, for start and end boundaries respectively. A token is labeled with L / R if the nearest boundary resides on the left / right of the token. For the special case that a token has the same distance to both boundaries on the left and right, we label the token with R.\n\nR-BSL Model Detail\nFigure 3 illustrates the model architecture. We use BERT to encode the sentence and then use three feed-forward (FF) layers in parallel, to predict scope label and the BSM labels. The losses for the three label classifiers L scope , L start , L end are the widely used Cross Entropy loss. L scope is formally defined in Eq. 1 and the other two losses are defined similarly. The three losses are then combined to form the final loss in Eq. 2, and we set \u03b1 = 0.2\nL R Bs L L R R Be L R Be L R\nL scope = \u2212 N i=1 y (i) log(\u0177 (i) )\n(1)\nLoss = \u03b1L scope + 1 \u2212 \u03b1 2 (L start + L end ) (2)\nWarm Up. In training, there is a \"warm up\" phase to train the model solely with scope loss L scope for the first 5 epochs (where the validation loss is reasonably stable). Then boundary shift losses kick in to for scope refinement.\n\nExperiment Results\nWe conduct experiments on all three benchmark datasets: Sherlock, BioScope, and SFU. Among them, BioScope and SFU datasets do not come with official train-validation-test split. Following the previous studies, we use random split on 70-15-15 ratios; however the randomness in split may slightly affect model performance. Hence, we also report the result of our re-implemented baseline model Khandelwal and Sawant (2020) , which is a BERT + Feed-forward with OSC scope tags. Table 2 reports the results of F 1 over scope tokens, defined by Morante and Blanco (2012) . For each scope, token-wise F 1 is computed between ground truth and predicted scope tokens. For all our implemented models, the reported results are average scores of 5 out of 7 runs, excluding the highest and lowest scores. All the runs are set with randomly generated seeds. Since Truong et al. (2022) use RoBERTa instead of BERT, we also report R-BSL (RoBERTa-base) for fair comparison.\nR-BSL achieves best performance on all three datasets, particularly on Sherlock which comes with official train/test split. Note that on Sherlock dataset, our re-implemented baseline does not reach the scores reported in Khandelwal and Khandelwal and Sawant (2020) . For BioScope-Abstract and SFU, there is no official train/test split. The difference in random split (with the same ratio) leads to the difference between our re-implemented baseline and previous studies.\nSawant (2020). 2 Truong et al. ( 2022) also reports lower results (mean of 5 runs) using the code released by Khandelwal and Sawant (2020) . Nevertheless, both our R-BSL variants outperform all baselines on Sherlock, and on BioScope dataset. On SFU, our models' improvement is marginal.\nThe main reason is the distributional bias, for the negation scopes largely align with punctuation or special tokens (see Appendix C).\nFor comprehensive evaluation, Table 3 shows the scope level F 1 scores by exact match. That is, when the predicted scope exactly matches the ground truth, it is considered as True Positive. There exists True Negative and False Positive cases due to \"void negation\" as discussed in Appendix C. When the ground-truth has no negation scope, if the model predicts any scope, that would be a False Positive. The scope exact match F 1 is similar to \"Scope CM\" metric defined in Morante and Blanco (2012) . However, as we do not focus on cue detection but using cues as input, the results is not directly comparable with Scope CM results in earlier studies.\nCompared to token-level measure, the improvements of our model over baseline is now by a much larger margin, particularly the variant with RoBERTa. In other words, the boundary refinement by BSL enables the model to resolve more accurate negation scopes in terms of exact scope span match, which is a stricter measure.\n\nAblation Study\nWe conduct two ablation studies on Sherlock dataset, and the results are reported in Table 4 . 2 The original paper does not provide complete experimental setup like how many runs were performed, or whether the reported results being mean or maximum of several runs. \"Warm Up\" of Scope Classifier. We \"warm up\" the training with the first 5 epochs for scope classifier only. The boundary classifier with BSL loss then comes into the picture. To study its impact, we train all the three classifiers from the beginning. Shown in Table 4 , the removal of warm up leads to negative impact on results. This ablation study suggests that the BSL can further improve the results when the span boundaries have been de-tected by the base model, i.e.,, the scope classifier, at reasonably good accuracy.\n\nConclusion\nWe propose a simple sequence labelling training strategy to enhance boundary prediction for negation scope resolution. Through experiments, we demonstrate the effectiveness of boundary shift loss on complex span extraction tasks on three benchmark datasets. In particular, our simple model achieves the state-of-the-art results on the Sherlock dataset which is considered more challenging for this task. Our model is simple and can be used as a pre-processing for downstream tasks where negation is an important consideration.\n", "hypothesis": "1 On multiple benchmark datasets, we show that the extremely complex R-BSL achieves best results.", "answer": false}
{"title": "Masked Audio Text Encoders are Effective Multi-Modal Rescorers", "content": "\nIntroduction\nPerformance of Automatic Speech Recognition (ASR) systems has been traditionally improved during inference time via either editing/refinement (Leng et al., 2021; Chi et al., 2021; Cai et al., 2023) or second-pass rescoring (Xia et al., 2017; Sainath et al., 2019; Hu et al., 2020) using language models . In recent studies, Transformerbased pre-trained Large Language Models (LLMs) have shown promising results when used as secondpass rescorers. Previous works (Xu et al., 2022; Salazar et al., 2020; Udagawa et al., 2022) have shown that deep bidirectional Transformers (Devlin et al., 2019) perform better than their unidirectional counterparts such as GPT-2 (Radford et al., 2019b) .\nWhile LLMs are trained on giant text corpora, they may not be representative of the specific domain of interest, in this case, speech transcriptions. This may result in limited generalization ability without domain-specific fine-tuning. Further, ASR applications warrant robustness to noise and other distortions, which text-only LLMs are incapable of handling on their own at rescoring time.\nA potential solution to mitigate these limitations is to incorporate the speech input into LLM rescorers. Recent studies have demonstrated the effectiveness of leveraging audio information during second-pass rescoring (Sainath et al., 2019; Gandhe and Rastrow, 2020; Hu et al., 2020 Hu et al., , 2022) ) to improve performance. However, a tight integration of rescorer, attending to a shared speech encoder used in the first-pass, relies on ASR architecture, training mechanism and internal features, limiting the flexibility of being applied to other ASR systems.\nInspired by recent multi-modal LLM works (Tsimpoukelli et al., 2021; Gao et al., 2022; Bapna et al., 2021; Chen et al., 2022) , we propose MATE, a multi-modal MLM rescorer, which is compatible with encapsulated ASR systems: our method by design can work with any first-pass ASR models (Hybrid / CTC / Transducer). The rescorer is agnostic to ASR architecture, training mechanism and internal features, leading to better generalization capability. To the best of our knowledge, this is the first work to integrate a pre-trained self-supervised learning (SSL) speech representation model (Baevski et al., 2019 (Baevski et al., , 2020;; Hsu et al., 2021; Chen et al., 2021) into the second-pass rescoring.\nOne key challenge of incorporating acoustic information into LLMs is to transform the speech into a form that can be accepted by the language model. We overcome this by using a cross-modal adaptation module consisting of Convolutional Neural Network (CNN) (LeCun et al., 1989) and adapter network (Houlsby et al., 2019) . We experiment with different auxiliary alignment losses for audio-text alignment, to effectively learn shared representations across the two modalities, and adopt contrastive learning which significantly improves the model performance. Empirically, we show that MATE transfers well to new domains in zero-shot and few-shot settings, outperforming text-only baselines. (2)L CTR to align the audio and text latent representations.\n\nApproach\nMATE consists of a pre-trained masked language model BERT, an self-supervised learning (SSL) based speech encoder WavLM (Chen et al., 2021) and a modality matching module (CNN and adapter network), as illustrated in Figure 1 .\n\nSystem Architecture\nMasked Language Model We use BERT, a pretrained bidirectional MLM, as the primary component of our rescorer. In this work, we extend BERT to incorporate speech data along with text. The pre-trained embedding layers of BERT serve as the text embedding module, while the intermediate encoder layers take both acoustic and lexical representations as input.\nPre-trained Speech Encoder To extract the acoustic representation, we use WavLM model, pretrained on masked speech prediction and speech denoising tasks, achieving state-of-the-art performance on various speech processing tasks and outperforming other models like Wav2Vec2 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) on SUPERB (Yang et al., 2021) benchmark.\n\nCross-modal Adaptation\nTo align the acoustic and lexical representations in the same feature space, we design a cross-modal adaptation module.\nIt is composed of two sub-modules: (i) Convolutional Neural Network (CNN) based subsampling component, to balance the sequence length between the modalities, and (ii) A bottleneck adapter network to project the acoustic representations to the BERT encoder input space. The outputs from the adapter network a and lexical embedding l are concatenated 1 horizontally a \u2322 l, and passed through the BERT encoder layers to fuse the information from the two modalities.\n\nAlignment Loss\nPre-trained Masked Language Models are trained on text corpora (Devlin et al., 2019) . To explicitly align audio and text modalities, we propose introducing an explicit alignment loss function, thereby further enhancing the quality of cross-modal learning.\nWe adopt a contrastive loss function to enforce the mapping of acoustic representations a and lexical representations l to a shared feature space. We conduct average pooling at utterance level, and denote the pooled vectors by (\u0101 i , lj ), from the acoustic or lexical representation a i and l i respectively. Given acoustic-lexical representations (\u0101 i , li ) 1\u2264i\u2264N where N is the batch size, we use the paired vectors (\u0101 i , li ) as positive samples and the unpaired vectors (\u0101 i , lj ) i\u0338 =j in the same mini-batch as negative samples. The training objective is to minimize the following contrastive loss L CTR with Negative Log-Likelihood (NLL) function:\nEQUATION\nwhere sim(\u2022, \u2022) is a similarity metric, implemented as dot product in our experiments. Contrastive loss promotes a higher level of similarity between paired acoustic and lexical representations, as compared to unpaired representations, thus enhancing the alignment between the two modalities.\n\nTraining and Inference\nTraining MATE is trained jointly on the MLM objective L MLM , similar to that employed in the pretraining of BERT, and the contrastive loss L CTR .\nL = L MLM + \u03b1 \u2022 L CTR (2)\nFollowing BERT pre-training, a portion of tokens in the text sequence are randomly selected for prediction, and are replaced by the [MASK] token, a random token or left unchanged. In order to optimize the model's performance, the model is trained end-to-end and all the parameters are updated during the training process.\nInference We use pseudo-log-likelihood (PLL) scoring (Wang and Cho, 2019; Salazar et al., 2020) to compute sequence level scores. Given an acoustic sequence a = (s 1 , ..., s R ) and a lexical sequence l = (t 1 , ..., t T ), let l \\k = (t 1 , ..., t k\u22121 , [MASK], t k+1 , ..., t T ), PLL score is computed by summing conditional log probabilities logP MLM (l i |a, l \\i ) of each masked lexical token:\nEQUATION\nThe final score of an utterance is computed as a linear interpolation of the first-pass ASR confidence score and second-pass PLL score, leveraging the complementary information to improve performance while allowing a trade-off between them.\n\nDatasets\nTraining Set The training corpora consist of 10K+ hours of paired audio-text data, sampled from both public and in-house datasets. This data regime is representative of a variety of ASR systems used for various speech applications, with a mix of accents, speakers, sampling rates, and background noise. Less than 5% of the data are synthetic audios generated using AWS Polly Text-to-Speech (TTS) 2 neural backend.\nEvaluation Set We evaluate the proposed MATE approach on both synthetic and real datasets from various domains: MTDialogue (movie-twitter), Lib-riSpeech (LS) (Panayotov et al., 2015) and Vox-Populi (Wang et al., 2021) are in-domain sets, as the training set includes their corresponding train data splits. Wall Street Journal (WSJ) (Garofolo et al., 1993) , ConvAI (in-house), SLURP (Bastianelli et al., 2020) datasets are out-of-domain (OOD) datasets for zero-shot evaluation.\nMTDialogue (movie-twitter) is based on a public lexical dialogue corpus 3 which consists of movie subtitles and twitter user interactions. The audios are generated from TTS system. MTDialogue dataset is a seen dataset for open-book evaluation; i.e., all its data samples are covered in training data. An subset of 1.2 hour is sampled for evaluation. LibriSpeech(LS) (Panayotov et al., 2015) is a read English speech corpus based on LibriVox audiobooks. We consider the two official evaluation sets: test-clean and test-other, each with 5.0 hours of test audios. VoxPopuli (Wang et al., 2021) consists of public political speech, sampled from 2009-2020 European Parliament event recordings. For our evaluation purpose, we utilize a 5-hour subset of VoxPopuli English data.\nWe also evaluate MATE on OOD evaluation sets: ConvAI, WSJ, and SLURP. The Wall Street Journal (WSJ) (Garofolo et al., 1993) corpus contains conventional and spontaneous dictation by journalists. The test_eval93 split of 0.4 hour is selected for our evaluation. ConvAI is based on in-house user utterances of a task-oriented conversational AI system. The typical usage scenarios include booking flights, ordering food and querying health insurance information, etc. The 2.0 hours of audios are generated from TTS system. SLURP (Bastianelli et al., 2020) is a public dataset for smart home virtual assistant development. Top usage scenarios include checking calendar, playing music, and asking about time, etc. We utilized the 10 hr test set for evaluation.\nEthical Considerations: We have reviewed all licenses of public datasets, which allow the usage for research and paper publication. The in-house dataset ConvAI is internally approved for research purposes. All datasets are sets are de-identified to ensure anonymity. We also make sure the datasets cover various English accents, speakers and backgrounds.\n\nEvaluation Metrics\nWe use word error rate (WER) and content word error rate (CWER) as the evaluation metrics. CWER is computed on content words only (e.g., \"pizza\", \"parliament\", \"airline\"), where we apply rule based method to filter out a predefined block-list of function words. Furthermore, we evaluate Spoken Language Understanding (SLU) performance on SLURP dataset using standard SLU metrics (accuracy and F1 score); SLU predictions (scenario, action and entity) are generated by a bi-directional Long Short-Term Memory (BiLSTM) NLU module (Appendix A). \n\nResults and Analysis\nWe summarize the observations and analysis of the results from our experiments 4 as follows:\nMATE excels at both in-domain and out-ofdomain generalization: Table 1 summarizes the performance of the proposed MATE and multiple baseline models, under various settings, across in-domain and OOD datasets. Overall, we observe that our proposed approach (row 8) significantly outperforms text-only baseline (row 3) on in-domain datasets indicating that audio information helps even when we have sufficient target domain corpus for fine-tuning. Furthermore, results on OOD datasets indicate that MATE generalizes much better to new domains in the complete absence of domain data (zero-shot setting), when compared to the text-only baseline, by utilizing the rich information from audio.\nMLMs are more effective multi-modal rescorers than autoregressive LMs: Rows 2-5 indicate a significant performance gap between BERT and autoregressive rescorers (LAS/GPT-2). BERT-Text, which is a text-only baseline, outperforms even the multi-modal GPT2 indicating the root cause of the gap is the lack of bi-directional (left and right) context in GPT2 which is necessary for reliable and effective LLM scoring, hence validating the choice of MLM in MATE.\n\nAlignment loss gives significant performance boost:\nTo study the effect of alignment loss, we train the multi-modal rescorer with two loss functions: Mean squared error (MSE) loss and contrastive loss. Significant performance gains (row 6 vs. row 7-8) in Table 1 indicate that explicit alignment techniques greatly improve learning of multimodal representations. Specifically, contrastive loss not only aligns relevant pairs like MSE loss, but also promotes distancing irrelevant samples, leading to improved generalization on OOD sets.\nParameter-efficient fine-tuning results in limited gains: Rows 9-11 study the performance of a multi-modal rescorer under different parameter efficient fine-tuning settings. We observe that performance degrades as we move from full finetuning to adapter-tuning and freezing the full BERT encoder layers, indicating that fine-tuning BERT encoder is the most beneficial in terms of performance improvement. As expected, in comparison to model with full fine-tuning (row 6), rows 9-11 exhibit lower performance. This suggests that frozen or parameter-efficient training methods may lack the model capacity to fully leverage the acoustic information present in the multi-modal data. \n\nMATE is the most effective few-shot learner:\nTo study the effect of few-shot learning, we plot the relative WER reduction (WERR) on Voxpopuli and WSJ datasets across different resource conditions as shown in Figure 2 . We observe that MATE transfers well to the new domains in the zero-shot setting with no training or domain data at all. Fewshot performance clearly improves with more examples and goes a reasonable way towards closing the gap from zero-shot performance to full finetuning performance. We also observe that MATE consistently has superior performance to text-only baseline across both datasets, confirming the ability to rapidly adapt to new domains by leveraging additional information from the audio modality.\nMATE achieves best zero-shot performance improvement on downstream SLU tasks: To evaluate the effectiveness of the proposed approach on the end goals in a dialog system, we compare it with other baselines using metrics such as scenario/action accuracy and entity F1 score in a zeroshot setting on SLURP dataset. From results in Table 2 , we observe that MATE consistently outperforms 5 the other baselines on end-to-end goals indicating that the improvements are mainly on recognition of content words and slot entities. 6\n\nConclusions\nWe propose a novel multi-modal rescorer, MATE, which achieves significant WER, CWER reduction on in-domain and OOD datasets. In zero-shot and few-shot settings, MATE performs well on unseen domains and adapts rapidly with limited data. The domain generalization capability of MATE makes it an effective choice as a second-pass rescorer for scaling ASR systems to new domains.\n", "hypothesis": " In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM.  We adopt contrastive learning for effectively aligning the modalities by learning shared representations.", "answer": true}
{"title": "A Holistic Approach to Reference-Free Evaluation of Machine Translation", "content": "\nIntroduction\nMachine translation evaluation has conventionally relied on reference, where outputs are compared against translations written by humans. This is in contrast to the reference-free manner in which translation quality is directly assessed with the source text. Reference-free evaluation (Napoles et al., 2016; Thompson and Post, 2020; Agrawal et al., 2021) has the potential to free the evaluation model from the constraints of labor-intensive annotations, allowing it to pivot easily to new domains. In this way, reference-free evaluation metrics are substantially more scalable and have lately been in the spotlight.\nThe history of reference-free evaluation for MT can trace back to \"QE as a Metric\" track of \u02daEqual contribution. : Corresponding author. 1 https://github.com/cocacola-lab/ Reference-Free-Evaluation-of-Machine-Translation. git WMT2019 Metrics Task (Ma et al., 2019) . YiSi-2 (Lo, 2019) and XBERTScore (Zhang* et al., 2020; Leiter, 2021) are embedding-based methods that adopt contextual word embeddings to calculate the lexical similarity between the source and candidate translation words. Quality estimation (Fonseca et al., 2019) system metrics such as UNI+ (Yankovskaya et al., 2019) and COMET-QE (Rei et al., 2020a (Rei et al., , 2021 ) also leverage contextual word embeddings and feed them into a feedforward network. However, they are trained to regress on human scores that are expensive to collect, and gross discrepancies exist when different humans are asked to label the scores.\nMore challenging but worthwhile, we focus on dispensing with references as well as human scores. Nevertheless, embedding-based methods are limited to token-level semantic similarity while neglecting sentence-level faithfulness (Song et al., 2021) . Besides, it's difficult for word embeddings to discriminate matched word pairs from random ones (Zhao et al., 2020a) .\nIn addition, current reference-free evaluation methods rarely take fluency into account. For the unfluent candidates whose content is roughly consistent with the source, the embedding-based metrics can hardly discriminate and provide accurate evaluation scores 2 . Moreover, the general goal of evaluation metrics is to estimate not only the semantic equivalence between source and candidate but also the general quality (i.e., fluency and naturalness) (Banchs et al., 2015; Feng et al., 2020; Yuan et al., 2021) .\nIn this work, we propose a holistic approach (i.e., ReFreeEval) to enhance the evaluation model in aspects of fluency and faithfulness, meanwhile on both word and sentence levels. With regard to fluency, we pose a data augmentation method and train a fluency discrimination module. For word-level faithfulness, we adopt a self-guided contrastive word-alignment method. For sentencelevel faithfulness, we execute knowledge distillation with SBERT (Reimers and Gurevych, 2019) to capture more fine-grained semantics. Our method builds on the framework of XBERTScore. Extensive experiments spanning WMT18/19/21 Metrics (Ma et al., 2018 (Ma et al., , 2019;; Freitag et al., 2021) segment-level daRR and MQM datasets demonstrate that our proposed reference-free approach, ReFreeEval, outperforms SOTA reference-free metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.\n\nApproach\nReference-free evaluation of MT can be characterized as two aspects: (1) fluency: how well it conforms to normal human language usage; and (2) faithfulness: how well the translated text reflects the source data. We assess faithfulness at different granularity: word level and sentence level. Figure 1 is the illustration of our ReFreeEval method.\n\nSentence-Level Fluency\nWe explore a data augmentation method to perturb the fluency of target sentences with noise which is difficult to be identified. Then we train a fluency discrimination module with contrastive learning (Gao et al., 2021; Zhang et al., 2021; Wu et al., 2022; Wang et al., 2022) to distinguish fluent samples from perturbed samples (namely, challenging negative samples).\n\nData Augmentation Using Clause Permutation\nA complex or compound sentence 3 has two or more clauses and relative clauses that are joined together with conjunctions or punctuation. As logical relations exist between these clauses, we manipulate and permute the clauses separated by punctuation, instead of words. In this way, the meaning is preserved inside the clauses, meanwhile, the sentence is often unfluent and unnatural. Similar to complex and compound sentences, for a simple sentence with only one clause 4 , we randomly split it into two fragments and permute the two fragments. Compared to permutation on the token level, clauselevel permutation has less influence on sentence fluency and semantic change. The clause-based permutation method brings perturbed samples that are more challenging and hard to be recognized.\n\nFluency Discrimination\nWe denote a source and target sentence in parallel data as x and y. Perturbed samples augmented from y are \u01771 , \u01772 , ..., \u0177k . A reliable metric has the ability to give the original fluent target y a higher evaluation score than those k perturbed unfluent samples.\nAs for the score, we adopt the same calculation measure as BERTScore but replace the pre-trained monolingual model (Devlin et al., 2019; Liu et al., 2019) with a cross-lingual model (Devlin et al., 2019; Conneau et al., 2019) to do reference-free evaluation (Zhou et al., 2020; Song et al., 2021) denominated as XBERTScore (Leiter, 2021) . We use 9th layer of XLM-Roberta-Base to extract contextual word embeddings. Here we only use F BERT as evaluation score between source x and targetside y or \u0177i , which is represented as s w px, yq or s w px, \u0177i q. Then we can obtain word-level faithfulness scores s w px, yq, s w px, \u01771 q, ..., s w px, \u0177k q of pk `1q pairs.\nIn order to discriminate fluent sentences from perturbed ones according to these scores, we treat the original target and its corresponding perturbed samples as opposite and assign them 1/0 hard labels. The cross-lingual model which produces XBERTScore is trained to classify target-side sentences with a cross-entropy loss function. The objective function on N training samples is as follows:\nL f l \" \u00b41 N \u00ff x,y log e swpx,yq\ne swpx,yq `\u0159k i\"1 e swpx,\u0177 i q\n(1)\n\nWord-Level Faithfulness\nAs for word-level faithfulness, each word in the source sentence should have a corresponding crosslingual representation in the target sentence and each word in the target sentence should be an accurate translation of its source word. This motivates us to do word-alignment training to enhance wordlevel evaluation. This module shares similar architecture with sentence-level fluency where word embeddings are derived from 9th layer of XLM-Roberta-Base.\nWe take the same steps as (Dou and Neubig, 2021) to extract alignments. First, we compute the dot product between source and target word embeddings to obtain the similarity matrix S. Then S is normalized in source and target dimensions. And we get source-to-target alignment matrix S xy and target-to-source alignment matrix S yx . A source/target token and a target/source token whose similarity value in alignment matrix S xy /S yx exceed threshold c 1 are regarded as aligned. The bidirectional alignment matrix A is deduced:\nEQUATION\nA ij \" 1 means x i and y j are aligned. Dou and Neubig (2021) also propose the self-training objective to align words with this bidirectional alignment, which improves alignment performance most.\nBased on this objective, we adopt a self-guided contrastive cross-lingual word-alignment method. By contrast, we not only pull semantic aligned words to have closer contextual representations but also push unrelated words away (Luo et al., 2021; Su et al., 2022; Meng et al., 2022) , which encourages the model to discriminate matched word embeddings from semantically unrelated ones.\nThe source token and target token are deemed to be unrelated if their similarity value is low. In our method, these unmatched pairs constitute negative samples and are pushed away. Moreover, we set threshold c 2 to further restrict the negative samples. The unmatched pairs whose similarity value is lower than c 2 are discarded from negatives as this unmatched relation can be easily distinguished by the model. In this way, we can control the difficulty of negative samples and only preserve those indistinguishable ones (hard negatives) to train the model.\nB \" pS xy \u0105 c 2 q \u02dapS T yx \u0105 c 2 q (3)\nB ij \" 1 means x i and y j are aligned or a part of hard negatives, which are preserved to train.\nIn Figure 1 , the dark blue positions mean bidirectional alignment while the light blue positions are hard negative examples.\nFinally, based on two dimensions of source and target, the positive and negative samples mentioned above, we construct a self-guided contrastive learning objective function on the word level as follows:\nEQUATION\nL word \" L x `Ly (6)\n\nSentence-Level Faithfulness\nThe main idea is to improve sentence-level faithfulness evaluation. Concretely, we distill sentencelevel semantic meaning from SBERT into the wordlevel shared model. We use SBERT to extract semantically meaningful sentence embeddings. Sentence semantic similarity between x and y is calculated with cosinesimilarity between sentence embeddings x and y:\nEQUATION\nThe semantic similarity reflects the sentencelevel faithfulness from target to source. Then we can obtain sentence-level faithfulness scores s s px, yq, s s px, \u01771 q, ..., s s px, \u0177k q. We use KLdivergence as the objective function to reduce the discrepancy between sentence-level and word-level similarity:\nL f a \" \u00ff x,y 1 PYx s s px, y 1 q log s s px, y 1 q s w px, y 1 q (8)\nIn this distillation module, SBERT plays a role of a teacher. Sentence-level semantic knowledge is distilled into the word-level shared model through these sentence-level faithfulness scores. In this way, evaluation is no longer limited to word level but incorporated sentence semantics.\nOn the other hand, SBERT plays a role as a corrector. It is unreasonable that a disturbed sample with slightly changed semantics is considered to be completely contrary to the original sentence. We correct the binary classification and convert the 0/1 discrete value in the fluency discrimination module to continuous variables.\nFor sentence-level training, we combine fluency with faithfulness. This joint architecture is motivated by (Ren et al., 2021) . The objective is:\nL sent \" L f l `\u03b1L f a (9)\n\u03b1 is a hyper-parameter to control the weight that the sentence-level faithfulness module accounts for.\n3 Experiment Embeddings We use the 9th layer of XLM-Roberta-Base to extract contextual word embeddings. This follows the default setting of BERTScore 6 . For sentence embeddings, we adopt xlm-r-bert-base-nli-stsb-mean-tokens model 7 the same as SentSim.\nBaselines For reference-based metrics, we choose sentBLEU (Papineni et al., 2002) and YiSi-1 (Lo, 2019) . For reference-free metrics, we choose XBERTScore (Leiter, 2021) , YiSi-2 (Lo, 2019) , SentSim (Song et al., 2021) and BERTScore-MKD (Zhang et al., 2022) . Most results of baseline models are reported in the original paper (Ma et al., 2018 (Ma et al., , 2019;; Freitag et al., 2021; Zhang et al., 2022) . We also implement experiments that have not been reported, such as XBERTScore, SentSim and BERTScore-MKD. \nFor WMT21 segment-level evaluation, conventional Kendall-tau statistic is used to measure the correlation between our scores and MQM scores.\n\nResults\nThe main results are displayed in Table 1 , 2, 3. First, we observe that fluency, word-level faithfulness, and sentence-level faithfulness module improve the evaluation performance respectively. We also find that the main improvement comes from sentencelevel fluency indicating that XBERTScore as a token-level evaluation metric lacks sentence-level knowledge. Then, the ensemble model combining the advantages of the three modules achieves even better results. And compared with some referencebased baselines it achieves comparable results or even outperforms them. More details of experimental results are in Appendix C.4.\n\nConclusion\nWe propose a reference-free evaluation approach ReFreeEval that comprehensively considers three aspects: aspect. ReFreeEval, combining the above three modules, achieves a higher correlation with human judgments, outperforming current SOTA referencefree metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.\n", "hypothesis": "In this paper, we propose a referencefree evaluation approach that characterizes evaluation as two aspects: (1) fluency: how well the candidate translation conforms to normal human language usage; (2) faithfulness: how well the candidate translation reflects the source data. We further split the fluency into word-level and sentence-level.", "answer": false}
{"title": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios", "content": "\nIntroduction\nReasoning plays a central role in human communication (Frank and Goodman, 2012) . While language models have demonstrated remarkable capacity on downstream tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) , it remains unclear to what extent predictions generated by language models are consequences of correlation with linguistic heuristics in the context, versus robust reasoning about causal relations grounded on understanding of world knowledge.\nIn this paper we leverage counterfactual conditionals to investigate the capacity of pre-trained LMs (PLMs) to distinguish hypothetical scenarios from reality, and to examine how this interacts with models' use of existing real world knowledge as well as shallower associative cues. Counterfactuals consist of a premise which is false in the real world but true in the hypothetical world (e.g., \"If cats were vegetarians\"), and an imaginary consequence of this premise (\"cats would love cabbages\"). Testing language models with counterfactuals allows us to use language to manipulate what is true and what is hypothetical, and to test models' ability to separate and use this information for predictions. Previous work has established the use of counterfactual scenarios to probe inference ability (Qin et al., 2019; Zellers et al., 2019; Mostafazadeh et al., 2016; Meng et al., 2022; Rajani et al., 2019; Saparov and He, 2022; Frohberg and Binder, 2021; Elazar et al., 2021; Rudinger et al., 2020) , but the datasets lack systematic control of lexical cues and world knowledge, which makes it likely that the performance could be attributable to spurious cues in the datasets (Niven and Kao, 2019) .\nFor our tests we draw on and adapt inputs from existing psycholinguistic experiments. We begin by testing models' ability to override existing world knowledge when the context indicates that the correct completion involves a hypothetical world (e.g., \"if cats were vegetarian, cats would love cabbages/fish\"). We test five popular PLMs, and find that models can increase their preference for counterfactual completions given counterfactual context-however, most models rely strongly on simple lexical cues. Next we control the effect of real world knowledge and lexical triggers, to test models' understanding of what counterfactual language implies about the world state. We find that most models fail to understand real-world implications of counterfactuals and largely rely on lexical triggers-with the exception of GPT-3, which shows greater sophistication, but continues to show non-trivial susceptibility to interfer-ence from lexical-associative cues. We discuss the implications and possible interpretations of these findings with respect to linguistic competence and predictive strategies of these models.\n\nExp1: overriding world knowledge\nOur first experiment investigates whether LMs are able to take a counterfactual scenario and predict a counterfactual-consistent completion that contradicts general world knowledge.\nItems We draw directly on counterfactual stimuli from the psycholinguistic study of Ferguson and Sanford (2008) . There are 128 items from the original psycholinguistic experiments, and we synthetically generate 10,720 additional items (see Appendix A.2 for illustration of data generation process). We match target nouns and syntactic constructions across conditions in order to control lexical properties that influence language models' predictions. Table 1 shows example items from the synthetic large-scale dataset (see Section A.1 for example items from the small-scale dataset).\n\nCond Sentence\nCW If cats were vegetarians, people would love them.\nFamilies would feed cats with fish/cabbages.\n\nRW\nBecause cats are carnivores, people love them.\nFamilies would feed cats with fish/cabbages.\n\nBB\nFamilies would feed cats with fish/cabbages The experiment includes two key conditions: Counterfactual-World (CW) and Real-World (RW) (Fig. 1 ). The CW condition presents a counterfactual scenario, e.g., in which cats are vegetarians. The logical target completion in this example is \"cabbages\", but because in reality cats are more likely to eat fish, this contradicts world knowledge. By contrast, in the RW condition the logical completion is consistent with the real world (\"feed cats with fish\"). We also include one Baseline Bias (BB) condition, for a more direct test of the strength of models' baseline preference for each completion.\nExperiments We test counterfactual reasoning in five pre-trained language models. We include autoregressive transformers in the GPT family (GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) ) and masked language models in the BERT family (BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) and MPNet (Song et al., 2020)) 2 .\nWe test models by comparing the log-probability that each model assigns to the CW-congruent (\"cabbages\") and RW-congruent (\"fish\") completions given the contexts. For all conditions, we compute the percentage of items in which the CW-congruent continuation has a higher probability than the RWcongruent continuation. This means that in RW and BB conditions, lower values reflect better predictions, since the CW-congruent completion is the less logical completion in these conditions. Table 2 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp1. In the CW condition, higher values reflect better predictions. In RW and BB conditions, lower values reflect better predictions.\nResults Table 2 shows the preferences for CWcongruent completions across all models and conditions, for the small-scale hand-designed items from the psycholinguistic experiment, and for the large-scale synthetic items. 3 We see that all mod-els show stronger preference for CW-congruent continuations in the counterfactual (CW) context than in the other conditions (though in the case of BERT on the small-scale data, this difference is negligible). All models show below-chance preference for CW-congruent continuations in the RW condition-which means above-chance preference for the correct RW-congruent continuations. However, though all model preferences for the correct CW-congruent continuation are higher in the CW condition than in the RW condition, even in the CW condition the preference for CW-congruent conditions is at best slightly above chance for most models. The exception is GPT-3, which is the only model to prefer the CW-congruent continuation in greater than 70% of items.\nWe also see that GPT-3 shows exceptionally strong performance on both BB and CW conditions. This suggests, slightly counterintuitively, that stronger grasp of relevant world knowledge may in fact be associated with models more effectively overriding that knowledge in a counterfactual. To investigate this effect further, we examine the impact of world knowledge at the item level. We quantify strength of world knowledge as the difference between models' log-probability of CW-and RW-congruent continuations for a given item in the BB condition, and the strength of counterfactual preference as the difference between log-probability of CW-and RW-congruent continuations for a given item in the CW condition. We then compute the Pearson correlation between these strength measures. We find a significant correlation between the robustness of world knowledge encoding and strength of counterfactual preference in the CW condition across all language models (see Appendix A.3), further supporting a relationship between strength of world knowledge and counterfactual sensitivity. While previous work has suggested that large language models may have difficulty avoiding memorized texts when explicitly prompted to end famous quotes differently (McKenzie et al., 2022) , our results suggest that world knowledge may in fact facilitate reasoning when accompanied with clear structural cues (e.g. \"if\"). To better understand how world knowledge informs language models' predictions and in-CW condition alone. However, to further address this concern, we calculate the proportion of items in which the model shows the correct preference in both CW and RW conditions. The results are presented in Section A.5 and suggest a comparable pattern in terms of relative model strengths.\nference, it will be important to continue expanding the scale of tests and more carefully operationalize definitions of world knowledge in future work.\n\nExp2: impact of cue words in context\nThe first experiment suggests that models can to an extent override world knowledge given a counterfactual, particularly in cases when models have a strong handle on the relevant world knowledge. However, it is possible that in these tests the models were not relying on sophisticated understanding of counterfactuals, but rather on simple lexical triggers in context. Consider, for instance, that models could perform well in Exp1 if they simply increase their preference for \"cabbages\" in the proximity of \"vegetarians\", etc. To test the impact of these lexical triggers, we incorporate an additional condition.\nItems Table 3 and Fig. 2 show a sample item and illustration of experimental set-up with the new added condition. In this Counterfactual-to-Reality (CR) condition, models see the same counterfactual context, but the subsequent sentence references actual reality. So the correct completion is consistent with reality, but inconsistent with the lexical trigger (\"vegetarians\"). We generate sentences in the CR condition by modifying CW sentences to include the discourse connective \"In reality\" and to include present tense in the second sentence.\n\nCR\nIf cats were vegetarians, people would love them.\nIn reality, families feed cats with fish/cabbages. Experiments As above, we calculate percentage of items in which models prefer the CW-congruent continuations. Models relying on information beyond simple lexical triggers should show a sharp drop in preference for the CW-congruent completion in the CR condition, where the correct completion should align with real world information.\nResults Table 4 shows the results. We see that most models show non-zero drop between CW and CR conditions-however, for most models this reduction is minor. It is only GPT-3 that shows a truly substantial drop in CW-congruent preference, and only in the large-scale synthetic dataset. This suggests that most models are largely following simpler lexical triggers, while GPT-3 has somewhat greater sensitivity to more detailed linguistic cues. Note, however that GPT-3's relative success on the synthetic data over the small-scale data may rely on larger distance between lexical triggers and target positions: see Appendix A.4 for evidence on GPT-3's sensitivity to linear distance. Table 4 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp2. In the CW condition, higher values reflect better predictions. In the CR condition, lower values reflect better predictions.\n\nExp3: Inferring real world state with counterfactual cues\nThe previous experiments indicate that models can override world knowledge in the face of counterfactual evidence, and that the ability to do this improves with stronger world knowledge-but for most models this performance appears to be driven largely by simple lexical triggers in the context, with the possible exception of GPT-3. In this section we remove the influence of pre-existing world knowledge, and hold constant lexical triggers across conditions, for a more direct test of models' sensitivity to linguistic indicators of counterfactuals, and what they say about the true state of the world. This task is particularly challenging because language models must infer the true state of the world based on the presence of counterfactuals, with lexical cues often being misleading.\nItems We adapt stimuli from a psycholinguistic study with 96 controlled sentences (Ferguson, 2012) . We additionally create a larger-scale synthetic dataset with 12,960 sentences, using the same events as the generated dataset from Section 2. We modify the subject noun phrases such that there is no influence of existing world knowledge. For example, we modify the subject \"cat\" to \"pet\", so that there is no prior knowledge about the subject's preference for \"cabbages\" or \"fish\". As a result, existing world knowledge cannot inform the correct completion-instead, models need to infer based on the counterfactual language that the true state of the world is different from what the counterfactual states. Further, we control the lexical items used across different conditions to minimize effects of lexical cues on condition differences (see Table 5 ).\n\nCond Sentence\nCWC If the pets were vegetarians, people would love them.\nIn fact, people feed the pets with fish/cabbages.\nRWCA Because the pets are vegetarians, people love them.\nIn fact, people feed the pets with fish/cabbages.\nBBC In fact, people feed the pets with fish/cabbages. Fig. 3 shows the set-up of conditions. In the Counterfactual-World Context (CWC) condition, the scenario described in the first sentence is neutral with respect to real world knowledge-it is the use of the counterfactual (\"if...were\") that tips us off that this scenario is not true in reality. The correct completion, then, cannot be informed by world knowledge, and is also misaligned with the lexical trigger (e.g., \"vegetarians\"), so models must rely specifically on this implication from the counterfactual in order to perform well.\nIn the Real-World Context Alternative (RWCA) condition, the context uses the same lexical triggers (\"vegetarians\") as the CWC condition. However, because there is no counterfactual language, the logical completion is now the word associated with the lexical trigger (e.g., \"cabbages\", associated with \"vegetarians\").\nGiven that the logical completions in CWC and RWCA differ, we also compare against a Baseline Bias Context (BBC) condition, to establish default model preference for the target factual completion in the presence of the new subject noun phrase.\nExperiments We compare proportion of CWCcongruent completions across conditions. Good performance will assign high values in the CWC condition and low values in the RWCA condition. Table 6 : Percentage of preference for CWC-congruent completion (e.g., \"fish\") in Exp3. In the CWC condition, higher values reflect better predictions. In the CWCA condition, lower values reflect better predictions. The BBC condition establishes models' default preference for the CWC-congruent completion.\nResults Table 6 shows the results. In the smallscale dataset, most models show a similar preference in CWC and RWCA, suggesting again that their predictions are largely driven by lexical triggers. Only GPT-3 shows substantial difference between CWC and RWCA, indicating finer-grained sensitivity to counterfactual structures. This sensitivity is, however, less pronounced in the largescale dataset. Closer inspection suggests that GPT-3's specific success on the small-scale data may in fact be attributable to canceling out of lexical triggers: in the small-scale dataset, there are lexical triggers supporting both continuations (see A.1 for more illustration of the characteristics of the smallscale dataset), which may cause lexical cues to cancel out, enabling more influence from other linguistic cues. To take one example, the small-scale dataset contains the item \"If Helen had received her student loan, her bank balance would now be in credit. When she checked her bank balance she was worried/happy about her finance.\" In this item, among the lexical triggers (\"student loan\", \"in credit\", \"bank balance\") there are potential associations with both the CWC-congruent completion \"worried\" and the CWC-incongruent completion \"happy\". By contrast, in the large-scale dataset, the major lexical trigger (\"vegetarians\") always favors the CWC-incongruent continuation (\"cabbages\"), causing strong lexical bias against the CWC-congruent continuation (see Appendix A.4 for further analysis on the role of conflicting lexical triggers and other linguistic factors). This suggests that GPT-3 does show real sensitivity to linguistic indicators of counterfactuals, but the effect of superficial lexical cues remains strong.\n\nConclusion\nThe experiments above have shown that when presented with counterfactual situations, PLMs are able to prefer completions that conflict with world knowledge-and counterintuitively, this sensitivity appears better in cases where that world knowledge is stronger. Our results also indicate, however, that models are in large part relying on simple lexical cues to inform these preferences. The only model that shows more sophisticated sensitivity to finegrained linguistic cues separating counterfactuals from reality is GPT-3-which successfully distinguishes conditions based on counterfactual cues, but nonetheless still shows strong influences from lexical associative cues. Why might world knowledge aid counterfactual sensitivity? Does GPT-3 truly understand counterfactuals? One possibility worth considering is that explanations in both of these cases involve volume of exposure. First, models' stronger world knowledge for a given fact suggests that models have encountered that fact more often in training-and this may in turn translate to more exposure to that type of knowledge in counterfactual contexts, enabling more straightforward memorization-based performance. Similarly, while GPT-3 may robustly understand counterfactuals, the massive data exposure for that model may enable a simpler path to success: GPT-3 could simply have developed lower-level knowledge of how linguistic cues like \"If/had\" versus \"Because\" mediate levels of association between nearby lexical cues and later words. We leave investigation of these hypotheses for future work.\n", "hypothesis": " We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions.", "answer": true}
{"title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark", "content": "\nIntroduction\nAlthough large language models (LLMs) are powerful tools for generating human-like language, they can also memorize false or outdated associations, limiting their applicability. Model editing techniques promise to solve this problem by correcting non-factual associations. It is important that model edits are highly specific in the sense of not introducing any unwanted associations as a side effect. In this paper, we discuss why the current benchmark for specificity falls short and propose a more challenging, dynamic specificity benchmark to evaluate model editing techniques. Using this benchmark, we evaluate recent model editing techniques and find previously unreported side effects. We highlight the importance of improved specificity benchmarks for the effective and safe use of LLMs subject to model edits.\nFigure 1 : Unintended side effects of model edits and how to measure them. (a) GPT-2-medium is edited using ROME to counter-factually associate the Louvre's location with Rome. However, this results in unintended associations (\"loud facts\") like the association of Obama with Rome, suggesting low specificity of the edit. The edit also significantly increases the maximum logit (shown in brackets), suggesting that the edit is not merely replacing \"Paris\" with \"Rome\" in the desired contexts. (b) Measuring specificity by the fraction of correctly completed test prompts (COUNTERFACT) suggests a high specificity for ROME. Prepending the edit prompt (like \"The Louvre is in Rome.\") to each test prompt (COUNTERFACT+) results in a significant drop in performance. A significant drop in measured specificity can also be observed if the model edit is implemented using constrained fine-tuning (FT-L).\nModel editing updates the parameters of a trained model in order to change its predicted probability distributions without retraining the entire model. This can be used to edit the associations that the model has memorized and hence, improve the accuracy of the model. Fig. 1 shows the example of a counter-factual model edit using ROME (Meng et al., 2022a) where the location of the Louvre is edited to be Rome instead of Paris. We use a counter-factual example since it makes it more evident that the new association is an effect of the model edit instead of the model training. Note that the examples in Fig. 1 are not taken from the COUNTERFACT+ dataset introduced below, but serve to intuitively illustrate the model editing failure modes we are interested in.\nAn important desideratum for model editing is specificity. Specificity captures how well the effect of the model edit is localized; in other words, specificity measures the absence of unintended side effects of model edits. Fig. 1 shows two examples of unintended side effects of ROME model editing, which we collectively call the problem of \"loud facts\". In the first example, mentioning \"Louvre\" (the subject of the model edit) leads the edited model to also complete unrelated test prompts (\"Obama was born in\") with \"Rome\" (the object of the model edit). In the second example, mentioning \"Louvre\" boosts the logits for words semantically related to \"Rome\", like \"Vatican\".\nThe existing specificity benchmark for model editing from the COUNTERFACT dataset (Meng et al., 2022a) suffers from two limitations which can be illustrated using these examples. First, COUNTERFACT does not prompt the model in a way that is likely to surface unwanted side effects. As demonstrated by the examples in Fig. 1 , mentioning the subject of the model edit can drastically change the behavior of the edited model, but the existing benchmark does not detect this. Second, COUNTERFACT considers only the probabilities for the original and edited object token (\"Paris\" and \"Rome\"). As shown by the last example in Fig. 1 , the edited model displays strongly changed logits not only for the original object (\"Paris\") and edit object (\"Rome\") but also for semantically related tokens (\"Vatican\"). Again, this would be overlooked by the current specificity evaluation since it does not consider the entire probability distribution.\nThese limitations mean that side effects of edits may be overlooked and specificity overestimated.\nOur main contributions are:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark.\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution instead of the currently used metrics which focus only on the tokens directly implicated in the model edit. (De Cao et al., 2021) and (Mitchell et al., 2022) . Elazar et al. (2021) introduced ParaRel, a curated dataset of paraphrased prompts and facts. Meng et al. (2022a) use this as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, including specificity.\nKnowledge extraction from LLMs. The assessment of knowledge within language models (LMs) has typically been done by evaluating whether the model is able to predict pieces of knowledge; Petroni et al. (2019 Petroni et al. ( , 2020 ) defined a fill-in-theblank prompt and asked the LM to complete it. Subsequent work has demonstrated that knowledge extraction can be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021) , or by fine-tuning a model on open-domain textual facts (Roberts et al., 2020) . However, constructing prompts from supervised knowledge extraction data is still prone to learning new knowledge instead of recalling existing knowledge in an LM (Zhong et al., 2021) .\n\nDataset\nWe investigate the specificity of recent model editing techniques using the COUNTERFACT benchmark introduced in (Meng et al., 2022a) . COUNTERFACT is a collection of 21,919 nonfactual statements of the form (subject, relation, object) (s, r, o * ), which have low probabilities prior to the model edit. For each of these non-factual statements, we perform a model edit targeting this specific statement. To measure specificity, we then check whether any other associations in the model change in undesired ways. COUNTERFACT supports this check by providing a set of so-called neighborhood prompts for every non-factual statement used in the model edit. These neighborhood prompts are constructed as follows: For a model edit of the form (s, r, o c ) \u2192 (s, r, o * ) (where o c is the correct object, and o * is the false, counterfactual object), COUNTERFACT samples a set of nearby subjects s n for which (s n , r, o c ) holds true. Neighborhood prompts are then paraphrases of the collected (s n , r).\nSuppose, for example, the edit request was (Darrieux, mother_tongue, French) \u2192 (Darrieux, mother_tongue, English). COUNTERFACT takes the relation and object from the edit request (mother_tongue, French), samples true factual associations for this relation, object pair; e.g., (Montesquieu, mother_tongue, French) and then samples a random paraphrase, such as \"The native language of Montesquieu is\". These neighborhood prompts can be used to inspect whether the model edit has undesired side effects on closely related factual associations. See appendix C for a sample from the COUNTERFACT dataset, including the full set of neighborhood prompts.\nMotivated by the example of loud facts shown in Fig. 1 and by the intuition that unwanted side effects are more likely when the model is primed with the linguistic context of the model edit, we now introduce a dynamic version of COUNTERFACT which we will refer to as COUNTERFACT+. To obtain COUNTERFACT+, we modify the neighborhood prompt by prepending the model edit. For example, if the original prompt is \"The native language of Montesquieu is\" the modified prompt would be \"The mother tongue of Danielle Darrieux is English. The native language of Montesquieu is\". See appendix D for a sample of the modified neighborhood prompts used for COUNTERFACT+.\nTo understand why we call COUNTERFACT+ a dynamic version of COUNTERFACT consider how either dataset would be applied to evaluate the success of a model edit: In both cases, we would need to identify the set N of neighborhood prompts in the dataset that are semantically closest to the intended model edit. But in COUNTERFACT, we would use N as is, whereas in COUNTERFACT+ we would change every prompt in N as a function of the model edit, as described above.\n\nMetrics\nTo evaluate the specificity of a model edit on COUNTERFACT, Meng et al. (2022a,b) use two metrics, called Neighborhood Score and Neighborhood Magnitude. Denoting the post-edit probabilities for the correct token o c and incorrect edit token o * by P * (o c ) and P * (o * ), respectively, these are defined as follows: The Neighborhood Score (NS) is defined as the fraction of neighborhood prompts for which P * (o c ) > P * (o * ). The Neighbourhood Magnitude (NM) is defined as P * (o c ) \u2212 P * (o * ), the difference in probability assigned to the correct token versus the incorrect edit token. High NS and NM indicate that the edit has small unwanted side effects.\nNS and NM, however, do not detect cases where the model edit significantly changes the predicted probability for tokens other than o c and o * , such as in the last example in Fig. 1 . To capture this possibility, we introduce as an additional metric the Kullback-Leibler (KL) divergence of the nexttoken distribution between the edited and unedited model, referred to as Neighborhood KL Divergence (NKL). Abbreviating the next token probability distribution for the unedited and edited models by P (w) and P * (w), respectively, and denoting the token vocabulatory by W, NKL is defined as KL divergence between P (w) and P * (w):\nEQUATION\nA large NKL is undesirable because it implies that the next-token probability distribution for neighborhood prompts has been strongly affected by the model edit.\n\nModels and Model Editing Algorithms\nWe use GPT-2-medium (355M parameters), GPT-2-XL (1.5B) (Radford et al., 2019) , and GPT-J (6B) (Wang and Komatsuzaki, 2021) to evaluate the following model editing methods:\n\u2022 ROME (Rank-One-Model-Editing) performs a rank-one update of a single MLP layer to implement the edit (Meng et al., 2022a) .\n\u2022 MEMIT (Mass-Editing Memory in a Transformer) extends ROME to updates across several MLP layers (Meng et al., 2022b) . Note that we do not test using multiple simultaneous edits.\n\u2022 FT-L: Fine-Tuning with an L \u221e norm constraint (Zhu et al., 2020) , constrained to a single layer, as described in (Meng et al., 2022a) .\nWe use FT-L as a simple baseline.\n\nResults\nFigure 2 shows the results for the ROME, MEMIT, and FT-L editing algorithms applied to the GPT-J (6B) model for different specificity metrics and datasets considered in this work. When evaluated using the Neighborhood Score (Fig. 2 , top), we observe significant drops in specificity for all editing algorithms when going from COUNTERFACT to COUNTERFACT+. Note that specificity measured on the unedited model (GPT-J (6B)) also drops suggesting that there is confounding from the test prompts in COUNTERFACT+, potentially due to recency bias (Zhao et al., 2021) . The drop in specificity is much more pronounced for ROME and MEMIT, compared to FT-L and the unedited model, however. This shows that:\n\u2022 ROME and MEMIT have undesired side effects which are not detected by COUNTERFACT\n\u2022 the improved benchmark COUNTERFACT+ is able to detect these unwanted side effects When evaluating specificity using the newly introduced Neighborhood KL Divergence (Fig. 2 , bottom), we observe a large spike in divergence for both ROME and MEMIT when going from COUNTERFACT to COUNTERFACT+. FT-L shows a much smaller increase in divergence from COUNTERFACT to COUNTERFACT+. Figure 3 in the appendix shows the results on COUNTERFACT and COUNTERFACT+ for the NM metric. (top) NS, the average fraction of correctly completed neighborhood test prompts after the model edit (larger is better). We see that COUNTERFACT+ is a much more challenging specificity benchmark: Success rates NS on it range from 33% to 54% across different editing algorithms while they are close to 80% for COUNTERFACT. (bottom) NKL, the KL divergence of the next-token probability distribution of the edited model from that of the unedited model, averaged over all neighborhood test prompts. A lower value indicates higher specificity (the edited model behaves more like the unedited model).\nResults across all three models are shown in tables 1 to 3. These tables list the mean scores on COUNTERFACT and COUNTERFACT+ for the Neighborhood Score (NS), Neighborhood Magnitude (NM), and Neighborhood KL divergence (NKL), respectively. The brackets give upper and lower bound of 99% confidence intervals obtained via bootstrap resampling (N=1,000 The results from tables 1 to 3 show that the significant drop in specificity when evaluating on\nNKL \u2193 COUNTERFACT COUNTERFACT+ GPT-2 M FT-L 1.4e-05 (1.3, 1.4) 1.4e-05 (1.3, 1.4) ROME\n1.6e-06 (1.4, 1.7) 2.5e-05 (2.5, 2.5)\nGPT-2 XL FT-L 7.2e-06 (6.9, 7.4) 9.5e-06 (9.3, 9.7) ROME 1.5e-06 (1.4, 1.6) 3.3e-05 (3.2, 3.3) MEMIT 2.9e-07 (2.5, 3.4) 9.0e-06 (8.8, 9.1) GPT-J (6B)\nFT-L 3.2e-06 (3.1, 3.4) 5.2e-06 (5.1, 5.3) ROME 3.5e-06 (3.2, 3.8) 1.8e-05 (1.8, 1.9) MEMIT 9.2e-07 (8.0, 10) 9.9e-06 (9.8, 10)\nTable 3 : Neighborhood KL Divergence NKL (\u00b5 & 99% CI) on COUNTERFACT and COUNTERFACT+. Note that the order of magnitude is suppressed for the confidence interval for visual clarity; it is the same as for the mean.\nCOUNTERFACT+ (compared to COUNTERFACT) holds across different model sizes and is not an artefact of using a particular model. Section B in the appendix discusses the scaling of specificity with model size in more detail.\n\nConclusion\nModel editing techniques for auto-regressive transformers exhibit unreported issues related to specificity. Although our fine-tuning baseline, FT-L, exhibits less vulnerability to these issues than ROME and MEMIT, it falls short in competing with them regarding crucial model editing metrics such as robustness to paraphrasing (Meng et al., 2022a,b) . This indicates that model editing still presents numerous complexities that require future attention. Additionally, we revealed that the existing COUNTERFACT benchmark fails to detect the low specificity in ROME and MEMIT. To address this limitation, our primary contributions include:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit.\n", "hypothesis": "However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing COUNTERFACT benchmark to include a dynamic component and dub our benchmark COUNTERFACT+. Additionally, we extend the metrics used for measuring specificity by a novel precision-based metric.", "answer": false}
{"title": "Transitioning from benchmarks to a real-world case of information-seeking in Scientific Publications", "content": "\nIntroduction\nScientific publications are one of the primary means by which researchers disseminate their findings and discoveries to the community, but the amount of information to go through can easily become daunting and challenging. Exploratory search, i.e, the process of conducting broad and open-ended searches to gain a better understanding of a research topic, is the type of search task that scientists typically spend the most time on. Unfortunately, specialized search engines designed for scientists only partially support this type of task, leaving researchers with limited options for efficiently accessing and extracting relevant information from the vast amount of available literature. We are however currently standing at an age of redefining the way we seek information as we make great advances in the whole development cycle of NLP technologies aiming to solve knowledgeintensive tasks. To this end, benchmarks constitute the backbone of this process and fundamentally influence the way we measure progress and identify where future research efforts should be focused. These datasets, almost solely, put the emphasis on performance-driven comparison and create an impression of reliable estimates of progress at a task scale, whereas, in reality, they might not be informative about the way the models would solve human problems or help solve them.\nIn this paper, we illustrate an example of transitioning from two biomedical IR benchmarks, i.e, NFCorpus and TREC-COVID, to a practical case of seeking information about a specific topic in scientific publications: vitamin B's impact on human health. In this context, we provide an expertannotated collection of relevance judgements on 1811 publications related to vitamin B and health. Our goal is to assess how models' comparison on benchmarks is meaningful to solving/assisting the expert seeking such information. We thus show through a zero-shot setting that narrowing the comparison down to a single metric might not be relevant to users' needs, even when a real-world case presents similarities with widely-used benchmarks.\nOur contributions can be summarized as follows: 1. we provide a real-world case of informationseeking in scientific publications that does not drift away from prominent benchmarks' characteristics, 2. we test in a zero-shot setting a few SOTA models reflecting the current paradigm in NLP and IR and we give an interpretation of their behavior in our case compared to the benchmarks, and 3. we discuss based on our observations the masked phenomena that the current process of evaluation might not reflect.\n\nInformation-seeking and relevance\nInformation-seeking strategies described by Belkin et al. (1995) represent how a searcher might use different methods and resources and have different aims. The broad information-seeking behavior of scientists is usually regarded as an exploratory search problem (Meho and Tibbo, 2003; Athukorala et al., 2013; Nedumov and Kuznetsov, 2019) . When searching for information in scientific publications, experts have specific information needs, and they seek information that is relevant to those needs. Effective search systems aim to retrieve highly relevant information and present it to the user in a way that is easy to understand and use. To this end, relevance refers to the degree to which a piece of information satisfies the information need of the seeker. There is however a certain degree of variability in the perception of relevance for a given task (Soufan et al., 2022) . Previous work argues that there is a strong relationship between the task singularity, the task carrier, and the type of expected relevance, leading to significant variability of performance levels across tasks (Zhang, 2014; Tamine et al., 2015; Hoeber and Storie, 2022) . Relevance can therefore be considered as a subjective measure that depends on both the informationseeker as well as the environment they seek in.\n\nSemantic Search on Scientific Publications\nIn the first steps of information seeking in scientific papers, the user may not have a clear understanding or ability to precisely articulate their information need (Vakkari, 2005) thus requiring a search system to understand the meaning behind the query and the content of the documents, rather than just matching the query terms with the terms in the papers. To this end, semantic search on scientific papers refers to the ability of a system to understand the meaning of the query and the content of the scientific papers and match them based on their semantic similarity.\nIn practice, given a collection of scientific papers 1 C, the goal is to rank the most relevant subset of candidate papers R \u2286 C by relatedness to q, i.e, R = {p \u2208 C|relevance q (p)}. A common approach to measuring the similarity between the query q embeddings and candidate paper p embeddings. The papers with the highest similarity scores are considered the most similar to the query and are thus returned at the top of the search results.\nAmong the recent research directions, there has been a focus on learning representations of sci-entific documents that can be used as rich input features for downstream tasks, thus alleviating the need for further fine-tuning (Cohan et al., 2020; Parisot and Zavrel, 2022; Singh et al., 2022) . Zero-shot robustness directions continued to show promising results as well, with state-of-the-art being dominated by models optimized to resist to natural dataset shifts (Yu et al., 2022) .\n\nEvaluation Paradigm\nBenchmarks are designed to replicate tasks and are useful for providing a standard method of comparison, reproducibility, and a concise way of tracking progress. For search on scientific papers, a widely adopted paradigm is to provide relevance annotations and evaluate model performance with top-k metrics, notably the Normalised Cumulative Discount Gain @k (Wang et al., 2013) which provides a good balance suitable for tasks involving binary and graded relevance judgements. Nonetheless, there are some concerns with this evaluation methodology.\nIt has long been argued that information seeking/retrieval is or should be considered as an interactive process with human involvement (Cool and Belkin, 2002; J\u00e4rvelin, 2011; Shah and Bender, 2022) where a user is more likely to navigate through different information-seeking strategies during a search session (Hoeber et al., 2019) . Current benchmarks are however non-interactive, whereas the information-seeking process is or should be considered an interactive process with human involvement. To this end, models that are deployed for interactive purposes should be evaluated as such (Lee et al., 2022) .\nFurther, top-k metrics assume that lower ranks are not of interest, with benchmarks usually 2 evaluating on k=10. This contributed to favoring speed and convenience, but in such knowledge-intensive settings like searching about a particular topic in scientific papers, the priority is to fill in the gaps of knowledge of the information-seeker (Hassan Awadallah et al., 2014) . Such small values of k present a very strong assumption on the quantity of information an expert requires to study their subject. We argue in the rest of the paper that such a method may not be ideal for evaluating systems that involve users in expert search situations, as it may not fully account for factors such as the user's interests and expertise when assessing relevance.\n\nExperimental Setup\nIn this section, we provide a description of our experimental setting of transitioning from information-seeking benchmarks on scientific papers to an industrial exploratory search about vitamin B's impact on health.\n\nDatasets\nTREC-COVID (Voorhees et al., 2021 ) is a test collection leveraging the TREC framework that aimed to address the search needs of clinicians and biomedical researchers during the COVID-19 pandemic, thus laying the groundwork for improved search systems in future public health emergencies. The document set is the one provided by CORD-19 (Wang et al., 2020) , which consists of new and historical publications on coronaviruses. TREC-COVID aims to rank papers in response to textual search queries and contains 50 queries with 69,318 candidate papers cumulatively judged by relevance.\nNFCorpus (Boteva et al., 2016 ) is a Medical Information Retrieval data set on nutrition facts where queries are harvested from Nutrition-Facts.org site and candidate documents are medical research papers mostly from PubMed. We use the dataset as it is contained in the BEIR benchmark (Thakur et al., 2021) : 323 queries and 3633 candidate documents.\n\nPractical case: Vitamin B's impact on health\nWe present a practical case study where an expert in immunology seeks to study the effects of vitamin B on human health. A corpus of candidate papers was retrieved from PubMed with the following query: (\"vitamin B\"[Title/Abstract]) AND (health[Title/Abstract] OR growth[Title/Abstract]), which resulted in 1811 papers 3 , out of which the expert identified 598 relevant documents (33%). Relevance judgement was carried out in two steps: 1. Search on title relevance: if a title is obviously out of scope, the expert does not investigate the abstract. Similarly, if the title is evidently in scope, the abstract is not judged. 2. Search on abstract relevance: the expert reads in detail and identifies the type of study that was carried out. On the models' side, the query used for ranking is: How do vitamins B impact health? Our vitamin B case has some similarities in nature 3 Retrieved in December 2022. All papers are in English.\nwith both NFCorpus and TREC-COVID (although not identical). While identifying and analyzing discrepancies between benchmarks and use-case results can provide valuable insights for improving the performance of models in practical real-world use, it can be difficult to know for certain whether a benchmark is representative of a real-world task, as this requires a careful investigation of the data, input format, expert input, and evaluation metrics.\n\nModels & Frameworks\nTransformer-based models have gained widespread popularity as retrieval models, due to their capability of acquiring semantic representations. We use BM25 as a generalizable baseline (Thakur et al., 2021) and test two sets of neural models that we port to sentence-transformers (Reimers and Gurevych, 2019) format known for its efficiency in semantic search tasks (Muennighoff, 2022) : 1. LMs pre-trained for scientific text similarity: SPECTER (Cohan et al., 2020) , SciNCL (Ostendorff et al., 2022), and ASPIRE (Mysore et al., 2022). All three have been trained with the intuition that learned scientific document representations can be substantially improved through contrastive learning objectives. 2. Robust models in zero-shot settings, COCO-DR (Yu et al., 2022) and monoT5 (Nogueira et al., 2020) , both transferred from MS-MARCO 4 .\nFinally, we use Haystack 5 as a framework and ElasticSearch to index embeddings along the papers. We did not alter the original trainings of models.\n\nResults & Discussion\nWe report in Table 1 the average nDCG@10 of the different models on both NFCorpus and TREC-COVID, as well as our use case. We experiment with three strategies of searching: based on title relevance, on abstract relevance, and titles and abstracts appended 6 .\nNFCorpus BM25 is leading on the three strategies, followed by scientific LMs mostly dominating the general robust models. The low scores (compared to the other datasets) can be partially explained with the fact that the percentage of relevant articles is smaller for most queries (\u22641-2%). All models, with the exception of BM25 SPECTER, perform better on titles rather than assessing abstracts' relevance.\nTREC-COVID On titles and titles+abstracts, COCO-DR is the best-performing model, whereas ASPIRE slightly outperforms it on abstracts. Models' performances are quite consistent on this benchmark, with scientific LMs having close scores and mostly best performing on titles. TREC-COVID's queries are more detailed than the other two datasets. This might explain the coherence of results between models; there is more relevant information to judge on.\nVitamin B & health Models' performance in our case seems to be divergent from what can be observed on the other two benchmarks. BM25's performance entirely drops with abstracts, which might be caused by the nomenclature of vitamin B (Appendix A) present in titles and abstracts. On the other hand, monoT5 outperforms all other models on strategies that include abstract relevance, whereas COCO-DR achieves perfect nDCG@10 on titles. Overall, our results show that models perform differently on datasets and suggest that there is an inconsistency in performance and difficulty in identifying the best model for seeking biomedical information in publications: if starting from NF-Corpus, one would suggest using BM25 as a decent model for the vitamin B case, whereas if comparing on TREC-COVID, one would prefer COCO-DR and entirely leave out monoT5. In reality, the perfect nDCG@10 on titles of COCO-DR might suggest the best fit, but the model is not actually placing all the relevant documents at the smallest ranks: Figure 1 illustrates this and shows that the nDCG@10 metric is not reflecting how \"early\" relevant documents are suggested to the user (the tendency is the same on TREC-COVID (Appendix B) for SciNCL, COCO-DR, and monoT5). The differences of scores in Table 1 suggest a big gap in the performance of models, however, if we consider the entire set of relevant papers in the vitamin B case, SciNCL (ndcg@10=0.534) is cumulatively suggesting the relevant elements \"faster\" than COCO-DR (ndcg@10=1.0), making it a better assistance to the expert seeking information.\n\nDiscussion\nWe further discuss in this section the misalignment between performance measures from the perspective of an expert seeking information in papers.\nExpert search strategy preferences Our expert expressed that they sort on titles for more speed, but abstract relevance remains the reference. The reason for this is that titles usually provide information on the study domain as a whole, and can be used to classify into big categories. The abstracts however are used when the title does not allow for immediate classification, since they contain the main question of the paper, the methodology, and the main results. Intuitively, models would find more relevance \"hints\" in abstracts, and thus have greater performance on search strategies that include them. This was rarely the case for all datasets, suggesting that many models might be better at matching shorter contexts (titles being closer to query length compared to abstracts).\nThe success of information-seeking is a process As we previously mentioned, tasks that are complex, such as learning about a new topic, often require multiple search queries and may extend over multiple sessions. It has to be noted that our ex- pert encountered 18 different themes out-of-scope (Appendix A) when annotating the entire collection of papers. These themes are discovered during the exploration process, emphasizing the fact that information-seeking is an interactive process and that the reported metric (designed for speed and convenience of ranking systems) is neither informative about the presence of such themes nor about the corresponding response of the different models. As we mentioned in Section 2.1, relevance is a subjective measure. Our expert investigated the ranked lists returned by different models on the vitamin B use case and categorized the first 100 irrelevant documents for each. We observed that the models' sensitivity to different topics is not the same when measuring similarity. For instance, on titles, COCO-DR (best performance) struggled most with practice recommendations, while SciNCL misjudged the prevalence of B vitamin deficiencies the most. Further, this was also the case on abstracts (Appendix A) as monoT5 struggled most with the vitamin content of food/diet, while BM25 suggested irrelevant studies the most. We illustrate these differences in Figure 2 : no agreement whatsoever between models about (ir)relevance of topics, which cannot be reflected by the NDCG@k measure. Such a disagreement further complicates the process of identifying the sources of differences, which are important to determine which model may be better suited for specific scenarios, given that such differences might have roots in the training data, model architectures, hyperparameters, or other factors.\n\nConclusion\nIn this paper, we illustrated the misalignment between single-metric performance and relevancy in practical expert information seeking. Through a transition from two biomedical IR benchmarks to a case of an expert seeking information about vitamin B's impact on human health, we showed that the current process of measuring performance may not fully capture the challenges of the task at hand. Our observations emphasized the misalignment between relying on top-k ranking metrics and the true nature of the information-seeking process' success. To this end, we provide an extensive description of the use-case creation and relevance judgements to foster future reconciliation between corpus-based evaluations and users' search experience.\n", "hypothesis": " We thus discuss the misalignment between solely focusing on single-metric performance as a criterion for model choice and relevancy as a subjective measure for meeting a user's need..", "answer": true}
{"title": "C-XNLI: Croatian Extension of XNLI Dataset", "content": "\nIntroduction\nNatural language processing has developed rapidly in recent years. Models are starting to achieve human-like performance, but most of these achievements are concentrated on only a small fraction of the world's 7000+ languages. This is to be expected due to the nature of linguistic annotation, which is not only tedious, subjective, and costly, but also requires domain experts, which are in decline (Lauscher et al., 2020) .\nThere are two main approaches commonly used to handle that problem from the models' perspective. The first approach relies on cross-lingual transfer, where the model is pretrained to learn multilingual representations (Conneau et al., 2020; Pires et al., 2019) , while the other approach relies heavily on Machine Translation (MT) systems to translate the text from a low-resource language to a high-resource language (or vice versa). Both approaches can be easily evaluated on cross-lingual benchmarks such as XTREME (Hu et al., 2020) or XGLUE (Liang et al., 2020) . They consist of crosslingual datasets grouped by task to allow comprehensive evaluation. Unfortunately, XTREME covers 40 languages and XGLUE only 19.\nSince none of these benchmarks include Croatian language in any of their datasets, and Crosslingual Natural Language Inference (XNLI; Conneau et al., 2018) corpus is included in both, we decided to extend XNLI with Croatian (C-XNLI). The task is to classify whether a premise contradicts, entails, or is neutral to the hypothesis. XNLI's development and test sets are crowdsourced in English and human-translated into 14 languages, while MultiNLI's (Williams et al., 2018) training set is used for training. It also consists of machine-translated sets required for the translatetrain and translate-test paradigms.\nOur Croatian extension is created in the same manner as its XNLI parent. The development and test sets are translated by a professional translator. Since XNLI provides translate-train, translate-dev and translate-test sets, we opted for Facebook's 1.2B parameter m2m_100 MT model (Fan et al., 2020) to create our own translations.\nIt has been shown that MT models still suffer from errors like mistranslations, non-translations and hallucinations (Freitag et al., 2021; Raunak et al., 2021) , which motivated us to analyze the quality of our dataset. For this purpose, we sampled 2000 sentences per language in both Croatian and German, and evaluated the translations using a variant of the Direct Assessment (DA) score proposed in the Multilingual Quality Estimation dataset (MLQE; Fomicheva et al., 2022) .\nTo summarize, our contributions are the following: (1) we create and analyze the Croatian exten-sion of XNLI and provide baseline models, (2) we create Quality Estimation (QE) datasets for Croatian and German to evaluate the quality of machinetranslated sentences from the translate-train sets, and (3) we quantify the textual overlap between hypothesis and premise and analyze its impact on baseline models.\n\nC-XNLI\nIn creating the dataset, we follow the same procedure as Conneau et al. (2018) . We hired a native Croatian professional translator to translate the English development (2490 samples) and test (5010 samples) sets of the XNLI dataset into Croatian. Premises and hypotheses were given to the translator separately to ensure that the premises did not provide context for the hypotheses. The English training set, derived from MultiNLI and containing 392,702 samples, was translated into Croatian using a selected MT model. We considered a total of eight models and opted for Facebook's multilingual m2m_100 model with 1.2B parameters because of its highest BLEU score (Papineni et al., 2002) on the FLORES dataset (Guzm\u00e1n et al., 2019) , as shown in Table 1 . All of m2m_100 and mbart models are available on fairseq 1 (Ott et al., 2019) , whereas opus models are available on Helsinki-NLP 2 (Tiedemann, 2020; Tiedemann and Thottingal, 2020) \n\nDA Scores\nTo evaluate the quality of the system used to translate English to Croatian, we compare the generated translations with the available translations from a high-resource language. We score a sample of Croatian and German translations from the train set and compare the results. The sentences were sampled using a semantic similarity-based metric that correlates with translation quality (Cer et al., 2017) to flatten the original distribution of scores and analyze samples of diverse quality. A cosine score between the multilingual sentence representations from both LASER (Artetxe and Schwenk, 2019) and SBERT (Reimers and Gurevych, 2019) were used to measure semantic similarity between the source and translated sentences. These models are commonly used at the Conference on Machine Translation (WMT) for QE task (Specia et al., 2021 (Specia et al., , 2020)) . The SBERT we used is a multilingual variant trained on the paraphrase dataset which has slightly better performance than the models trained on similarity tasks (Reimers and Gurevych, 2020) .\nBy utilizing a histogram of cosine scores with a bin size of 0.05, we adopted a circular sampling approach to randomly select one premise from each bin until a total of 50 premises were obtained. Similarly, we followed the same procedure for hypotheses, alternating between SBERT and LASER cosine scores. Furthermore, we implemented an additional criterion to ensure the inclusion of all premises and hypotheses that share a common premise. This entire process was repeated until we reached a 1000 samples each, for both SBERT and LASER cosine scores (2000 in total) .\nWe scored the samples using the procedure described by Fomicheva et al. (2022) . Annotators were asked to rate the translation quality for each sentence on a scale 0-100. Sentences were initially annotated by three annotators. If the range of the most diverging scores exceeded 30 points, an additional annotator was asked to replace the most diverging one until convergence was achieved. The annotators' raw scores were converted to z-scores 3 ; the final score is the average of all scores after convergence. More information about annotators, and annotation procedure is presented in Appendix A.\n\nC-XNLI and DA Scores\nTo demonstrate that our extension has similar properties to its parent XNLI, we perform the following analyses. We tokenize C-XNLI's sentences with MOSES tokenizer and obtain the average number 2018) provide is the BLEU score of their MT systems translating to and from the target language. We have extended their results to include those for the Croatian language (Table 2 ). Our translations from English to Croatian (EN-XX in the table) have the fourth-best BLEU score. These findings are not too surprising since the MT we use is more recent. The distribution of DA scores for Croatian and German is shown in Figure 1 . We can observe that Croatian, although is a lower-resourced language, it has a slightly higher translation quality, as the mean of Croatian DA scores is almost identical to a German one. The correlations between the LASER and SBERT cosine scores and DA scores for both languages are shown in Table 3 , with p < 0.05. The correlations for German are higher, and the LASER cosines tend to correlate less. In Figure 2 we can see that the Croatian model is more likely to make a mistake on premises compared to the German model. \n\nOverlaps\nThe analysis presented here extends Artetxe et al.'s (2020) work where authors demonstrate that the overlap between hypotheses and premises is an overlooked bias in the XNLI dataset, caused by access to premise during hypothesis generation in English, and no access to it during translation into other languages. They decrease the bias by back-translating data and improve their results. To demonstrate the existence of that bias, we take a more direct approach and define a metric that represents overlap -the proportion of copied text from premise to hypothesis. It is the number of character N -grams which occur in both hypothesis and premise, divided by the number of possible character N -grams in the hypothesis. In Table 4 we presented those overlaps using bi-grams, N = 2. We can observe that in the training set, the overlap is 5% to 20% higher compared to development and test sets. In order to investigate that even further, we asked our professional translator to translate 1% of our C-XNLI dataset: 100 sentences which consist of 25 premises and 75 of their hypotheses. We made sure that the premise was given alongside each hypothesis so that it provides context to it in order to measure the influence on the overlap since, in the translation effort, premises and hypotheses were given separately. Our representative sample contained similar genre distribution, overlap distribution, and similar development vs. test overlap ratio. Our results show that when using N = 2, biased sample has 8% increase in overlap, whereas for N = {3, 4, 5}, it increased by \u223c 17%. Table 5 : We present the accuracy of baseline XLM-R Base models on each XNLI language, with the addition of Croatian, together with an average accuracy for all languages without Croatian (Avg) and with Croatian (Avg +hr ).\nOur XLM-R models are averaged over three different seeds. We also calculate the Spearman's correlation between accuracies of each model's setup and train set overlaps (C tr ), development set overlaps (C de ), and test set overlaps (C te ). For overlaps we used N = 2.\n\nXLM-R Setups\nWe tested cross-lingual transfer using zero-shot and translate-based setups. For each, we employ pretrained XLM-R Base model (Conneau et al., 2020) , implemented in Transformers library (Wolf et al., 2020) . In the zero-shot approach, we fine-tune our model on English samples. In the translate-train approach, we fine-tune on translations of a training set, whereas in translate-train-all, we fine-tune it on concatenated training translations. Evaluations are done in all languages. In the translate-test approach, we use the same model from our zero-shot approach and evaluate it on English translations of other languages. We experimented with various hyperparameter configurations and found appropriate ranges. Hyperparameter optimization is done for each setup, and details are presented in the Appendix B.\nResults of baseline setups are shown in Table 5 . To demonstrate the comparability of our training setup, we compare XLM-R's reported accuracy with ours, which is only 0.6 points lower in the train-translate-all setup. The performance of the Croatian model is consistently among the TOP5 models. The reason for that might be in the high BLEU score shown in Table 2 . Focusing on the best overall model -translate-train-all, we notice that adding Croatian did not drastically change the average performance and decreased it only for dis-tant languages like Urdu and Swahili. Whereas for other languages, it increased or did not change significantly.\nFinally, Table 5 also shows how the performance of models on the test set of each language correlates with the bi-gram overlaps in the train, development, and test sets of that particular language. There is a consistent high correlation between the overlap in all sets and models' performance (p < 0.05). However, a lower correlation is seen in the development and test sets. This observation could be attributed to the fact that increasing the overlap of a particular language makes it more similar to the English set, in terms of overlap, thus improving the performance. However, as we showed in Subsection 3.2, the overlap in the development and test sets is artificially lower due to biased translation. Alternatively, high training overlaps might indicate that the model is learning to detect the occurrence of overlapping cues.\n\nConclusion\nIn this work, we extended XNLI to include the Croatian language. The development and test sets were translated by a professional translator. We have successfully demonstrated that the quality of the development and test sets is comparable to that of the other languages. To validate the machine-translated training set, we compare our Croatian translations with those available for a high-resourced language -German. The comparison is based on 2000 manually scored sentences from German and Croatian train sets using a variant of DA scores normalized by z-score. Our results show that the Croatian MT model performs slightly better because it's more up-to-date, even though it's a lower-resourced language. We also found that the Croatian translation model performs poorly on longer sentences -premises.\nFinally, we present an overlap metric to measure the textual overlap between the premise and hypothesis. We find that the training set has larger overlaps than the development and test sets. These overlaps resulted in a high correlation between the models' scores, indicating that a model uses cues from the data that also correlate with overlaps.\nWe provide our datasets under the same license 4 as the XNLI dataset, and also make the accompanying code available on GitHub 5 . We hope that by sharing our datasets, researchers will have the opportunity to gain further insights and expand their knowledge in the field of cross-lingual transfer.\n", "hypothesis": "The comparison is based on 2000 manually scored sentences per language using a variant of the Direct Assessment (DA) score commonly used at the Conference on Machine Translation (WMT). Our findings reveal that a less-resourced language like Croatian has higher translation quality of longer sentences compared to German. However, both sets have a substantial amount of high-quality translations, which should be considered in translation-based training or evaluation setups.", "answer": false}
{"title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning", "content": "\nIntroduction\nIntelligent assistants have become increasingly popular in recent years, but they require users to explicitly describe their tasks within a single domain. Yet, the exploration of gradually guiding users through individual task-oriented dialogues has been relatively limited (Chiu et al., 2022) . This limitation is amplified when tasks extend across multiple domains, compelling users to interact with numerous bots to accomplish their goals (Sun et al., 2016) . For instance, planning a trip might involve interacting with one bot for flight booking and another for hotel reservation, each requiring distinct, taskspecific intentions like \"Book a flight ticket\" to activate the corresponding bot, such as an airline bot. In contrast, human assistants can manage highlevel intentions spanning multiple domains, utiliz-ing commonsense knowledge. This approach renders conversations more pragmatic and efficient, reducing the user's need to deliberate over each task separately. To overcome this limitation of current intelligent assistants, we present a flexible framework capable of recommending task-oriented bots within a multi-domain dialogue system, leveraging commonsense-inferred implicit intents as depicted in Figure 1 . Sun et al. (2016) pinpointed the challenges associated with a multidomain dialogue system, such as 1) comprehending single-app and multi-app language descriptions, and 2) conveying task-level functionality to users. They also gathered multi-app data to encourage research in these directions. The HELPR framework (Sun et al., 2017) was the pioneering attempt to grasp users' multi-app intentions and consequently suggest appropriate individual apps. Nevertheless, previous work focused on understanding individual apps based on high-level descriptions exclusively through user behaviors, necessitating a massive accumulation of personalized data. Due to the lack of paired data for training, our work leverages external commonsense knowledge to bridge the gap between high-level utterances and their task-specific bots. This approach enables us to consider a broad range of intents for improved generalizability and scalability.\n\nMulti-Domain Realization\nCommonsense Reasoning Commonsense reasoning involves making assumptions about the nature and essence of typical situations humans encounter daily. These assumptions encompass judgments about the attributes of physical objects, taxonomic properties, and individuals' intentions. Existing commonsense knowledge graphs such as ConceptNet (Bosselut et al., 2019) , ATOMIC (Sap et al., 2019), and TransOMCS (Zhang et al., 2021) facilitate models to reason over human-annotated commonsense knowledge. This paper utilizes a generative model trained on ATOMIC 20 20 (Hwang et al., 2021) to predict potential intents linking given user high-level utterances with corresponding task-oriented bots. The inferred intents can activate the relevant task-oriented bots and also serve as justification for recommendations, thereby enhancing explainability. This work is the first attempt to integrate external commonsense relations with task-oriented dialogue systems.\nZero-Shot Prompting Recent research has revealed that large language models (Radford et al., 2019; Brown et al., 2020) have acquired an astounding ability to perform few-shot tasks by using a natural-language prompt and a handful of task demonstrations as input context (Brown et al., 2020) . Guiding the model with interventions via an input can render many downstream tasks remarkably easier if those tasks can be naturally framed as a cloze test problem through language models. As a result, the technique of prompting, which transposes tasks into a language model format, is increasingly being adopted for different tasks (Zhao et al., 2021; Schick and Sch\u00fctze, 2021) . Without available data for prompt engineering (Shin et al., 2020) , we exploit the potential of prompting for bot recommendation in a zero-shot manner. This strategy further extends the applicability of our proposed framework and enables it to accommodate a wider variety of user intents and tasks, thus contributing to a more versatile and efficient multidomain dialogue system.\n\nFramework\nFigure 2 illustrates our proposed two-stage framework, which consists of: 1) a commonsenseinferred intent generator, and 2) a zero-shot bot recommender. Given a user's high-level intention utterance, the first component focuses on generating implicit task-oriented intents. The second component then utilizes these task-specific intents to recommend appropriate task-oriented bots, considering the bots' functionality through a large pretrained language model.\n\nCommonsense-Inferred Intent Generation\nThe commonsense-inferred implicit intents function not only as prompts for bot recommendation but also as rationales for the suggested bots, thereby establishing a solid connection between the highlevel intention and task-oriented bots throughout the conversation. For instance, the multi-domain system shown in Figure 1 recommends not only the AirlineBot but also describes its functionality-\"can book a flight ticket\"-to better convince the user about the recommendation.\n\nRelation Trigger Selection\nATOMIC 20 20 is a commonsense knowledge graph featuring commonsense relations across three categories: social-interaction, event-centered, and physical-entity relations, all of which concern situations surrounding a specified event of interest. Following Hwang et al. (2021) , we employ a BART model (Lewis et al., 2020) pre-trained on ATOMIC 20 20 to generate related entities and events based on the input sentence. However, despite having a total of 23 commonsense relations, not all are suitable for inferring implicit intents in assistant scenarios. We utilize AppDialogue data (Sun et al., 2016) to determine which commonsense relations can better trigger the task-specific intents. Given a high-level intention description u i and its task-specific sentences s ij , we calculate the trigger score of each relation r as an indicator of its \n\nRelation Definition\nSocial xIntent the likely intent or desire of an agent (X) behind the execution of an event \"X gives Y gifts\" \u2192 X wanted \"to be thoughtful\" xNeed a precondition for X achieving the event \"X gives Y gifts\" \u2192 X must first \"buy the presents\" xWant post-condition desires on the part of X \"X gives Y gifts\" \u2192 X may also desire \"to hug [Y]\" Event isAfter events that can precede an event \"X is in a hurry to get to work\" \u2192 \"X wakes up late\" isBefore events that can follow an event \"X is in a hurry to get to work\" \u2192 \"X drives too fast\" suitability as a trigger relation.\nEQUATION\nwhere P BART ([u i , r, s ij ]) represents the probability of the sentence beginning with the high-level user description u i , followed by a relation trigger r, and the corresponding task-specific sentences s ij . By summing up multiple task-specific sentences over j and all samples over i, a higher T (r) implies that the relation r can better trigger implicit task-oriented intents in assistant scenarios. We identify a total of five relations with the highest T (r) and present their definitions (Sap et al., 2019) in Table 1 . These relations are also reasonable from a human perspective to trigger implicit user intents.\n\nCommonsense Knowledge Generation\nGiven the selected relations R = {r 1 , r 2 , ..., r 5 }, where r i represents the i-th relation from {xIntent, xNeed, xWant, isAfter, isBefore}, we concatenate each relation with a user utterance u to serve as the context input for our pre-trained BART model:\n<s> u r i [GEN] </s>,\nwhere <s> and </s> are special tokens in BART, and [GEN] is a unique token employed during the pre-training of BART to initiate the commonsenserelated events. BART accepts this input and decodes the commonsense events into implicit taskoriented intents Y = y 1 1:k , y 2 1:k , ..., y 5 1:k , where y i k denotes the k-th generated commonsense event of the relation r i .\n\nZero-Shot Bot Recommendation\nWith the inferred intents, the second component aims to recommend appropriate bots capable of executing the anticipated tasks. To pinpoint the task-specific bots based on the required functionality, we leverage the remarkable capacity of a large pre-trained language model, assuming that app descriptions form a part of the pre-trained data.\n\nPre-trained Language Model\nThe language model used in this study is GPT-J 6B 2 , an GPT-3-like causal language model trained on the Pile dataset 3 (Radford et al., 2019) , a diverse, open-source language modeling dataset that comprises 22 smaller, high-quality datasets combined together. Making the assumption that app descriptions in mobile app stores are incorporated in the pre-training data, we exploit the learned language capability to suggest task-oriented bots based on the given intents.\n\nPrompting for Bot Recommendation\nTo leverage the pre-trained language capability of GPT-J, we manually design prompts for each relation type. For social-interaction relations, the prompt is formulated as \"The user r i y i 1:k by using a popular app called\". For instance, Figure 2 generates a prompt \"The user needs to go to the restaurant and make the reservation by using a popular app called\". For event-centered relations, we simply concatenate the generated events and appprompt to trigger the recommended task-oriented apps/bots.\n\nExperiments\nTo evaluate the zero-shot performance of our proposed framework, we collected a test set specific to our multi-domain scenarios. We recruited six volunteers who were knowledgeable about the target scenarios to gather their high-level intention utterances along with the associated task-oriented bots.\nUpon filtering out inadequate data, our test set incorporated a total of 220 task-oriented bots and 92 high-level utterances, each linked with an average of 2.4 bots. The number of bot candidates considered in our experiments is 6,264, highlighting the higher complexity of our tasks. Our primary aim is to connect a high-level intention with its corresponding task-oriented bot recommendation by leveraging external commonsense knowledge. Therefore, we assess the effectiveness of the proposed methodology and compare it with a 1-stage prompting baseline using GPT-J to maintain fairness in comparison. For this baseline, we perform simple prompting on the user's high-level utterance concatenating with a uniform app-based prompt: \"so I can use some popular apps called.\" In response to these context prompts, GPT-J generates the associated (multiple) app names, serving as our baseline results.\nTo further investigate whether our proposed commonsense-inferred implicit intent generator is suitable for our recommendation scenarios, we introduce another 2-stage prompting baseline for comparison. Taking into account that contemporary large language models exhibit astonishing proficiency in commonsense reasoning, we substitute our first component with the state-of-the-art GPT-3 (Brown et al., 2020) to infer implicit intents, serving as another comparative baseline.\n\nAutomatic Evaluation Results\nConsidering that multiple bots can fulfill the same task (functionality), we represent each app by its category as defined on Google Play, then compute precision, recall, and F1 score at the category level. This evaluation better aligns with our task objective; for instance, both \"WhatsApp\" and \"Line\" belong to the same category-\"communication\" as demonstrated in Table 3 .\nTable 2 presents that the 2-stage methods significantly outperform the 1-stage baseline, suggesting that commonsense knowledge is useful to bridge high-level user utterances with task-oriented bots. Further, our proposed approach, which leverages external commonsense knowledge, achieves superior precision over GPT-3, a quality that is more important in recommendation scenarios. The reason is that GPT-3 may generate hallucinations for inferring more diverse but may not suitable intents.\n\nHuman Evaluation Results\nGiven that our goal can be interpreted as a recommendation task, the suggested bots different from user labels can be still reasonable and useful to users. Therefore, we recruited crowd workers from Amazon Mechanical Turk (AMT) to evaluate the relevance of each recommended result given its high-level user utterance. Each predicted bot or app is assessed by three workers on a three-point scale: irrelevant (1), acceptable (2), and useful (3). The human-judged scores are reported in the right part of Table 2 , and our proposed framework achieves the average score of 2.18, implying that most recommended tasks are above acceptable. Compared with the 1-stage baseline with a score below 2, it demonstrates that commonsense inferred implicit intents can more effectively connect the reasonable task-oriented bots. Considering that the score of 2-stage prompting is also good, we report the pairwise comparison in Table 4 , where we can see that humans prefer ours to 2-stage prompting baseline for 57% of the data.\nIn additon to simply suggesting task-oriented bots, providing the rationale behind their recommendation could help users better judge their utility. Within our proposed framework, the commonsenseinferred implicit intents, which are automatically generated by the first component, can act as the explanations for the recommended task-oriented bots, as illustrated in Table 3 . Consequently, we provide these rationales alongside the recommended results using the predicted intents and undergo the same human evaluation process. Table 4 validates that providing these justifications results in improved performance from a human perspective, further suggesting that commonsense-inferred intents are useful not only for prompting task-oriented bots but also for generating human-interpretable recommendation.\n\nDiscussion\nTable 5 showcases the implicit intents generated by our proposed COMeT generator and GPT-3. It is noteworthy that GPT-3 occasionally produces hallucinations, which can render the recommended bots unsuitable. For instance, given the text prompt \"My best friend likes pop music.\", GPT-3 infers an intent to \"buy a ticket to see Justin Bieber\", which may not align accurately with the user's need.\nHence, our experiments reveal that while the 2-stage prompting achieves higher recall, its precision is lower. As our objective is to recommend reasonable task-specific bots, a higher precision is more advantageous in our scenarios.\n\nConclusion\nThis paper introduces a pioneering task centered around recommending task-oriented dialogue systems solely based on high-level user intention utterances. The proposed framework leverages the power of commonsense knowledge to facilitate zero-shot bot recommendation. Experimental results corroborate the reasonability of the recommended bots through both automatic and human evaluations. Experiments show that the recommended bots are reasonable for both automatic and human evaluation, and the inferred intents can provide informative and interpretable rationales to better convince users of the recommendation for practical usage. This innovative approach bridges the gap between user high-level intention and actionable bot recommendations, paving the way for a more intuitive and user-centric conversational AI landscape.\n", "hypothesis": " By leveraging commonsense knowledge, our framework recommends associated bots in a zero-shot manner, enhancing interaction efficiency and effectiveness.  This approach substantially reduces interaction complexity, seamlessly integrates various domains and tasks, and represents a significant step towards creating more human-like intelligent assistants that can reason about implicit intents, offering a superior user experience.", "answer": true}
{"title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code", "content": "\nIntroduction\nHuman beings rely heavily on the capacity for causal reasoning (Sloman, 2005; Hagmayer et al., 2007) . People understand the observed facts, predict future events, and speculate about what might have happened if things had been different with the help of their causal reasoning skills. For instance, when we go home and find a mess, we probably want to figure out why it happened. If we determine that a bird flew into the house, we might then consider whether the mess could have been avoided if we had closed the window.\nAlthough large language models (LLMs) demonstrate great language understanding and generation abilities, it is still challenging for them to perform complex causal reasoning such as the example above. Powerful LLMs are able to understand single cause-and-effect relations (Brown et al., 2020;  Figure 1 : Causal relationships between events in two causal reasoning tasks. Wang et al., 2021) , like a man losing his balance causes him to fell. However, when it comes to more complex causal structures involving multiple events and alternative branches (like close the window or not), LLMs perform much inferior to humans (Bhagavatula et al., 2019; Qin et al., 2019) . In this paper, we consider two challenging causal reasoning tasks: abductive reasoning and counterfactual reasoning. Abductive reasoning requires models to generate a plausible reason for the ending while being consistent with the premise. Counterfactual reasoning asks what will occur in the counterfactual branch. Causal relationships between events in these tasks are shown in Figure 1 .\nA potential difficulty for LLMs to learn complex causal structures is that they are rarely expressed explicitly in the text. News articles or narratives may contain multiple events with causal relationships, like an incident and a chain of consequences. However, these events are often written chronologically, and it is hard to extract the causal structure from the text without further annotation. Branches are expressed rarer in text, except for the multi-branching storytelling style (Nisi and Haahr, 2006) .\nOn the other hand, causal relations are exhibited more commonly in code. Conditional statements like if direct the computer to execute certain commands, provided a condition is met. This explicitly demonstrates the causal relationship between the condition block and the execution block. Code can also express branching with elif or switch statements, and the nesting feature enables code to describe more complex structures 1 . This motivates us to utilize code models in natural language causal reasoning. Recently, large language models of code (Code-LLMs) are receiving increasing attention (Chen et al., 2021; Xu et al., 2022) . They exhibit strong code generation performance, and their structural prediction abilities help complete structural natural language tasks like argument graph generation (Madaan et al., 2022) and event argument extraction (Wang et al., 2022b) . Being pre-trained on code with abundant causal expressions, Code-LLMs may also have gained better causal reasoning abilities.\nWe conduct experiments on the unsupervised abductive reasoning and counterfactual reasoning tasks. To generate task outputs, we design code prompts like Figure 2 to clearly represent the causal structures of the tasks. Results show that Code-LLMs with code prompts perform much better than text-only LLMs and previous methods. To better understand why the code prompts are effective, we break down the prompts and analyze the influence of different aspects. We find that Code-LLMs are very sensitive to the programming structure (specifically, the conditional statements), while being robust towards format perturbations and programming language changes.\nOur main contributions are as follows: 1) We design code prompts to tackle causal reasoning tasks, by leveraging conditional statements in code to represent causal structures. 2) We evaluate Code-LLMs with code prompts on the abductive reasoning and counterfactual reasoning tasks, and exhibit that code models with code prompts are better causal reasoners than text models. 3) We break down the code prompt in detail and find that the programming structure is crucial to the performance.\n\nModeling Causal Structure with Code\nWe convert the input of causal reasoning tasks into the form of code prompt for Code-LLMs to understand better. We expect the prompts to meet two requirements: 1) clearly represent the causal relationships between events, and 2) as most Code-LLMs only support generating at the end, the target output should appear at the end of the prompts. The first requirement is addressed with conditional statements. However, for the second, the target prediction is not always the last part of the conditional statements, e.g., in abductive reasoning we want to predict the hypothesis, which is the condition in the if structure. To address this, we uniformly use functions to represent events. As shown in Figure 2 , the causal structure is described in the main function. All the event functions are listed afterwards, leaving the target event function at the last.\nAbductive Reasoning. Abductive reasoning requires models to generate a plausible hypothesis H given the observations: premise P and ending E. The chronological order of these three events is P \u2192 H \u2192 E, and the hypothesis causes the ending to occur.\nIn Figure 2 , we regard the task definition as an instruction and place it as a comment at the beginning of the prompt. The causal structure is represented in the main function like: executing the premise, and if the hypothesis is met, executing the ending 2 . The content of each event is presented as a comment of its function. The hypothesis function is placed at the last, leaving for models to complete. The generation process stops with a line break.\nCounterfactual Reasoning. Counterfactual reasoning aims to rewrite a story under a counterfactual condition. As in Figure 1 , the input consists of four parts: the premise P , the initial context C 1 , the original ending E 1 , and the counterfactual context C 2 . Models are asked to generate the counterfactual ending E 2 that minimally modifies the original ending E 1 and is coherent with the counterfactual context C 2 .\nThe causal relationships are represented with the if-elif structure. The premise P is executed first, and then if the initial context C 1 is met, the original ending E 1 is executed; otherwise, if the counterfac-tual context C 2 is met, the counterfactual ending E 2 will be executed. For ease of exposition, we call the context hypothesis as well, being consistent with the former task. The event contents are also written as comments for event functions. We use # end to mark the finish of the ending. provides a brief introduction of these methods.\nAutomatic Evaluation. We use the following automatic evaluation metrics: BLEU 4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) 1 reports the automatic evaluation results in the zero-shot setting. CODEX significantly outperforms previous methods and DAVINCI on both tasks (with significance level \u03b1 = 0.01), exhibiting strong causal reasoning ability. Although the two DAVINCI models are based on CODEX, their causal reasoning abilities may be weakened during instruction tuning, and this phenomenon is called alignment tax (Ouyang et al., 2022) . DAVINCI 003 underperforms DAVINCI 002 on most metrics, probably because it tends to generate longer and more discursive outputs, which do not comply with the tasks.\nHuman Evaluation. We conduct pairwise comparison between CODEX and DAVINCI 002 on 100 test examples. Annotators are asked to choose the better output given the task requirements. For abductive reasoning, the outputs are rated from three aspects: coherence with the premise, coherence with the ending, and the overall coherence. For counterfactual reasoning, the outputs are rated from coherence with the context and the extent of preserving the original ending. Each example is rated by at least two annotators, and the average interrater reliability is 0.64.\nThe results are shown in Table 2 . CODEX outperforms DAVINCI 002 in all aspects. It better considers the context in generation, and is able to preserve the original content in counterfactual reasoning.\nContributions of the Model and the Prompt. We exchange the prompts of code and text models, to measure the contributions of the model and the prompt. The results are in Table 3 . We find that CODEX performs better with the code prompt, as the code prompt clearly describes the causal relation between events. Code prompts benefit the text model DAVINCI 002 on abductive reasoning, but have negative impacts on counterfactual reasoning. A possible reason is that the causal structure in counterfactual reasoning is more complicated, leading to a more complex code which is harder for text models to understand.\n\nWhat are Crucial in Code Prompts?\nTo paint a better picture of the key points in the code prompts, we intervene on the prompts from four aspects and measure the influences of the interventions. The four aspects we select are information, structure, format, and language. The former two, the prior information provided and the programming structure of functions, are contentrelated; the latter two, the code format and programming languages, are form-related. An ideal model should rely on the content and be insensitive to form perturbations. Information. We study two types of prior information: task instructions and function names. In No Instruction, we remove the task instruction from the prompts. In Function Name Perturbation, we replace original function names with anonymous functionX. For example, we replace premise() and hypothesis() in Figure 2 with functionA() and functionB(), respectively. It eliminates the information in function names and only allows models to learn the event relations from programming structures.\nStructure. The first way to intervene in the programming structure is to convert the conditional structures into sequential structures, referred to as Sequential Structure. The events are executed sequentially, like premise(), hypothesis(), ending() in abductive reasoning. In the second way called Disruption, we randomly disrupt the positions of the functions in the conditional structure.\nFor instance, if hypothesis(): ending() can be disrupted into if ending(): hypothesis().\nWe also apply the function name perturbation in disruption to eliminate the impact of function names.\nFormat. We test three formats besides the original one: Class, Print and Return. The first one converts the original code into a class. We define the programming structure in the __init__ method, and move the event functions into the class. In Print, we represent the content of events as a string and print it in the function body, like def premise(): print(\"The Smiths ...\"). And in Return, the string is the return value of event functions.\nLanguage. We also convert the original Python programs into two other languages, Java and C, to evaluate the influence of programming languages. CODEX is quite robust towards format and language changes. Settings like Class and Java are even better than the original one, revealing that the performance can be further improved with delicate prompt engineering.\n\nConclusion\nWe investigate the causal reasoning ability of Code-LLMs. With code prompts of conditional statements, Code-LLMs achieve great performance in abductive and counterfactual reasoning, outperforming text-only LLMs significantly. Our study on different aspects of code prompts shows that providing a reasonable causal structure in code can help generate plausible outputs, and Code-LLMs are robust towards format perturbations.\n", "hypothesis": "Our experiments show that compared to textonly LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is not as important as the format perturbations in code prompt design, while Code-LLMs are robust towards programming language changes.", "answer": false}
{"title": "Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data", "content": "\nIntroduction\nCross-lingual Information Retrieval (CLIR) is the task of retrieving relevant documents written in a language different from a query language. The large number of languages and limited amounts of training data pose a serious challenge for training ranking models. Previous work address this issue by using machine translation (MT), effectively casting CLIR into a noisy variant of monolingual retrieval (Li and Cheng, 2018; Shi et al., 2020 Shi et al., , 2021;; Moraes et al., 2021) . MT systems are used to either train ranking models on translated train-ing data (translate train), or by translating queries into the document language at retrieval time (translate test). However, CLIR approaches relying on MT systems are limited by their language coverage. Because training MT models is bounded by the availability of parallel data, it does not scale well to a large number of languages. Furthermore, using MT for IR has been shown to be prone to propagation of unwanted translation artifacts such as topic shifts, repetition, hallucinations and lexical ambiguity (Artetxe et al., 2020; Litschko et al., 2022a; Li et al., 2022) . In this work, we propose a resourcelean MT alternative to bridge the language gap and propose to use artificially code-switched data.\nWe focus on zero-shot cross-encoder (CE) models for reranking (MacAvaney et al., 2020; Jiang et al., 2020) . Our study is motivated by the observation that the performance of CEs diminishes when they are transferred into CLIR and MLIR as opposed to MoIR. We hypothesize that training on queries and documents from the same language leads to monolingual overfitting where the ranker learns features, such as exact keyword matches, which are useful in MoIR but do not transfer well to CLIR and MLIR setups due to the lack of lexical overlap (Litschko et al., 2022b) . In fact, as shown by Roy et al. (2020) on bi-encoders, representations from zero-shot models are weakly aligned between languages, where models prefer non-relevant documents in the same language over relevant documents in a different language. To address this problem, we propose to use code-switching as an inductive bias to regularize monolingual overfitting in CEs.\nGeneration of synthetic code-switched data has served as a way to augment data in cross-lingual setups in a number of NLP tasks (Singh et al., 2019; Einolghozati et al., 2021; Tan and Joty, 2021) . They utilize substitution techniques ranging from simplistic re-writing in the target script (Gautam et al., 2021) , looking up bilingual lexicons (Tan and Joty, 2021) to MT (Tarunesh et al., 2021) . Previous work on improving zero-shot transfer for IR includes weak supervision (Shi et al., 2021) , tuning the pivot language (Turc et al., 2021) , multilingual query expansion (Blloshmi et al., 2021) and crosslingual pre-training (Yang et al., 2020; Yu et al., 2021; Yang et al., 2022; Lee et al., 2023) . To this end, code-switching is complementary to existing approaches. Our work is most similar to Shi et al. (2020) , who use bilingual lexicons for full termby-term translation to improve MoIR. Concurrent to our work, Huang et al. (2023) show that codeswitching improves the retrieval performance on low-resource languages, however, their focus lies on CLIR with English documents. To the best of our knowledge, we are the first to systematically investigate (1) artificial code-switching to train CEs and (2) the interaction between MoIR, CLIR and MLIR.\nOur contributions are as follows: (i) We show that training on artificially code-switched data improves zero-shot cross-lingual and multilingual rankers. (ii) We demonstrate its robustness towards the ratio of code-switched tokens and effectiveness in generalizing to unseen languages. (iii) We release our code and resources. 1\n\nMethodology\nReranking with Cross-Encoders. We follow the standard cross-encoder reranking approach (CE) proposed by Nogueira and Cho (2019) , which formulates relevance prediction as a sequence pair (query-document pair) classification task. CEs are composed of an encoder model and a relevance prediction model. The encoder is a pre-trained language model (Devlin et al., 2019) \nthat transforms the concatenated input [CLS] Q [SEP] D [SEP]\ninto a joint query-document feature representation, from which the classification head predicts relevance. Finally, documents are reranked according to their predicted relevance. We argue that fine-tuning CEs on monolingual data biases the encoder towards encoding features that are only useful when the target setup is MoIR. To mitigate this bias, we propose to perturb the training data with code-switching, as described next.\nArtificial Code-Switching. While previous work has studied code-switching (CS) as a natural phenomenon where speakers borrow words from other 1 https://github.com/MaiNLP/CodeSwitchCLIR languages (e.g. anglicism) (Ganguly et al., 2016; Wang and Komlodi, 2018) , we here refer to codeswitching as a method to artificially modify monolingual training data. In the following we assume availability of English (EN-EN) training data. The goal is to improve the zero-shot transfer of ranking models into cross-lingual language pairs X-Y by training on code-switched data EN X -EN Y instead, which we obtain by exploiting bilingual lexicons similar to Tan and Joty (2021) . We now describe two CS approaches based on lexicons: one derived from word embeddings and one from Wikipedia page titles (cf. Appendix A for examples).\nCode-Switching with Word Embeddings. We rely on bilingual dictionaries D induced from crosslingual word embeddings (Mikolov et al., 2013; Heyman et al., 2017) and compute for each EN term its nearest (cosine) cross-lingual neighbor. In order to generate EN X -EN Y we then use D EN )X and D EN )Y to code-switch query and document terms from EN into the languages X and Y, each with probability p. This approach, dubbed Bilingual CS (BL-CS), allows a ranker to learn interlingual semantics between EN, X and Y. In our second approach, Multilingual CS (ML-CS), we additionally sample for each term a different target language into which it gets translated; we refer to the pool of available languages as seen languages.\nCode-Switching with Wikipedia Titles. Our third approach, Wiki-CS, follows (Lan et al., 2020; Fetahu et al., 2021) and uses bilingual lexicons derived from parallel Wikipedia page titles obtained from inter-language links. We first extract word n-grams from queries and documents with different sliding window of sizes n P t1, 2, 3u. Longer n-gram are favored over shorter ones in order to account for multi-term expressions, which are commonly observed in named entities. In Wiki CS we create a single multilingual dataset where queries and documents from different training instances are code-switched into different languages.\n\nExperimental Setup\nModels and Dictionaries. We follow Bonifacio et al. (2021) and initialize rankers with the multilingual encoder mMiniLM provided by Reimers and Gurevych (2020) . We report hyperparameters in Appendix C. For BL-CS and ML-CS we use multilingual MUSE embeddings 2 to induce bilingual lexicons (Lample et al., 2018) , which have been aligned with initial seed dictionaries of 5k word translation pairs. We set the translation probability p \" 0.5. For Wiki-CS, we use the lexicons provided by the linguatools project. 3\nBaselines. To compare whether training on CS'ed data EN X -EN Y improves the transfer into CLIR setups, we include the zero-shot ranker trained on EN-EN as our main baseline (henceforth, Zero-shot). Our upper-bound reference, dubbed Fine-tuning, refers to ranking models that are directly trained on the target language pair X-Y, i.e. no zero-shot transfer. Following Roy et al. (2020) , we adopt the Translate Test baseline and translate any test data into EN using using our bilingual lexicons induced from word embeddings. On this data we evaluate both the Zero-shot baseline (Zero-shot Translate Test ) and our ML-CS model (ML-CS Translate Test ).\n\nResults and Discussion\nWe observe that code-switching improves crosslingual and multilingual re-ranking, while not impeding monolingual setups, as shown next.\nTransfer into MoIR vs. CLIR. We first quantify the performance drop when transferring models trained on EN-EN to MoIR as opposed to CLIR and MLIR. Comparing Zero-shot results between different settings we find that the average MoIR performance of 25.5 MRR@10 (Table 1 ) is substantially higher than CLIR with 15.7 MRR@10 (Table 2 ) and MLIR with 16.6 MRR@10 (Table 3 ).\nThe transfer performance greatly varies with the language proximity, in CLIR the drop is larger for setups involving typologically distant languages (AR-IT, AR-RU), to a lesser extent the same observation holds for MoIR (AR-AR, RU-RU). This is consistent with previous findings made in other syntactic and semantic NLP tasks (He et al., 2019; Lauscher et al., 2020) . The performance gap to Fine-tuning on translated data is much smaller in MoIR (+4 MRR@10) than in CLIR (+11.1 MRR@10) and MLIR (+8.3 MRR@10). Our aim to is close this gap between zero-shot and full finetuning in a resource-lean way by training on codeswitched queries and documents.\nCode-Switching Results. Training on codeswitched data consistently outperforms zero-shot models in CLIR and MLIR (Table 2 and Table 3 ). In AR-IT and AR-RU we see improvements from 7.7 and 7.1 MRR@10 up to 15.6 and 14.1 MRR@10, rendering our approach particularly effective for distant languages. Encouragingly, Table 1 shows that the differences between both of our CS approaches (BL-CS and ML-CS) versus Zero-shot is not statistically significant, showing that gains can be obtained without impairing MoIR performance. Table 2 shows that specializing one zero-shot model for multiple CLIR language pairs (ML-CS, Wiki-CS) performs almost on par with specializing one model for each language pair (BL-CS).\nThe results of Wiki-CS are slightly worse in MoIR and on par with ML-CS on MLIR and CLIR.\nTranslate Test vs. Code-Switch Train. In MoIR (Table 1 ) both Zero-shot Translate Test and ML-CS Translate Test underperform compared to other approaches. This shows that zero-shot rankers work better on clean monolingual data in the target language than noisy monolingual data in English. In CLIR, where Translate Test bridges the language gap between X and Y, we observe slight improvements of +0.2 and +2.2 MRR@10 (Table 2 ). However, in both MoIR and CLIR Translate Test consistently falls behind code-switching at training time.\nMultilingual Retrieval and Unseen Languages.\nHere we compare how code-switching fares against Zero-shot on languages to which neither model has been exposed to at training time. Table 3 shows the gains remain virtually unchanged when moving from six seen (+4.1 MRR@10 / +3.8 MRR@10) to fourteen languages including eight unseen languages (+3.9 MRR@10 / +4.0 MRR@10). Results in Appendix B confirm that this holds for unseen languages on the query, document and both sides, suggesting that the best pivot language for zeroshot transfer (Turc et al., 2021) may not be monolingual but a code-switched language. On seen languages ML-CS is close to MT (Fine-tuning).\nAblation: Translation Probability. The translation probability p allows us to control the ratio of code-switched tokens to original tokens, with p \" 0.0 we default back to the Zero-shot base- Table 4 : MLIR results on seen languages (MRR@10) broken down into queries that share no common tokens (no overlap), between one and three tokens (some overlap) and more than three tokens (significant overlap) with their relevant documents. Gains of ML-CS are shown in brackets. EN-X has 3,116 queries with no overlap, 3,095 with some overlap and 769 with significant overlap. X-EN has 3,147 queries with no overlap, 2,972 with some overlap and 861 with significant overlap. X-X has 3,671 queries with no overlap, 2,502 with some overlap and 807 with significant overlap.\nline, with p \" 1.0 we attempt to code-switch every token. 6 Figure 1 (top) shows that code-switching a smaller portion of tokens is already beneficial for the zero-shot transfer into CLIR. The gains are robust towards different values for p. The best results are achieved with p \" 0.5 and p \" 0.75 for BL-CS and ML-CS, respectively. Figure 1 (bottom) shows that the absolute differences to Zero-shot are much smaller in MoIR.\nMonolingual Overfitting. Exact matches between query and document keywords is a strong relevance signal in MoIR, but does not transfer well to CLIR and MLIR due to mismatching vocabularies. Training zero-shot rankers on monolingual data biases rankers towards learning features that cannot be exploited at test time. Code-Switching reduces this bias by replacing exact matches with translation pairs, 7 steering model training towards learning interlingual semantics instead. To investigate this, we group queries by their average token overlap with their relevant documents and evaluate each 6 Due to out-of-vocabulary tokens the percentage of translated tokens is slightly lower: 23% for p \" 0.25, 45% for p \" 0.5, 68% for p \" 0.75 and 92% for p \" 1.0. In Wiki CS 90% of queries and documents contain at least one translated n-gram, leading to 20% of translated tokens overall. 7 We analyzed a sample of 1M positive training instances and found a total of 4,409,974 overlapping tokens before and 3,039,750 overlapping tokens after code-switching (ML-CS, p \" 0.5), a reduction rate of ~31%. group separately on MLIR. 8 The results are shown in Table 4 . Unsurprisingly, rankers work best when there is significant overlap between query and document tokens. However, the performance gains resulting from training on code-switched data (ML-CS) are most pronounced for queries with some token overlap (up to +5.4 MRR@10) and no token overlap (up to +6.8 MRR@10). On the other hand, the gains are much lower for queries with more than three overlapping tokens and range from -0.5 to +1.4 MRR@10. This supports our hypothesis that code-switching indeed regularizes monolingual overfitting.\n\nConclusion\nWe propose a simple and effective method to improve zero-shot rankers: training on artificially code-switched data. We empirically test our approach on 36 language pairs, spanning monolingual, cross-lingual, and multilingual setups. Our method outperforms zero-shot models trained only monolingually and provides a resource-lean alternative to MT for CLIR. In MLIR our approach can match MT performance while relying only on bilingual dictionaries. To the best of our knowledge, this work is the first to propose artificial code-switched training data for cross-lingual and multilingual IR.\n", "hypothesis": " We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR).  Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR.  Encouragingly, the gains are especially pronounced for distant languages (up to 2x absolute gain).  We further show that our approach is robust towards the ratio of codeswitched tokens and also extends to unseen languages.  Our results demonstrate that training on code-switched data is a cheap and effective way of generalizing zero-shot rankers for crosslingual and multilingual retrieval..", "answer": true}
{"title": "Evaluating Paraphrastic Robustness in Textual Entailment Models", "content": "\nIntroduction\nRecognizing Textual Entailment (RTE), the task of predicting whether one sentence (hypothesis) would likely be implied by another (premise), is central to natural language understanding (NLU; Dagan et al., 2005) , as this task captures \"all manners of linguistic phenomena and broad variability of semantic expression\" (MacCartney, 2009) . If an RTE model has a sufficiently high capacity for reliable, robust inference necessary for full NLU (Mac-Cartney, 2009) , then the model's predictions should be consistent across paraphrased examples.\nWe introduce P aRT E, a test set to evaluate how reliable and robust models are to paraphrases (Table 1 includes an example). The test set consists of examples from the Pascal RTE1-3 challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) rewritten with a lexical rewriter and manually verified to preserve the meaning and label of the original RTE sentence-pair. We use this evaluation set to determine whether models change their predictions when examples are paraphrased.\nWhile this may not be a sufficient test to determine whether RTE models fully understand language, as there are many semantic phenomena that RTE models should capture (Cooper et al., 1996; Naik et al., 2018) , it is necessary that any NLU system be robust to paraphrases.\n\nP\nThe cost of security when world leaders gather near Auchterarder for next year 's G8 summit, is expected to top $150 million. P' The cost of security when world leaders meet for the G8 summit near Auchterarder next year will top $150 million.\n\nH\nMore than $150 million will be probably spent for security at next year's G8 summit. H' At the G8 summit next year more than $150 million will likely be spent on security at the event.\nTable 1 : An original and paraphrased RTE example.\nThe top represents an original premise (P) and its paraphrase (P'). The bottom depicts an original hypothesis (H) and its paraphrase (H'). A model robust to paraphrases should have consistent predictions across the following pairs: P-H, P'-H, P-H', and P'-H'.\nOur experiments indicate that contemporary models are robust to paraphrases as their predictions do not change on the overwhelmingly large majority of examples that are paraphrased. However, our analyses temper this claim as models are more likely to change their predictions when both the premise and hypothesis are phrased compared to when just one of the sentences is rewritten. We release P aRT E 1 to encourage others to evaluate how well their models perform when RTE examples are paraphrased.\n\nRelated Work\nWith the vast adoption of human language technology (HLT), systems must understand when different expressions convey the same meaning (paraphrase) and support the same inferences (entailment). Paraphrasing and entailment are closely connected as the former is a special case of the latter where two sentences entail each other (Nev\u011b\u0159ilov\u00e1, 2014; Fonseca and Alu\u00edsio, 2015; V\u00edta, 2015; Ravichander et al., 2022) . Para-phrasing has been used to improve RTE predictions (Bosma and Callison-Burch, 2006; Sun et al., 2021) and RTE has been used for paraphrase identification (Seethamol and Manju, 2017) and generation (Arora et al., 2022) . Furthermore, both phenomena are key to NLU (Androutsopoulos and Malakasiotis, 2010) and work such as Zhao et al. (2018) ; Hu et al. (2019) have explored rewriting RTE examples to create more robust models.\nWe follow a long tradition of evaluating linguistic phenomena captured in RTE models (Cooper et al., 1996) . Recent tests focus on evaluating how well contemporary RTE models capture phenomena such as monotonicity (Yanaka et al., 2019a,b) , verb veridicality (Ross and Pavlick, 2019; Yanaka et al., 2021) , presuppositions (Parrish et al., 2021) implicatures (Jeretic et al., 2020) , basic logic (Richardson et al., 2020; Shi et al., 2021) , figurative language (Chakrabarty et al., 2021) , and others (Naik et al., 2018; Poliak et al., 2018a; Vashishtha et al., 2020) . Unlike many of those works that evaluate models' accuracy on examples that target specific phenomena, we use a contrastive approach (Prabhakaran et al., 2019; Gardner et al., 2020) to determine whether RTE models' predictions change when examples are paraphrased.\n\nP aRT E\nTo explore whether these RTE models are robust to paraphrases, we create P aRT E, a modified version of the Pascal RTE1-3 challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007) . P aRT E contains 1,126 examples of an original unmodified RTE sentence-pair grouped with a sentence-pair with a modified premise, hypothesis, or both. We use the examples in RTE1-3 to create our test set, as opposed to other RTE datasets due to its long-standing history.\n\nParaphrase Generation & Verification\nFor each RTE premise-hypothesis pair (P-H), we created three paraphrased premises (P') and hypotheses (H') using a T5-based paraphraser 2 finetuned on the Google PAWS dataset (Zhang et al., 2019) . To ensure lexically diverse paraphrases, we filter out any paraphrases that have high lexical overlap with the original sentences using Jaccard index threshold of 0.75. Out of 14,400 generated sentences, 2,449 remained -956 paraphrased premises (P') and 1,493 paraphrased hypotheses (H'). Next, we retained 550 paraphrased premises and 800 paraphrased hypotheses paraphrases that crowdsource workers identified as grammatical and similar in meaning to the original sentences. 3 We include a grammatical check since an existing RTE evaluation set focused on paraphrases (White et al., 2017) contains hypothesis-only biases related to grammaticality (Poliak et al., 2018b) .\nIf at least one P' or one H' passes this filtering process, we retain the original RTE example and pair it with a corresponding paraphrased example (i.e. P'-H', P'-H, or P-H'). In the case where more than one P' or H' passes the filtering, we retained the P' or H' that crowdsource workers deemed most similar to the original sentence. Out of the original 2,400 RTE test pairs, we retain 914 pairs with a high-quality P' or H', resulting in 1,178 original and paraphrased RTE pairs. 4\n\nOvercoming Semantic Variability\nMacCartney (2009) argues that in addition to being reliable and robust, RTE models must deal with the broad variability of semantic expression. In other words, though two sentences may be semantically congruent, it is possible that small variations in a paraphrased sentence contain enough semantic variability to change what would likely, or not likely be inferred from the sentence. Despite all P' and H' being deemed to be semantically congruent with their corresponding original sentences, the semantic variability of paraphrases might change whether H or H' can be inferred from P' or P.\nTherefore, propagating an RTE label from an original sentence pair to a modified sentence pair might be inappropriate. We manually determined that this issue occurs in just 52 (4%) examples, and retained 1,126 examples. This ensures an evaluation set of high-quality examples that can be used to determine whether models are sensitive to paraphrases and change their prediction on paraphrased examples. Our dataset contains 402 examples with just a paraphrased premise P', 602 with just a paraphrased hypothesis H', and 122 with both a paraphrased premise and hypothesis. \n\nExperimental Setup\nWe explore models built upon three different classes of sentence encoders: bag of words (BoW), LSTMs, and Transformers. Our BoW model represents premises and hypotheses as an average of their tokens' 300 dimensional GloVe embeddings (Pennington et al., 2014b) . The concatenation of these representations is fed to an MLP with two hidden layers. For the BiLSTM model, we represent tokens with GloVe embeddings, extract sentence representations using max-pooling, and pass concatenated sentence representations to an MLP with two hidden layers.\nOur transformer-based models are pre-trained BERT (Devlin et al., 2019) and Roberta (Liu et al., 2020) encoders with an MLP attached to the final layer. Additionally, we use GPT-3 in a zero-shot setting where we ask it to label the relationship between a premise and hypothesis. 5 The RTE training sets do not contain enough examples to train deep learning models with a large number of parameters. We follow the common practice of training models on MNLI and using our test set to evaluate how well they capture a specific phenomenon related to NLU. During testing, we map the MNLI 'contradiction' and 'neutral' labels to the 'not-entailed' label in RTE, following common practice (Wang et al., 2018; Yin et al., 2019; Ma et al., 2021; Utama et al., 2022, inter ailia) .\n\nResults\nTable 2 report the results. The RTE and P aRT E columns respectively report the models' accuracy on the 1,126 unmodified and paraphrased sentence pairs. 6 Comparing the difference in accuracy be-5 See Appendix A for more details, including hyperparameters, model sizes, and GPT-3 prompt design and configurations. Our code is available at https://github.com/ stonybrooknlp/parte 6 Although there are just 914 unmodified sentence pairs, for the sake of a head-to-head comparison, we retain all instances tween unmodified and paraphrased examples can be misleading. If the number of times a model changes a correct prediction is close to the number of times it changes an incorrect prediction, then the accuracy will hardly change. Figure 1 demonstrates why the accuracies do not change by much when models' predictions change on paraphrased examples. Furthermore, if a model is robust to paraphrases, then it should not change its predictions when an example is paraphrased, even if the prediction on the original unmodified example was incorrect. Hence, our test statistic is the percentage of examples where a model's predictions change (% \u2206 P aRT E column in Table 2 ) rather than a change in accuracy. Compared to the Transformer based models, the BoW and BiLSTM models seem to be more sensitive, and less robust to paraphrasing, as they change their predictions on 15.27% and 16.69% respectively of the 1,126 examples. However, this might be associated with how word xembedding models only just outperform random guesses in and perform much worse on RTE compared to the Transformer models.\nof the unmodified sentence pairs when computing accuracy. Focusing on the Transformer models, we noticed that RoBERTa performs the best on the datasets and is the most robust to paraphrasing -changing its predictions on just under 8% of paraphrased examples. Interestingly, when the models are trained specifically to perform this task, the models change their predictions on fewer paraphrased examples as these models' accuracy increases. However, improving performance alone might not automatically improve models' robustness to paraphrases. GPT-3's accuracy noticeably outperforms BERT's accuracy, but GPT-3 changes its predictions on more paraphrased examples compared to BERT. P'-H' compared to P-H' or P'-H Figure 2 shows noticeable increases in the percentage of changed predictions when both premise and hypothesis are paraphrased compared to when just one of the sentences is paraphrased. Specifically, for BoW and BiLSTM we see an increase of 4.01 and 6.01 percentage points respectively, and for BERT, Roberta, GPT-3 increases of 4.97, 4.83, and 3.55. As the transformer-based models changed their predictions on 12-14% of examples where both sentences are paraphrased compared to 9-11% in general, this analysis further suggests that these models are not as robust to paraprhases as desired.\nEntailed vs Not-entailed examples RTE analyses often differentiate how models perform on entailed vs not entailed examples (Liu et al., 2022) . In Figure 3 , we do not see meaningful differences in how models' predictions change on paraphrased examples based on the gold label. This might suggest that our dataset does not contain statistical irregularities based on the RTE labels. Correct vs Not-Correct Predictions Figure 4 shows that the Transformer models' predictions is more likely to change when it's prediction on an original example was incorrect (right red bars) compared to when the prediction for an original example was correct (left blue bars). For example, when RoBERTa's prediction for an original RTE example was correct, the model changed its prediction on just 5.5% of the corresponding paraphrased examples. When RoBERTa's predictions for an original RTE example were incorrect, RoBERTa's predictions changed for 20.88% corresponding paraphrased examples. Analyzing differences in models' confidences assigned to predictions might provide more insight (Marc\u00e9 and Poliak, 2022) . We leave this for future work.\nSource Task RTE1-3 examples originated from multiple domains and downstream tasks, e.g. question-answering (Moldovan et al., 2006) , information extraction (Grishman and Sundheim, 1996) , and summarization (Evans et al., 2004; Radev et al., 2001) . This enables researchers to evaluate how \n\nConclusion\nWe introduced P aRT E, a high-quality evaluation set of RTE examples paired with paraphrased RTE examples. We use our evaluation set to determine whether RTE models are robust to paraphrased examples. Our experiments indicate that while these models predictions are usually consistent when RTE examples are paraphrased, there is still room for improvement as models remain sensitive to changes in input (Jia and Liang, 2017; Belinkov and Bisk, 2018; Iyyer et al., 2018) . We hope that researchers will use P aRT E to evaluate how well their NLU systems perform on paraphrased data.\n", "hypothesis": "We use the evaluation set to determine if RTE models' predictions change when examples are rephrased.", "answer": false}
{"title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark", "content": "\nIntroduction\nAlthough large language models (LLMs) are powerful tools for generating human-like language, they can also memorize false or outdated associations, limiting their applicability. Model editing techniques promise to solve this problem by correcting non-factual associations. It is important that model edits are highly specific in the sense of not introducing any unwanted associations as a side effect. In this paper, we discuss why the current benchmark for specificity falls short and propose a more challenging, dynamic specificity benchmark to evaluate model editing techniques. Using this benchmark, we evaluate recent model editing techniques and find previously unreported side effects. We highlight the importance of improved specificity benchmarks for the effective and safe use of LLMs subject to model edits.\nFigure 1 : Unintended side effects of model edits and how to measure them. (a) GPT-2-medium is edited using ROME to counter-factually associate the Louvre's location with Rome. However, this results in unintended associations (\"loud facts\") like the association of Obama with Rome, suggesting low specificity of the edit. The edit also significantly increases the maximum logit (shown in brackets), suggesting that the edit is not merely replacing \"Paris\" with \"Rome\" in the desired contexts. (b) Measuring specificity by the fraction of correctly completed test prompts (COUNTERFACT) suggests a high specificity for ROME. Prepending the edit prompt (like \"The Louvre is in Rome.\") to each test prompt (COUNTERFACT+) results in a significant drop in performance. A significant drop in measured specificity can also be observed if the model edit is implemented using constrained fine-tuning (FT-L).\nModel editing updates the parameters of a trained model in order to change its predicted probability distributions without retraining the entire model. This can be used to edit the associations that the model has memorized and hence, improve the accuracy of the model. Fig. 1 shows the example of a counter-factual model edit using ROME (Meng et al., 2022a) where the location of the Louvre is edited to be Rome instead of Paris. We use a counter-factual example since it makes it more evident that the new association is an effect of the model edit instead of the model training. Note that the examples in Fig. 1 are not taken from the COUNTERFACT+ dataset introduced below, but serve to intuitively illustrate the model editing failure modes we are interested in.\nAn important desideratum for model editing is specificity. Specificity captures how well the effect of the model edit is localized; in other words, specificity measures the absence of unintended side effects of model edits. Fig. 1 shows two examples of unintended side effects of ROME model editing, which we collectively call the problem of \"loud facts\". In the first example, mentioning \"Louvre\" (the subject of the model edit) leads the edited model to also complete unrelated test prompts (\"Obama was born in\") with \"Rome\" (the object of the model edit). In the second example, mentioning \"Louvre\" boosts the logits for words semantically related to \"Rome\", like \"Vatican\".\nThe existing specificity benchmark for model editing from the COUNTERFACT dataset (Meng et al., 2022a) suffers from two limitations which can be illustrated using these examples. First, COUNTERFACT does not prompt the model in a way that is likely to surface unwanted side effects. As demonstrated by the examples in Fig. 1 , mentioning the subject of the model edit can drastically change the behavior of the edited model, but the existing benchmark does not detect this. Second, COUNTERFACT considers only the probabilities for the original and edited object token (\"Paris\" and \"Rome\"). As shown by the last example in Fig. 1 , the edited model displays strongly changed logits not only for the original object (\"Paris\") and edit object (\"Rome\") but also for semantically related tokens (\"Vatican\"). Again, this would be overlooked by the current specificity evaluation since it does not consider the entire probability distribution.\nThese limitations mean that side effects of edits may be overlooked and specificity overestimated.\nOur main contributions are:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark.\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution instead of the currently used metrics which focus only on the tokens directly implicated in the model edit. (De Cao et al., 2021) and (Mitchell et al., 2022) . Elazar et al. (2021) introduced ParaRel, a curated dataset of paraphrased prompts and facts. Meng et al. (2022a) use this as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, including specificity.\nKnowledge extraction from LLMs. The assessment of knowledge within language models (LMs) has typically been done by evaluating whether the model is able to predict pieces of knowledge; Petroni et al. (2019 Petroni et al. ( , 2020 ) defined a fill-in-theblank prompt and asked the LM to complete it. Subsequent work has demonstrated that knowledge extraction can be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021) , or by fine-tuning a model on open-domain textual facts (Roberts et al., 2020) . However, constructing prompts from supervised knowledge extraction data is still prone to learning new knowledge instead of recalling existing knowledge in an LM (Zhong et al., 2021) .\n\nDataset\nWe investigate the specificity of recent model editing techniques using the COUNTERFACT benchmark introduced in (Meng et al., 2022a) . COUNTERFACT is a collection of 21,919 nonfactual statements of the form (subject, relation, object) (s, r, o * ), which have low probabilities prior to the model edit. For each of these non-factual statements, we perform a model edit targeting this specific statement. To measure specificity, we then check whether any other associations in the model change in undesired ways. COUNTERFACT supports this check by providing a set of so-called neighborhood prompts for every non-factual statement used in the model edit. These neighborhood prompts are constructed as follows: For a model edit of the form (s, r, o c ) \u2192 (s, r, o * ) (where o c is the correct object, and o * is the false, counterfactual object), COUNTERFACT samples a set of nearby subjects s n for which (s n , r, o c ) holds true. Neighborhood prompts are then paraphrases of the collected (s n , r).\nSuppose, for example, the edit request was (Darrieux, mother_tongue, French) \u2192 (Darrieux, mother_tongue, English). COUNTERFACT takes the relation and object from the edit request (mother_tongue, French), samples true factual associations for this relation, object pair; e.g., (Montesquieu, mother_tongue, French) and then samples a random paraphrase, such as \"The native language of Montesquieu is\". These neighborhood prompts can be used to inspect whether the model edit has undesired side effects on closely related factual associations. See appendix C for a sample from the COUNTERFACT dataset, including the full set of neighborhood prompts.\nMotivated by the example of loud facts shown in Fig. 1 and by the intuition that unwanted side effects are more likely when the model is primed with the linguistic context of the model edit, we now introduce a dynamic version of COUNTERFACT which we will refer to as COUNTERFACT+. To obtain COUNTERFACT+, we modify the neighborhood prompt by prepending the model edit. For example, if the original prompt is \"The native language of Montesquieu is\" the modified prompt would be \"The mother tongue of Danielle Darrieux is English. The native language of Montesquieu is\". See appendix D for a sample of the modified neighborhood prompts used for COUNTERFACT+.\nTo understand why we call COUNTERFACT+ a dynamic version of COUNTERFACT consider how either dataset would be applied to evaluate the success of a model edit: In both cases, we would need to identify the set N of neighborhood prompts in the dataset that are semantically closest to the intended model edit. But in COUNTERFACT, we would use N as is, whereas in COUNTERFACT+ we would change every prompt in N as a function of the model edit, as described above.\n\nMetrics\nTo evaluate the specificity of a model edit on COUNTERFACT, Meng et al. (2022a,b) use two metrics, called Neighborhood Score and Neighborhood Magnitude. Denoting the post-edit probabilities for the correct token o c and incorrect edit token o * by P * (o c ) and P * (o * ), respectively, these are defined as follows: The Neighborhood Score (NS) is defined as the fraction of neighborhood prompts for which P * (o c ) > P * (o * ). The Neighbourhood Magnitude (NM) is defined as P * (o c ) \u2212 P * (o * ), the difference in probability assigned to the correct token versus the incorrect edit token. High NS and NM indicate that the edit has small unwanted side effects.\nNS and NM, however, do not detect cases where the model edit significantly changes the predicted probability for tokens other than o c and o * , such as in the last example in Fig. 1 . To capture this possibility, we introduce as an additional metric the Kullback-Leibler (KL) divergence of the nexttoken distribution between the edited and unedited model, referred to as Neighborhood KL Divergence (NKL). Abbreviating the next token probability distribution for the unedited and edited models by P (w) and P * (w), respectively, and denoting the token vocabulatory by W, NKL is defined as KL divergence between P (w) and P * (w):\nEQUATION\nA large NKL is undesirable because it implies that the next-token probability distribution for neighborhood prompts has been strongly affected by the model edit.\n\nModels and Model Editing Algorithms\nWe use GPT-2-medium (355M parameters), GPT-2-XL (1.5B) (Radford et al., 2019) , and GPT-J (6B) (Wang and Komatsuzaki, 2021) to evaluate the following model editing methods:\n\u2022 ROME (Rank-One-Model-Editing) performs a rank-one update of a single MLP layer to implement the edit (Meng et al., 2022a) .\n\u2022 MEMIT (Mass-Editing Memory in a Transformer) extends ROME to updates across several MLP layers (Meng et al., 2022b) . Note that we do not test using multiple simultaneous edits.\n\u2022 FT-L: Fine-Tuning with an L \u221e norm constraint (Zhu et al., 2020) , constrained to a single layer, as described in (Meng et al., 2022a) .\nWe use FT-L as a simple baseline.\n\nResults\nFigure 2 shows the results for the ROME, MEMIT, and FT-L editing algorithms applied to the GPT-J (6B) model for different specificity metrics and datasets considered in this work. When evaluated using the Neighborhood Score (Fig. 2 , top), we observe significant drops in specificity for all editing algorithms when going from COUNTERFACT to COUNTERFACT+. Note that specificity measured on the unedited model (GPT-J (6B)) also drops suggesting that there is confounding from the test prompts in COUNTERFACT+, potentially due to recency bias (Zhao et al., 2021) . The drop in specificity is much more pronounced for ROME and MEMIT, compared to FT-L and the unedited model, however. This shows that:\n\u2022 ROME and MEMIT have undesired side effects which are not detected by COUNTERFACT\n\u2022 the improved benchmark COUNTERFACT+ is able to detect these unwanted side effects When evaluating specificity using the newly introduced Neighborhood KL Divergence (Fig. 2 , bottom), we observe a large spike in divergence for both ROME and MEMIT when going from COUNTERFACT to COUNTERFACT+. FT-L shows a much smaller increase in divergence from COUNTERFACT to COUNTERFACT+. Figure 3 in the appendix shows the results on COUNTERFACT and COUNTERFACT+ for the NM metric. (top) NS, the average fraction of correctly completed neighborhood test prompts after the model edit (larger is better). We see that COUNTERFACT+ is a much more challenging specificity benchmark: Success rates NS on it range from 33% to 54% across different editing algorithms while they are close to 80% for COUNTERFACT. (bottom) NKL, the KL divergence of the next-token probability distribution of the edited model from that of the unedited model, averaged over all neighborhood test prompts. A lower value indicates higher specificity (the edited model behaves more like the unedited model).\nResults across all three models are shown in tables 1 to 3. These tables list the mean scores on COUNTERFACT and COUNTERFACT+ for the Neighborhood Score (NS), Neighborhood Magnitude (NM), and Neighborhood KL divergence (NKL), respectively. The brackets give upper and lower bound of 99% confidence intervals obtained via bootstrap resampling (N=1,000 The results from tables 1 to 3 show that the significant drop in specificity when evaluating on\nNKL \u2193 COUNTERFACT COUNTERFACT+ GPT-2 M FT-L 1.4e-05 (1.3, 1.4) 1.4e-05 (1.3, 1.4) ROME\n1.6e-06 (1.4, 1.7) 2.5e-05 (2.5, 2.5)\nGPT-2 XL FT-L 7.2e-06 (6.9, 7.4) 9.5e-06 (9.3, 9.7) ROME 1.5e-06 (1.4, 1.6) 3.3e-05 (3.2, 3.3) MEMIT 2.9e-07 (2.5, 3.4) 9.0e-06 (8.8, 9.1) GPT-J (6B)\nFT-L 3.2e-06 (3.1, 3.4) 5.2e-06 (5.1, 5.3) ROME 3.5e-06 (3.2, 3.8) 1.8e-05 (1.8, 1.9) MEMIT 9.2e-07 (8.0, 10) 9.9e-06 (9.8, 10)\nTable 3 : Neighborhood KL Divergence NKL (\u00b5 & 99% CI) on COUNTERFACT and COUNTERFACT+. Note that the order of magnitude is suppressed for the confidence interval for visual clarity; it is the same as for the mean.\nCOUNTERFACT+ (compared to COUNTERFACT) holds across different model sizes and is not an artefact of using a particular model. Section B in the appendix discusses the scaling of specificity with model size in more detail.\n\nConclusion\nModel editing techniques for auto-regressive transformers exhibit unreported issues related to specificity. Although our fine-tuning baseline, FT-L, exhibits less vulnerability to these issues than ROME and MEMIT, it falls short in competing with them regarding crucial model editing metrics such as robustness to paraphrasing (Meng et al., 2022a,b) . This indicates that model editing still presents numerous complexities that require future attention. Additionally, we revealed that the existing COUNTERFACT benchmark fails to detect the low specificity in ROME and MEMIT. To address this limitation, our primary contributions include:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit.\n", "hypothesis": " We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity.  Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects..", "answer": true}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": "In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering.  We apply computationally lightweight ensemble-based UE methods to seq2seq models and show that they often outperform density-based methods on the task of OOD detection.", "answer": false}
{"title": "Controllable Mixed-Initiative Dialogue Generation through Prompting", "content": "\nIntroduction\nMixed initiative dialogue systems allow all interacting agents to initiate actions to control the interaction. These systems dynamically adapt interaction styles to regain control and progress towards specific goals (Allen et al., 1999; Chu-Carroll, 2000) , unlike others which passively respond to users' input (e.g. some assistants like ChatGPT), Mixed initiative dialogue systems thus often involve complex policy planning sub-tasks to determine optimal turn-level system dialogue intents (Peng et al., 2018; Hiraoka et al., 2013; Muise et al., 2019; Liu et al., 2020) . These policies define when it is optimal for a system to regain initiative (e.g., when a moderator should interject in a conversation, or when a companion should ask questions or change a conversation topic).\nHowever, \"optimal\" planned dialogue intents still need to be executed through \"optimal\" response models. The standard practice in recent dialogue research has been to fine-tune a pretrained language model for conditional generation My girlfriend dumped me for someone else! I've been really shook up.\nI'm so sorry to hear that :( Do you have any idea why? I think she just lost interest in me? It happened a couple months ago. I wish I could get over it. I can't get her off my mind. She's dating this total loser.\n\nRestatement or Paraphrasing\nFine-Tuning: I'm so sorry to hear that Ground Truth: That's tough... so you're saying you grew apart a little? Or do you think it was more one-sided?\nPrompting: It sounds like you're feeling really hurt and frustrated by the situation with your ex-girlfriend. It can be really difficult to see her move on with someone else. to achieve semantic control through some combination of innovations in model architectures or learning processes (Liu et al., 2021; Chen et al., 2019) . Such generation approaches still leave room for error. Assuming that there exists a truly optimal dialogue policy planner, a response model may still generate according to the wrong intent (partially due to the fact that dialogue datasets often have annotation errors (Qian et al., 2021; Zang et al., 2020) ). Or, a model may learn to generate correct intents but fail to create a response consistent with conversational context (Chen et al., 2022b) . Additionally, training corpora often differ in demographic and distribution compared to production environments, which can lead to deteriorating response quality (Koh et al., 2021) .\nWe propose using vanilla large pre-trained language models (LLMs) such as GPT-3 (Brown et al., 2020) as drop-in replacements to traditional finetuned conditional generation models for mixedinitiative dialogue systems. LLMs typically have been trained on massive corpora with large amounts of linguistic variety, making them more robust to overfitting specific tasks. Recent work demonstrates that LLMs have reasonable semantic control through few-shot prompting (Brown et al., 2020; Chen et al., 2023; Meng et al., 2022) . Here, we demonstrate how 1 to systematically prompt LLMs for mixed-initiative dialogue generation. Evaluations yielded strong performance on two popular English mixed-initiative tasks: Emotional Support Conversations (ESC; Liu et al. (2021) ) and Persua-sionForGood (P4G; Wang et al. (2019b) ).\n\nRelated Work\nControllable Generation approaches often involve fine-tuning a model conditioned on control codes (Keskar et al., 2019; Ficler and Goldberg, 2017) , additional attribute representations in hidden states (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019a) . Other work has attempted to mitigate the computational cost of fine-tuning, e.g. by training an auxiliary networks to guide the original LM (Dathathri et al., 2020; Yu et al., 2021; Pascual et al., 2021) . Here, we attempt controllable generation that replaces fine-tuning by prompting LLMs.\nPrompting in Dialogue Research typically has focused on understanding tasks such as dialogue planning (Kuo and Chen, 2022) or state tracking (Lee et al., 2021; Mi et al., 2022) . More recent dialogue research has examined using prompting for generating conversational data with varying levels of control (Kim et al., 2022; Chen et al., 2022a; Mehri et al., 2022; Chen et al., 2023) , citing the difficulty of using vanilla language models in production. Studies focusing on response generation looked at prompting LLMs specifically for knowledge-grounded dialogue generation (Liu et al., 2022; Madotto et al., 2021; Shuster et al., 2022) . Our work is the first to construct an interactive prompt-based mixed initiative dialogue system and evaluate the semantic control of prompting.\n1 Code to reconstruct all prompts available at https://github.com/maxlchen/Controllable-Mixed-Initiative-Dialogue-Generation\n\nDatasets\nWe examined ESC (Liu et al., 2021) ) and P4G (Wang et al., 2019b) . ESC consists of 1053 conversations between emotional help-seekers and supporters. Each conversation is annotated with the help-seeker's description of their problem, and the type of issues they are facing. Each turn by the supporters is annotated with one of eight emotional support strategies (Table A1 ). P4G contains 300 annotated conversations between persuaders who attempt to persuade persuadees to donate to a charity called Save the Children. Persuader turns are annotated with one of 10 strategies (Table A2 ).\n\nBaselines\nIn mixed-initiative dialogue, interacting parties continuously exchange control throughout the conversation. However, in order for agents to regain control, they must be able to properly execute items from their conversational agenda, e.g. generating a response that matches a desired strategy/intent. Liu et al. (2021) fine-tuned BlenderBot (Roller et al., 2021) on ESC using input representations consisting of flattened dialogue history and the predicted emotional support strategy for a specific turn. The best-performing model in their experimental setting is \"Oracle-BlenderBot\" which conditions on the ground truth strategy for a given turn.\nChen et al. (2022b) proposed a persuasive dialogue system called RAP, which combined targeted user response with conditional generation. The conditional generation component of RAP involves fine-tuning BART (Lewis et al., 2020) using a penalized loss to force the model to artificially create semantic control through dialogue intents.\n\nMixed-Initative Dialogue Prompting\nRAP required introducing a dialogue intent classifier to weakly supervise the training process, as there is not an oracle for whether the dialogue intent of a candidate response is correct. But, this confounds errors, as classifiers are imperfect. Moreover, fine-tuning approaches like both RAP and Oracle-BlenderBot involve balancing a tradeoff between response quality and semantic control accuracy. Prompting LLMs avoids both issues as it does not involve adjusting model weights to learn representations of control codes for individual tasks.\nIn this paper, we systematically prompt Instruct-GPT \"text-davinci-003.\" Rather than requiring expert-level prompt engineering, we create general prompt templates which directly fill slots using roles and annotations from both ESC and P4G. Specifically, we split up prompt construction into Task Background and Conversation History.\nFigure 2 breaks down an example of a prompt for ESC. The Task Background is a paragraph formed from the \"emotion type,\" \"problem type,\" and \"situation\" annotations provided by the corpus. The Conversation History consists of each prior utterance, prepended by labels for each speaker. The system-side turns are also prefixed by a natural language form of the annotated emotional support strategy, derived from the annotation scheme in Liu et al. ( 2021) (e.g. \"The Therapist acknowledges the Patient's feelings by paraphrasing their situation.\"). Figure 2 contains the contextual dialogue turns in order, along with the three support strategies used.\nThe P4G prompting style is similar. Unlike personalized emotional support conversations, the task does not change, so the Task Background is fixed with relevant factual background information. The Conversation History still interweaves narrative directions for each persuasive strategy (e.g. \"The Persuader uses a logical appeal.\"). Example provided in Figure A1 . The natural language intent mappings for both tasks are provided in Tables A1,A2 .\n\nExperiments\nWe evaluated prompting statically and interactively.\n\nStatic Evaluation\nWe quantified how much semantic and pragmatic control vanilla LLMs can provide in conversation. We randomly sampled 100 responses from ESC (supporters) and P4G (persuaders). Each response's conversational history and strategy annotation was used to generate responses via prompting and fine-tuned models. We used Oracle-BlenderBot for ESC and RAP's conditional generation module for P4G.\nWe asked crowdworkers on Amazon Mechanical Turk 2 to evaluate candidate responses' accuracy with respect to its prescribed dialogue intents, coherence, consistency, and engagingness. We paired the dialogue responses from each source (fine-tuning, prompting, or ground truth) with the corresponding responses from each of the other 2 Details for all human evaluation tasks in Appendix A. sources, allowing us to compute preference winrates between each pair. Each job presented only one pair of responses, in a random order. Additionally, we examined automatic metrics through Distinct-N (N \u2208 {3, 4}), as well QuantiDCE (Ye et al., 2021) , a BERT-based automatic dialogue coherence metric for open-domain conversation.\nTable 1 shows that prompt-generated responses are more highly rated in terms of quality compared to responses generated from competitive fine-tuned dialogue models as well as ground truth responses, in terms of all human evaluation metrics. This is also the case for Distinct-N in both tasks, and Quan-tiDCE in P4G. Oracle-BlenderBot slightly outperforms the prompt-generated responses in terms of QuantiDCE for ESC, but this difference is not statistically significant. Table 1 also shows that the prompt-generated responses are consistently preferable to the responses generated from fine-tuned dialogue models as well as the ground truth.\nFinally, we also see that prompting appears to provide the best semantic control over generated responses. Prompt-generated responses had the highest probability of matching the desired dialogue intent, even surpassing that of the ground truth utterances in both corpora. This further demonstrates the difficulty of performing annotation for supervised training -the conversational strategies are subjective, and even the ground truth responses may have annotation errors. The prompt-generated responses are generally of higher quality than both fine-tuned models, which may be a result of the aforementioned difficulty of balancing control accuracy with response quality during generation.\n\nInteractive Evaluation\nWe evaluated prompting as a generation module for mixed-initiative systems. This requires holding fixed other components, including policy planning. RAP is a recently proposed framework for P4G using an \"optimal\" persuasive strategy ordering. But, it built rapport with users by hierarchically integrating social chit-chat and knowledge retrieval with semantically-controlled generation (details in Chen et al. ( 2022b)). We built a system which replaces RAP's fine-tuned BART module with a module that systematically prompts InstructGPT. As with the original implementation of RAP, our prompting module conditions on the knowledge retrieved for factual question answering 3 .\nWe asked crowdworkers to evaluate our system according to the criteria in Table 2 . The system using prompting for generation was consistently rated more favorably than RAP, including in terms of convincingness, persuasiveness, and being a strong reason for donation. We discuss conversation examples in Appendix C. We see that our system was robust to a variety of input language patterns.\n\nDiscussion\nPrompting yields strong performance in mixedinitiative tasks in the low resource regime 4 . Promptgenerated responses are often preferable even compared to ground-truth responses in ESC and P4G. From 17 paired evaluations of ESC where crowdworkers rated ground truth utterances as not matching the ground truth intent annotation, the promptgenerated response was rated as correct 13 times. However, this is likely because many dialogue corpora are created or annotated by crowdworkers, so the data may vary in quality. While LLMs may generate \"better\" responses than crowdworkers, we do not expect them to be better than expert therapists.\nThe results do indicate that prompting may be appropriate for building systems for tasks with limited data. As made evident by our ratings, annotating dialogue intents is a difficult and subjective process prone to errors which can further propagate to fine-tuned task models. This could potentially be addressed by the high semantic control demonstrated through prompting, despite not requiring downstream fine-tuning label supervision.\nThis prompting approach could be applied to other mixed-initiative tasks, including chit-chat and task-oriented dialogue. For instance, many real-world systems such as customer service chatbots already have pre-defined policies for what systems are allowed to say, despite not necessarily having many labeled conversations. A system can be designed as long as there is a policy planner, which could simply be a hierarchical ruleset. While there is some human-effort involved in writing natural language forms of fixed dialogue intents, it is a much less costly process than annotating highquality dialogue data.\n\nConclusion\nWe find encouraging results for prompting on mixed-initiative dialogue tasks, indicating that generated responses are high quality and follow semantic controls. Strong low resource performance opens the possibility of future work building mixedinitiative systems around novel settings which would require subjective data annotation.\n", "hypothesis": " Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner.  The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents.  However, these supervised generation models are limited by the cost and quality of data annotation.  We instead prompt large language models as a drop-in replacement to finetuning on conditional generation.  We formalize prompt construction for controllable mixedinitiative dialogue.", "answer": true}
{"title": "Silver Syntax Pre-training for Cross-Domain Relation Extraction", "content": "\nIntroduction\nRelation Extraction (RE) is the task of extracting structured knowledge, often in the form of triplets, from unstructured text. Despite the increasing attention this task received in recent years, the performance obtained so far are very low (Popovic and F\u00e4rber, 2022) . This happens in particular when considering realistic scenarios which include outof-domain setups, and deal with the whole taskin contrast to the simplified Relation Classification which assumes that the correct entity pairs are given (Han et al., 2018; Baldini Soares et al., 2019; Gao et al., 2019) . One main challenge of RE and other related Information Extraction tasks is the \"domain-specificity\": Depending on the text domain, the type of information to extract changes. For example, while in the news domain we can find entities like person and city, and relations like city of birth (Zhang et al., 2017) , in scientific texts, we can find information about metrics, tasks and comparisons between computational models (Luan et al., 2018) . While high-quality, domain-specific data for fine-tuning the RE models would be ideal, as for many other NLP tasks, annotating data is expensive and time-consuming. 1 A recent approach that leads to improved performance on a variety of NLP tasks is intermediate task training. It consists of a step of training on one or more NLP tasks between the general language model pre-training and the specific end task fine-tuning (STILT, Supplementary Training on Intermediate Labeled-data Tasks; Phang et al., 2018) . However, STILT assumes the availability of additional high quality training data, annotated for a related task.\nIn this paper, we explore intermediate pretraining specifically for cross-domain RE and look for alternatives which avoid the need of external manually annotated datasets to pre-train the model on. In particular, we analyze the affinity between syntactic structure and semantic relations, by considering the shortest dependency path between two entities (Bunescu and Mooney, 2005; Fundel et al., 2006; Bj\u00f6rne et al., 2009; Liu et al., 2015) . We replace the traditional intermediate pre-training step Linear-fractional programming ( LFP ) is a generalization of linear programming ( LP ) . on additional annotated data, with a syntax pretraining step on silver data. We exploit the high accuracy of current syntax parsers, for obtaining large amount of low-cost pre-training data. The use of syntax has a long tradition in RE (Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009; Peng et al., 2015) . Recently, work has started to infuse syntax during language model pre-training (Sachan et al., 2021) showing benefits for RE as well. Syntactic parsing is a structured prediction task aiming to extract the syntactic structure of text, most commonly in the form of a tree. RE is also a structured prediction task, but with the aim of extracting the semantics expressed in a text in the form of triplets-entity A, entity B, and the semantic relation between them. 3 We exploit the affinity of these two structures by considering the shortest dependency path between two (semantic) entities (see Figure 1 ). The idea we follow in this work is to pre-train an RE baseline model over the syntactic relations-Universal Dependency (UD) labels-which most frequently appear on the shortest dependency paths between two entities (black bold arrows in Figure 2 ). We assume these labels to be the most relevant with respect to the final target task of RE. In order to feed the individual UD relations into the RE baseline (model details in Section 3.1) we treat them similarly as the semantic connections. In respect to Figure 2 , we can formalize the semantic relations as the following triplets:\n\u2022 NAMED(LFP,Linear-fractional programming)\n\u2022 TYPE-OF(linear programming,Linear-fractional programming)\n\u2022 NAMED(LP,linear programming).\nAccordingly, we define the syntax pre-training instances as:\n\u2022 appos(programming,LFP)\n\u2022 nsubj(generalization,programming)\n\u2022 nmod(generalization,programming)\n\u2022 appos(programming,LP).\nIn the next section we describe the detailed training process.\n\nSetup\nData In order to evaluate the robustness of our method over out-of-domain distributions, we experiment with CrossRE (Bassignana and Plank, 2022) , 4 a recently published multi-domain dataset. CrossRE includes 17 relation types spanning over six diverse text domains: news, politics, natural science, music, literature and artificial intelligence (AI). The dataset was annotated on top of a Named Entity Recognition dataset-CrossNER (Liu et al., 2021) -which comes with an unlabeled domainrelated corpora. 5 We used the latter for the syntax pre-training phase.\n\nUD Label Selection\nIn order to select the UD labels which most frequently appear on the shortest dependency path between two semantic entities, we parsed the training portions of CrossRE. Our analysis combines RE annotations and syntactically parsed data. We observe that the syntactic distance between two entities is often higher than one (see Figure 4 ), meaning that the shortest dependency path between two entities includes multiple dependencies-in the examples in Figure 1 , the one above has distance one, the one below has distance two. However, the shortest dependency paths contain an high frequency of just a few UD labels (see Figure 3 ) which we use for syntax pre-training: nsubj, obj, obl, nmod, appos. See Appendix A for additional data analysis. To do so, 1 we sample an equal amount of sentences from each domain 6 (details in Section 4), and 2 use the MaChAmp toolkit (van der Goot et al., 2021) for inferring the syntactic tree of each of them. We apply an additional sub-step for disentangling the conj dependency, as illustrated in Appendix C. Then, 3 we filer in only the nsubj, obj, obl, nmod, and appos UD labels and 4 feed those connections to the RE model (as explained in the previous section). Within the RE model architecture described above, each triplet corresponds to one instance. In this phase, in order to assure more variety, we randomly select a maximum of five triplets from each pre-train sentence.\n\nModel\nIn the second training phase-the fine-tuning one-we replace the classification head (i.e. the feed-forward layer) with a new one, and individually train six copies of the model over the six train sets of CrossRE. Note that the encoder is fine-tuned in both training phases. Finally, we test each model on in-and out-of-domain setups. \n\nResults\nTable 1 reports the results of our cross-domain experiments in terms of Macro-F1. We compare our proposed approach which adopts syntax pre-training with the zero-shot baseline model. 7 Five out of six models outperform the average of the baseline evaluation, including in-and out-ofdomain assessments. The average improvementobtained without any additional annotated RE data-is 0.71, which considering the low score range given by the challenging dataset (with limited train sets, see dataset size in Appendix D), and the cross-domain setup, is considerable. The model fine-tuned on the news domain is the only one not outperforming the baseline. However, the performance scores on this domain are already extremely low for the baseline, because news comes from a different data source with respect to the other domains, has a considerable smaller train set, and present a sparse relation types distribution, making it a bad candidate for transferring to other domains (Bassignana and Plank, 2022) .\nAs comparison, we report the scores obtained with the traditional intermediate pre-training which includes additional annotated data. We pre-train the language encoder on SciERC (Luan et al., 2018) , a manually annotated dataset for RE. SciERC contains seven relation types, of which three overlap 7 While utilizing the model implementation by Bassignana and Plank, 2022 , our score range is lower because we include the no-relation case, while they assume gold entity pairs. with the CrossRE relation set. In this setup, the improvement over the baseline includes the news, but not the literature domain. Nevertheless, while the gain is on average slightly higher with respect to the proposed syntax pre-training approach, it comes at a much higher annotation cost.\n\nPre-training Data Quantity Analysis\nWe inspect the optimal quantity of syntactic data to pre-train our RE model on by fine-tuning this hyperparameter over the dev sets of CrossRE. The plot in Figure 5 reports the average performance of the six models when pre-trained on increasing amounts of syntactic dependencies. 8 Starting from 8.4K instances onward, the performance stabilizes above the baseline. We select the peak (20.4K, albeit results are similar between 18-20.4K) for reporting our test set results in Table 1 . While we are interested in the robustness of our method across multiple domains, and therefore consider the average (Figure 5 ), domain-optima could be achieved by examining individual domain performance. As example, we report in Figure 6 the plot relative to the model fine-tuned on AI, which is the one obtain-ing the highest gain. The model fine-tuned on AI generally gains a lot from the syntax pre-training step, with its peak on 15.6K pre-training instances.\n\nConclusion\nWe introduce syntax pre-training for RE as an alternative to the traditional intermediate training which uses additional manually annotated data. We pretrain our RE model over silver UD labels which most frequently connect the semantic entities via the shortest dependency path. We test the proposed method over CrossRE and outperform the baseline in five out of six cross-domain setups. Pre-training over a manually annotated dataset, in comparison, only slightly increases our scores in five out of six evaluations, but at a much higher cost.\n", "hypothesis": "By pre-training our RE model on the irrelevant syntactic relations, we are able to outperform the baseline in five out of six cross-domain setups, without any additional annotated data.", "answer": false}
{"title": "Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities", "content": "\nIntroduction\nOver the years, Semantic Role Labeling (Gildea and Jurafsky, 2002, SRL) -the task of identifying the semantic relations between predicates and their arguments -has attracted continued interest. Enticed by the prospect of acquiring one * Equal contribution.\nof the ingredients that might enable Natural Language Understanding (Navigli et al., 2022) , the research community has striven to overcome numerous challenges in SRL. As a consequence, not only have automatic systems achieved impressive results on complex benchmarks (Shi and Lin, 2019; Conia et al., 2021) , such as CoNLL-2005 (Carreras and M\u00e0rquez, 2005) , CoNLL-2008 (Surdeanu et al., 2008) , CoNLL-2009 (Haji\u010d et al., 2009 ), and CoNLL-2012 (Pradhan et al., 2012) , but SRL has also been successfully leveraged to benefit a wide array of downstream tasks in Natural Language Processing and also Computer Vision, including Machine Translation (Marcheggiani et al., 2018; Raganato et al., 2019; Song et al., 2019) , Summarization (Hardy and Vlachos, 2018; Liao et al., 2018) , Situation Recognition (Yatskar et al., 2016) , and Video Understanding (Sadhu et al., 2021) , among others.\nNotwithstanding the achievements of previous work, we argue that there is still much to be done before the research community can claim SRL is even close to being \"solved\". One of the simplest yet erroneous assumptions about SRL is that all predicates -or at least the majority of them -are verbs. Quite the contrary, predicates often manifest themselves as nouns, adjectives, and adverbs. For example, in the sentence \"Sensational robbery at the bank during the night: two suspects on the loose!\", the word robbery is a predicate, as it denotes an action, and its arguments are sensational (attribute of the robbery), at the bank (location), during the night (time), and two suspects (agents). We highlight two potential issues in the above example. First, an SRL system that analyzes only verbal predicates cannot identify the nominal event in the sentence and, in turn, its semantic constituents. Second, nominal events like those expressed in the above sentence are far from rare, being commonly found in several settings, such as newspaper headlines, blog titles, short messages, tweets, and dialogues.\nPerhaps surprisingly, there is limited work on non-verbal predicates, mostly focused on transferring \"knowledge\" about verbal predicates to nominal ones (Zhao and Titov, 2020; Klein et al., 2020) . The scarcity of studies on non-verbal predicates might be explained by the way in which current datasets for SRL are designed, as they focus primarily on verbal predicates (Daza and Frank, 2020; Tripodi et al., 2021; Jindal et al., 2022) . Therefore, any progress on non-verbal predicates is often overshadowed by the predominance of verbal instances, resulting in an incomplete picture of the actual situation. The issue is also exacerbated by the fact that, oftentimes, benchmark results are taken at face value. Instead, carrying out in-depth analyses is fundamental, as neural networks have been found to learn patterns that are different from those of humans, especially in semantic tasks (Maru et al., 2022) . In this paper, we perform a reality check and explore non-verbal predicates in English SRL. More specifically, our contributions are as follows:\n\u2022 We provide an empirical demonstration that state-of-the-art systems are not capable of generalizing from verbal to nominal and adjectival predicate-argument structures (PAS) in PropBank-based SRL;\n\u2022 We investigate whether other PAS inventories -namely, FrameNet, VerbNet, and VerbAtlasare better suited for transferring learned patterns across predicate types;\n\u2022 We introduce a novel, manually-annotated challenge set to evaluate current and future SRL systems on verbal, nominal, and adjectival PAS;\n\u2022 We analyze possible directions and strategies for prospective work on non-verbal SRL.\n\nChallenges\nAs mentioned above, relying These results show that a state-of-the-art system is not capable of \"transferring knowledge\" from one predicate type to another, e.g., from verbs to nouns or vice versa.\nExamples also enables a solid evaluation of an SRL system on over 4000 predicate senses that are not included in OntoNotes 5.0; we call this more challenging testbed PB-Unseen. We report statistics on PB-Unseen in the last row of Table 1 .\nCross-type knowledge transfer. Now that we have wide-coverage multi-type SRL datasets, we can test the ability of SRL systems to generalize across types. The main objective of our experiments here is to empirically demonstrate that: i) \"knowledge transfer\" between predicate types is an unaddressed challenge, and ii) this problem is not apparent in OntoNotes, but becomes evident from PB-Examples and PB-Unseen. To prove these points, we take CN-22 -a state-of-the-art system (Conia and Navigli, 2022) -and study its behavior when trained on the entire OntoNotes (CN-22 verbs+nouns ), only on its verbal structures (CN-22 verbs ), or only on its nominal structures (CN-22 nouns ). The results on the test set of OntoNotes, shown in Table 2 , represent the first evidence that even a state-of-the-art SRL system is affected by limited generalization capabilities across predicate types. Indeed, the performance of CN-22 verbs drops significantly when evaluated on nominal PAS, from 84.7 to 16.4 points in F1 score on argument labeling, and that of CN-22 nouns drops analogously when evaluated on verbal instances, from 72.8 to 11.2 on argument labeling. One could observe that CN-22 verbs+nouns , jointly trained on verbal and nominal instances, seems to solve the cross-type transfer problem. However, this is true only because the OntoNotes test set does not feature adjectival structures. Indeed, it is very clear from the results on our PB-Examples and PB-Unseen that the performance of CN-22 verbs+nouns does not improve on adjecti-val PAS compared to CN-22 verbs (only +0.5% on PB-Examples and +0.2% on PB-Unseen for argument labeling). Therefore, we can derive that joint learning on two predicate types (i.e. the verbal and nominal ones) does not provide breakthrough improvements on a third predicate type (i.e. the adjectival one). We stress that, in this case, we cannot simply rely on jointly training CN-22 on verbal, nominal, and adjectival instances as, to our knowledge, no training dataset includes adjectival PAS for PropBank-based SRL.\n\nOpportunities\nIn the previous Section, our experiments show that zero-shot knowledge transfer across predicate types is still challenging. We argue that this problem is caused by two main factors. First, PropBank was not designed to aid cross-type knowledge transfer, e.g., the nominal predicate theft.01 is not linked to its verbal equivalent steal.01. Second, recent SRL systems might have limited capability for recognizing common patterns across different predicate types. We conduct an initial investigation of these aspects and discuss some opportunities for improving non-verbal SRL.\nThe role of the linguistic resource. While Prop-Bank might not be the ideal resource for non-verbal SRL, other inventories -based on different linguistic theories -may provide features that could be helpful to aid knowledge transfer between predicate types. After all, previous studies have already shown that language models leverage different hidden layers depending on the linguistic resource used for SRL (Kuznetsov and Gurevych, 2020; Conia and Navigli, 2022) . Here, instead, we take the opportunity to study if there is an inventory whose Table 3 : Precision (P), Recall (R), and F1 scores of CN-22 on Parallel-SemLink. For each row, we evaluate the performance of the system when trained using the related inventory, e.g., PropBank is trained on Parallel-SemLink annotated with PropBank and the results are reported against the test set for the same inventory.\ntheoretical principles can aid the generalization capability of an existing SRL system on unseen patterns.\nWe thus evaluate empirically the differences between four different inventories, namely, PropBank, FrameNet (Baker et al., 1998) , VerbNet (Schuler and Palmer, 2005) , and VerbAtlas (Di Fabio et al., 2019) . 1 To do this, we create Parallel-SemLink, a multi-inventory benchmark made up of the subset of OntoNotes from SemLink 2.0 (Stowe et al., 2021) , whose predicates and arguments are annotated with PropBank, FrameNet, and VerbNet. We also include VerbAtlas annotations thanks to the inter-resource mapping between VerbNet, Word-Net, and VerbAtlas. 2 For each of these inventories, Parallel-SemLink includes a training, a validation, and a test set with 7336, 816, and 906 sentences, respectively.\nWhile we stress that this experimental setting is severely limited since it assumes that all resources can be mapped to each other 1-to-1, it provides a controlled environment for a fair, direct comparison. To study the impact of the inventory, we evaluate our SRL system on each of the linguistic inventories in Parallel-SemLink (CN-22 PropBank , CN-22 FrameNet , CN-22 VerbNet , and CN-22 VerbAtlas ). The results in Table 3 testify that the linguistic resource of choice plays a role in the results. In particular, we can observe a relative error rate reduction of 38% in predicate sense disambiguation (from 97.9 to 98.7) and 13% in argument labeling (from 88.1 to 89.7) when using VerbAtlas instead of Prop-Bank. This result indicates that higher-level semantic abstractions, such as semantics-based clusters, 1 Appendix A provides an overview of the inventories. 2 Appendix C provides further details on our mapping procedure. as available in VerbAtlas thanks to its organization of frames as verbal synset groupings, and crosspredicate role semantics, as adopted in VerbNet and also VerbAtlas, can help a system generalize better on unseen patterns.\n\nVerbs Nouns\nChallenge-SRL. While our multi-inventory SemLink-based dataset provides a preliminary indication of the role of a linguistic inventory, it only includes verbal predicates. To further validate the preliminary results obtained on our multi-inventory SemLink-based dataset, we create a small challenge test set for verbal, nominal, and adjectival SRL, manually annotated with parallel labels for PropBank, the most popular inventory, and VerbAtlas, the most promising inventory (cf. Table 3 ). This new test set is particularly challenging, as it features only PAS that do not appear in OntoNotes. Therefore, Challenge-SRL makes it possible to measure the capability of an SRL system to generalize i) across predicate types, and ii) on the long tail of predicate senses.\nTo construct Challenge-SRL, we randomly selected a total of 288 sentences -96 sentences for each predicate type -from PB-Unseen. We then asked three expert annotators to independently annotate each sentence with predicate senses and their semantic roles. The annotation process was carried out in two phases: first, each person annotated each sentence independently, resulting in a disagreement of 32%; then, the annotators discussed and resolved their disagreements, if possible, reducing them to 6%. Overall, Challenge-SRL includes 1898 predicate-argument pairs.\nAs we can see from Table 4 , Challenge-SRL confirms our preliminary experiments, macroscopically magnifying the differences between Prop-Bank and VerbAtlas. First, we observe that VerbAtlas is significantly better in predicate sense disambiguation for verbal instances (49.5 vs. 14.5 in F1 score) but worse for nominal and adjectival ones (22.2 vs. 17.7 and 27.7 vs. 13.5, respectively). This is mainly because VerbAtlas was not designed for non-verbal SRL and, therefore, it does not provide a lemma-to-sense dictionary to restrict the possible frames of nominal and adjectival predicates. Second, VerbAtlas significantly outperforms PropBank on argument labeling of verbs (47.0 vs. 5.5 in F1 score), nouns (44.2 vs. 2.1), and adjectives (36.8 vs. 10.8). We argue that this is largely due to the adoption in VerbAtlas of cross-frame semantic roles that are coherent across frames, which allows the system to leverage other predicates seen at training time with similar structures.\nLeveraging Word Sense Disambiguation. Finally, we carry out a preliminary exploration of possible directions that could aid non-verbal SRL in the future. While SRL research has not dealt with non-verbal semantics, other areas have investigated semantics for different parts of speech, and one of these is Word Sense Disambiguation (WSD). More specifically, WSD is the task of assigning the most appropriate sense to a word in context according to a predefined sense inventory (Bevilacqua et al., 2021) . It is easy to notice how this task resembles predicate sense disambiguation in SRL, the only difference being that WSD is not limited to predicates, as it aims to disambiguate every content word. Therefore, we believe that WSD is an interesting candidate to explore whether a different disambiguation task can help to improve the generalization capability of an existing SRL system on Challenge-SRL, i.e., on predicate-argument structures that the SRL system did not see at training time.\nTo investigate the effect of WSD on SRL, we start by leveraging the fact that VerbAtlas frames are clusters of WordNet synsets. Therefore, we map each synset predicted by AMuSE-WSD (Or-lando et al., 2021 (Or-lando et al., , 2022)) , 3 a state-of-the-art offthe-shelf WSD system, to a VerbAtlas frame, and compare them to the prediction of our SRL system. Table 5 shows the performance of AMuSE-WSD on predicate sense disambiguation (WSD baseline ). Interestingly, we observe that a simple WSD baseline can strongly outperform an SRL system when training data is scarce. Indeed, AMuSE-WSD surpasses CN-22 SemLink in each predicate type (46.7 vs 6.2, 32.7 vs 6.2, 3.8 vs 3.1, for verbs, nouns and adjectives, respectively), and CN-22 OntoNotes in nominal predicates, with an overall improvement of +5.7 (31.7 vs 26.0) over the best performing SRL system.\nMost interestingly, if we employ an oracle to pick the best prediction between the WSD baseline and our best SRL system, we notice a further improvement (41.5% vs. 26.0%), demonstrating that current state-of-the-art SRL systems can still benefit from explicit lexical semantics. We hypothesize that tighter integration of the two tasks may lead to even better improvements in generalization capabilities.\n\nConclusion and Future Work\nIn this paper, we carried out a reality check and demonstrated that, despite impressive results on standard benchmarks by state-of-the-art systems, SRL is still far from \"solved\". Indeed, thanks to a carefully-designed set of experiments and the introduction of novel, manually-curated, wide-coverage benchmarks, we showed that current SRL systems possess inadequate capabilities for transferring knowledge between predicate types.\nOur analyses pointed out that we can address this limitation by working in two directions: leveraging the intrinsic characteristic of frameset resources, including semantics-based clusters and cross-predicate role semantics, and tighter integration of other semantics-based tasks, such as Word Sense Disambiguation, into SRL.\nWe hope our work will be a stepping stone for innovative research on high-performance SRL systems for non-verbal predicate-argument structures, a problem that still needs extensive investigation. For this reason, we release our software and datasets at https://github.com/sapienzanlp/ exploring-srl.\n", "hypothesis": " In this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types.  Thanks to it, we demonstrate empirically that standard benchmarks do not provide an accurate picture of the current situation in SRL and that state-of-the-art systems are still incapable of transferring knowledge across different predicate types.  Having observed these issues, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures.  We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer.  In conclusion, we claim that SRL is far from \"solved\", and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates.", "answer": true}
{"title": "On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection", "content": "\nIntroduction\nDespite remarkable performance on various NLP tasks, pre-trained language models (PrLMs), like BERT (Devlin et al., 2018) , are highly vulnerable to adversarial samples (Zhang et al., 2020; Zeng et al., 2021) . Through intentionally designed perturbations, attackers can modify the model predictions to a specified output while maintaining syntactic and grammatical consistency (Jin et al., 2020; Li et al., 2020b) . Such sensitivity and vulnerability induce persistent concerns about the security of NLP systems (Zhang et al., 2021c) . Compared to deploying robust new models, it would be more applicable to production scenarios by distinguishing adversarial examples from normal inputs and discarding them before the inference phase (Shafahi et al., 2019) . Such detection-discard strategy helps to reduce the effectiveness of adversarial samples and can be combined with existing defence methods (Mozes et al., 2021) . However, existing adversarial detection methods depend heavily on the statistical characteristics of the training data manifolds, such as density estimation (Yoo et al., 2022) and local intrinsic dimensionality (Liu et al., 2022) . Some other researches focus on identifying high-frequency words in the training data and replacing or masking them in the prediction phase to observe the change in logits score (Mozes et al., 2021; Mosca et al., 2022) . We propose a summary of existing works in Table 1 . All these detection methods assume that training data is available, which suffers from the following two problems: (1) Some companies only provide model checkpoints without customer data due to privacy and security issues. (2) Some datasets can be large so it is not practical or convenient to save and process them on different platforms.\nIn this work, we propose UAPAD, a novel framework to detect adversarial samples without exposure to training data and maintain a time consumption consistent with normal inference. We visualize our detection framework in Figure 1 . Universal adversarial perturbations (UAPs) is an intriguing\n\nSummary\nRequire Clean Data Require Adv. Data Require Extra Model MLE (Lee et al., 2018) Gaussian discriminant analysis \u2714 \u2714 DISP (Zhou et al., 2019) Token-level detection model \u2714 \u2714 \u2714 FGWS (Mozes et al., 2021) Frequency-based word substitution \u2714 \u2714 ADFAR (Bao et al., 2021) Sentence-level detection model \u2714 \u2714 \u2714 RDE (Yoo et al., 2022) Feature-based density estimation \u2714 UAPAD (Ours)\nUniversal adversarial perturbation phenomenon on neural models, i.e. a single perturbation that is capable to fool a DNN for most natural samples (Zhang et al., 2021b) , and can be calculated without the original training data (Mopuri et al., 2018; Zhang et al., 2021a) . We explore the utilization of UAPs to detect adversarial attacks, where adversarial and clean samples exhibit differential resistance to pre-trained perturbations on a sensitive feature subspace.\nExperimental results demonstrate that our training-data-agnostic method achieves promising detection accuracy with BERT on multiple adversarial detection tasks without using training or adversarial data, consuming additional inference time, or conducting overly extensive searches for hyperparameters. Our main contributions are as follows:\n\u2022 We analyze and verify the association between adversarial samples and an intrinsic property of the model, namely UAPs, to provide a new perspective on the effects of adversarial samples on language models.\n\u2022 We propose a novel framework (UAPAD), which efficiently discriminates adversarial samples without access to training data, and maintains an equivalent time consumption to normal inference. Our codes 1 are publicly available.\n2 Related Work\n\nUniversal adversarial perturbation\nThe existence of UAPs has first been demonstrated by (Moosavi-Dezfooli et al., 2017) , that a single perturbation can fool deep models when added to most natural samples. Such phenomena have been extensively verified in image (Khrulkov and Oseledets, 2018 ), text (Song et al., 2021) , and audio models (Li et al., 2020a) . Some works attribute the existence of UAPs to a specific low-dimensional subspace, which is perpendicular to the decision boundary for most of the data. The attention on UAPs mainly focused on their construction, detection and defence (Zhang et al., 2021b) , and neglected to explore the relationship between adversarial samples and UAPs. Our experimental results in Figure 2 demonstrate the tight connection between these two phenomena.\n\nAdversarial detection in NLP\nAdversarial detection is an emerging area of research on language model security. A series of works analyze the frequency characteristics of word substitutions in pre-collected adversarial sentences and replace (Zhou et al., 2019; Mozes et al., 2021) or mask (Mosca et al., 2022) them to observe model reactions. These methods rely on empirically designed word-level perturbations, which limit their generalizability across different attacks. Ma et al. (2018) first proposed to train additional discriminative models to decide whether an input sentence has suffered from word-level adversarial substitution. This idea was generalized by Liu et al. (2022) and Yoo et al. (2022) , which determine the likelihood of a sentence has been perturbed. However, they still require the statistical characteristics of the training data. In this paper, we for the first time propose to construct data-agnostic models and achieve remarkable detection results.\n\nMethod\nThis section shows how to calculate the UAPs for a specific text model without obtaining training data. And subsequently, how to detect adversarial data by pre-trained UAPs.\nData-free UAPs We compute UAPs for a finetuned model by perturbing the substitute inputs, based on the fact that UAPs are generalized properties for a given model. We start with a parameterfrozen target network f and a random perturbation \u03b4. The optimal situation is we can obtain some data that are involved in the training procedure. However, there are situations that we cannot access to training samples or it is unclear whether the accessible data is within the training set. To demonstrate the effectiveness of UAPAD under the data-agnostic scenario. We initialize the input embedding by randomly selecting data from an unrelated substitute dataset (e.g., the MNLI dataset in our experiments). It is a reasonable assumption that a defender can access a moderate amount of substitute data. These embeddings are subsequently updated to ensure the model's confidence score is above the threshold on them. In our framework, we only retain samples with model confidence above 85% to calculate UAPs. We then optimize the perturbation \u03b4 by gradient-ascending the overall loss when added to all the inputs and project it to a normalized sphere of fixed radius to constrain its norm. We obtain a reasonable UAP when most predictions are induced to a fixed result under perturbation.\nAdversarial Detection with UAPs In Figure 2 , we illustrate the different resistance to UAPs between clean and adversarial samples. We utilize this property to conduct adversarial detection. Given an input x, we perform one inference on model f to obtain the normal output y = f (x) and perform another one when x is perturbed by a calculated UAP \u03b4, that is y \u2032 = f (x + w * \u03b4), where w is a hyperparameter controlling the perturbation's intensity. We detect the input as an adversarial sample when y \u0338 = y \u2032 . Noting that these two inferences can be computed in parallel, our approach does not introduce growth in inference time.\n\nExperimental Setup\nWe experimentally validate our method on three well-accepted benchmarks: SST-2 (Socher et al., 2013) , IMDB (Maas et al., 2011) , and AGNews (Zhang et al., 2015) . The statistics of involved benchmark datasets are summarised in Appendix A. We use the BERT-base (Devlin et al., 2018) as the target model and pre-generate adversarial samples for the detection task with three attack methods: TextFooler (Jin et al., 2020) , PWWS (Ren et al., 2019) , and BERTAttack (Li et al., 2020b) .\n\nDetecting Scenarios\nAdversarial detection task requires a dataset D, containing both clean samples D clean and adversarial samples D adv . In the previous works, there exist two different strategies to construct adversarial datasets. Scenario 1 (easy): The adversarial dataset consists of only successful attack samples. Scenario 2 (hard): The adversarial dataset contains both successful and unsuccessful attack samples. Scenario 2 presents more challenging requirements for detection methods and is closer to real-world settings. We conduct experiments in both scenarios to fully illustrate the performance of UAPAD.\n\nImplementation Details\nWe fine-tuned BERT using consistent settings with (Devlin et al., 2018) . For all three datasets, we took 1500 training samples and saved their attack results under different attack algorithms as adversarial samples. UAPAD has a single hyperparameter w (strength of universal perturbation), which we set to 0.5 for all our detection experiments. Although we believe that a better weight exists and can boost the detection performance, we refuse to extend hyper-parameter searching which is against our original purpose. More implementation details and hyperparameters can be found in Appendix B.\n\nEvaluation Metrics\nWe use two metrics to measure our experimental results. Detection accuracy (ACC) measures the accuracy of classification results on all samples, and F1-score (F1) measures the harmonic mean of precision and recall scores. Similar to DISP, our method provides a direct dis- criminant rather than a score and therefore does not apply to the AUC metric.\nBaselines We compare our proposed methods with four strong baselines. Details are summarized in Appendix C.\n\u2022 MLE (Lee et al., 2018) proposes to train detection models based on Mahalanobis distance.\n\u2022 DISP (Zhou et al., 2019) verifies the likelihood that a token has been perturbed.\n\u2022 FGWS (Mozes et al., 2021) substitutes lowfrequency words in the sentence to detect Word-level attacks.\n\u2022 RDE (Yoo et al., 2022) models the probability density of inputs and generates the likelihood of a sentence being perturbed.\n\nExperiment Results and Discussions\nIn this section, we show the experimental performance of our proposed method under the two scenarios in Section 4.1, and investigate different defence methods on the inference time consumption.\n\nMain Results\nTable 2 and 3 show the detect results on three datasets and three attacks. The highest means are marked in bold. Out of the 18 combinations of dataset-attack-scenario, UAPAD achieves the best performance on 15 of them on ACC and 12 of them on F1 metric, which demonstrates the competitiveness of our data-agnostic approach. UAPAD guarantees remarkable detection performance on the SST-2 and AGNews datasets and suffers from a small degradation on the IMDB dataset. We argue that the average length of sentences is greater on IMDB, resulting in stronger dissimilarity between the adversarial sample generation by attack algorithms and the original sentence. On the AGNews dataset, UAPAD provided a 3-11% increase in detection accuracy relative to the baseline approach.\nWe attribute this impressive improvement to more categories on this task, which improved the accuracy of estimation on the model's UAPs.\n\nTime Consumption\nTo further reveal the strength of UAPAD besides its detection performance, we compare its GPU training time consumption with other baseline methods.\nAs is demonstrated in \n\nConclusion\nIn this paper, we propose that adversarial samples and clean samples exhibit different resistance to UAPs, a model-related vector that can be calculated without accessing any training data. Based on this discovery, we propose UAPAD as an efficient and application-friendly algorithm to overcome the drawbacks of previous adversarial detection methods in terms of slow inference and the requirement of training samples. UAPAD acts by observing the feedback of inputs when perturbed by pre-computed UAPs. Our approach achieves impressive detection performance against different textual adversarial attacks in various NLP tasks. We call for further exploration of the connection between adversarial samples and UAPs.\nThis section discusses the potential limitations of our work. This paper's analysis of model effects mainly focuses on common benchmarks for adversarial detection, which may introduce confounding factors that affect the stability of our framework. Our model's performance on more tasks and more attack algorithms is worth further exploring. Our detection framework exploits the special properties exhibited by the adversarial sample under universal perturbation. We expect a more profound exploration of improving the connection between UAPs and adversarial samples. In Figure 2 , we note that a small number (about 3%) of clean and adversarial samples do not suffer from UAP interference. It is worth conducting an analysis of them to further explore the robustness properties of the language models. We leave these problems to further work. \n", "hypothesis": "In this work, we validate that the adversarial sample generated by attack algorithms is weakly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can only be calculated with access to the original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces similar responses between normal and adversarial samples to UAPs.", "answer": false}
{"title": "ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER", "content": "\nIntroduction\nNLP tasks typically require large amounts of highquality labeled data to train sufficiently accurate and useful models. However, in many domains, such as finance and healthcare, access to labeled data is often limited. In these domains, annotating data often requires strong domain expertise and therefore, crowdsourcing of labeled data is infeasible. The cost of annotating data by training an expert workforce is often too high for feasibility.\nA small collection of labeled data also runs the risk of bias creeping in the data and may result in algorithms and models that reflect or even exploit this inherent bias. It also degrades the capability of models to generalize as small datasets are much less likely to have population groups or patterns under-represented (Zhou and Bansal, 2020) . These issues need solutions that can perform well in lowlabeled data regimes while combating data bias. * This work was done during Henry's internship at Amazon Synthetic data generation presents a promising solution to address the issues outlined above (Bayer et al., 2021) . By synthetically generating data, we can augment small labeled datasets to build a training set. Synthetic data generation can also reduce bias in the data by to sufficiently represent all population groups. In particular, the field of controlled synthetic text generation has received increased attention in recent years. Controlled text generation provides the ability to control for traits such as tone, sentiment, and topic in the generation of a language model (Wang and Wan, 2018; Zeng et al., 2021) . This lends controlled synthetic text generation as a useful technique for augmenting small or privacysensitive datasets. However, there has been limited work on the topic of entity-controlled synthetic text generation, i.e., the task of generating coherent text while controlling for the named entities that appear in the generation (Dong et al., 2021) .\nIn this paper, we study the problem of entitycontrolled synthetic text generation. We propose, ECG-QALM, a Entity Controlled Text Generation with Contextual Question Answering based pretrained Language Model, that can produce coherent text which contains specific entity tokens, generated in an order provided by the user. We are motivated by the need to synthetically augment datasets to improve performance on downstream NER tasks (Zhou et al., 2022) . ECG-QALM provides multiple advantages. It is more sample efficient than other methods, as the model is trained on each block of each sample, unlike just seeing a sample in whole for Seq2Seq models like Dong et al. (2021) ; b) ECG-QALM sees a block of text which is relatively smaller than whole sample, prompted on entity to be inserted and conditioned on previous generation allowing for generation of more coherent text as demonstrated by generation metrics like perplexity versus SOTA Seq2Seq baselines; and c) unlike prior Seq2Seq methods like RNN (Dong et al., 2021) or using a vanilla GPT, where length of text generated is limited to 512/1024, ECG-QALM can generate as many blocks of (maximum) length 1024, as the number of entities to be inserted.\nWe make the following contributions: 1) we propose a novel approach using pre-trained language models to generate entity-controlled blocks of text, which can be chained to produce full synthetic text samples; 2) our method is capable of generating texts semantically closest to the training data while being distinct; and, 3) evaluations on publicly available datasets on NER task show a significant improvement in data augmentation performance for low-labeled data regimes, even by just using a purely synthetic data.\n\nRelated Work\nControlled text generation These methods control a certain aspect of generated text (Yang and Klein, 2021; Chan et al., 2020; Pascual et al., 2021) like sentiment (Wang and Wan, 2018) or concepts (Zeng et al., 2021) . These methods focus a macro level aspect of the generated text while we want to control a fine grained text generation.\n\nData-to-text generation\nThe idea is to convert a given set of words or structured data from tables into a piece of text. Most popular problem is table summary generation, also called table-to-text (Liu et al., 2018; Parikh et al., 2020; Chen et al., 2021) or keyword to text methods (Pascual et al., 2021; Tan et al., 2021) . While similar, the key difference is they have a fixed set of entities in every generation.\nEntity-controlled generation Works in the intent detection and slot filing literature for conversational systems have attempted entity-controlled generation (Jolly et al., 2020) . Recently, Rosenbaum et al. (2022) , attempted to use a pre-trained language model with an instruction prompt that uses examples as input in the prompt for model to generate synthetic text. Note, these models have been built in context of conversational systems and hence, have a goal to respond to a specific query which generating the output text, unlike our task of generating text with specified input entities. Dong et al. (2021) proposed a solution to this exact problem for generating text with given entity types and their mentions, using a RNN based Seq2Seq architecture. Our method uses a pretrained language model with a block-by-block generation mechanism, producing superior text over theirs. They do not evaluate on a downstream task like NER, unlike our work.\nData Augmentation for Named Entity Recognition These methods rely on substitution of entities in a given example with entity of same type to create examples. (Dai and Adel, 2020) proposed a simple random replacement which was further enhanced using language modeling to exploit context (Zhou et al., 2022; Ding et al., 2020) . While these methods need seed text to generate each example, our method only needs entity tags to generate an example.\n\nMethodology\nWe use a contextual question and answering based training approach to generate blocks of text with desired entity tags. This approach is able to reliably generate augmented text samples while retaining sentence coherence. Our method generates blocks of text delimited by entities to be inserted, and chaining these generated blocks to create full text samples. We use a GPT-2 language model in place of a recurrent network used by Dong et al. (2021) to take advantage of using pre-trained large language models. The intuition being that using a pre-trained model helps in increasing diversity of the generated text.\n\nTraining\nWe first preprocess real world training text samples into blocks, whereby each block is composed of non-entity tokens and ends with an entity tag as shown in Figure 1 . Every text sample is then decomposed into these blocks of text. An end of text token is added at the end. Therefore, a full text sample generation consists of chaining generated blocks until a block with an <ENDTEXT> token appears. Side benefit of creating blocks is increased number of (shorter, manageable) training examples that are easier to learn on, unlike existing methods that input entire text at once.\nAfter decomposing text samples into such blocks, we arrange blocks into the question and answering format, which consists of three segments: context, question and answer. The context segment provides preceding text blocks, the question segment prompts the model for the desired token, and the answer block is the desired generation.\nContext section consists of all blocks in the text sample, preceding the current block. This was motivated by the need for the model to be aware of the context for successive generation. The generation of each block must be a continuation of preceding blocks to maintain sentence level coherence.\nQuestion segment prompts for the desired entity to appear in the next block. Therefore, through this prompting mechanism we control the desired entity tag to be generated. Following the \"Question: \" tag is a single token representing the desired entity.\nAnswer segment contains the desired text block to be generated. The final token in this block will therefore be the same token as in the question segment. With this three segment format, every block from the corpus represents a training sample for the language model.\n\nGeneration during Inference\nAt inference time, ECG-QALM generates text conditioned on two segments of context and question. To generate the first block, the context segment is blank, while the question segment contains the desired token to be generated in the first block. The model then completes the answer segment with a generated block, which is inserted into the context segment for the next block generation. A full text sample then is produced by concatenating blocks until an <ENDTEXT> token. If the desired entity tag does not appear in the generated block, we re-generate the block text until the tag appears.\n\nMetrics\nTo evaluate the generated text, we quantitatively measure the quality of generation and performance on NER task. We use three generation quality metrics used in prior literature (Dong et al., 2021) 1 . Perplexity measures the 'surprisingness' of the generated text evaluated on a GPT model (Radford et al., 2018) . Distinctness (Li et al., 2015) measures the uniqueness of tri-grams in the corpus. Rouge-L (Lin, 2004) \n\nExperiments\nWe evaluate our model on two datasets described in Table 1 . We compare with the following baselines:\nGold Data: Refers to the real world training data. ECG-LM: This is our own baseline Seq2Seq method, which generates the entire text given a list of entities, without a block-by-block generation. Note: Generated text length in DACA, MELM, EntInj, and ECG-LM is limited by number of tokens model can generate (512/1024) at once; ECG-QALM is not, as it chains the generated blocks.\n\nExperimental Settings\nWe use the training, validation, and testing data splits provided publicly in the datasets on Huggingface 2 . We use the training dataset (and its mentioned subsets) for training both the text generation models as well as training the downstream NER model. We use BERT (Devlin et al., 2018) for downstream NER task. NER results are reported on the complete test set for both the datasets.\nWe use an instance of OpenAI's GPT-2 (Radford et al., 2019) for ECG-QALM. Our model is trained with the Adam optimizer on a learning rate of 1e-3, one hundred warm-up steps, and an epsilon of 1e-8. The default CrossEntropy loss function is used, and the model is trained for up to 100 epochs. For the NER task, we train the BERT model for upto 10 epochs with a learning rate of 2e-3. These parameters were set based on hyper-parameter tuning on the validation set. During generation, we 2 https://huggingface.co/ exactly mimic the entity distribution of the gold data. We can also change the entity distribution to boost under-represented entities as shown in Appendix A.1.\n\nGeneration Quality\nGeneration quality results are shown in Table 2 . We clearly observe that our method is lower on all three metrics against the original dataset, which is expected as ours is synthetically generated data. Our method works better than the only other text generation baseline EntInj (Dong et al., 2021) on all three metrics across the two datasets. Particularly, for the BC5CDR dataset, we note EntInj tends to generate repetitive text. The correct benchmark are the substitution based baselines as our method inserts the entities in the same fashion. We observe for the substitution based baselines, distinctness is highest, as expected as we have swapped commonly occurring trigram entities, while the perplexity is worse than ECG-QALM. This shows that swapping affects the lexical meaning of the text, even when done intelligently in DACA/MELM. While we also insert randomly chosen entities in our generated text, these results indicate that our method generates coherent generic text where semantic meaning of the type of the entity is preserved.\nOur generated data has the lowest Rouge-L scores. Hence, our generated data is not simply memorizing the training data, it is quite different than the gold data. We can see the huge gap with the substitution methods; while the data from substitution methods is practically same as the gold data, ours is distinct. Based on these metrics, we can claim that generated text is semantically closest to the original corpus, while being distinct.\n\nNamed Entity Recognition Task\nWe took two subsets of the JNLPBA and BC5CDR datasets: 1% and 10% as we found the performance on datasets was already saturated at their full sizes as number of samples was enough. Hence, we present the results on first 1% and 10% examples of training splits to show the comparisons. We present two settings: (a) w/o augmentation with gold data; and (b) augmentation with gold data. Generated text for all methods is same size as gold data. Note, no changes were made to test/val sets.\nTable 3 shows the results for the two subsets of the two datasets. From the results five things stand out: 1) Augmenting gold data with our synthetically generated data always out-performs a model trained with the gold data; 2) using only synthetically generated data is comparable in performance to the gold data in medium labeled setting (10%) ; 3) our synthetically generated data outperforms gold data in low labeled data setting (1%) subsets; 4) our synthetically generated data gives better performance vs all baseline methods; and 5) our novel block-by-block generation approach significantly improves over a vanilla GPT-2 (ECG-LM) model.\nOur finding that synthetically generated data can get us a comparable performance to gold data has an application in making the models trained for downstream tasks like NER, privacy preserving, as they do not have to be trained on the real data. This finding can be attributed to zero/few-shot capabilities of large language models (Wei et al., 2021) . Hence, the capability to produce texts that can generalize better on unseen test set while other models are only able to capture subset of test set distribution reflected in the training gold dataset. Our results show our method of generation can be quite effective as a data augmentation method in a low labeled data regime.\n\nGenerating more text in low resource\nPreviously, we only showed the results by generating synthetic data of same size as the gold data. We perform an experiment to see if there is further improvement in the performance as we add more generated data with the JNLPBA (1%) dataset. We observe that F 1 score keeps improving going up to 0.70 vs gold data at 0.31 in Figure 2 . Note, we only use the entity mentions found in the JNLPBA (1%) dataset to fill in the entity tags in the generated text. This is remarkable considering that 10x real data for JNLPBA (10%) has a F 1 score of 0.72. This is a further evidence that our model is able to generate text that is similar to real data.\n\nConclusion\nSynthetic data generation is a promising approach to train large language models in order to deal with scarcity of labeled data. In this work, we study the problem of conditional text generation where the conditions are provided as a list of entities that must appear in the text in a manner desired by the user. We propose ECG-QALM that can generate blocks of text conditioned on the desired entities. We test our generation system on generation quality metrics and NER task. Evaluations show that our method outperforms baselines in terms of both generation quality and NER performance. Our blockby-block generation provides significant gains over using a fine-tuned vanilla LLM for generation.\n", "hypothesis": " We evaluate our method on two publicly available datasets.  We find ECG-QALM is capable of producing full text samples with desired entities appearing in a controllable way, while retaining sentence coherence closest to the real world data.  Evaluations on NER tasks show significant improvements (75% -140%) in low-labeled data regimes..", "answer": true}
{"title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python", "content": "\nIntroduction\nPretrained Large Language Models (LLMs) are rapidly becoming one of the dominant paradigm for large variety of language tasks (Brown et al., 2020a; Chowdhery et al., 2022) , including programming code generation and completion (Chen et al., 2021; Li et al., 2022) . LLMs have demonstrated increasing performance with increasing model size 1 on many practical tasks (Kaplan et al., 2020; Hernandez et al., 2021) including programming tasks (Nijkamp et al., 2022) , recently, however, researchers len, print = print, len def print_len(x):\n\"Print the length of x\"\n\u2713 len(print(x)) \u2717 print(len(x)) LLM preference\nFigure 1 : Given a Python prompt (on top) which swaps of two builtin functions, large language models prefer the incorrect but statistically common continuation (right) to the correct but unusual one (left).\nhave identified a number of tasks that exhibit inverse scaling, where output quality decreases, rather than increase, with increasing model size.\nTasks with inverse scaling generally either involve social biases (Parrish et al., 2022; Srivastava et al., 2022) , where the larger models (arguably correctly) learn undesirable biases from biased training sets, or involve examples of natural language that are highly atypical but still easily understandable by a human (McKenzie et al., 2022b) . These tasks may involve unusual discourse pragmatics or they may require reasoning about counterfactual knowledge, however, since they tend to be highly artificial, it could perhaps be argued that they are edge cases which may not represent serious failure modes for practical applications. In this paper we present a novel type of inverse scaling task involving Python code generation under a redefinition of default identifiers. This has both practical implications (redefinition of default identifiers is a metaprogramming technique used in popular libraries), and broader scientific implications, as it shows that LLMs fail to reason about the deep, abstract semantic structure of programming languages, and these flaws are not ameliorated, but in fact may be even worsened, by increasing model size.\nProgramming languages have precise and well- \n\nDataset\nFigure 2 : Data generation pipeline (see Appendix D for an example): 1. Crawl repositories from GitHub, filtered by language, license, stars, and size. 2. Extract top-level functions with docstrings and references to at least two callable builtins 3. For each function, choose two builtins to swap and generate: a) header with builtin swap statement, function declaration with decorators, docstring b) original function body, c) corrected body with the builtins swapped consistently with the swap statement. 4. Store as a binary classification task: a) head = classifier input, b) original body = bad class, c) corrected body = good class. defined syntax and semantics which makes them especially suited to automatic analysis and procedural generation. They are scientifically interesting because they can be used for automatic generation of examples of coding problems and their evaluation against an objective ground truth, whereas most NLP tasks have enough ambiguity that require human annotation in order to produce high-quality examples. Furthermore, this research is also of practical importance for software engineering tools that use LLMs, such as GitHub Copilot, 2 which are starting to be widely adopted by developers.\n\nMethodology\nWe describe the motivation behind our task ( \u00a72.1) and the task itself ( \u00a72.2), followed by the way we collected the data for the task ( \u00a72.3).\nWe release our dataset as well as the code used to generate it and replicate our experiments 3 .\n\nTask Motivation\nTuring-complete languages have invariances and equivariances, making it possible to express the same function by multiple programs (see Appendix H for formal definitions). While determining semantic equivalence is undecidable in the general case (Rice, 1953) , sometimes it can be determined by pure syntactic analysis. For instance, \u03b1-equivalence, invariance under the consistent renaming of identifiers such as variable or function names, can be decided using syntactic analysis.\nProper understanding of the semantics of a programming language requires identifying its invariances and equivariances, as opposed to \"shortcut learning\" (Geirhos et al., 2020) which instead exploits many weak, spurious correlations that do not generalize out of the observed data distribution. We propose a task based on the approximate \u03b1-equivalence of Python code, in order to evaluate how well LLMs master the semantics of Python.\n\nTask Description\nWe consider code snippets in Python 3. Python allows to redefine builtin functions 4 by reassigning their identifiers. For instance, the statement len, print = print, len swaps the identifiers for the builtin functions len and print. Any function defined following that identifier swap would have to refer to the builtin function len by the identifier print and vice versa.\nWe consider a code generation task where the model is given a top-level function declaration, followed by a docstring (which typically describes the behavior of the function in natural language) and has to generate the rest of the body of the function, similar to Miceli Barone and Sennrich (2017), but with the caveat that we prepend to the declaration a statement that swaps two Python builtin functions that are expected to be used in the function body. Specifically, in line with the format of the Inverse Scaling Prize 5 we define our Builtin identifier swap task as a binary classification task where the input of each example is the concatenation of a swap statement, function declaration (with optional decorators) and docstring. A \"bad\" output for such input is a function body that uses the builtin functions according to their usual meaning, ignoring the swap statement. In contrast, the \"good\" output is a function body where the builtin functions are used consistently with the swap statement. To assess the success of the model in distinguishing between the \"bad\" and the \"good\" output, we compute the likelihood of each output given the input provided as a prompt (Figure 1 , Appendix D).\n\nData Collection\nSimilar to Miceli Barone and Sennrich (2017) , our dataset collection procedure involves scraping code from GitHub using the PyCodeSuggest library 6 (Bhoopchand et al., 2016) to download Python repositories with at least 100 stars, of size at most 200 MB and which mention the use of the Open Source CC-BY-4.0 license 7 in their README. Our final dataset includes 559 repositories downloaded on 16 December 2022. We then parse the .py files in each repository with the Python 3 ast module to make sure that they contain valid code. We extract 1,000 randomly chosen top-level functions that each contain a docstring and that reference at least two callable builtin identifiers, as defined by the builtins module. For each of these extracted functions, we randomly choose two builtin functions and generate the corresponding swap statement, function declaration (with decorators) and docstring as the example prompt, the original function body (regenerated from the abstract syntax tree with the astunparse module 8 ) as the \"bad\" output and the function body where the two selected builtins are swapped consistently with the swap statement as the \"good\" output (Figure 2 ).\nNote that functions can in principle access the builtin identifiers as strings using reflection and evaluation facilities, which may require a full static analysis of the code to identify and is undecidable in the general case. Since our method uses purely syntactic substitutions, there might be cases where the \"good\" outputs do not maintain the expected function behavior. In practice, this dynamic access of identifiers at runtime is rare with builtin identifiers and therefore does not pose an issue.\n\nExperiments\nWe next describe our experiments with a likelihood calculation of correct and incorrect completions ( \u00a73.1) and chat LLMs ( \u00a73.2), and then present a qualitative analysis ( \u00a73.3).\n\nComputational resources\nWe spent approximately 130 US dollars, including donated credits, to use the OpenAI LLMs through their publicly accessible API.\nWe also used a small amount of machine-hours on the Baskerville Tier 2 HPC platform 9 equipped with NVIDIA A100 GPUs. While this is a highend system, our experiments on the open source models can be also practically run on consumergrade machines with gaming GPUs.\n\nCompletion Likelihood\nFor our main set of experiments, we evaluate our dataset on families of auto-regressive language models (OpenAI GPT-3, Salesforce Code-Gen, Meta AI OPT) and one family of sequenceto-sequence conditional auto-regressive language models (Google FLAN-T5). All models are based on the Transformer architecture (Vaswani et al., 2017) and pretrained on large datasets scraped from the Internet (full details in Appendix A).\nResults We evaluate our datasets on the models using a modified version of the Inverse Scaling Prize evaluation code. 10 We report the results for all models in Figure 3 All tested models always prefer the incorrect output resulting in zero classification accuracy, the log-likelihood of the incorrect output is always significantly higher than the uniform baseline, but it varies with the model. Specifically:\n\u2022 The Meta AI OPT and OpenAI text-based GPT-3 families exhibit strong inverse scaling, with the larger models more strongly preferring the incorrect output. The trend is monotonic for the \"First generation\" GPT-3 family, and somewhat nonmonotonic for the OPT and InstructGPT families. The InstructGPT models perform worse than the base GPT-3 models.\n\u2022 The Salesforce CodeGen models exhibit mostly flat scaling. The \"mono\" models which are further fine-tuned on Python-only data perform worse than the \"multi\" models they are based on. \u2022 The OpenAI Codex models are the only models that seem to show positive scaling (which may be spurious since they are only two data points). However, the two GPT-3.5 models (text-davinci-002 and text-davinci-003, shown in the figures as red crosses) that further fine-tune code-davinci-002 on English demonstrations, lose their edge and end up performing worse than the base GPT-3 model of the same size (davinci). \u2022 Google FLAN-T5 shows an unclear, oscillating scaling trend, with large error bars at each point.\nWe report numerical correlation results between model size and mean loss 11 in Table 1 . Due to the small number of model sizes per family, some of the p-values are quite high, but the numerical results are consistent with the qualitative analysis.\nOverall, our analysis shows that autoregressive text-based LLMs (even when previously pretrained on code-based models) exhibit inverse scaling on our task, while the code-based models exhibit flat scaling which might possibly transition to positive scaling at the largest tested size, but fail to substantially improve over the text-based models.\n\nChat LLMs Accuracy\nWe perform additional experiments on chat LLMs by OpenAI and Anthropic, whose APIs became recently available. These models constrain both the input text and the generated output to take the form of a dialogue between the user and the \"assistant\" (the model itself). Notably, the APIs of these models do not report log-probabilities, hence they cannot be used to score arbitrary texts. This prevents us from using the same experimental protocol of the other experiments. We instead reformulate the task as binary classification where the model is presented with both the correct and incorrect forms of the same program in the same user message and is asked to select the correct one. We describe the models and the prompt templates in Appendix C. \n\nResults\nWe report the results in Figure 5 . All the models strongly prefer the incorrect programs, although the classification accuracy is non-zero. This may not be necessarily comparable to the zero classification accuracy of the previous experiments, due to the different experimental protocol. The Anthropic models (claude-instant and claude) show better accuracy (10-18%) with positive scaling and never produce invalid outputs. The OpenAI models (gpt-3.5-turbo and gpt-4) show low accuracy (< 4%) with flat or inverse scaling and occasionally produce invalid outputs.\n\nQualitative Experiments\nWe perform a small number of manual two-shot experiments on GPT-3.5. We also carry out manual experiments on OpenAI ChatGPT-3.5 12 and GPT-4 models, where we interact with the models in multiple rounds of dialogue, trying to hint the correct solution. The models are still unable to provide the correct continuations. See Appendices E-G. \n\nConclusions\nWe explored the ability of large language models to predict the correct continuations of fragments of Python programs in scenarios where the correct continuations are statistically uncommon due to the redefinition of identifiers caused by a statement that we included in the prompt. Not only all the tested models fail at this task, but some model families even display inverse scaling: they become worse, rather than better, with increasing model size. These results suggest that LLMs rely on \"shortcut learning\", i.e., weak, unstable, mostly lexical correlations in the data, rather than an understanding of the semantics of the data (in this case, Python code) at a deep level. We believe that our results are important both for a better scientific understanding of the capabilities of LLMs and for their practical relevance as a core technology for automated code generation tools. Future work could investigate scaling effects at larger model sizes, as well as on other programming languages.\n", "hypothesis": "We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become less confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.", "answer": false}
{"title": "Distinguishing Address vs. Reference Mentions of Personal Names in Text", "content": "\nIntroduction\nNamed entity recognition (NER) in text has long been a core task in the NLP community (Sundheim, 1995; Yadav and Bethard, 2018) . However, not much work has looked into distinguishing whether an entity mention is an instance of addressing the entity or referring to them:\n\u2022 John, would you turn the light off? (Address) \u2022 John turned the light off. (Reference)\nThe address usage is also called a vocative phrase: \"a noun phrase which does not belong to the thematic grid of a predicate and is used to attract someone's attention\" (Moro, 2003) . Many languages have explicit morphological vocative case markers: e.g., in \"Et tu, Brute?\", Brute marks the vocative case of the nominative Brutus. However, many 1 https://stavatir.com/s/address-vs-reference.xlsx modern Indo-European languages, including English, do not have vocative case markers, and the distinction is left to be interpreted based on context.\nDistinguishing vocative phrases is important in many NLP tasks, such as sentiment analysis (Karami et al., 2020) , offensiveness detection (Mubarak et al., 2020) and information extraction (Makazhanov et al., 2014) . For instance, Karami et al. (2020) point out the difference in interpretations between \"Let's eat, Grandma\" and \"Let's eat Grandma\". The vocative distinction is also important for NLP-aided computational social sciences, since the pragmatics and the patterns of usage vary between these two types of name mentions (Dickey, 1997) , and since name mentions capture various societal biases (Prabhakaran et al., 2019) . This aspect is especially crucial in studies analyzing political discourse, with the goal of understanding the rhetoric by and about political personalities (Prabhakaran et al., 2014; Gupta, 2022) .\nDespite the prevalence of NER as a useful task in various NLP applications (Marrero et al., 2013) , efforts to make this distinction have largely been limited to languages that have explicit vocative case markers such as Portuguese (Baptista and Mamede, 2017) , Hebrew (Tsarfaty et al., 2019) , Korean (Nam and Choi, 1997), and Sindhi (Muslim and Bhatti, 2010) , and not much work has looked into detecting vocative name mentions in English.\nIn this paper, we present a dataset of social media text in the political domain in English language, with person mentions annotated with the address vs. reference distinction. We then build a tagger that is able to make this distinction automatically, with an accuracy of 85%. We use this tagger to demonstrate the importance of this distinction in two largescale computational socio-linguistic analysis. First, we demonstrate that female personalities are more likely to be mentioned in the addressing context than male personalities, across three different social medial corpora, which has implications for NLP research on gender bias in data and models. Second, we demonstrate that sentences with address mentions are significantly more likely to be toxic than those with reference mentions. This finding has important implications for the active area of NLP research on detecting online abuse.\n\nAddress vs. Reference Mentions\nHow a person is addressed or referenced in language, and its associated pragmatics has long been of interest in sociolinguistics (Brown et al., 1960; Brown and Ford, 1961) . While most of this research focused on the different address pronouns and the T/V distinction, much less work has looked into the difference in the social meaning of a mention when used as an address vs. when used as a reference (Dickey, 1997) . While this distinction is not limited to persons (for instance, organizations may also be mentioned in an addressing context, as in Hey Doordash, where is my food?), person name mentions add additional nuance owing to the social relations. For instance, Dickey (1997) show that the words used to address a person by a speaker may differ from the words used to refer to them depending on the social power relations between the speaker, the referent, and the addressee.\nForms of address has been studied in NLP-aided computational sociolinguistics, for instance, in the context of how they relate to social power relations (Prabhakaran et al., 2013) . The address vs. references distinction has also been shown to be of value in NLP tasks, for instance, Mubarak et al. (2020) extracts Arabic tweets with the vocative particle \"yA\" as it indicates directing speech to a person or a group, increasing the likelihood of offensiveness. However NLP work on making this distinction is largely limited to languages that have explicit vocative case markers. In the absence of any vocative markers, as in English, this becomes a task that relies on the syntactic context. In this paper, we build resources to perform and evaluate this distinction, and demonstrate its utility in NLP applications.\nThere is related work in NLP on detecting addressees in multi-party dialog (op den Akker and op den Akker, 2009; Ouchi and Tsuboi, 2016; Le et al., 2019; Ek et al., 2018) , which is a substantially different task from ours. First, addressee detection in multi-party dialog takes into account the larger dialog/content context (e.g., prior utterances). For instance, Ouchi and Tsuboi (2016) jointly captures \"who is talking about what at each time step\" in order to determine the addressee. Ours is a simple linguistic task that relies on the local syntactic context of named mentions, making it applicable in broader contexts. Second, the above work crucially looks into the implicit cues about addressees. In contrast, our work focuses only on explicit mentions, primarily motivated by the computational social science analyses anchored on them.\n\nData\nSource: We use the corpus of Facebook comments on politicians' posts released by (Voigt et al., 2018) for this study. Our choice is motivated by three reasons. First, the comments in this corpus are all made in response to a individual's Facebook post and hence it is likely for it to have more instances of comments addressing the person than general social media data with mentions of that person. Second, the corpus captures the individual's name within the metadata, making it easy to detect and disambiguate different mentions referring to the same person. Finally, the corpus also captures the gender information of the person the comments are in response to (unlike most other gender-labeled data that captures the gender of the speaker/writer) as it was originally developed to study gender bias in social media, which is one of our goals too.\nPre-processing: Since the metadata captures the politician's name that each comment is in response to, we use a regex-based approach to determine if that politician is mentioned in the comment or not. We made sure the regex captures different forms of address including full name mentions, first name mentions, and last name mentions. Furthermore, since the corpus contained comments directed at only 402 politicians, we manually coded different common variations and misspellings of their first and last names. For instance, the first name of the politician Jim Boozman could be mentioned as Jim, James, or Jimmy, and the common variations of his lastname included Boozman, Boozeman, and Bozeman. While some of these choices may be genuine misspellings, some others may indicate pragmatic connotations: Jimmy instead of Jim may have been used to evoke familiarity, while Boozeman instead of Boozman may have been intended to evoke humor or disrespect. We do not analyze these distinctions in this paper, however, we included them in our regex to ensure that we capture such diverse associated linguistic contexts.\nAnnotation: We sampled 800 comments with at most 100 words (to avoid exceedingly long comments) from the corpus. We restricted ourselves to only those comments with a single mention of the individual (i.e., removed comments with no or multiple mentions). Multiple mentions were rare in our data (less than 1%), and when they do happen they were almost exclusively all reference mentions, as it is unlikely for someone to address someone by name, and then refer to them in third person in the same sentence itself. We trained two annotators to make the address vs. reference distinction. The annotators were undergraduate students majoring in Psychology at Yale University. Annotators were provided with the comments, the individual whose post the comment was in response to, as well as the mention of that individual detected in the comment. They were asked to label whether the mention was addressing the individual vs. referencing the individual, along with examples.\nAnalysis: All comments were double annotated, obtaining an inter-annotator agreement of \u03ba = 0.898, suggesting that the task is relatively easy for trained humans, and that our annotations capture reliable data. We then performed an adjudication round where both annotators met with one of the authors and arrived at a final label through discussion. While most disagreements were due to misinterpretations, some cases were inherently ambiguous. For instance, in \"Yes!!! Sen. Booker\", it is ambiguous whether the commenter is addressing Sen. Booker or just mentioning him.\nThe annotation and adjudication process revealed 15 comments where the name mention was not valid; e.g., within a URL contained in the comment, and 11 comments where the comment did not have enough linguistic context to make the distinction; e.g., when the comment was just a name mention. We removed these comments as they will add noise, resulting in 774 comments in the dataset, each with a mention labeled as either address or reference. There were 250 (32.3%) instances that were the address usage compared to 524 (67.7%) instances that were the reference usage.\n\nAutomatic Tagger\nWe now investigate automatically distinguishing address vs. reference, given a text and a name men-tion in it. Since contextualized embeddings such as BERT (Devlin et al., 2019) are proven to capture syntactic information (Clark et al., 2019) , we expect the positional embedding of the name mention to capture its syntactic context and hence help make this distinction. Further, we use the intuition that reference mentions are more likely to occur in syntactic contexts where third person pronouns could fit, while address mentions are more likely to fit second person pronouns or address terms. We consider three settings, each with two sets of words that fit with the address vs. reference contexts: S1: you/your vs. he/him/his/she/her S2: you/your vs. he/him/his/she/her/they/them S3: you/your/hey/hi vs. he/him/his/she/her S1 uses singular pronouns, S2 includes the (usually) plural pronouns they/them, S3 includes addressing terms (hey/hi). For each setting, we use a contextual embedding, replace the mention with [MASK] and calculate the score for each word in the list to fit the masked slot. If the top scored word from the list is of the address category, we predict the mention as address, otherwise, as reference. To illustrate, the top candidate from S3 above for the input \"[MASK], would you turn the light off?\" as per BERT is hey, while the top candidate for \"[MASK] turned the light off \" is he, then she. This approach is not entirely foolproof, but as Table 1 shows, this simple approach yielded good performance of 85% accuracy. We report results using BERT and DistillBERT models across all three settings outlined above. Adding addressing terms hey and hi increased the accuracy, while adding the third person pronouns they and them that are usually used in plural context (but also has singular usage) resulted in reducing the accuracy.\nMost errors happen when the sentence is not well-formed or uses non-standard language. An approach to circumvent this issue is to fine-tune a pre-trained model using our data. In our preliminary experiments, fine-tuning a BERT model only yields marginal (\u223c1%) improvement in accuracy at sentence level. Using more advanced models and hyper parameter tuning may yield better performance. However, our goal in this paper is not to build the best tagger possible for this task, rather to demonstrate the utility of this task in NLP and computational social science applications. Given the high performance of the Slot-filling model, we use it for all analyses in the rest of this paper. \n\nGender Effects in Addressing\nWe first look into the RtGender dataset (Voigt et al., 2018) built to study differential responses to gender. They found that responses to female posters or speakers were more likely to be about the individuals (e.g., their appearance) rather than about the content they posted or talked about. As a complementary analysis, we analyze whether these responses were addressed to the speaker or poster, or referring to them. We apply the tagger to 5K comments each, chosen at random, from three different sub-corpora in the RtGender corpus: comments in response to (1) Facebook posts by politicians (FB Congress), (2) Facebook posts by celebrities (FB Wiki), and (3) TED talk videos (Ted Talks). We ensured that the tagger does not introduce systematic gender bias; t-test revealed no association between gender and error (p = 0.166). Across board, mentions of female personalities were more likely to be in the address rather than reference contexts (Figure 1 ). This difference was statistically significant in all three cases: t(4999) = 3.51, p < .001 (FB Congress); t(4999) = 3.87, p < .001 (FB Wiki); and t(4999) = 4.41, p < .001 (TED Talks). For the congress dataset, we also have access to the political party they belong to; we added it as a control causing the effect size to decrease (2.72) suggesting that political party affiliation plays an important role. In fact, Figure 2 shows that the gender disparity is present only for the Republican party politicians.\nAddressing someone directly could be an expression of friendliness or familiarity, and its prevalence in comments directed at female personalities is notable. These insights enable adding nuance to many NLP-aided studies of gender and power. Moreover, this finding adds to research on gender influences on communication with and about professionals (Atir and Ferguson, 2018) .\n\nAddress vs. Reference and Toxicity\nWe now turn to online abuse detection, an NLP task where address vs. reference distinction is important. Prior work has shown that 2nd person pronouns are spuriously associated with toxic comments (Hede et al., 2021) . In languages such as Arabic that has explicit vocative markers, researchers have used vocative markers to curate comments with higher likelihood of offensiveness (Mubarak et al., 2020) . In this section, we use our tagger to analyze the tox- icity dataset annotated by Jigsaw (Jigsaw, 2018) to see if this pattern holds true. In the Jigsaw dataset, we do not have access to the mentions of people in text. Hence, we created a tagger for the Jigsaw dataset by first using the SpaCy python package to detect person mentions, then used the BERT Slotfilling (S3) tagger to detect whether each person is addressed or referenced in the message.\nWe find significant difference in address vs. reference in toxic vs. non-toxic tweets. The average toxicity score of sentences with address mentions were 0.088, compared to 0.070 for those without; this difference is statistically significant using the standard Student's t-test (p < .001) and a permutation test (p < .001). Figure 3 shows differences in the ratios of address to reference mentions in toxic and non-toxic texts. This finding is important for NLP-aided content moderation, especially in detecting targets of abuse.\n\nDiscussion/Conclusion\nIn this paper, we introduced the basic NLP task of distinguishing a name mention to be address or reference, annotated a new dataset for it in the English language, and presented a simple tagger using contextual word embeddings. Our annotation and tagging experiments reveal this to be a relatively easy task, however our accuracy being only at 85% suggests room to improve. We also demonstrate the utility of this capability in computational social science work anchored on name mentions through two analyses: first, on gender bias in mention patterns, and second, in toxic comments online.\nThis capability is important, but often ignored, for tasks that assume entity mentions to be part of the expressed propositional meaning; e.g., belief modeling (Prabhakaran et al., 2015) , and social relation extraction (Massey et al., 2015) . It will also aid in tasks that model relationships between interactants, such as power (Prabhakaran and Rambow, 2014) and influence (Rosenthal and Mckeown, 2017) . The vocative usage is arguably already being implicitly modeled in tasks such as dialog act tagging. However, it may be important to model it explicitly in certain cases, e.g., our work could contribute to ongoing efforts in detecting addressees in multi-party dialog (Ouchi and Tsuboi, 2016; Le et al., 2019) . Future work should look into these applications, and more advanced modeling techniques such as few-shot training for this task.\n", "hypothesis": " In this paper, we present a new annotated dataset that captures the address vs.  reference distinction in English, 1 an automatic tagger that performs at 85% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language..", "answer": true}
{"title": "Unsupervised Task Graph Generation from Instructional Video Transcripts", "content": "\nIntroduction\nTasks in the real-world are composed of multiple key steps with specific dependencies that dictate the order in which they can be performed (e.g., one has to check for breathing before performing CPR). Exposing these dependencies between key steps has many downstream applications including assisting human users in troubleshooting and building artificial agents that efficiently learn and perform new tasks. However, information about tasks is typically available in unstructured and noisy form in the wild (e.g., 'how to' descriptions or instructional video transcripts), presenting a major challenge in extracting structured representations.\nThere is a long history of work on reasoning about tasks, events and temporal ordering, broadly referred to as 'script understanding' ( of script understanding problems include generating a sequence of steps from a given task description (e.g., bake a cake) (Lyu et al., 2021; Sancheti and Rudinger, 2021; Sun et al., 2022) and generating flow graphs from goal and event descriptions (Pal et al., 2021; Sakaguchi et al., 2021) . Script generation also manifests in interactive settings such as simulated embodied environments where agents are expected to reason about subgoals in order to complete tasks (Logeswaran et al., 2022; Huang et al., 2022). Many of these prior approaches either fine-tune language models on human-annotated scripts or rely on knowledge encoded in language models to generate scripts. In contrast, we attempt to use pre-trained language models as an information extraction system to perform zero-shot script inference from noisy ASR (Automatic Speech Recognition) transcriptions of instructional videos describing a task.\nOur focus in this work is to generate a directed graph that represents dependency relationships between the key steps relevant to a real-world task. Figure 1 (a) shows a graph predicted by our approach for performing CPR. An example depen-\n\nGet out two slices of bread\nSpread peanut butter on one slice\n\nSpread jelly\nJoin the slices \"hello and welcome. this is episode number one of traditional school lunches. today I'm going to explain how to make a simple peanut butter and jelly sandwich. before we start making our ..\" log p LM = -270 \"how to make a peanut butter and jelly sandwich. so we came out here all the way today to make one of these. that's right a peanut butter and jelly sandwich. there's a lot of science behind ..\" Given multiple text transcripts of a task, we 1) Summarize the steps described in the transcript, 2) Identify the key steps, 3) Re-label summary steps with key steps, 4) Rank key step sequences using a language model and 5) Consolidate top-k sequences to generate a task graph for the given task. dency that can be read from the graph is that checking for safety hazards has to have happened before any other step (i.e., it is a precondition that needs to be satisfied). In this paper, we will use the term task graph to refer to such dependency graphs.\n\nSpread peanut butter and jelly\nMore formally, consider a real-world task \u03c4 . We assume that multiple text transcripts t 1 , . . . , t n describing how this task is performed are available. 1 We assume that having access to such multiple transcripts helps robustly identify the dependencies between key steps so that an accurate task graph can be generated. For instance, if step y frequently follows step x, it is highly likely that step x needs to happen before step y (i.e., is a precondition). Our goal is to generate a task graph for the given task \u03c4 which models these dependencies. In particular, this involves (i) Identifying the key steps K = {k 1 , . . . , k m } relevant to performing the task and (ii) Generating a graph with nodes k i and edges representing precondition relationships.\nOur contributions in this work are as follows. \u2022 We propose an unsupervised task graph generation approach that uses pretrained language models to infer key steps and their dependencies from multiple text descriptions of a real-world activity. \u2022 We propose ranking and filtering mechanisms to improve the quality of generated task graphs. \u2022 We demonstrate the effectiveness of the proposed approach compared to strong supervised and unsupervised baselines on two datasets. 1 Each transcript is a text document derived from an instructional video using Automatic Speech Recognition.\n\nApproach\nOur approach to task graph generation consists of multiple steps, illustrated in Figure 2 . First, we use an instruction-tuned language model to generate a summary of steps (in free-form text) from a transcript (Section 2.1). Given these summary step sequences generated from multiple such transcripts for the task, we identify the key steps relevant to the task using a clustering approach (Section 2.2). We then re-label summary step sequences using the identified key steps to obtain key step sequences (Section 2.3) and rank them using a language model (Section 2.4). Finally, we generate a task graph from the key step sequences (Section 2.5).\n\nGenerating Summary Steps\nThe first step of our pipeline extracts a summary of steps g i = (g 1 i , g 2 i , . . .) for performing the task described in each transcript t i . We use an instructiontuned language model for this purpose. We prompt the model with a transcript, followed by a query such as 'Based on this description list down the key steps for making coffee using short phrases.' and let the model generate a completion. We use the 'Davinci' version of the InstructGPT (Ouyang et al., 2022) model in our experiments. We observed that the model consistently generates the steps in the format '1. <step 1>\\n 2. <step 2>\\n ..', occasionally using bullet points instead of numbers. The sentences g j i on each line are extracted and treated as the summary steps identified from the transcript. Appendix B shows example summary step sequences generated by InstructGPT.\n\nIdentifying Key Steps Relevant to the Task\nGiven summary step sequences g 1 , . . . , g n generated in the previous step, we seek to identify correspondences between steps in different summaries and capture the salient steps that appear frequently. We use a clustering approach for this purpose. Sentences g j i are represented as embeddings using a sentence encoder (We use the MiniLMv2 encoder from the SentenceTransformers library (Reimers and Gurevych, 2019; Wolf et al., 2019), which was identified as the best sentence embedding method for semantic search/retrieval). We obtain highconfidence clusters by identifying max cliquesclusters of sentences that are similar (determined by a threshold -cosine similarity \u2265 0.9) to each other, and retain cliques with more than 5 sentences. We noticed that this often yields multiple clusters that represent the same key step. For instance, the steps 'fill the moka pot with water' and 'fill the bottom chamber with water' represent the same key step of filling water, but are placed in different clusters. Identifying such redundant clusters based on sentence similarity alone is difficult. We define the notion of sequence overlap between two clusters -how often a sentence from one cluster and a sentence from the other cluster appear in the same summary step sequence. Intuitively, if two clusters have high inter-cluster similarity and low sequence overlap, it is likely that they represent the same key step, and we merge the clusters. The resulting clusters obtained are treated as the key steps k 1 , . . . , k m . 2 Appendix C shows example clusters discovered for different tasks.\n\nRe-labeling Summary Step Sequences\nWe re-label each summary step sequence g (subscript i dropped for brevity) with the identified key steps k 1 , . . . , k m to produce a key step sequence h using the greedy algorithm described in Algorithm 1. The algorithm sequentially picks the most similar 3 candidate summary step and cluster pair (g a , k b ) at each step, assuming each key step only appears once in the sequence. The process terminates when the highest cosine similarity drops below zero.\n\nAlgorithm 1: Key Step Sequence Inference\nInput g = (g 1 , g 2 , . . .) \u25b7 Summary step sequence Input K = {k1, k2, . . .} \u25b7 Key steps For each summary step identify most similar sentence from each cluster:\nCij \u2190 max s\u2208k j cos(g i , s) Hij \u2190 arg max s\u2208k j cos(g i , s) S \u2190 {} \u25b7 Predicted alignments while maxi,j Cij > 0 do a, b \u2190 arg max i,j Cij S \u2190 S \u222a {(a, b)} Caj \u2190 0, C ib \u2190 0 \u2200i, j Sort (ai, bi) \u2208 S so that a1, a2, . . . are in increasing order Output h = (H a 1 b 1 , H a 2 b 2 , . . .)\n\u25b7 Key step sequence\n\nRanking\nOne shortcoming of the labeling algorithm described in the previous section is that it does not take the sequential nature of steps into account. 565 tokens on average.\n\nSetup\nThe datasets come with key steps annotations (i.e., K) for each task and key step sequence annotations for each transcript. Our approach is unsupervised and does not make use of these annotations. However, for evaluation purposes, we consider two settings. The first setting assumes ground truth K and evaluates the performance of the full pipeline ignoring the clustering component (since key steps are known). In the above setting, we use ground truth human annotated graphs from Jang et al. (2023) for evaluation. In the second setting, we use K inferred from Section 2.2 and perform qualitative comparisons with ground truth graphs. Note that we did not use key step sequence annotations from the datasets in either setting.\nBaselines. We compare our approach against the following baselines. Proscript (Sakaguchi et al., 2021) is a language model fine-tuned on manually curated script data. Given a task description and a set of key steps, Proscript generates a partial order of the key steps. In addition, we consider several variations of our approach as baselines in Table 1 . In contrast, we exploit large language models in order to extract key phrases from the transcript. Third, we observe that ranking and filtering key step sequences using a language model ( 5 ) further improves performance, with a significant improvement for ProceL. Finally, our approach comes closest to graphs generated from human annotated key step sequences in the datasets ( 6 ). 5 Unknown Key Steps Next, we consider the full pipeline where key steps are identified automatically. Since ground truth reference task graphs are unavailable in this case we perform a qualitative comparison of graphs generated using our approach and the ground truth, human annotated graph. Figures 1 and 2 show predicted graphs for the tasks perform cpr and make pbj sandwich, respectively. We observe that the predicted graph for perform cpr is more detailed and fine-grained than the ground truth graph and captures many of the ground truth precondition relationships. On the other hand, the graph for make pbj sandwich is less fine-grained compared to the ground truth (Figure 6 of Appendix D). For instance, the ground truth annotations distinguish between putting jelly on the bread and spreading jelly on the bread, whereas our approach treats them as a single step. In addition, spreading peanut butter and spreading jelly are independent of each other and have no sequential dependency. However, the predicted graph fails to capture this and assumes that the former is a precondition for the latter. Appendix D shows more examples of predicted graphs.\n\nAblations Summary\nStep Sequence Generation We perform an ablation to study the effect of the model used to generate summary step sequences from transcripts. We replace the InstructGPT model (Ouyang et al., 2022) with a FLAN-T5 model (Chung et al., 2022) and evaluate graph prediction performance. We find that InstructGPT consistently outperforms FLAN-T5 across all the tasks (Table 2 ). In addition, we found that plain language models (not fine-tuned with instructions) struggled to produce usable summaries. This shows that models trained with instructions and human-preference data are better at producing task graphs from transcripts compared to other forms of supervision such as language modeling and supervised multi-task training with NLP tasks.\n\nRanking Language Model\nWe perform an ablation to understand the impact of the choice of language model for the ranking process in Section 2.4. We present the average performance on tasks in the ProceL dataset with different language model choices in Table 3 . First, we find that performance does not degrade much when switching to a smaller model in the GPT2 family. Second, we notice that scale alone does not guarantee better ranking performance as the larger GPT-J model (Wang and Komatsuzaki, 2021) is inferior to the GPT2 models. These findings suggest that the choice of pre-training data influences the script knowledge present in a model and can be more important than model scale. \n\nConclusion\nThis work presented an unsupervised approach to generate task graphs from text transcripts of instructional videos. Our framework exploits multiple text transcripts which describe a task in order to robustly identify the key steps relevant to a task and the depencies between these steps. We demonstrated the effectiveness of our approach compared to supervised and unsupervised baselines on instructional video transcripts from the ProceL and CrossTask datasets.\n", "hypothesis": "We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a semi-supervised manner.", "answer": false}
{"title": "Temporal Relation Classification using Boolean Question Answering", "content": "\nIntroduction\nEvents in stories are not necessarily mentioned in a chronological order. The timeline of events is important for understanding the main narrative of a story as well as the correct order of actions. For example, the timeline may be used directly by clinicians looking for a convenient way to explore the disease course of their patients, or by algorithms to follow instructions in the right order, given as text, such as in cooking recipes. Building the timeline is done based on two main subtasks: (1) event extraction, that is, detecting the most important events in a given textual input, and (2) temporal relation classification (TRC), also known as temporal relation extraction, which is about putting two events, given as gold spans, in the right chronological order. For example, consider the following text: \"Before you put the cake in the oven, say a little prayer.\" In the first subtask, known as event extraction, we would like to detect only the relevant events for our domain of interest. In this case, the words put and say are both verbs representing some relevant actions; therefore, we mark them as events. In the second subtask, TRC, we put every two events in a chronological order by classifying them using a closed set of temporal relations. In this case, the two events put and say should be assigned with the label AFTER indicating that put is happening after say in a chronological order.\nIn this study we focus on TRC, which is typically handled as a classification problem of two events provided along with the context in which they are mentioned. MATRES (Ning et al., 2018b) is one of the dominant datasets for TRC comprised of news documents manually annotated with temporal relation labels. The events are deterministically chosen to be all actions (mostly verbs) mentioned in the documents. Every pair of events (n, m) are manually labeled with one of four labels: BEFORE (n happened before m), AFTER (n happened after m), EQUAL (n and m happened at the same time), and VAGUE (it is impossible to know which event happened before the other).\nTraditional classification approaches have already been demonstrated for TRC. In this work, we get inspiration from a relatively new promising approach for solving natural language processing (NLP) tasks, in which the target algorithm is based on a reduction of the task to another problem. In our case, we solve the TRC problem using a model that handles the boolean question-answering (QA) task, which is about answering a Yes/No question given a passage used as a context. We decide to use boolean QA as our proxy problem due to the way the annotation work for building MATRES has been done. In the main annotation guidelines of MATRES (Ning et al., 2018b) , the annotators are asked to assign a label to a pair of events (n, m) by answering the two following questions: (1) Is it possible that the start time of n is before the start time of m? and (2) Is it possible that the start time of m is before the start time of n? There are four possible answer combinations, each is mapped to one label: (yes, no) \u21d2 BEFORE, (no, yes) \u21d2 AFTER, (no, no) \u21d2 EQUAL, and (yes, yes) \u21d2 VAGUE. Therefore, we transform an instance of TRC, composed of a pair of events and a document, into a pair of Yes/No QA instances, one for each of the two questions, and then fine-tune a Yes/No QA model to answer them. The final prediction is made based on the combination of the Yes/No answers retrieved by the QA model.\n\nRelated Work\nTRC has received increasing levels of attention in the past decade. There is a relatively long list of related shared tasks (Verhagen et al., 2007 (Verhagen et al., , 2010;; Bethard et al., 2016; MacAvaney et al., 2017) . Modern approaches for TRC use some sort of a neural network as a classifier. For example, Dligach et al. (2017) showed that a neural network that uses only words as input, performs better than the traditional models that process features which were manually created. A more modern approach for TRC is based on large pre-trained language models. Han et al. (2021) continued to pre-train a language model before fine-tuning it on TRC; Zhou et al. (2021) incorporated a global inference mechanism to tackle the problem at the document level; Han et al. (2019a) combined a recurrent neural network (RNN) over BERT (Devlin et al., 2019) embedding and a structured support vector machine (SSVM) classifier to make joint predictions; Ning et al. (2019) integrated BERT with a temporal commonsense knowledge base, and improved accuracy significantly by 10% over the previously known best result; and Han et al. (2019b) developed a multitask model for the two related subtasks, event extraction and TRC. Mathur et al. (2021) train a gated relational graph convolution network using rhetorical discourse features and temporal arguments from semantic role labels, in addition to some traditional syntactic features. Wang et al. (2022b) 2021) built a syntactic graph constructed from one or two continuous sentences and combined it with a pre-trained language model. The best result so far has been reported recently by Zhou et al. (2022) , who extract relational syntactic and semantic structures, and encode them using a graph neural network. In another recent work (Man et al., 2022) , the authors introduce a novel method to better model long document-level contexts by detecting and encoding important sentences in the document. None of those studies use QA to address the TRC problem.\nOur boolean QA-based approach continues to improve on Zhou et al.'s (2022) work, achieving a new stat-of-the-art result for TRC.\n\nDatasets\nWe conduct experiments with two datasets. MA-TRES (Ning et al., 2018b ) is a composition of three datasets (TIMEBANK, AQUAINT and PLAT-INUM) which were re-annotated following new guidelines. Following previous work, we use TIMEBANK and AQUAINT together as a training set and PLATINUM as a testing set. For validation and development we use a different dataset named TCR (Ning et al., 2018a) , which has been used similarly in other works (Zhang et al., 2021) . As mentioned above, MATRES has four labels: BEFORE, AFTER, EQUAL, and VAGUE. TimeBank-Dense (Cassidy et al., 2014) , or TB-Dense in short, is the second dataset which we use in this work. TB-Dense has two additional labels: INCLUDES and IS-INCLUDED. Following common practices, we evaluate our models using the relaxed micro-average F1 score (i.e., for MA-TRES ignoring all mistakes on VAGUE instances during evaluation, and for TB-Dense completely removing VAGUE instances from the validation and testing sets). Overall, MATRES contains 12, 736 training instances, 837 testing instances, and 2, 600 validation instances from TRC. TB-Dense contains 4, 032 training instances, 1, 427 testing instances, and 629 validation instances. The label distributions is summarized under Appendix B.\n\nMethodology\nWe design our problem as Yes/No question answering problem. Therefore, we fine-tune a pre-trained language model (PLM) by taking a Yes/No QA classification approach for which every instance is composed of a passage (text) and a question, provided along with a Yes/No answer. Our QA model is designed as a traditional classifier; the input is a concatenation of the passage and the question with a special separator token in between, and the output is a two-way label distribution vector. We use RoBERTa (Liu et al., 2019) , which comes in two sizes, base and large; we use both.\nAn instance of TRC is composed of a document, two event spans, and a label. In order to use our QA model for TRC, we convert each such instance into two or three Yes/No QA instances, which we use for fine-tuning and testing. Each QA instance Yes/No QA Instance Passage: Before you @put@ the cake in the oven, @say@ a little prayer. Question: Is it possible that @put@ started before @say@? Yes/No QA Instance Passage: Before you @put@ the cake in the oven, @say@ a little prayer. Question: Is it possible that @say@ started before @put@?\nYes is composed of a passage and a question. Therefore, we cut the sentence from the input document, containing the spans of the two events, and use it as a passage. Sentence breaks are detected using full stops (e.g., a dot followed by a white space). The passage is paired with the Yes/No questions, generating multiple QA instances. MATRES uses a label set of size four, and TB-Dense has two additional labels: INCLUDES and IS-INCLUDED. Therefore, for MATRES we compose the following two question templates (<EVENT 1> and <EVENT 2> are used here as placeholders), inspired by the TRC annotation guidelines: (1) Is it possible that <EVENT 1> started before <EVENT 2>? and\n(2) Is it possible that <EVENT 2> started before <EVENT 1>? For TB-Dense, we add another question template: (3) Is it possible that <EVENT 1> ended before <EVENT 2>? We experiment with additional phrasing, as described in the following section. The answers to the questions are determined by the label of the TRC instance, using Table 1 . In TB-Dense, question 3 is not used only when the answers to questions 1 and 2 are either (no,no) or (yes,yes), respectively.\nEach QA instance is processed independently during fine-tuning. At inference time we run the instances through the model and assign a TRC label based on the answers.\nNaturally, a document may contain more events than the two relevant ones. Therefore, we use markers (Baldini Soares et al., 2019) in order to mark the two relevant events. Specifically, each relevant event is surrounded by the '@' character in both, the passage and the question. Figure 1 demonstrates how we process a MATRES instance.\n\nExperiments and Results\nTable 2 summarizes our evaluation results on MATRES and TB-Dense, using the two sizes of RoBERTa. We compare our results with two baseline models, and some previous work. We experiment with three variations for the questions (only for the two MATRES-related questions; for TB-Dense we only use the best out of the three), 1 as reported in the three first rows of Table 2 : QV1: <EVENT1> before <EVENT2>? QV2: Is it possible that the start time of <EVENT1> is before the start time of <EVENT2>? QV3: Is it possible that <EVENT1> started before <EVENT2>?\nWe fine-tune our models for the duration of five epochs and evaluate them on the validation set every epoch; we use the best checkpoint as the output model. We run every experiment three times using different seeds and report on the averaged accuracy and standard deviation on the testing set. 2 The MA-TRES model with the best question variation (QV3) has been further processed with two additional procedures: Perturbation and fine-tuning with BoolQ.\nPerturbation. To achieve better model generalization, we perturb the instances of the training set, using nlpaug, 3 a data augmentation library for text. We employ the optical-character recognition (OCR) error simulation, using the default argument values, which replaces about 30% of the characters (except the characters of the events) with random letters or digits considered as common OCR mistakes (e.g., l vs. 1). We modify the original training instances in place; therefore, we do not increase the size of the training set. In Table 2 we refer to this procedure as AUG. It adds about 1% to F1 in the base model, and a slightly higher percentage in the large model, on both datasets. BoolQ. Before fine-tuning on MATRES, we finetune the model on the BoolQ dataset (Clark et al., 2019) in which every instance is composed of a passage (text) and a question, provided along with a Yes/No answer. Overall, BoolQ has 9, 427 training instances, which we use for fine-tuning. In Table 2 we refer to this procedure as BoolQ. As reported, this step does not improve performance. Therefore, we did not use it for TB-Dense.\nBaseline Algorithms. To assess the contribution of our Yes/No QA design, we define two baseline algorithms. The first baseline is a traditional multiclass QA model, which is given with the same passage as in our original Yes/No QA model, paired with only one question that takes one of the labels as an answer. We experiment with two question variations:\nQV1: What is the chronological order of the two marked events: <EVENT 1> and <EVENT 2>? QV2: Is <EVENT 1> happening before, after or at the same time as <EVENT 2>?\nThe second baseline is a simple multiclass sentence-classification RoBERTa model, which receives as input for this model comprises only the passage, and the output is one of the labels from the dataset. As seen in Table 2 , our models outperform the baselines and previous work, introducing a new state-of-the-art result for TRC on both datasets. 4\n\nConclusions\nWe proposed a novel approach for TRC using a pretrained language model fine-tuned for a Yes/No QA classification task. Our model was fine-tuned to answer questions which were originally designed to support decision making during the annotation process. We believe we have demonstrated the potential of this method to leverage the Yes/No QA design to break down the prediction process into a set of Yes/No questions; our approach outperforms existing methods, achieving a new state-of-the-art result for TRC on two datasets. There is a potential practical limitation to this work, which is related to time complexity and speed performance. Since every instance is transformed into multiple QA instances, it may take a relatively long time to process a document.\n", "hypothesis": " We propose an efficient approach for temporal relation classification (TRC) using a boolean question answering (QA) model which we fine-tune on questions that we carefully design based on the TRC annotation guidelines, thereby mimicking the way human annotators approach the task.", "answer": true}
{"title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English", "content": "\nIntroduction\nPrivacy policies are documents that outline how a company or organization collects, uses, shares, and protects individuals' personal information. Without a clear understanding of privacy policies, individuals may not know how their personal information is being used or who it is being shared with. The privacy violation might cause potential harm to them. However, privacy policies are lengthy and complex, prohibiting users from reading and understanding them in detail (Commission et al., 2012; Gluck et al., 2016; Marotta-Wurgler, 2015) .\nVarious natural language understanding (NLU) technologies have recently been developed to understand privacy policies (Wilson et al., 2016a; Harkous et al., 2018; Ravichander et al., 2019 ; * Work done while at UCLA. Ahmad et al., 2020; Parvez et al., 2022; Ahmad et al., 2021; Bui et al., 2021) . These tasks focus on understanding specific privacy practices at different syntax or semantics levels and require significant effort for data annotations (e.g., domain experts). It is hard to develop generic pre-trained language models (e.g., BERT (Devlin et al., 2019) ) with task-specific fine-tuning using limited annotated data. Besides, the unique characteristics of privacy policies, such as reasoning over ambiguity and vagueness, modality, and document structure (Ravichander et al., 2021) , make it challenging to directly apply generic pre-trained language models to the privacy policy domain.\nTo address these problems and encourage research to develop NLU technologies in the privacy policy domain, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, to evaluate the privacy policy language understanding across six tasks, including text classification, question answering, semantic parsing, and named-entity recognition. PLUE also includes a pre-training privacy policy corpus that we crawl from the websites to enable privacy policy domainspecific language model pre-training. We use this corpus to pre-train BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , Electra (Clark et al., 2020) , and SpanBERT (Joshi et al., 2020) and finetune them on the downstream tasks. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. We will release the benchmark to assist natural language processing (NLP) researchers and practitioners in future exploration.\n\nPolicy Language Understanding Evaluation (PLUE) Benchmark\nPLUE is centered on six English privacy policy language understanding tasks. The datasets and tasks are selected based on the following principles: (1) usefulness: the selected tasks can help practitioners in the domain quickly understand privacy practices without reading the whole privacy policy; (2) task diversity: the selected tasks focus on different semantic levels, e.g., words (phrases), sentences, and paragraphs;\n(3) task difficulty: the selected tasks should be adequately challenging for more room for improvement; (4) training efficiency: all tasks can be trainable on a single moderate GPU (e.g., GeForce GTX 1080 Ti) for no more than ten hours;\n(5) accessibility: all datasets are publicly available under licenses that allow usage and redistribution for research purposes.\n\nDatasets and Tasks\nPLUE includes six tasks in four categories. Table 1 presents an overview of the datasets and tasks within PLUE, and Thus, we report the results for collection-related and share-related entities, respectively.\n\nPre-training Corpus Collection\nThe existing pre-trained language models (PLMs) mostly use data from BooksCorpus (Zhu et al., 2015) and English Wikipedia. Language models pre-trained on text from those sources might not perform well on the downstream privacy policy language understanding tasks, as privacy policies are composed of text written by domain experts (e.g., lawyers). Gururangan et al. (2020) suggested that adapting to the domain's unlabeled data (domainadaptive pre-training) improves the performance of domain-specific tasks. Therefore, we collect a large privacy policy corpus for language model pre-training. In order to achieve broad coverage across privacy practices written in privacy policies (William, 2020; Ahmad et al., 2021) , we collect the privacy policies from two sources: mobile application privacy policies and website privacy policies.\nAppendix B provides more details about how we collect these two types of privacy policies.\n\nModels & Training\nBaselines We benchmark pre-trained language models (PLMs), BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , SpanBERT (Joshi et al., 2020) , Electra (Clark et al., 2020) , and LEGAL-BERT (Chalkidis et al., 2020) . We present the details of the PLMs in Appendix C.\n\nDomain-specific Continual Pre-training\nIn order to adapt PLMs to the privacy policy domain, we continue to train BERT, Electra, SpanBERT, and RoBERTa on the pre-training corpus described in Section 2.2. We refer to them as PP-BERT, PP-RoBERTa, PP-SpanBERT, and PP-Electra, respectively. 1 We present details in Appendix D.1.\nTask-specific Fine-tuning We fine-tune PLMs for each PLUE task. We only tune the learning rate for each task, as we found in the preliminary experiments that model performances are highly sensitive to the learning rate. We present more details in Appendix D.2.\n\nExperiment Results\nTables 2 and 3 present the results for all the experiment models for PLUE tasks. Rows 2-9 show the results of the base PLMs and their corresponding variants with privacy policy domain-specific continual pre-training. Similar to GLUE (Wang et al., 2019) , we also provide the average scores of all PLUE tasks in the last column of Table 3 .\nWe observe that the language models (PP-BERT, PP-SpanBERT, PP-Electra, PP-RoBERTa) adapted to the privacy policy domain outperform the general language models consistently in all the tasks, and PP-RoBERTa performs the best among all base models in terms of the average scores of all PLUE tasks. In particular, PP-RoBERTa performs the best for OPP-115, APP-350, PrivacyQA, 2 and PI-Extract, among all base models. PP-BERT and PP-RoBERTa perform the best for PolicyQA; PP-Electra and PP-RoBERTa achieve the best performance for PolicyIE. In contrast, LEGAL-BERT (row 10) performs comparably or shows moderate improvements over BERT, indicating that pretraining on the general legal corpus does not necessarily help privacy policy language understanding. It is interesting to see that continual pre-training of the language models using the privacy policy domain data benefits them differently. For example, in the text classification tasks (i.e., , the performance difference between SpanBERT and PP-SpanBERT are most significant, while models using MLM (BERT and RoBERTa) already shows relatively high performance before continual pre-training and continual pre-training brings moderate gains to BERT and RoBERTa.\nWe further investigate the improvement of large variants of PLMs over base variants of PLMs on PLUE tasks. Since PP-RoBERTa BASE performs the best among all base models, we also continue pretrain RoBERTa LARGE (PP-RoBERTa LARGE ). As shown in the last five rows in Tables 2 and 3 , the large pre-trained language models mostly outperform their base counterparts. Noticeably, PP-RoBERTa LARGE is the best-performing model in APP-350, PolicyQA, PI-Extract, and sub-tasks in PolicyIE, and it also achieves the highest average scores of all PLUE tasks among all models.\nLastly, even though domain-specific pre-training and large PLMs help boost the performance for all tasks, the performance of some tasks and datasets (e.g., APP-350, PrivacyQA, slot filling in PolicyIE) remains low, which indicates much potential for further work on NLP for the privacy policy domain.\n\nRelated Work\nPrivacy Policy Benchmarks The Usable Privacy Policy Project (Sadeh et al., 2013) is the most significant effort to date, resulting in a large pool of works (Wilson et al., 2016a,b; Sathyendra et al Table 3 : Performance comparison of pre-trained models on intent classification and slot filling tasks (PolicyIE) and average scores of all PLUE tasks. We fine-tune all the models three times with different seeds and report average performances. Human performances are reported from the respective works.\n2016; Mysore Sathyendra et al., 2017; Bhatia and Breaux, 2015; Bhatia et al., 2016; Hosseini et al., 2016; Zimmeck et al., 2019) to facilitate the automation of privacy policy analysis. A wide range of NLP techniques have been explored accordingly (Liu et al., 2014; Ramanath et al., 2014; Wilson et al., 2016a; Harkous et al., 2018; Zimmeck et al., 2019; Shvartzshanider et al., 2018; Harkous et al., 2018; Ravichander et al., 2019; Ahmad et al., 2020; Bui et al., 2021; Ahmad et al., 2021) .\nPre-trained Language Models In the last few years, NLP research has witnessed a radical change with the advent of PLMs like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) . PLMs achieved state-of-the-art results in many language understanding benchmarks. Consequently, PLMs have been developed for a wide range of domains, e.g., scientific (Beltagy et al., 2019) , medical (Lee et al., 2020; Rasmy et al., 2021; Alsentzer et al., 2019) , legal (Chalkidis et al., 2020) , and cybersecurity (Ranade et al., 2021; Bayer et al., 2022) . This work investigates the adaptation of PLMs to facilitate NLP research in the privacy policy domain.\n\nConclusion and Future work\nReliable aggregation of datasets and benchmarking foundation models on them facilitate future research. This work presents PLUE, a benchmark for training and evaluating new security and privacy policy models. PLUE will help researchers benchmark policy language understanding under a unified setup and facilitate reliable comparison. PLUE also presents some challenges in language understanding evaluation for privacy policies. For example, the imbalance data issue for privacy practices is a major challenge in the Priva-cyQA task (Parvez et al., 2022) . Data efficiency is also a challenge for continual pre-training as the amount of unlabeled data is also small for this domain. Approaches such as (Qin et al., 2022 ) could be investigated to continually adapt LMs for the emerging data in this domain.\n", "hypothesis": "We demonstrate that domain-specific continual pre-training offers no significant performance improvements across all tasks.", "answer": false}
{"title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark", "content": "\nIntroduction\nAlthough large language models (LLMs) are powerful tools for generating human-like language, they can also memorize false or outdated associations, limiting their applicability. Model editing techniques promise to solve this problem by correcting non-factual associations. It is important that model edits are highly specific in the sense of not introducing any unwanted associations as a side effect. In this paper, we discuss why the current benchmark for specificity falls short and propose a more challenging, dynamic specificity benchmark to evaluate model editing techniques. Using this benchmark, we evaluate recent model editing techniques and find previously unreported side effects. We highlight the importance of improved specificity benchmarks for the effective and safe use of LLMs subject to model edits.\nFigure 1 : Unintended side effects of model edits and how to measure them. (a) GPT-2-medium is edited using ROME to counter-factually associate the Louvre's location with Rome. However, this results in unintended associations (\"loud facts\") like the association of Obama with Rome, suggesting low specificity of the edit. The edit also significantly increases the maximum logit (shown in brackets), suggesting that the edit is not merely replacing \"Paris\" with \"Rome\" in the desired contexts. (b) Measuring specificity by the fraction of correctly completed test prompts (COUNTERFACT) suggests a high specificity for ROME. Prepending the edit prompt (like \"The Louvre is in Rome.\") to each test prompt (COUNTERFACT+) results in a significant drop in performance. A significant drop in measured specificity can also be observed if the model edit is implemented using constrained fine-tuning (FT-L).\nModel editing updates the parameters of a trained model in order to change its predicted probability distributions without retraining the entire model. This can be used to edit the associations that the model has memorized and hence, improve the accuracy of the model. Fig. 1 shows the example of a counter-factual model edit using ROME (Meng et al., 2022a) where the location of the Louvre is edited to be Rome instead of Paris. We use a counter-factual example since it makes it more evident that the new association is an effect of the model edit instead of the model training. Note that the examples in Fig. 1 are not taken from the COUNTERFACT+ dataset introduced below, but serve to intuitively illustrate the model editing failure modes we are interested in.\nAn important desideratum for model editing is specificity. Specificity captures how well the effect of the model edit is localized; in other words, specificity measures the absence of unintended side effects of model edits. Fig. 1 shows two examples of unintended side effects of ROME model editing, which we collectively call the problem of \"loud facts\". In the first example, mentioning \"Louvre\" (the subject of the model edit) leads the edited model to also complete unrelated test prompts (\"Obama was born in\") with \"Rome\" (the object of the model edit). In the second example, mentioning \"Louvre\" boosts the logits for words semantically related to \"Rome\", like \"Vatican\".\nThe existing specificity benchmark for model editing from the COUNTERFACT dataset (Meng et al., 2022a) suffers from two limitations which can be illustrated using these examples. First, COUNTERFACT does not prompt the model in a way that is likely to surface unwanted side effects. As demonstrated by the examples in Fig. 1 , mentioning the subject of the model edit can drastically change the behavior of the edited model, but the existing benchmark does not detect this. Second, COUNTERFACT considers only the probabilities for the original and edited object token (\"Paris\" and \"Rome\"). As shown by the last example in Fig. 1 , the edited model displays strongly changed logits not only for the original object (\"Paris\") and edit object (\"Rome\") but also for semantically related tokens (\"Vatican\"). Again, this would be overlooked by the current specificity evaluation since it does not consider the entire probability distribution.\nThese limitations mean that side effects of edits may be overlooked and specificity overestimated.\nOur main contributions are:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark.\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution instead of the currently used metrics which focus only on the tokens directly implicated in the model edit. (De Cao et al., 2021) and (Mitchell et al., 2022) . Elazar et al. (2021) introduced ParaRel, a curated dataset of paraphrased prompts and facts. Meng et al. (2022a) use this as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, including specificity.\nKnowledge extraction from LLMs. The assessment of knowledge within language models (LMs) has typically been done by evaluating whether the model is able to predict pieces of knowledge; Petroni et al. (2019 Petroni et al. ( , 2020 ) defined a fill-in-theblank prompt and asked the LM to complete it. Subsequent work has demonstrated that knowledge extraction can be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021) , or by fine-tuning a model on open-domain textual facts (Roberts et al., 2020) . However, constructing prompts from supervised knowledge extraction data is still prone to learning new knowledge instead of recalling existing knowledge in an LM (Zhong et al., 2021) .\n\nDataset\nWe investigate the specificity of recent model editing techniques using the COUNTERFACT benchmark introduced in (Meng et al., 2022a) . COUNTERFACT is a collection of 21,919 nonfactual statements of the form (subject, relation, object) (s, r, o * ), which have low probabilities prior to the model edit. For each of these non-factual statements, we perform a model edit targeting this specific statement. To measure specificity, we then check whether any other associations in the model change in undesired ways. COUNTERFACT supports this check by providing a set of so-called neighborhood prompts for every non-factual statement used in the model edit. These neighborhood prompts are constructed as follows: For a model edit of the form (s, r, o c ) \u2192 (s, r, o * ) (where o c is the correct object, and o * is the false, counterfactual object), COUNTERFACT samples a set of nearby subjects s n for which (s n , r, o c ) holds true. Neighborhood prompts are then paraphrases of the collected (s n , r).\nSuppose, for example, the edit request was (Darrieux, mother_tongue, French) \u2192 (Darrieux, mother_tongue, English). COUNTERFACT takes the relation and object from the edit request (mother_tongue, French), samples true factual associations for this relation, object pair; e.g., (Montesquieu, mother_tongue, French) and then samples a random paraphrase, such as \"The native language of Montesquieu is\". These neighborhood prompts can be used to inspect whether the model edit has undesired side effects on closely related factual associations. See appendix C for a sample from the COUNTERFACT dataset, including the full set of neighborhood prompts.\nMotivated by the example of loud facts shown in Fig. 1 and by the intuition that unwanted side effects are more likely when the model is primed with the linguistic context of the model edit, we now introduce a dynamic version of COUNTERFACT which we will refer to as COUNTERFACT+. To obtain COUNTERFACT+, we modify the neighborhood prompt by prepending the model edit. For example, if the original prompt is \"The native language of Montesquieu is\" the modified prompt would be \"The mother tongue of Danielle Darrieux is English. The native language of Montesquieu is\". See appendix D for a sample of the modified neighborhood prompts used for COUNTERFACT+.\nTo understand why we call COUNTERFACT+ a dynamic version of COUNTERFACT consider how either dataset would be applied to evaluate the success of a model edit: In both cases, we would need to identify the set N of neighborhood prompts in the dataset that are semantically closest to the intended model edit. But in COUNTERFACT, we would use N as is, whereas in COUNTERFACT+ we would change every prompt in N as a function of the model edit, as described above.\n\nMetrics\nTo evaluate the specificity of a model edit on COUNTERFACT, Meng et al. (2022a,b) use two metrics, called Neighborhood Score and Neighborhood Magnitude. Denoting the post-edit probabilities for the correct token o c and incorrect edit token o * by P * (o c ) and P * (o * ), respectively, these are defined as follows: The Neighborhood Score (NS) is defined as the fraction of neighborhood prompts for which P * (o c ) > P * (o * ). The Neighbourhood Magnitude (NM) is defined as P * (o c ) \u2212 P * (o * ), the difference in probability assigned to the correct token versus the incorrect edit token. High NS and NM indicate that the edit has small unwanted side effects.\nNS and NM, however, do not detect cases where the model edit significantly changes the predicted probability for tokens other than o c and o * , such as in the last example in Fig. 1 . To capture this possibility, we introduce as an additional metric the Kullback-Leibler (KL) divergence of the nexttoken distribution between the edited and unedited model, referred to as Neighborhood KL Divergence (NKL). Abbreviating the next token probability distribution for the unedited and edited models by P (w) and P * (w), respectively, and denoting the token vocabulatory by W, NKL is defined as KL divergence between P (w) and P * (w):\nEQUATION\nA large NKL is undesirable because it implies that the next-token probability distribution for neighborhood prompts has been strongly affected by the model edit.\n\nModels and Model Editing Algorithms\nWe use GPT-2-medium (355M parameters), GPT-2-XL (1.5B) (Radford et al., 2019) , and GPT-J (6B) (Wang and Komatsuzaki, 2021) to evaluate the following model editing methods:\n\u2022 ROME (Rank-One-Model-Editing) performs a rank-one update of a single MLP layer to implement the edit (Meng et al., 2022a) .\n\u2022 MEMIT (Mass-Editing Memory in a Transformer) extends ROME to updates across several MLP layers (Meng et al., 2022b) . Note that we do not test using multiple simultaneous edits.\n\u2022 FT-L: Fine-Tuning with an L \u221e norm constraint (Zhu et al., 2020) , constrained to a single layer, as described in (Meng et al., 2022a) .\nWe use FT-L as a simple baseline.\n\nResults\nFigure 2 shows the results for the ROME, MEMIT, and FT-L editing algorithms applied to the GPT-J (6B) model for different specificity metrics and datasets considered in this work. When evaluated using the Neighborhood Score (Fig. 2 , top), we observe significant drops in specificity for all editing algorithms when going from COUNTERFACT to COUNTERFACT+. Note that specificity measured on the unedited model (GPT-J (6B)) also drops suggesting that there is confounding from the test prompts in COUNTERFACT+, potentially due to recency bias (Zhao et al., 2021) . The drop in specificity is much more pronounced for ROME and MEMIT, compared to FT-L and the unedited model, however. This shows that:\n\u2022 ROME and MEMIT have undesired side effects which are not detected by COUNTERFACT\n\u2022 the improved benchmark COUNTERFACT+ is able to detect these unwanted side effects When evaluating specificity using the newly introduced Neighborhood KL Divergence (Fig. 2 , bottom), we observe a large spike in divergence for both ROME and MEMIT when going from COUNTERFACT to COUNTERFACT+. FT-L shows a much smaller increase in divergence from COUNTERFACT to COUNTERFACT+. Figure 3 in the appendix shows the results on COUNTERFACT and COUNTERFACT+ for the NM metric. (top) NS, the average fraction of correctly completed neighborhood test prompts after the model edit (larger is better). We see that COUNTERFACT+ is a much more challenging specificity benchmark: Success rates NS on it range from 33% to 54% across different editing algorithms while they are close to 80% for COUNTERFACT. (bottom) NKL, the KL divergence of the next-token probability distribution of the edited model from that of the unedited model, averaged over all neighborhood test prompts. A lower value indicates higher specificity (the edited model behaves more like the unedited model).\nResults across all three models are shown in tables 1 to 3. These tables list the mean scores on COUNTERFACT and COUNTERFACT+ for the Neighborhood Score (NS), Neighborhood Magnitude (NM), and Neighborhood KL divergence (NKL), respectively. The brackets give upper and lower bound of 99% confidence intervals obtained via bootstrap resampling (N=1,000 The results from tables 1 to 3 show that the significant drop in specificity when evaluating on\nNKL \u2193 COUNTERFACT COUNTERFACT+ GPT-2 M FT-L 1.4e-05 (1.3, 1.4) 1.4e-05 (1.3, 1.4) ROME\n1.6e-06 (1.4, 1.7) 2.5e-05 (2.5, 2.5)\nGPT-2 XL FT-L 7.2e-06 (6.9, 7.4) 9.5e-06 (9.3, 9.7) ROME 1.5e-06 (1.4, 1.6) 3.3e-05 (3.2, 3.3) MEMIT 2.9e-07 (2.5, 3.4) 9.0e-06 (8.8, 9.1) GPT-J (6B)\nFT-L 3.2e-06 (3.1, 3.4) 5.2e-06 (5.1, 5.3) ROME 3.5e-06 (3.2, 3.8) 1.8e-05 (1.8, 1.9) MEMIT 9.2e-07 (8.0, 10) 9.9e-06 (9.8, 10)\nTable 3 : Neighborhood KL Divergence NKL (\u00b5 & 99% CI) on COUNTERFACT and COUNTERFACT+. Note that the order of magnitude is suppressed for the confidence interval for visual clarity; it is the same as for the mean.\nCOUNTERFACT+ (compared to COUNTERFACT) holds across different model sizes and is not an artefact of using a particular model. Section B in the appendix discusses the scaling of specificity with model size in more detail.\n\nConclusion\nModel editing techniques for auto-regressive transformers exhibit unreported issues related to specificity. Although our fine-tuning baseline, FT-L, exhibits less vulnerability to these issues than ROME and MEMIT, it falls short in competing with them regarding crucial model editing metrics such as robustness to paraphrasing (Meng et al., 2022a,b) . This indicates that model editing still presents numerous complexities that require future attention. Additionally, we revealed that the existing COUNTERFACT benchmark fails to detect the low specificity in ROME and MEMIT. To address this limitation, our primary contributions include:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit.\n", "hypothesis": " However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.  We extend the existing COUNTERFACT benchmark to include a dynamic component and dub our benchmark COUNTERFACT+.  Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric.", "answer": true}
{"title": "C-XNLI: Croatian Extension of XNLI Dataset", "content": "\nIntroduction\nNatural language processing has developed rapidly in recent years. Models are starting to achieve human-like performance, but most of these achievements are concentrated on only a small fraction of the world's 7000+ languages. This is to be expected due to the nature of linguistic annotation, which is not only tedious, subjective, and costly, but also requires domain experts, which are in decline (Lauscher et al., 2020) .\nThere are two main approaches commonly used to handle that problem from the models' perspective. The first approach relies on cross-lingual transfer, where the model is pretrained to learn multilingual representations (Conneau et al., 2020; Pires et al., 2019) , while the other approach relies heavily on Machine Translation (MT) systems to translate the text from a low-resource language to a high-resource language (or vice versa). Both approaches can be easily evaluated on cross-lingual benchmarks such as XTREME (Hu et al., 2020) or XGLUE (Liang et al., 2020) . They consist of crosslingual datasets grouped by task to allow comprehensive evaluation. Unfortunately, XTREME covers 40 languages and XGLUE only 19.\nSince none of these benchmarks include Croatian language in any of their datasets, and Crosslingual Natural Language Inference (XNLI; Conneau et al., 2018) corpus is included in both, we decided to extend XNLI with Croatian (C-XNLI). The task is to classify whether a premise contradicts, entails, or is neutral to the hypothesis. XNLI's development and test sets are crowdsourced in English and human-translated into 14 languages, while MultiNLI's (Williams et al., 2018) training set is used for training. It also consists of machine-translated sets required for the translatetrain and translate-test paradigms.\nOur Croatian extension is created in the same manner as its XNLI parent. The development and test sets are translated by a professional translator. Since XNLI provides translate-train, translate-dev and translate-test sets, we opted for Facebook's 1.2B parameter m2m_100 MT model (Fan et al., 2020) to create our own translations.\nIt has been shown that MT models still suffer from errors like mistranslations, non-translations and hallucinations (Freitag et al., 2021; Raunak et al., 2021) , which motivated us to analyze the quality of our dataset. For this purpose, we sampled 2000 sentences per language in both Croatian and German, and evaluated the translations using a variant of the Direct Assessment (DA) score proposed in the Multilingual Quality Estimation dataset (MLQE; Fomicheva et al., 2022) .\nTo summarize, our contributions are the following: (1) we create and analyze the Croatian exten-sion of XNLI and provide baseline models, (2) we create Quality Estimation (QE) datasets for Croatian and German to evaluate the quality of machinetranslated sentences from the translate-train sets, and (3) we quantify the textual overlap between hypothesis and premise and analyze its impact on baseline models.\n\nC-XNLI\nIn creating the dataset, we follow the same procedure as Conneau et al. (2018) . We hired a native Croatian professional translator to translate the English development (2490 samples) and test (5010 samples) sets of the XNLI dataset into Croatian. Premises and hypotheses were given to the translator separately to ensure that the premises did not provide context for the hypotheses. The English training set, derived from MultiNLI and containing 392,702 samples, was translated into Croatian using a selected MT model. We considered a total of eight models and opted for Facebook's multilingual m2m_100 model with 1.2B parameters because of its highest BLEU score (Papineni et al., 2002) on the FLORES dataset (Guzm\u00e1n et al., 2019) , as shown in Table 1 . All of m2m_100 and mbart models are available on fairseq 1 (Ott et al., 2019) , whereas opus models are available on Helsinki-NLP 2 (Tiedemann, 2020; Tiedemann and Thottingal, 2020) \n\nDA Scores\nTo evaluate the quality of the system used to translate English to Croatian, we compare the generated translations with the available translations from a high-resource language. We score a sample of Croatian and German translations from the train set and compare the results. The sentences were sampled using a semantic similarity-based metric that correlates with translation quality (Cer et al., 2017) to flatten the original distribution of scores and analyze samples of diverse quality. A cosine score between the multilingual sentence representations from both LASER (Artetxe and Schwenk, 2019) and SBERT (Reimers and Gurevych, 2019) were used to measure semantic similarity between the source and translated sentences. These models are commonly used at the Conference on Machine Translation (WMT) for QE task (Specia et al., 2021 (Specia et al., , 2020)) . The SBERT we used is a multilingual variant trained on the paraphrase dataset which has slightly better performance than the models trained on similarity tasks (Reimers and Gurevych, 2020) .\nBy utilizing a histogram of cosine scores with a bin size of 0.05, we adopted a circular sampling approach to randomly select one premise from each bin until a total of 50 premises were obtained. Similarly, we followed the same procedure for hypotheses, alternating between SBERT and LASER cosine scores. Furthermore, we implemented an additional criterion to ensure the inclusion of all premises and hypotheses that share a common premise. This entire process was repeated until we reached a 1000 samples each, for both SBERT and LASER cosine scores (2000 in total) .\nWe scored the samples using the procedure described by Fomicheva et al. (2022) . Annotators were asked to rate the translation quality for each sentence on a scale 0-100. Sentences were initially annotated by three annotators. If the range of the most diverging scores exceeded 30 points, an additional annotator was asked to replace the most diverging one until convergence was achieved. The annotators' raw scores were converted to z-scores 3 ; the final score is the average of all scores after convergence. More information about annotators, and annotation procedure is presented in Appendix A.\n\nC-XNLI and DA Scores\nTo demonstrate that our extension has similar properties to its parent XNLI, we perform the following analyses. We tokenize C-XNLI's sentences with MOSES tokenizer and obtain the average number 2018) provide is the BLEU score of their MT systems translating to and from the target language. We have extended their results to include those for the Croatian language (Table 2 ). Our translations from English to Croatian (EN-XX in the table) have the fourth-best BLEU score. These findings are not too surprising since the MT we use is more recent. The distribution of DA scores for Croatian and German is shown in Figure 1 . We can observe that Croatian, although is a lower-resourced language, it has a slightly higher translation quality, as the mean of Croatian DA scores is almost identical to a German one. The correlations between the LASER and SBERT cosine scores and DA scores for both languages are shown in Table 3 , with p < 0.05. The correlations for German are higher, and the LASER cosines tend to correlate less. In Figure 2 we can see that the Croatian model is more likely to make a mistake on premises compared to the German model. \n\nOverlaps\nThe analysis presented here extends Artetxe et al.'s (2020) work where authors demonstrate that the overlap between hypotheses and premises is an overlooked bias in the XNLI dataset, caused by access to premise during hypothesis generation in English, and no access to it during translation into other languages. They decrease the bias by back-translating data and improve their results. To demonstrate the existence of that bias, we take a more direct approach and define a metric that represents overlap -the proportion of copied text from premise to hypothesis. It is the number of character N -grams which occur in both hypothesis and premise, divided by the number of possible character N -grams in the hypothesis. In Table 4 we presented those overlaps using bi-grams, N = 2. We can observe that in the training set, the overlap is 5% to 20% higher compared to development and test sets. In order to investigate that even further, we asked our professional translator to translate 1% of our C-XNLI dataset: 100 sentences which consist of 25 premises and 75 of their hypotheses. We made sure that the premise was given alongside each hypothesis so that it provides context to it in order to measure the influence on the overlap since, in the translation effort, premises and hypotheses were given separately. Our representative sample contained similar genre distribution, overlap distribution, and similar development vs. test overlap ratio. Our results show that when using N = 2, biased sample has 8% increase in overlap, whereas for N = {3, 4, 5}, it increased by \u223c 17%. Table 5 : We present the accuracy of baseline XLM-R Base models on each XNLI language, with the addition of Croatian, together with an average accuracy for all languages without Croatian (Avg) and with Croatian (Avg +hr ).\nOur XLM-R models are averaged over three different seeds. We also calculate the Spearman's correlation between accuracies of each model's setup and train set overlaps (C tr ), development set overlaps (C de ), and test set overlaps (C te ). For overlaps we used N = 2.\n\nXLM-R Setups\nWe tested cross-lingual transfer using zero-shot and translate-based setups. For each, we employ pretrained XLM-R Base model (Conneau et al., 2020) , implemented in Transformers library (Wolf et al., 2020) . In the zero-shot approach, we fine-tune our model on English samples. In the translate-train approach, we fine-tune on translations of a training set, whereas in translate-train-all, we fine-tune it on concatenated training translations. Evaluations are done in all languages. In the translate-test approach, we use the same model from our zero-shot approach and evaluate it on English translations of other languages. We experimented with various hyperparameter configurations and found appropriate ranges. Hyperparameter optimization is done for each setup, and details are presented in the Appendix B.\nResults of baseline setups are shown in Table 5 . To demonstrate the comparability of our training setup, we compare XLM-R's reported accuracy with ours, which is only 0.6 points lower in the train-translate-all setup. The performance of the Croatian model is consistently among the TOP5 models. The reason for that might be in the high BLEU score shown in Table 2 . Focusing on the best overall model -translate-train-all, we notice that adding Croatian did not drastically change the average performance and decreased it only for dis-tant languages like Urdu and Swahili. Whereas for other languages, it increased or did not change significantly.\nFinally, Table 5 also shows how the performance of models on the test set of each language correlates with the bi-gram overlaps in the train, development, and test sets of that particular language. There is a consistent high correlation between the overlap in all sets and models' performance (p < 0.05). However, a lower correlation is seen in the development and test sets. This observation could be attributed to the fact that increasing the overlap of a particular language makes it more similar to the English set, in terms of overlap, thus improving the performance. However, as we showed in Subsection 3.2, the overlap in the development and test sets is artificially lower due to biased translation. Alternatively, high training overlaps might indicate that the model is learning to detect the occurrence of overlapping cues.\n\nConclusion\nIn this work, we extended XNLI to include the Croatian language. The development and test sets were translated by a professional translator. We have successfully demonstrated that the quality of the development and test sets is comparable to that of the other languages. To validate the machine-translated training set, we compare our Croatian translations with those available for a high-resourced language -German. The comparison is based on 2000 manually scored sentences from German and Croatian train sets using a variant of DA scores normalized by z-score. Our results show that the Croatian MT model performs slightly better because it's more up-to-date, even though it's a lower-resourced language. We also found that the Croatian translation model performs poorly on longer sentences -premises.\nFinally, we present an overlap metric to measure the textual overlap between the premise and hypothesis. We find that the training set has larger overlaps than the development and test sets. These overlaps resulted in a high correlation between the models' scores, indicating that a model uses cues from the data that also correlate with overlaps.\nWe provide our datasets under the same license 4 as the XNLI dataset, and also make the accompanying code available on GitHub 5 . We hope that by sharing our datasets, researchers will have the opportunity to gain further insights and expand their knowledge in the field of cross-lingual transfer.\n", "hypothesis": "The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs. The train set is translated using Facebook's 1.2B parameter m2m_100 model. We thoroughly analyze the Croatian train set and find that it has a higher quality than the existing machinetranslated German set.", "answer": false}
{"title": "A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models", "content": "\nIntroduction\nRecently, Distillation from Weak Teacher (DWT) (Yuan et al., 2020; Qin et al., 2022) , a reversed Knowledge Distillation (KD) technique, has gained attention from researchers. Unlike the traditional KD (Sanh et al., 2019; Wang et al., 2020b,a; Sun et al., 2019; Jiao et al., 2020) , which compresses a pre-trained model by transferring its knowledge to a smaller model, DWT distills knowledge from a smaller (or weaker) pre-trained model to a larger model to improve its quality during training.\nDWT is well-suited for practical real-world scenarios such as:\n\u2022 Train a larger (scaled-up) model with an existing (smaller) pre-trained model to improve model quality using the same dataset. * Work done while interning at Meta AI.\n\u2022 Train a new, large-scale model with an old, smaller model to improve performance using the same dataset.\n\u2022 It is not feasible to use a large teacher model during KD training due to training resource constraints.\nFor the above cases, DWT can utilize the existing pre-trained models and improve the learning of new (larger) models.\nStudies (Yuan et al., 2020; Qin et al., 2022) have shown that DWT allows a larger student model to leverage the knowledge of a weaker, smaller pre-trained teacher model in both the computer vision and NLP pre-training stages. While previous research by Qin et al. (2022) has demonstrated the potential of DWT in the NLP domain, it did not fully explore the key aspects of DWT such as the impact of teacher model quality and a student model initialization technique for DWT.\nHowever, to truly unlock the potential of DWT for real-world applications, we need a deeper understanding of the key conditions and factors that contribute to its performance. For example, the effect of DWT might differ from traditional KD and potentially harm the student model, depending on the quality of its teacher.\nTherefore, this work conducts in-depth studies and uncovers crucial insights to optimize DWT in the pre-training stage of NLP as follows:\n\u2022 First, we investigate the effectiveness of DWT in relation to the quality of the teacher model. We find that an extremely weak teacher can negatively impact the student model's quality, which is different from the vision domain where even an extremely weak teacher still improves performance (Yuan et al., 2020) .\n\u2022 Second, we examine the impact of distillation by adjusting the weighting value of the soft loss. We demonstrate that adjusting the weighting value for the DWT loss (soft loss) can improve training speed but may lead to suboptimal performance. To mitigate this issue, we recommend starting with a large weighting value and gradually decaying it during training.\n\u2022 Lastly, we study the effectiveness of Parameter Remapping (PR) (Chen et al., 2015; Cai et al., 2018; Fang et al., 2020a; Lee et al., 2022) , which is a popular student parameter initialization technique for conventional KD, as an initialization technique for DWT. We observe that PR leads to suboptimal solutions, contrary to its effectiveness in conventional KD scenarios. Random initialization is better than PR for DWT.\nWe believe that these observations provide useful guidelines to better utilize DWT techniques for real-world applications.\n\nDistillation from Weak Teacher\nIn this section, we formulate the Distillation from Weak Teacher (DWT) strategy, which involves training the target (student) model using both the teacher's predictions (soft labels) and the ground truth (hard labels).\nTask Given a classification task with c classes, for each training instance x and its corresponding ground truth label y, the ground truth distribution over the labels is denoted as q(c|x) (abbreviated as q(c)) where for each label c in the set {1...C}, q(y) = 1 and q(c) = 0 for all c not equal to y.\nModel The teacher model, with learnable parameters \u03c9, and the student model, with learnable parameters \u03b8, are utilized to predict the probability of each label c for a given instance x. The probability predicted by the teacher model, denoted as p \u03c4 \u03c9 (c|x), and the probability predicted by the student model, denoted as p \u03c4 \u03b8 (c|x), are expressed as follows:\np \u03c4 \u03c9 (c|x) = sof tmax(z \u03c9 ) = exp(z \u03c9 c /\u03c4 ) C i=1 exp(z \u03c9 i /\u03c4 ) p \u03c4 \u03b8 (c|x) = sof tmax(z \u03b8 ) = exp(z \u03b8 c /\u03c4 ) C i=1 exp(z \u03b8 i /\u03c4 ) where z \u03c9 = {z \u03c9 i } C i=1 is the output logit of the teacher model, z \u03b8 = {z \u03b8 i } C i=1\nis the output logit of the student model, and \u03c4 is the temperature used to soften the probabilities p \u03c9 (c) and p \u03b8 (c).\n\nWeak (Small) Teacher\nWe assume that the parameter of the teacher model is pre-trained as \u03c9 * . While conventional KD typically assumes that the size of the teacher model is larger than or equal to the size of the student model, i.e., |\u03c9 * | \u2265 |\u03b8|, DWT considers the case where the size of the teacher model is smaller than the size of the student model, i.e., |\u03c9 * | < |\u03b8|, or the quality of the pre-trained teacher model with parameters \u03c9 * is inferior to the quality of the pre-trained student model with parameters \u03b8 * obtained through stand-alone training.\nHard Loss is the cross-entropy loss H(q, p \u03b8 ) between the ground truth q and student's prediction p \u03b8 , used to train the student model:\nEQUATION\nFollowing BERT (Devlin et al., 2019) , H(q, p \u03b8 ) is the Masked Language Modeling loss (MLM loss).\nSoft Loss is the Kullback-Leibler divergence (KL divergence) S(p \u03c4 \u03c9 , p \u03c4 \u03b8 ) between the predictions of the student and the teacher models, and is given by: Final Objective The objective function L(\u03b8) aims to train the student model by minimizing a weighted sum of the hard loss and the soft loss:\nEQUATION\nL(\u03b8) = \u03b1 h \u2022 H(q, p \u03b8 ) + \u03b1 s \u2022 S(p \u03c4 \u03c9 , p \u03c4 \u03b8 ) (3)\nwhere the weighting hyperparameters for the hard loss and the soft loss are denoted by \u03b1 h and \u03b1 s , respectively.\n\nExperiment\nWe conducted a study to analyze the efficacy of the DWT method and present key observations for optimizing its impact in three core elements: (i) the quality of the teacher model, (ii) the degree of soft knowledge transfer, and (iii) the initialization type (parameter remapping) of the student model.\nTraining setting we use a default loss weight ratio of \u03b1 h : \u03b1 s = 1:1 for the hard loss and soft loss during distillation. The learning rate is set to 5e \u2212 4, and the models are trained for 20 epochs with the application of quantization, linear warmup (5%), the Adam optimizer (Kingma and Ba, 2014), 16 batch sizes per GPU, and 8 A100 GPUs per run. In the pre-training stage, we utilize a reduced dataset of 30 million sentences generated by uniformly selecting one sentence out of every four sentences from the original dataset, which consists of a combination of BookCorpus (Zhu et al., 2015) and Wikipedia (Foundation). The performance of the distilled models is evaluated on the dev sets of the GLUE benchmark (Wang et al., 2019), comprising nine sentence-level classification tasks (Please see the supplementary file for more details.).\n\nImpact of Teacher Model Quality\nIn Figure 2 , we examine the performance of distilled student models based on the quality of the teacher model. We conduct a distillation from a teacher model during the pre-training stage and fine-tune the distilled student models on the dev sets of the GLUE benchmark. We report the average performance and the performance gap between the distilled student and a student trained standalone. We categorize the weak teacher quality into three levels compared to the standalone student model, which has a model size of 67M and achieves an average performance of 79.89 on the GLUE benchmark dev sets. 1) Weak: 0.78\u00d7 smaller size, -2.23 lower performance\n2) Very Weak: 0.57\u00d7 smaller size, -13.44 lower performance\n3) Extremely Weak: 0.46\u00d7 smaller size, -26.02 lower performance.\nWhile distillation from weak teachers, even extremely weak ones, consistently improves the performance of the student model in the vision field due to the regularization effect (Yuan et al., 2020), we found that in language model pre-training, the effectiveness of DWT on the student model heavily depends on the quality of the teacher model. The student model (the red mark) clearly benefits from the Weak teacher model (score is 77.66), represented by the blue mark in the red box, as it shows an improvement of 1.44 points, from 79.89 to 81.33. However, when the performance gap between the teacher and student is too large, such as in the cases of Very Weak and Extremely Weak teachers, distillation from these teachers may negatively impact the student's performance by -1.37 and -2.76, respectively. Our observations provide valuable guidance for researchers aiming to utilize existing pre-trained models in training new models.\n\nThe Impact of Soft Loss\nIn Figure 3 , we investigate the impact of the soft loss in DWT during the pre-training stage by adjusting the weights in the following two versions:\n(1) Strong: We fix the weight for the hard loss at 1 and multiply the weight for the soft loss by 4 to increase the intensity of distillation. (2) Normal:\nThe ratio between the hard loss and soft loss is equal, with the soft loss weight set to 1. Finally, we fine-tune the models pre-trained with different soft loss weights on the GLUE benchmark tasks. Conventional KD has shown that using large weights for the soft loss can improve both training convergence and model performance (Sanh et al., 2019) . However, we reveal that DWT requires careful tuning of the soft weights. Our observations show that using a large weight for the soft loss (Strong) leads to faster convergence in most downstream tasks (e.g., MNLI (Williams et al., 2018) , COLA (Warstadt et al., 2019) , QQP (Iyer et al., 2017) , SST2 (Socher et al., 2013 )) compared to using a small weight (Normal). However, as training continues, using a small weight for the soft loss (Normal) leads to better fine-tuning performance than using a large weight (Strong). Therefore, we believe that gradually decreasing the soft loss weights (e.g., from 4 to 1) during training would benefit both convergence and performance.\n\nImpact of Parameter Remapping\nParameter remapping (PR) (Chen et al., 2015; Cai et al., 2018; Fang et al., 2020a,b ) is a popular technique used in conventional KD methods. It involves copying the parameters of a pre-trained teacher model and initializing the student model with these parameters before starting KD training (See the supplementary file for more details.). PR can accelerate convergence speed and improve the final performance of the distilled model. For example, DistilBERT (Sanh et al., 2019) uniformly samples six layers out of twelve from the BERT model (teacher) and initializes the corresponding layers in DistilBERT (student) with the copied parameters.\nIn Figure 4 , we investigate the effectiveness of \n\nConclusion\nDistillation from Weak Teacher (DWT) is a technique that improves the performance of a larger student model by transferring knowledge from a weaker, smaller teacher model. Despite the potential of DWT, the optimal conditions to use DWT have yet to be fully investigated in NLP pre-training. This study investigated three crucial factors for optimizing DWT in NLP pre-training, which differ from those in vision or traditional KD. These factors include the impact of teacher model quality, the use of parameter remapping as an initialization technique for DWT, and guidelines for adjusting the weighting value of the DWT loss.\n", "hypothesis": " However, the optimal conditions for using DWT have yet to be fully investigated in NLP pretraining.  Therefore, this study examines three key factors to optimize DWT, distinct from those used in the vision domain or traditional knowledge distillation.  These factors are: (i) the impact of teacher model quality on DWT effectiveness, (ii) guidelines for adjusting the weighting value for DWT loss, and (iii) the impact of parameter remapping as a student model initialization technique for DWT..", "answer": true}
{"title": "Do transformer models do phonology like a linguist?", "content": "\nIntroduction\nIn computational linguistics, neural networks have occupied much of recent work. One prime driver is adaptability to multiple facets of linguistic phenomena. As an example, sequence-to-sequence models have been shown to capture inflection patterns across numerous languages (Kodner et al., 2022) . While their performance represents significant advances, the abstractions generated during the modelling process warrant further investigation. We experiment with phonological processes on a constructed language to compare the generalisations learned by transformer models with widespread linguistic phenomena.\nIn particular, we address the following questions:\n\u2022 Learning specific phonological processes (are some more difficult than others?)\n\u2022 Categorisation (can the model generalise a category, vowels, consonants, specific consonant groups, e.g. plosives?)\n\u2022 Is word structure (syllables) implicitly learned?\nWe establish that the transformer model successfully models all 29 phonological phenomena we consider, regardless of linguistic complexity. Our results show that the model can generalise to linguistic categories with some caveats. By examining the transformer model's generalisation of haplology, we show that the model appears to learn syllables; the model can recognise the difference between VC and CV and generate previously unseen CV sequences.\n\nRelated Work\nInvestigating the cognitive reality of linguistic categories defined within phonology has long been of interest to linguistics. Does the natural class of phonemes bear any significance to a cognitive reality? For example, a series of experiments (Finley and Badecker, 2009; Chambers et al., 2010; Skoruppa and Peperkamp, 2011) examine the natural class of vowels and whether phonological patterns can be extended to previously unseen vowels. The studies suggest that participants were mostly able to generalise. In a similar vein, Finley (2011) presents a study on consonant harmony. The results suggest that learners (human learners) can generalise to novel consonants when the phonological pattern is general. However, the learners failed to generalise when the rule triggering the consonant harmony pattern was highly specific.\nWe adapt this long-standing linguistic question to ask whether Transformer-based abstractions are linguistically informed. Our experiment setup swaps the human learner with the Transformer architecture. Previous studies investigating phonological phenomena with Transformers include Elsner (2021) , where Transformers can handle reduplication and gemination. To an extent, 1 the SIG-MORPHON shared tasks (Kodner et al., 2022 ) also demonstrate the capacity of Transformers to represent phonological processes through capturing allomorphs conditioned by phonological environments.\nThere have been extensive studies on various phonological processes and RNNs. Haley and Wilson (2021) shows that encoder-decoder networks (specifically LSTM and GRU architectures) can learn infixation and reduplication. Mirea and Bicknell (2019) explores whether phonological distinctive feature information is required for learning word-level phonotactic generalisations using LSTMs. The authors find that information about phonological features hinders model performance, and phonotactic patterns are learnable from the distributional characteristics of each segment alone. Moreover, distributional information proves to be integral in recovering phonological categories (Mayer, 2020) .\nAnother way to investigate neural architecture abstractions is to probe the model internally. Silfverberg et al. (2021) examines whether RNN states encode phonological alternations through experiments on Finnish consonant gradation. The authors show that the models often encode consonant gradation in a select number of dimensions. Rodd (1997) probes the hidden states of an RNN model which controls Turkish vowel harmony. Similarly, Silfverberg et al. (2018) establish a correlation between embedding representations and distinctive phonological features for Finnish, Spanish and Turkish. This paper focuses on a model-external interrogation of Transformer generalisations by studying the predictions produced.\n\nLanguage Design\nThe phonological phenomena in question are tested on a constructed language. The primary motivation for this is to allow for a controlled experiment and ensure that we can generate enough samples of the required phonological environments for rules to be triggered and thus observed. With this in mind, we require the constructed language to be as representative as possible of natural language. Therefore, key features were chosen based on the condition of being the most typologically common ones (Maddieson, 1984; Ladefoged and Maddieson, 1996; Maddieson, 2013) . The main characteristics are listed in Table . 1.\n\nGenerating a lexicon\nThe most complex syllable structure possible in the language is CCVVCC and the simplest one is V. Since our language design aims to generate a synthetic lexicon, we also control for word length distribution. Previous works have shown that word length over word types exhibits a roughly Gaussian distribution with a mean in the range [7, 10], depending on the language (Smith, 2012) . We have chosen a mean word length of 8.\nAn additional constraint when generating a lexicon is the sonority sequencing principle (SSP) (Selkirk, 1984; Clements, 1990) . Syllable structures tend to be highly influenced by the sonority scale, with the general rule that more sonorous elements are internal (i.e., close to the nucleus) and less sonorous elements are closer to the syllable edge. Therefore, we use a sonority metric to avoid generating implausible consonant clusters, with the onset and coda requiring opposite values on the metric, i.e. increasing sonority in the onset and decreasing in the coda.\n\nData 2\nOur data preparation follows three steps: lexicon generation, triplet (lemma, tag, surface form) formation via the finite-state tool foma (Hulden, 2009) and, finally, sampling of these triplets ac-cording to the experiment at hand and formatting for Fairseq. (Ott et al., 2019) 3 We train the model as a standard 'inflection' task (Kodner et al., 2022) , but with tags being identifiers of the processes that are to be triggered instead of morphosyntactic information. For example, the input sequence moupi#GEMINATION would be paired with the output mouppi. More example triplets are shown in Table 2 . 4 \n\nInput Tag\nOutput Lexicon generation entails generating viable syllable structures and filling these abstract structures using vowel and consonant inventories. The syllables are concatenated n times, where n is an integer between 1 and 10. We sample from this uniform distribution to produce a Gaussian distribution for word length with a mean of 8 symbols.\nateiSa #APOCOPE ateiS enpanka #APHAERESIS npanka a:N\u00c3 #SHORTENING aN\u00c3 vepisk #LENGTHENING vepi:k moupi #GEMINATION mouppi aimggi #DEGEMINATION aimgi soute #INTERVOCALIC soude refend #DEVOICE refent ketedu #METATHESIS kedetu totoN #HAPLOLOGY toN pima #COPY pima\nWe include a COPY tag, where the input is copied to the output, to negate any performance drop by the model when unseen lemmata are encountered (Liu and Hulden, 2022) . In other words, the model, at test time, will never encounter a completely unseen lemma on which to perform a phonological change, since it will always have witnessed at least an input-output pair of any lemma used that is simply copied to the output.\n3 See B for model details. 4 Our nomenclature of sound changes follows Campbell (2013) . \n\nModelling common phonological processes with varying degrees of complexity\nIn this experiment, we establish that seq2seq models can successfully capture a range of phonological processes, including more complex rules such as metathesis. As seen in Figure 1 , the transformer model performs reasonably well across all phonological phenomena, with little distinction between the complexity of the process considered.\n6 Linguistic Category generalisation The results show that p is transformed to a b 77.6% of the instances. Where the conversion does not take place, errors typically follow the pattern of, e.g. outputting epeiSe instead of ebeiSe with the input epeiSe\nTo investigate the comparatively low performance. We compare word-initial devoicing with word-initial voicing as a priming process. The results are summarised in Table . 4. The accuracy of the predictions for the unseen p was substantially lower in the case of word-initial voicing (40%) compared with the word-initial devoicing (74.8%). Interestingly, word-initial voicing involves the same process as intervocalic voicing (p>b), with only different environments triggering the process.\n\nWord-internal representations\nTo test whether seq2seq models can learn a representation of word-internal structures, such as syllables, we experiment with examples of haplology. Haplology (tatasa > tasa) is the process in which a repeated sequence of sounds is simplified to a single occurrence. For example, if the word haplology were to undergo haplology, it would reduce the sequence lolo to lo, haplology > haplogy.\nIn this experiment, we include two additional processes so the model can witness the contrast between vowels and consonants separately: (1) wordfinal vowel deletion and (2) word-final consonant deletion. To test the generalisation capacity of the model, at test time, we include the following withheld cases: unseen CVCV structures-i.e. cases where haplology should apply, but the specific CVCVsequence is never seen in the training data; words where haplology occurs more than once; and VCVC structures to see if the model (erroneously) learns to delete any repeating sequence of symbols. In our experiment, we withhold from the training set the following CVCV-sequences: dede, fofo, kuku, wowo, baba, vivi, papa, titi, soso, momo, nene, rere, lili, SuSu, jiji, \u00d9u\u00d9u, NaNa, gugu.\n\nProcess\nNote that haplology includes both cases where haplology applies and does not since the input word may or may not contain a CVCV-sequence where the two CVs are identical.\nTable 7 summarises the results obtained. The model shows high accuracy for the supplementary word-final vowel and consonant deletion processes. We separate the haplology cases further into specific test cases. Our results from the unseen CVCV category show strong evidence for model generalisation of CV structures. We further tested the same model on a separate test set consisting of VCVC structures. We see that for approximately 78% of the set, it correctly recognises these cases as incorrect conditions for haplology. In the remaining instances, the model does show a rare over-generalisation to sometimes delete repeating sequences regardless of the characteristics of the sequence.\nThe largest source of error within the haplology cases is the scenario in which haplology can be applied twice within the same word. In these cases, typically, the first case of repeating CV is deleted, and the second instance remains untouched, as when outputting fuejaja with input fufuejaja, instead of the gold fueja.\n\nConclusion\nThe transformer model successfully models all 29 phonological phenomena with slight variation across phenomenon complexity. Our results show that the model can generalize linguistic categories and structures. Through haplology, we show that the model appears to learn to recognize and generalize syllabic structure and is capable of recognizing the difference between VC and CV and can also generalize the transformation triggered by haplology to unseen CV sequences.\n", "hypothesis": "Our results show that the transformer model can successfully model all 29 phonological phenomena considered, regardless of perceived process difficulty. However, the model struggled in generalizing linguistic categories and structures, such as vowels and syllables. The model did not show a clear understanding of word structure when examining haplology, and it struggled to generate previously unseen CV sequences.", "answer": false}
{"title": "BANGLABOOK: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews", "content": "\nIntroduction\nThe resources publicly available for scholarly investigation in the realm of Sentiment Analysis (SA) for the Bangla language are scarce and limited in quantity (Khatun and Rabeya, 2022; Sazzed, 2021; Rahman et al., 2019) despite its literary gravitas as the 6 th most spoken language 1 in the world with approximately 200 million speakers. In the existing literature on Bangla Text SA, as shown in Table 5 , the largest dataset consists of 20,468 samples (Islam et al., 2022) while the smallest has a mere 1,050 samples (Tabassum and Khan, 2019) . Besides these, Islam et al. ( 2020) created a dataset consisting of 17,852 samples and Islam et al. ( 2021) utilized a dataset of 15,728 samples. All other datasets apart from these either have <15,000 samples or are publicly unavailable. Another limitation of the existing research works in Bangla Text SA is the deficiency of datasets having product-specific review samples. Most of the available Bangla SA datasets are focused on usergenerated textual content from cyberspace. The insights derived from these may not accurately represent sentiment in the context of product reviews, thus hindering their usefulness for businesses. The tonal and linguistic analysis of reviews from product-specific datasets can aid businesses to gain valuable insights into customer attitudes, preferences, and experiences which can then be leveraged to improve products and services, design targeted marketing campaigns, and make more informed business decisions. In this paper, we introduce a large-scale dataset, BANGLABOOK, consisting of 158,065 samples of book reviews collected from online bookshops written in the Bangla language. This is the largest dataset for Bangla sentiment analysis to the best of our knowledge. We perform an analysis of the dataset's statistical characteristics, employ various ML techniques to establish a performance benchmark for validating the dataset, and also conduct a thorough evaluation of the classification errors.\n\nDataset Construction\nIn order to create this dataset, we collect a total of 204,659 book reviews from two online bookshops (Rokomari 2 and Wafilife 3 ) using a web scraper developed with several Python libraries, including BeautifulSoup, Selenium, Pandas, Openpyxl, and Webdriver, to collect and process the raw data.\nFor the data collection and preparation process of the BANGLABOOK dataset, we first compile a list of URLs for authors from online bookstores. From there, we procure URLs for the books. We meticulously scrape information such as book titles, author names, book categories, review texts, reviewer names, review dates, and ratings by utilizing these book URLs. \n\nLabeling & Translation\nIf a review does not have a rating, we deem it unannotated. Reviews with a rating of 1 or 2 are classified as negative, a rating of 3 is considered neutral, and a rating of 4 or 5 is classified as positive. Two manual experiments are carried out to validate the use of ratings as a measure of sentiment in product reviews. In the first experiment, around 10% of the reviews are randomly selected and annotated manually. The annotated labels are cross-checked with the original labels, resulting in a 96.7% accuracy in the corresponding labels. In addition, we consult the work of Wang et al. (2020) that explored the issue of incongruous sentiment expressions with regard to ratings. Specifically, the study scrutinized two categories of reviews: high ratings lacking a positive sentiment, and low ratings lacking a negative sentiment. We perform an analysis to identify such inconsistencies within our dataset and discovered that only a minuscule 3.41% of the samples exhibited this pattern. This figure is rela-tively insignificant when considering the substantially large scale of our dataset. After discarding the unannotated reviews, we curate a final dataset of 158,065 annotated reviews. Of these, 89,371 are written entirely in Bangla.\nThe remaining 68,694 reviews were written in Romanized Bangla, English, or a mix of languages.\nThey are translated into Bangla with Google Translator and a custom Python program using the googletrans library. The translations are subsequently subjected to manual review and scrutiny to confirm their accuracy. The majority of inaccurate translations primarily comprise spelling errors and instances where English words remain untranslated within samples containing a combination of Bangla and English text. The meticulous evaluation process of untranslated samples involves a thorough assessment by post-graduate native Bangla speakers, who critically compare the translated text against the original untranslated text to ascertain the correctness of the translation.\n\nStatistical Analysis\nTables 1 and 2 spond to each rating on a scale of 1 to 5. Upon analyzing the sentiment chart, it appears that the majority of the reviews (124,084 + 17,503 = 141,587 samples) are positive, with a significant portion also being negative (2,728 + 6,946 = 9,674 samples). A relatively small fraction of the reviews are neutral (6,804 samples). This suggests that overall, the books have been well received by the readers, with the majority expressing favorable opinions. The distribution of the dataset is representative of real-world scenarios and it tessellates well with previous content analysis works on book reviews (Lin et al., 2005; Sorensen and Rasmussen, 2004) . In Figure-2, we can visualize an illustration of the sentiment distribution among the 5 most frequently reviewed categories of books. We can gain some salient insights from the popularity of these genres. Contemporary novels are bestsellers as they reflect current events, social issues, and trends, making them relatable and thought-provoking for the readers while self-help and religious books provide guidance, inspiration, and a sense of purpose, catering to individuals' quest for personal growth and spiritual fulfillment.\n\nDeveloping Benchmark for BANGLABOOK\nA series of baseline models with combinations of different lexical and semantic features are chosen to evaluate the BANGLABOOK dataset. An overview of the models, evaluation metrics, results, and analysis of the experimental results are provided in this section.\n\nBaseline Models & Features\nFor the lexical features, we extract bag-of-words (BoW), char n-grams (1-3), and word n-grams (1-3) from the reviews as these representations have performed well in different classification tasks (Islam et al., 2022) . After extracting the features, they are vectorized using TF-IDF and count vec- torizer and trained on a series of ML models such as Random Forest (Breiman, 2001) , XG-Boost (Chen and Guestrin, 2016) , linear SVM (Cortes and Vapnik, 1995) , Logistic Regression (le Cessie and van Houwelingen, 1992) and Multinomial Naive Bayes (John and Langley, 1995). We choose LSTM (Hochreiter and Schmidhuber, 1997) with GloVe (Pennington et al., 2014) embedding for its ability to understand context along with recent dependency. We also fine-tuned two available transformer-based models in Bangla: Bangla-BERT(base-uncased) (110M parameters) (Sarker, 2020) and Bangla-BERT(large) (2.5B parameters) (Bhattacharjee et al., 2022) , due to the recent success of BERT (Devlin et al., 2019) in various downstream NLP tasks. We select F1-score and weighted average F1-score to evaluate the models because the dataset has an un- even class distribution. F1-score is the harmonic mean of precision and recall and it helps balance the metric across the imbalanced positive/negative samples (Sokolova et al., 2006) . All our experiments are done using scikit-learn, pytorch, and transformers (Vaswani et al., 2017) and run on Google Colaboratory. The training, testing, and validation split of the entire dataset was 70-20-10 with previously unseen samples in the test and validation set. To summarize, the utilization of pre-trained models (i.e. Bangla-BERT) that undergo training on extensive corpora, leading to exposure to extensive general language knowledge, has significantly contributed to their superior classification performance compared to other models and word embeddings. Additionally, models trained on handcrafted features also perform significantly well. It should be noted that Bangla pre-trained models are currently undergoing development, and further training on expansive corpora has the potential to enhance their ability to generalize and achieve even more impressive results.\n\nError Analysis\nIn the 'Positive' class, all the models produce excellent classification results. While some models perform reasonably well on the 'Negative' class, nearly all of the models perform poorly on the 'Neutral' class. The class imbalance of the dataset, as shown in Figure 1 , is one obvious cause of this fluctuation in results. The confusion matrix for Bangla-BERT on our dataset, presented in Figure-3, reveals that most of the 'Negative' and 'Neutral' samples are misclassified as 'Positive' samples by our classifiers. To further analyze the misclassifications, we examine the W1 (word unigrams) of these three classes. We find 124,796 unique W1 for the 'Positive' class, 20,714 unique W1 for the 'Negative' class, and 19,096 unique W1 for the 'Neutral' class. 77.57% of the W1 from the 'Neutral' class and 79.83% of the W1 from the 'Negative' class are found in the 'Positive' class. Table 4 depicts the most frequent W1 conveying the strongest sentiments for each class. With only one distinct 'Neutral' W1 and even the 'Negative' class having multiple positive W1, the dominance of 'Positive' sentiment W1 over the other two classes is evident. This may have contributed to the lack of distinctive words in the 'Negative' and 'Neutral' classes, which inevitably prevented the feature-based models from generalizing.\n\nMorphology and Negation Patterns of Bangla\nUnderstanding the morphology and negation patterns of a language holds paramount importance in the realm of sentiment analysis because negation can alter the meaning of words and phrases, thereby affecting the overall sentiment conveyed by a text. We provide a concise yet insightful recapitulation of the topic in the case of Bangla accompanied by review samples from our dataset BANGLABOOK as the respective examples. From the linguistic typological standpoint, Bangla is categorized as a subject-object-verb (SOV) language because the subject, object, and verb generally adhere to said order in its sentential structure (Ramchand, 2004) . The most common juxtaposition of polarity from positive to negative is the use of ni (\u09bf\u09a8) as a tensed negative. For example,\n\u0986\u09bf\u09ae \u09a4\u09be\u0981 \u09b0 \u0985\u09a8\u09c1 \u09b0\u09be\u0997\u09c0 \u09b9\u0993\u09df\u09be\u09df \u0986\u09bf\u09ae \u098f\u0987 \u09ac\u0987\u09bf\u099f \u09c7\u0995\u09a8\u09be \u09c7\u09a5\u09c7\u0995 \u09bf\u09a8\u09c7\u099c\u09c7\u0995 \u09aa\u09f0\u09cd\u09bf\u09a4\u09c7\u09b0\u09be\u09a7 \u0995\u09b0\u09c7\u09a4 \u09aa\u09be\u09bf\u09b0\u09bf\u09a8 !!!!\nTranslation: As I am a fan of his I couldn't resist myself from buying this book!!!! Another negational feature is expressed by placing na (\u09a8\u09be) prior to the non-finite verb and after the finite verb in a sentence (although there are some exceptions). For example, \u0985\u09ac\u09b6\u09af\u09cd \u09b9\u09c1\u09ae\u09be\u09df\u09c1 \u09a8 \u0986\u09b9\u09c7\u09ae\u09a6 \u09bf\u09b2\u09c7\u0996\u09c7\u099b\u09a8 \u098f\u0987 \u09ac\u0987\u099f\u09be\u09b0 \u0989\u09aa\u09c7\u09b0 \u09bf\u09a4\u09bf\u09a8 \u09bf\u09a8\u09c7\u099c\u0993 \u09b8\u09a8\u09cd\u09a4\u09c1\u09b7\u09cd\u099f \u09a8\u09be \u0964 Translation: Of course, Humayun Ahmed wrote that he himself is not satisfied with this book.\nThe Bangla language consists of no negative adverbs or pronouns (Thompson, 2006) . This is why the negative element responsible for the reversal of polarity transcends from the word-level to the sentence-level rendering the occurrences of almost all negations in Bangla manifest on the syntactic level (Thompson, 2006) .\nIn the cases of double negatives, we see the involvement of lexical negation, a morphological feature that works with negative affixes (prefixes and suffixes) attached to a root word. The prefixes in Bangla have two different phonetic variations or allophones depending on whether the prefix precedes a vowel or a consonant. The same is true for prefixes that imbue a negative connotation to a root word, e.g. o (\u0985) and on (\u0985\u09a8\u09cd ). For example, \u09bf\u0995\u09a8\u09cd\u09a4\u09c1 \u098f\u0987 \u09ac\u0987\u09bf\u099f \u098f\u0987 \u0985\u09aa\u09c2 \u09a3\u09b0\u09cd \u09a4\u09be \u09c7\u09a2\u09c7\u0995 \u09c7\u09ab\u09c7\u09b2\u09c7\u099b\u0964 Translation: But this book has covered up this incompleteness .\n\u0993\u09ae\u09b0 \u09c8\u0996\u09df\u09be\u09c7\u09ae\u09b0 \u09ad\u09be\u09b7\u09be\u09df \u09bf\u0995\u099b\u09c1 \u09ac\u0987 \u0985\u09a8\u09a8\u09cd\u09a4 \u09c7\u09af\u09d7\u09ac\u09c7\u09a8\u09b0 \u09ac\u0987, \u09af\u09be\u09c7\u09a6\u09b0 \u09c7\u0995\u09be\u09a8 \u0995\u09cd\u09b7\u09df \u09c7\u09a8\u0987\u0964 Translation: In the words of Omar Khayyam, some books are books of never-ending youth, which have no decay.\nAnother negative prefix that precedes a root word to invert its polarity is nir (\u09bf\u09a8\u09b0\u09cd ). For example,\n\n\u09c7\u09b2\u0996\u09c7\u0995\u09b0 \u09bf\u09a8\u09b0\u09b2\u09b8 \u09b6\u09f0\u09cd\u09ae \u09c7\u09b2\u0996\u09be\u09df \u09ab\u09c1 \u09c7\u099f \u0989\u09c7\u09a0\u09c7\u099b\u0964\nTranslation: The relentless effort of the author is reflected in the writing.\nOn the contrary, the suffix hin (\u09b9\u09c0\u09a8) succeeds a root word to convert it to the corresponding negative form. For example, \u098f\u09b0\u0995\u09ae \u09bf\u09ad\u09bf\u09a4\u09cd\u09a4\u09b9\u09c0\u09a8 \u0995\u09be\u09b2\u09cd\u09aa\u09bf\u09a8\u0995 \u0997\u09b2\u09cd\u09aa \u09bf\u09b6\u09b6\u09c1\u09c7\u09a6\u09b0 \u09a8\u09be \u09aa\u09dc\u09be\u0987 \u09ad\u09be\u09c7\u09b2\u09be\u0964 Translation: It is better for children not to read such baseless fictional stories.\nThe expression of negative sentiment is, therefore, very nuanced in the Bangla language as every occurrence of negative is intertwined with features like the tense, hierarchy of syntax, verb status, case-specific issues, and sequential arrangement of words (Thompson, 2006) .\n\nConclusion\nThis paper introduces BANGLABOOK, the largest Bangla book review dataset with 158,065 samples, each labeled with 1 of 3 user sentiments. We provide extensive statistical analysis and strong baselines facilitating the utility of the dataset. Given its massive size and fine-grained sentiment distribution, BANGLABOOK has the potential to alleviate the resource scarcity in Bangla language research.\n", "hypothesis": " Our findings demonstrate a substantial performance advantage of pretrained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain.  Additionally, we conduct an in-depth error analysis by examining sentiment unigrams, which may provide insight into common classification errors in under-resourced languages like Bangla.", "answer": true}
{"title": "Mind the Gap between the Application Track and the Real World", "content": "\nIntroduction\nModern NLP systems, powered by large language models (LLMs), now have the ability to perform well at foundational natural language understanding and generation tasks (Wang et al., 2018; Brown et al., 2020) . Such systems have also increased access and made inter-disciplinary contributions possible across fields such as medicine, law, education, and science. In NLP venues like ACL, the growth in applied and inter-disciplinary work can be witnessed in the NLP Applications track, which received the second-highest number of submissions at EMNLP 2022.\nRecently published research from these tracks includes work on complex and important tasks such as synthesizing code for visualization (Chen et al., 2021) , classifying operational risk in finance (Zhou et al., 2020) , and verifiying scientific claims (Wadden et al., 2020) . However, the inherent complex- ities associated with real-world data distributions and workflows can lead to the actual problem being simplified into an artificial setting that does not realistically reflect the original motivation. For instance, systems may make assumptions about the input available (e.g., require providing pseudocode/docstrings for code generation), or only evaluate on manually curated clean data as opposed to noisier data such as automatic speech recognition (ASR) outputs.\nMotivated by this observation and in line with the ACL 2023 theme track, we set out to investigate the relationship between the motivation described in the introductions and the actual experiments in application-focused NLP papers. We survey papers from the NLP applications tracks of ACL 2020 and EMNLP 2020. Specifically, we ask if there are gaps between motivation and experimentation, in the form of i) sub-tasks that are required for the application, but haven't been mentioned in the paper ii) data distributions that are expected in real-world conditions, but haven't been included in the paper's modeling or evaluation. We find that authors do not always explicitly mention assumptions they make, and often operate in con- strained scenarios highly different from their intended motivation.\nTo empirically demonstrate the severity of this problem, we then present a case study investigating the performance of an educational dialog system, when the inputs are changed from manually transcribed data to transcripts from a state-of-the-art ASR system. The purpose of the system is to classify utterances made by a student in a classroom into talkmoves (Michaels and O'Connor, 2015; O'Connor and Michaels, 2019 ) that reflect the communication strategies they use, such as making a claim, relating to another student. We find that performance drops by 14.6 points (21.2%) when evaluting on Google ASR instead of human transcripts. However, ASR was not identified as a key component of the evaluation pipeline by the original work. We argue that as the field grows and NLP models get better and better at simulated and constrained settings, it is important for us to explicitly consider additional complexities of our systems in practice. We then present suggestions for authors and organizers of conferences, towards this end.\n\nMethod\nFor the survey of application-oriented research papers, we look at all papers from the NLP Applications track of two recent NLP conferences, ACL 2020 and EMNLP 2020, which have a total of 115 papers. These conferences, which were conducted virtually, provide publicly available interfaces, 1 that allow automatically filtering papers by the track they were submitted to.\nWe then manually filter papers to identify those that propose and work on new tasks. We choose these since papers that tackle existing tasks, such as fact checking, might be restricted to existing benchmarks and datasets that are established in a topic (Thorne et al., 2018) . In contrast, papers that propose a new task, such as recommending fonts suitable for written text (Shirani et al., 2020) , can integrate considerations about the environment where the task will be used, into their problem formulation and evaluation setup. We end up with 12 papers from EMNLP 2020, and 3 papers from ACL 2020 that deal with new tasks.\nWe then answer four questions about each paper:\n1. Does the paper comprehensively describe the use case for a reader to understand? This question helps us establish that the motivations of the authors are clear to us before proceeding with the survey. We discard papers if the answer is no here.\n2. Is the paper dealing with an entire task or a sub-task only? An example of the sub-task only would be if the desired application was assisting students with writing by providing feedback, but the actual task worked on was detecting errors in writing, with the task of formulating feedback being a sub-task for future work.\n3. Does the paper mention the other missing subtasks explicitly? We investigate if the authors either mention existing systems that work on the other sub-tasks, or explicitly describe the remaining steps as future work. This is only collected when the answer to Q2 is \"sub-task only\".\n4. Is the downstream evaluation realistic? An example of the answer being No, is if the expected use-case requires classifying spoken dialog in real-time, but the paper only evaluates on manually transcribed data.\nThe survey is conducted by three authors of this paper, who have all been working on NLP for 3+ years. In cases where agreement is not perfect, we report the majority answer. While all four questions take either yes or no for an answer, we optionally collect reasons for answering no on Questions 1 and 4. We only accept unsure as an answer when no decision can be made.\n\nFindings\nThe results of the survey are presented in Table 1 . In response to the second question, we find that 4 out of 15 papers work on sub-tasks of the overall system; however, only one of these papers explicitly mentions the other sub-tasks as components of the pipeline. Overlooked are tasks such as machine translation, performing grammatical error correction, and performing document retrieval prior to classification. In response to the fourth question, we find that 7 out of 15 papers do not include evaluations that are realistic for the setting in which they might be deployed. Some comments provided by the annotators as evidence include \"evaluating only on transcribed dialog and not on ASR\", \"evaluating only on data translated from the original language\", \"not incorporating retrieval performance into evaluation pipeline\" and \"not checking the validity of integrated evidence.\" One of the responses to the last question is unsure, provided by two of the annotators, while the third annotator answered yes. One annotator's rationale for being unable to decide is that the output space modeled in the paper does not adequately reflect that seen by a user, while the second annotator claims that the task is highly subjective.\nWe compute inter-rater agreement using Krippendorff's \u03b1, used when there are more than two annotators (Artstein and Poesio, 2008) . On Questions 2,3 and 4, the \u03b1 values are 0.39, 0.44, and 0.44. While the relatively low values reflect the subjective nature of assessing application-oriented work qualitatively, our three-way annotation process and majority voting reduces the effect of an overly strict or lenient annotator. Overall, our findings indicate that application-oriented papers display some gaps that need to be addressed before the intended application is viable. While this gap often occurs in the evaluation pipeline, we highlight the importance of adequately describing all components or sub-tasks essential for an application in practice.\n\nCase Study\nIn this section, we present a case study of an application from the domain of education. The task involves classifying student utterances into talk moves (Michaels and O'Connor, 2015) , which are strategies provided by the Academically Productive Talk framework (Michaels et al., 2008) , that students and teachers use for maintaining productive and respective discourse in a classroom. We empirically analyze the impact of evaluating this task only on a constrained, artificial environment, as opposed to a more realistic setting.\n\nDataset and Models\nDataset The data consists of conversations among middle school students performing collaborative work in science classrooms, documented in more detail in Southwell et al. (2022) . Groups of 2-4 consenting students are seated at each table, and audio is collected through table-top Yeti Blue microphones. In total, 31 five-minute dialogue sessions are chosen for the talk moves analysis. Like most papers in our survey, we build a high-quality dataset for our application: samples were filtered and transcribed manually (\"human\" transcript) by a team of three annotators, resulting in 2003 student utterances. There are five student talk moves under the APT scheme, including Relating to another student, Asking for more info, Making a Claim, Providing evidence or reasoning, and None. We additionally include the label Not enough context when the annotators cannot make a decision. Examples of all labels can be found in Appendix A. Due to label imbalance, we cluster the labels into 3 categories (NONE, LEARNING COMMUNITY (LC) and OTHER) . Our clustering follows the higher-level grouping of talk moves into Learning Community, Content Knowledge, and Rigorous Thinking as defined in (Resnick et al., 2018) . The dataset is then divided by session into training/dev/test splits for our model.\nModel Following the state-of-the-art model for classifying teacher talk moves (Suresh et al., 2022) , we build our student talk moves model by finetuning the RoBERTa-base (Liu et al., 2019) model for sequence classification. We use the previous N = 6 utterances as the context when predicting the talkmove label for the current utterance, after experimenting with multiple context windows (N) on our development set. As a baseline, we develop a random classifier using the scikit-learn Dummy-Classifier (Pedregosa et al., 2011) transcripts for the current case study, results for this setting can be found in Cao et al. (2023) .\n\nDistribution Shift: Human vs. ASR\nHowever, when deploying our models in the classroom, we do not have access to clean human transcripts, and instead need to work with the outputs of ASR systems. To compare the differences between both, we look at two state-of-the-art ASR systems: Google (Google, 2023) and OpenAI Whisper (Radford et al., 2022) . 2 Table 2 shows the distribution shift between human and ASR transcripts. Because of the noisy small-group classroom setting, some student utterances are difficult to recognize, resulting in imperfect ASR transcriptions with incomplete or empty utterances. This causes the input distributions to vary between human and ASR transcripts. Additionally, when the empty utterances are filtered out, the label distribution also shifts across human and different ASRs. To provide as fair a comparison as possible with the original human transcripts, we create two versions of the ASR data. The first version, denoted using the subscript 'filter' is filtered such that empty utterances are removed, which results in its size varying from the human transcripts. The second version, denoted by the subscript 'all', retains all ASR utterances where the corresponding human transcription is not empty, thus resulting in the same number of utterances as the original human transcripts.\n\nResults\nTo show the performance gap caused by the above distribution shift, we evaluate our model on both human transcriptions and transcriptions from the two ASR systems. For each ASR transcript, we report both performances on their filtered version (Google filter , Whisper filter ) and the all ver- sion (Google all , Whisper all ). We report macro F1 as well as class-wise F1 for all models, as shown in Table 3 . The top rows show performance of the random baseline. Because of the shift in label distributions, as described in Section 3.2, even the input-agnostic random baselines vary for the different versions. Looking at the model performances, we see that overall macro F1 drops by 8.91 points for Whisper all (a 12% drop) and 14.6 points (a 21% drop) for Google all when comparing across transcripts that have the same length.\nWhen considering real-world deployment, the potential for such a dramatic drop in performance should be taken into account by both the designer (including researchers) and the user (such as teachers). However, for similar applications based on classroom discourse analysis, such as classifying teacher talk moves (Suresh et al., 2022) , predicting appropriate next teacher talk moves (Ganesh et al., 2021) or measuring teacher uptake of student ideas (Demszky et al., 2021) , comparisons to ASR transcriptions to illustrate real-world performance are rarely made, and, in many cases, ASR as a component is never mentioned.\n\nDiscussion\nThrough the above survey and case study, we qualitatively and quantitatively examine the gap between task-focused solutions in NLP research, and realistic use cases. We first acknowledge that there has existed a long-standing tradition in NLP to contextualize current research efforts through potential future applications. Looking at task-oriented dialog systems for example, early work such as Deutsch (1975) was motivated by the need to design computational assistants to support humans in mechanical tasks, and discussed the construction of essential components such as discourse processors, despite missing key upstream and downstream components such as ASR or dialog generation. Investigating sub-problems and their respective solutions in environments that are distinct from real-word settings has largely been unavoidable and sometimes even desirable. However, we argue that with the growth of the field and with the progress enabled by LLMs and related advances, we now have the opportunity to examine how closely our experimental setups can reflect our long term goals. Additionally, for papers that are explicitly in the Applications track, which present new applications intended to satisfy a real-world user need, we believe it is even more important to consider the bigger picture, and accurately describe necessary next steps for making the application a reality.\nTo bridge this gap, we propose a few initial recommendations: i) we suggest including a question on the Responsible NLP Checklist 3 pertinent to application-oriented papers, asking if the experimental setup has taken into account the real-world conditions of the application, ii) we recommend that authors describe any potential gaps between their motivation and proposed solution, and if so, state what is lost in the gap (such as ASR), and iii) we call for work to investigate ways to explicitly account for the gap, such as simulating noisy input data in cases where accessing the true distributions is not possible. We invite discussion from the research community on other ways forward.\n\nRelated Work\nOur paper adds to a body of work on meta-analysis of NLP papers and the state of NLP research, particularly from the recently introduced theme tracks at *ACL conferences (Bianchi and Hovy, 2021; Bowman, 2022; Kann et al., 2022) . Similarly to us in that the authors examine evaluation practices, Bowman and Dahl (2021) points out problems with benchmarking, while Rodriguez et al. (2021) proposes ways to improve leaderboards in order to truly track progress. Other papers that critically examine evaluation and leaderboards include Ribeiro et al. (2020); Dodge et al. (2019) and Ethayarajh and Jurafsky (2020) . In contrast, we focus on discrepancies between proposed experimental settings and the stated motivation of research endeavours.\nIn addition, Bowman (2022) discusses that, similar to problematic hype, underclaiming when talking about NLP models comes with risks, and Bianchi and Hovy (2021) highlights multiple concerning trends in NLP research. More broadly, Lipton and Steinhardt (2019) discuss concerns with ML scholarship, and Church (2020) draws attention to downward trends in reviewing quality and how these can potentially be mitigated.\n\nConclusions\nWe investigate the \"gap\" between the motivations of application-focused NLP papers and their actual experimental setting. Through a survey of NLP Applications papers from two NLP conferences, we find that i) necessary components for the application get overlooked when papers focus on subtasks and ii) realistic input sources such as ASR are not being considered in downstream evaluations. We further highlight the severity of the latter issue through a case study on a dialog understanding system intended for classrooms, showing the drop in performance when ASR input, expected in the real-world, is used. While we outline potential strategies to address this issue, we hope our work will spur further discussion about future steps.\n", "hypothesis": "We find that many papers fall short of considering real-world input and output conditions due to adopting simplified modeling or evaluation settings. As a case study, we then empirically show that the performance of an educational dialog understanding system remains consistent when used in a realistic classroom environment.", "answer": false}
{"title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing", "content": "\nIntroduction\nNeural networks are vulnerable to adversarial attacks: small perturbations to the input ,which do not fool humans (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2017) . In NLP tasks, previous studies (Alzantot et al., 2018; Jin et al., 2019; Li et al., 2020; Garg and Ramakrishnan, 2020) demonstrate that simple word-level text attacks (synonym substitution, word insertion/deletion) easily fool state-of-the-art models, including pre-trained transformers like BERT (Devlin et al., 2019; Wolf et al., 2020) . Further, it has recently been shown models are overconfident 1 on examples which are easy to attack (Qin et al., 2021) and indeed, such over-confident predictions plague much of modern deep learning (Kong et al., 2020; Guo et al., 2017; Nguyen et al., 2015; Rahimi et al., 2020) . Label smoothing is a regularization method that has been proven effective in a variety of applications, and modalities (Szegedy et al., 2016; Chorowski and Jaitly, 2017; Vaswani et al., 2017) . Importantly, it has been shown to reduce overconfident predictions and produce better confidence calibrated classifiers (Muller et al., 2019; Zhang et al., 2021; Dan and Roth, 2021; Desai and Durrett, 2020; Huang et al., 2021; Liu and JaJa, 2020) .\nIn this work, we focus on the question: does label smoothing also implicitly help in adversarial robustness? While there has been some investigation in this direction for adversarial attacks in computer vision, (Fu et al., 2020; Goibert and Dohmatob, 2019; Shafahi et al., 2019) , there is a gap in understanding of whether it helps with discrete, text adversarial attacks used against NLP systems. With the increasing need for robust NLP models in safety-critical applications and a lack of generic robustness strategies, 2 there is a need to understand inherent robustness properties of popular label smoothing strategies, and the interplay between confidence and robustness of a model.\nIn this paper, we extensively study standard label smoothing and its adversarial variant, covering robustness, prediction confidence, and domain transfer properties. We observe that label smoothing provides implicit robustness against adversarial examples. Particularly, we focus on pre-trained transformer models and test robustness under various kinds of black-box and white-box word-level adversarial attacks, in both in-domain and out-ofdomain scenarios. Our experiments show that label smoothing (1) improves robustness to text adversarial attacks (both black-box and white-box), and (2) mitigates over-confident errors on adversarial textual examples. Analysing the adversarial exam-ples along various quality dimensions reveals the remarkable efficacy of label smoothing as a simple add-on robustness and calibration tool.\n\nText Adversarial Attacks\nOur experiments evaluate the robustness of text classification models under three state-of-the-art text adversarial attacks TextFooler (black-box), BAE (black-box) and SemAttack (white-box), described below. 3 For a particular victim NLP model and a raw text input, the attack produces semantically-similar adversarial text as output. Importantly, only those examples are attacked, which are originally correctly predicted by the victim model. The attacks considered are word-level, i.e. they replace words in a clean text with their synonyms to maintain the meaning of the clean text, but change the prediction of the victim models.\n\u2022 TextFooler (TF): (Jin et al., 2019) proposes an attack which determines the word importance in a sentence, and then replaces the important words with qualified synonyms.\n\u2022 BAE: (Garg and Ramakrishnan, 2020) uses masked pre-trained language models to generate replacements for the important words until the victim model's prediction is incorrect.\n\u2022 SemAttack (SemAtt): (Wang et al., 2022) introduces an attack to search perturbations in the contextualized embedding space by formulating an optimization problem as in (Carlini and Wagner, 2016) . We specifically use the white-box word-level version of this attack.\n\nLabel Smoothing\nLabel Smoothing is a modified fine-tuning procedure to address overconfident predictions. It introduces uncertainty to smoothen the posterior distribution over the target labels. Label smoothing has been shown to implicitly calibrate neural networks on out-of-distribution data, where calibration measures how well the model confidences are aligned with the empirical likelihoods (Guo et al., 2017) .\n\u2022 Standard Label Smoothing (LS) (Szegedy et al., 2013; Muller et al., 2019 ) constructs 3 The black-box attacks keep querying the model with its attempts until the victim model is fooled while the white-box attack has access to the gradients to the model. Further details of the attacks are in (Jin et al., 2019; Garg and Ramakrishnan, 2020; Wang et al., 2022) . a new target vector (y LS i ) from the one-hot target vector (y i ), where y LS i = (1 \u2212 \u03b1)y i + \u03b1/K for a K class classification problem. \u03b1 is a hyperparameter selection and its range is from 0 to 1. ) with a probability of 1 \u2212 \u03b1 on the target label and \u03b1 on the label to which the classification model assigns the minimum softmax scores, thus introducing uncertainty.\nFor both LS and ALS, the cross entropy loss is subsequently minimized between the model predictions and the modified target vectors y LS i , y ALS i .\n\nExperiments\nIn this section, we present a thorough empirical evaluation on the effect of label smoothing on adversarial robustness for two pre-trained transformer models: BERT and its distilled variant, dBERT, which are the victim models. 4 We attack the victim models using TF, BAE, and SemAttack. For each attack, we present results on both the standard models and the label-smoothed models on various classification tasks: text classification and natural language inference. For each dataset we evaluate on a randomly sampled subset of the test set (1000 examples), as done in prior work (Li et al., 2021; Jin et al., 2019; Garg and Ramakrishnan, 2020) . We evaluate on the following tasks, and other details about the setting is in Appendix A.8:\n\u2022 Text Classification: We evaluate on movie review classification using Movie Review (MR) (Pang and Lee, 2005) and Stanford Sentiment Treebank (SST2) (Socher et al., 2013) against various attacks for in-domain data. We show clean accuracy, attack success rate and average confidence on successful adversarial texts. For each dataset, the left column are the results for standard model, and the right column are for LS models where \u03b1 denotes the label smoothing factor (\u03b1=0: no LS). \u2191 (\u2193) denotes higher (lower) is better respectively. dBERT denotes the distilBERT model.\non the matched genre test-set in the OOD setting presented in subsection 3.2 .\n\nIn-domain Setting\nIn the in-domain setting (iD), the pre-trained transformer models are fine-tuned on the train-set for each task and evaluated on the corresponding testset. For each case, we report the clean accuracy, the adversarial attack success rate (percentage of misclassified examples after an attack) and the average confidence on successfully attacked examples (on which the model makes a wrong prediction). 5 Table 1 shows the performance of BERT and dBERT, with and without label-smoothing. We choose label smoothing factor \u03b1 = 0.45 for standard labelsmoothed models in our experiments.\nWe see that label-smoothed models are more robust for every adversarial attack across different datasets in terms of the attack success rate, which is a standard metric in this area (Li et al., 2021; Lee et al., 2022) . Additionally, the higher confidence of the standard models on the successfully attacked examples indicates that label smoothing helps mitigate overconfident mistakes in the adversarial setting. Importantly, the clean accuracy remains almost unchanged in all the cases. Moreover, we observe that the models gain much more robustness from LS under white-box attack, compared 5 Details of each metric are presented in Appendix A.2.\nto the black-box setting. We perform hyperparameter sweeping for the label smoothing factor \u03b1 to investigate their impact to model accuracy and adversarial robustness. Figure 1 shows that the attack success rate gets lower as we increase the label smooth factor when fine-tuning the model while the test accuracy is comparable 6 . However, when the label smoothing factor is larger than 0.45, there is no further improvement on adversarial robustness in terms of attack success rate. Automatic search for an optimal label smoothing factor and its theoretical analysis is important future work. We also investigate the impact of adversarial label smoothing (ALS) and show that the adversarial label smoothed methods also improves model's robustness in Table 2 . \n\nOut-of-Domain setting\nWe now evaluate the benefits of label smoothing for robustness in the out-of-domain (OOD) setting, where the pre-trained model is fine-tuned on a particular dataset and is then evaluated directly on a different dataset, which has a matching label space. Three examples of these that we evaluate on are the Movie Reviews to SST-2 transfer, the SST-2 to Yelp transfer, and the SNLI to MNLI transfer.\nIn Table 3 helps produce more robust models in the OOD setting although with less gain compared to iD setting. This is a challenging setting, as evidenced by the significant performance drop in the clean accuracy as compared to the in-domain setting. We also see that the standard models make over-confident errors on successfully attacked adversarial examples, when compared to label-smoothed models.\n\nQualitative Results\nIn this section, we try to understand how the generated adversarial examples differ for label smoothed and standard models. First we look at some qualitative examples: in We also performed automatic evaluation of the quality of the adversarial examples for standard and label smoothed models, adopting standard metrics from previous studies (Jin et al., 2019; Li et al., 2021) . Ideally, we want the adversarial sentences to be free of grammar errors, fluent, and semantically similar to the clean text. This can be quantified using metrics such as grammar errors, perplexity, and similarity scores (compared to the clean text). Table 5 shows that the quality of generated adversarial examples on label smoothed models is worse than those on standard models for different metrics, suggesting that the adversarial sentences generated by standard models are easier to perceive. This further demonstrates that label smoothing makes it harder to find adversarial vulnerabilities.\n\nConclusion\nWe presented an extensive empirical study to investigate the effect of label smoothing techniques on adversarial robustness for various NLP tasks, for various victim models and adversarial attacks. Our results demonstrate that label smoothing imparts implicit robustness to models, even under domain shifts. This first work on the effects of LS for text adversarial attacks, complemented with prior work on LS and implicit calibration (Desai and Durrett, 2020; Dan and Roth, 2021) , is an important step towards developing robust, reliable models. In the future, it would be interesting to explore the combination of label smoothing with other regularization and adversarial training techniques to further enhance the adversarial robustness of NLP models.\n", "hypothesis": " Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks.  We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples..", "answer": true}
{"title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives", "content": "\nIntroduction\nThe dual encoder architecture applied to information retrieval has shown excellent performance in a wide range of tasks (Gillick et al., 2018; Karpukhin et al., 2020; Ni et al., 2021 Ni et al., , 2022)) .\nRecently, the Information Retrieval community has transitioned towards Deep Learning models that leverage large unsupervised corpus pretraining (Devlin et al., 2019; Raffel et al., 2020) , which offers more powerful semantic and contextual representation for queries and documents. These models can be successfully applied to scoring tasks, e.g. Dehghani et al. (2017) , or retrieval tasks, e.g. Gillick et al. (2018) . In contrast, classic SearchQA show that sharing a projection layer in Asymmetric Dual Encoders (ADE-SPL) (Dong et al., 2022) may not guarantee that the embeddings from the two encoder towers are in coinciding parameter spaces. However SamToNe can effectively achieve that.\nretrieval models, such as BM25 (Robertson and Zaragoza, 2009) , rely on bag-of-words lexical overlap, term frequency heuristics, inverse document frequency and document length. This type of retrieval models does not require any training and can generalize reasonably well, but they fall short of finding documents that have low term overlap but high semantic similarity.\nA dual encoder (Gillick et al., 2018; Yang et al., 2020; Karpukhin et al., 2020; Reimers and Gurevych, 2019) consists of two encoding towers that map queries and documents, respectively, into a shared low-dimensional dense representation, namely, the embedding space. The model is usually optimized by a contrastive loss (Chopra et al., 2005) , which moves the embeddings of the queries and documents from the same positive examples closer to each other, and the embeddings from negative examples farther away. Training the dual encoder in batches allows to use, for each question, the passages that answer all the other questions within the batch as negatives (Gillick et al., 2018) , namely \"in-batch negatives\". At indexing time, all the documents in a corpus are encoded via bulk inference and indexed. To run retrieval, a query is encoded and its most relevant documents can be retrieved through Nearest Neighbours Search ( Vanderkam et al., 2013; Johnson et al., 2021) over the embedding space using a measure of similarity, e.g. the dot-product or cosine distance of the embedding vectors.\nMotivation. In this work, we consider two major types of dual encoder architectures: \"Symmetric Dual Encoder\" (SDE) 1 , with parameters shared between two encoder towers, and \"Asymmetric Dual Encoder\" (ADE), with two distinctly parameterized encoder towers. Dong et al. (2022) demonstrated that sharing projection layers can significantly improve the performance of ADEs. They empirically explained the efficacy of SDE and ADE-SPL by claiming that the shared projection layers help mapping the embeddings of the two encoder towers into a coinciding parameter space.\nBy repeating this embedding space analysis on a variety tasks, we find that ADE-SPL may not be enough to ensure that the embedding spaces from two encoder towers are coinciding, as shown in Figure 1 . This motivates us to further improve the dual encoder retrieval quality beyond the architectural change explored in Dong et al. (2022) . Although the projection layers are shared, our analyses suggest that an extra mechanism, other than using the standard contrastive loss with in-batch negatives, is required to ensure the adjacency of the embeddings of a ground truth pair.\nContributions. In this paper, we propose an improved training objective for dual encoder models: contrastive loss with Same Tower Negatives (SamToNe). In Section 3, we demonstrate its usefulness on a variety of Information Retrieval tasks, including both tasks with in-task fine-tuning and a zero-shot benchmark suite. Across all the tasks explored, SamToNe performs competitively comparing to the traditional training setup, with a significant improvement on the metrics averaged across tasks. Finally, through an analysis of the produced embeddings, in Section 4, we further make evident the superiority of SamToNe from the perspective of regularisation. \n\nMethod\nDual Encoder Architecture. We follow the standard setup of information retrieval: given a query, q, and a corpus of retrieval candidates, P, the goal is to retrieve k relevant candidates, p k \u2208 P. The candidate can be a phrase, a sentence, a passage, or a document.\nRecent research (Dong et al., 2022) demonstrated that sharing projection layers can significantly improve the performance of ADEs and we use this shared projection layer for ADEs (ADE-SPL) throughout our experiments. Figure 2 illustrates the SDE and ADE-SPL architectures we use in this work. Our dual encoders are initialized from pre-trained t5.1.1 encoders (Raffel et al., 2020) . Following Ni et al. (2022) ; Dong et al. (2022) , we encode a query, q i , or a candidate, p i , by averaging the T5 encoder outputs and projecting them to the final embedding vector.\nContrastive Loss. A standard way to train a dual encoder model is optimizing an in-batch sampled softmax loss for contrastive learning (Henderson et al., 2017) :\nEQUATION\n)\nwhere sim is cosine similarity, B is a mini-batch of examples, and \u03c4 is the softmax temperature. p i is the ground-truth relevant passage for the query q i in a batch of retrieval candidates p * , where all the other passages p k (k \u0338 = i) are treated as the negative examples for contrastive learning. Bi-directional in-batch sampled softmax loss is commonly applied to improve the embedding quality of both towers, where the contrastive loss is computed for both query to passage matching and passage to query matching (Yang et al., 2019) . We use the bi-directional loss throughout this work.\nSame Tower Negatives. The in-batch sampled softmax loss is a contrastive loss that only considers the contrastive estimation between the target example pair {q i , p i }, and the in-batch sampled negative pairs {q i , p j } (j \u0338 = i).\nOne way to improve the quality of the retrieval is to improve the contrast among the embeddings of the queries. Therefore, we propose a novel contrastive loss using Same Tower Negatives, which we abbreviate as SamToNe:\nLS = e sim(q i ,p i )/\u03c4 j\u2208B e sim(q i ,p j )/\u03c4 + j\u2208B,j\u0338 =i e sim(q i ,q j )/\u03c4 , (2\n)\nwhere the second term in the denominator is the contribution from the same tower negatives. SamToNe can be interpreted as a regularized version of the in-batch sampled softmax loss, where the term j\u2208B,j\u0338 =i e sim(q i ,q j )/\u03c4 is a regularizer. When query embeddings are not well distributed, max sim(q i , q j ) \u226b max sim(q i , p j ), and the second term in the denominator will dominate the contribution from the negative examples. Thus, it will drive the separation of the query embeddings in contrastive learning. In Section 4, we provide empirical evidence of the effects of SamToNe as a regularizer of the embedding space. Ren et al. (2021) proposed an improved contrastive loss, PAIR, which is a hybrid loss\nL P AIR = \u2212(1 \u2212 \u03b1) log L c \u2212 \u03b1 log L P , where LP =\ne sim(q i ,p i )/\u03c4 j\u2208B,j\u0338 =i e sim(p i ,p j )/\u03c4\n(3) penalizes the similarities between passages / documents. Despite both SamToNe and PAIR are penalizing the similarities among the same tower inputs, there are two significant differences. single stage training and guaranteed improvement on embedding space quality, make SamToNe much easier to use.\n\nQuestion-Answering Retrieval Tasks\nWe evaluate SamToNe on 5 question-answering (QA) retrieval tasks including MS MARCO (Nguyen et al., 2016) and MultiReQA (Guo et al., 2021) . For MS MARCO, the retrieval candidates are relevant passages, and for the 4 tasks in Mul-tiReQA, the retrieval candidates are answer sentences.\nTo make a fair comparison across the results of our experiments, the same fine-tuning hyperparameters are applied to all our model variants. The models are optimized for 20, 000 steps using Adafactor optimizer (Shazeer and Stern, 2018) , with softmax temperature \u03c4 = 0.01, batch size 512, and a linearly decaying learning rate starting from 10 \u22123 to 0 at the final step. To compare SamToNe and PAIR, we use the hyperparameter \u03b1 = 0.1 for PAIR as reported in Ren et al. (2021) , and keep all the other experimental setups identical. SamToNe is applied only on the query side, as it is more robust across different datasets. For experiments and analysis on applying SamToNe on both encoder towers, please refer to Section 3.4. We benchmark Model Loss MSMARCO NQ SQuAD TriviaQA SearchQA Average P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR the fine-tuned models using precision at 1 (P @1) and mean reciprocal rank (MRR).\nAs shown in Table 1 , SamToNe greatly improves the retrieval performance of both SDE and ADE-SPL models. Using SamToNe, ADE-SPL models can outperform SDE ones, especially for TriviaQA and SearchQA, by a great margin. Relative to PAIR, SamToNe provides better performance across different datasets in both types of models.\n\nScaling the Model Size\nTo assess the impact of the model size, we evaluate the dual encoders initialized from t5.1.1-base (\u223c 250M parameters), t5.1.1-large (\u223c 800M parameters), and t5.1.1-XXL (\u223c 11B parameters). Figure 3 and Appendix Table 4 show that SamToNe consistently improves the performance of dual encoders across different model sizes.\n\nBEIR Generalization Tasks\nWe further demonstrate the efficacy of the dual encoders trained with SamToNe on BEIR (Thakur et al., 2021) , a heterogeneous benchmark for zeroshot evaluations.\nBEIR has 18 information retrieval datasets 2 across 9 domains, including Bio-Medical, Finance, News, Twitter, Wikipedia, StackExchange, Quora, Scientific, and Misc. The majority of the datasets have binary query relevance labels. The other datasets have 3-level or 5-level relevance judgements.\nAs BEIR is evaluating generalization capabilities and SDEs are commonly used for general purpose retrieval (Ni et al., 2021) , we focus on evaluating the impact of SamToNe on BEIR using the SDE architecture. In this evaluation, we reuse the model fine-tuned with MS MARCO, as described in Section 3.1.\nEvaluated with the same setting as GTR (Ni et al., 2021) , SamToNe demonstrates strong performance on BEIR, as shown in Table 2 and Figure 4 . On average, SamToNe improves NDCG@10 by 1.4% for SDE with XXL size. SDE trained with SamToNe significantly outperform BM-25, a sparse retrieval method, and GTR, a dense retrieval method that shares the same architecture and the same model size as SDE but fine-tuned with different corpora.\n\nApplying SamToNe to Both Towers\nJust as with the query tower, SamToNe can be applied to the document tower which leads to better query-document alignment. However, it is common that the training data contains a large fraction of duplicated documents for a diverse set of queries. For example, only 17% of the documents in the train-split are unique for TriviaQA, but 98% for MSMARCO. For datasets with a low rate of unique documents, applying SamToNe on the document side will penalize sim(p i , p j ) with p i = p j and may hinder the performance, as shown in Table 3 .\n\nEmbedding Space Analysis\nAs shown in the top row of Figure 1 , for MS MARCO and SearchQA, ADE-SPL generates two connected but topologically separable embedding spaces. It requires an extra mechanism, beyond the shared projection layers, to ensure the adjacency of the embeddings from a ground truth pair. SamToNe is proposed as the \"force\" drawing the embeddings of each ground truth training pair together. Its efficacy is illustrated in the bottom half of Figure 1 .\n\nSamToNe: an Embedding Distance Regularizer\nTo further understand SamToNe's role as a regularizer of embedding distances, we evaluate the distribution of the distances between the embeddings of the queries and their top-1 retrieval results in the test set of MS MARCO and SearchQA. The embedding distance is measured by cosine similarity, where 1.0 means perfect alignment with a range of [\u22121.0, 1.0]. As shown in Figure 5 , SamToNe drastically shifts the distribution of the (query, top-1 retrieval result) pairs towards 1.0, demonstrating the regularizing effect of SamToNe over the embedding distances.\nBy placing the regularizing query-query similarity terms e sim(q i ,q j )/\u03c4 and the standard inbatch negative query-document similarity terms e sim(q i ,p j )/\u03c4 together in the denominator with same weight, SamToNe pushes the similarity ratio between query-query and query-documents, sim(q i , q j )/sim(q i , p j ), to be centered around 1.0. This is a self-balancing regularization effect. The query and document spaces are set to closely overlap each other and the embeddings of a positive pair are more likely to be located in the same region of the embedding space.\nTo empirically illustrate this effect, we plotted histograms of the sim(q i ,q j ) sim(q i ,p j ) ratios for randomly selected i and j in Figure 6 . The regularization effect only shows when SamToNe is used, but not when PAIR (Ren et al., 2021) is. This is because the self-balancing effect does not exist in a hybrid loss such as PAIR.\n\nConclusions\nEvaluating on QA retrieval tasks and zero-shot generalization benchmarks, we demonstrate that training with SamToNe can significantly improve the dual encoder retrieval quality. With t-SNE maps of query and document embeddings, we show that the embedding spaces from the two encoding towers of models trained with SamToNe are better aligned. Through the distributions of similarity distances between the embeddings of queries and their nearest neighbours, we empirically explain the efficacy of SamToNe from a regularisation prospective. In general, we recommend using SamToNe to train dual encoders for information retrieval tasks.\n", "hypothesis": "By evaluating on question answering retrieval benchmarks from MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval benchmarks (BEIR), we demonstrate that SamToNe can effectively improve the retrieval quality for both symmetric and asymmetric dual encoders.  By directly probing the embedding spaces of the two encoding towers via the t-SNE algorithm (van der Maaten and Hinton, 2008), we observe that SamToNe ensures the misalignment between the embedding spaces from the two encoder towers.  Based on the analysis of the embedding distance distributions of the top-1 retrieved results, we further explain the ineffectiveness of the method from the perspective of regularization.", "answer": false}
{"title": "Putting Natural in Natural Language Processing", "content": "\nIntroduction\nThe ACL 2023 theme track urges the community to check the reality of the progress in NLP. This position paper adopts an expansive interpretation of this question. It is definitely worth inquiring into the apparent advances of current NLP in their own terms. Here, however, I question these terms and argue that our field has focused on only a limited subset of human language which happens to be convenient to work with, and thus misses major aspects of human communication.\n\nHuman Language is Primarily Spoken\nHumans are an exceptional species in many ways, and out of these, human language is one of the most salient. Unlike communication systems used by other organisms, human language is open-ended, capable of expressing abstract concepts, and of reference to events displaced in time and space. While the capacity to acquire language is universal and largely innate (Darwin, 1874; Pinker and Bloom, 1990) it also is culturally mediated and likely arose via gene-culture co-evolution (Deacon, 1998; Richerson and Boyd, 2010) .\nOne revolutionary technology which turbocharged human language was writing, which was invented a handful of times in the most recent few thousand years of the human story (Fischer, 2003) . Writing, followed by the printing press, followed by the Internet, have made written text ubiquitous to the extent that it is easy to forget that the primary and universal modality for most human communication throughout history has been spoken. 1 Even today many of the world's languages do not have a standardized written form. For those that do, the written modality originated as a compressed, symbolic representation of the spoken form.\nChildren acquire a spoken language (and not infrequently two or more) within the first few years of their life with no or little explicit instruction, largely relying on weak, noisy supervision via social interaction and perceptual grounding. In contrast, they require hundreds of hours of explicit instruction and arduous conscious practice to learn to read and write, and most are only able to learn the written modality a couple of years at best after becoming fluent communicators in one or more spoken languages.\n\nReality check\nThus, arguably, the natural language for which we are biologically equipped is spoken. Written language is a secondary development, which happens to be very useful and widespread, but is nevertheless derivative of speech. This appears to be the consensus view in linguistics going back at least a century (de Saussure, 1916; Bloomfield, 1933). 2 Given these facts, is then the field of Natural Language Processing (NLP) a misnomer? Are we making less progress with getting machines to communicate via human language than current advances with processing written text would have us believe?\n\nNLP is Written Language Processing\nTo anyone with experience reading, reviewing and publishing papers in NLP conferences and journals (such the ACL conferences and TACL) it is evident that the field is very strongly focused on processing written language. While this is evident to practitioners, it is also largely tacit and implicit.\n\nUnstated assumptions\nThe fact that a paper is concerned with written as opposed to spoken oral or sign language is almost invariably assumed to be the default and not explicitly stated. Furthermore, even if there is some interest in tackling a dataset of originally spoken language (for example in much work on dialog and child language acquisition), the usual approach is to use a written transcription of this data rather than the actual audio. This is partly a matter of convenience, but partly due to the assumption that the written form of language is the canonical one while the audio modality is just a weird, cumbersome encoding of it.\nTo some extent such an implicit belief also lurks in much work within the speech community: the main thrust of speech research has always been on so called Automatic Speech Recognition (ASR), by which is meant automatically transcribing spoken language into a written form. Written text is treated as an interface and an abstraction barrier between the field of speech processing and NLP. In Sections 3 and 4 I address problems arising from the above assumptions, as well as the challenges and opportunities we have once we discard them. Firstly, however, it will be instructive to briefly quantify the assertion that NLP is Written Language Processing. by looking at historical publication patterns. \n\nPublication patterns\nFigure 1 shows the proportion of NLP papers explicitly mentioning speech-related terms in their title over the years covered by the ACL anthology (1950 through 2022), which is a comprehensive database of NLP papers from a wide variety of relevant conferences, workshops and journals. 3 The fraction of speech-focused NLP papers varies quite a bit over the years, but mostly stays below 10%.\nThere is a large peak going to 20% in 1989, followed by three years with around 10% of speech papers. A look at the underlying data reveals that the 1989 peak is associated with the inclusion in the anthology of the proceedings of the Speech and Natural Language Workshop (Hirshman, 1989) organized by the US Defense Advanced Research Projects Agency (DARPA), and featuring 79 papers. This workshop ran until 1992 and is thus largely responsible for the four-year run of sizable representation of spoken language research in the ACL anthology.\nThe overview of the last edition of this event notes the then ongoing \"paradigm shift in natural language processing towards empirical, corpus based methods\" (Marcus, 1992). It is likely that this shift in NLP methodology was at least partly driven by this workshop, the associated DARPA program, and the resulting increased interaction between researchers working on spoken and written language.\nIn recent years (since 2010) the proportion of NLP papers explicitly mentioning spoken language has resolutely stayed below 6%. While the major ACL events typically include speech processing as a topic in their calls for papers, as well as a track including the term speech in its name, such as Speech and Multimodality, processing of spoken language it clearly a rather minor concern of these conferences. Instead, speech work is published in different venues organized by a separate speech processing community.\n\nSpoken Language is Richer\nWhile the primacy of the spoken modality as means of communication is the consensus view in linguistics, Section 2.1 identifies unstated assumptions among NLP practitioners which amount to the opposite view. Here I outline why these assumptions contradicting the scientific view are not only incorrect but also detrimental to progress on understanding and processing real human language.\n\nKey features of spoken language\nSpeech and writing are two different modalities with different affordances, and there is no straightforward mapping between them. Some writing systems such as those used for English, Arabic or Chinese do not even represent the phonology of the spoken language in a direct way. More crucially, writing only captures a small proportion of the information carried in the equivalent audio signal. Writing discards most of the information falling within the general category of paralinguistic phenomena, such as that related to speaker identity, speaker emotional state and attitude; likewise, information conveyed by speech tempo and amplitude, including most of suprasegmental phonology such as intonation and rhythm is typically not present in writing. In addition to the auditory signal, oral spoken language can also feature visual clues in the form of accompanying gestures, facial expressions and body posture. Sign languages rely on the visual channel exclusively, and in fact there are no widely used writing systems for any of them (Grushkin, 2017) . Unlike most text, speech also typically contains a variable amount of channel noise (Shannon, 1948) such as environmental sounds.\nNatural spontaneous speech contains fillers, hesitations, false starts, repairs and other disfluencies (Dinkar et al., 2023) which are usually edited out in the written form of language. Even more critically, spontaneous speech typically takes the form of a dialog between two or more participants. Dialog is unlike common written genres: crucially it features turn-taking behavior which is governed by com-plex and incompletely understood rules (Skantze, 2021) . These features of natural dialog also mean that the traditional cascaded approach of ASR followed by NLP faces serious limitations, not least due to low ASR performance in this regime (Szyma\u0144ski et al., 2020) , but also due to its inherently interactive nature.\nFor all these reasons, spoken language is more informationally rich than written language; 4 the same factors also make it more variable, complex and noisy, and consequently more challenging for automated processing (Shriberg, 2005) . Thus any understanding of language as a human faculty gained via the written modality does not necessarily generalize to the spoken modality. The same is also the case about language applications: for example the successes and shortcomings of state-of-the-art text chatbot systems (e.g. Stiennon et al., 2020) are likely to be substantially different from those of spoken dialog systems.\n\nChallenges of speech\nAs an illustrative example, let us consider the effectiveness of self-supervision: inducing representations of words and phrases from just listening to speech or reading text. For text, this general family of methods has been successful since around the time of Latent Semantic Analysis (Dumais, 2004) , and currently large written language models exhibit a constantly expanding range of abilities (Wei et al.) . In contrast, self-supervision with spoken language has met with a limited amount of success only in the last few years (e.g. Baevski et al., 2020; Hsu et al., 2021) , and these models as of now are usually only fine-tuned on the task of ASR. One obvious difference is that items such as words and morphemes are either explicitly delimited or easily discovered in text, but finding them is an unsolved research problem in speech, due to the inherent variability of this modality.\nOn the other hand, learning spoken language becomes much more tractable when self-supervision is augmented with grounding in perception. The cross-modal correlations, though unreliable and noisy, are often sufficient to substantially facilitate the discovery and representation of words (Peng and Harwath, 2022; Nikolaus et al., 2022) and syllables (Peng et al., 2023) in spoken language. For written language, grounding in the visual modality has also been found to help in some cases (e.g. Tan and Bansal, 2020) but it does not appear crucial, as the dominance of text-only language models demonstrates.\nSince spoken language is richer in information content, it should in principle be possible to exploit this extra signal for improving performance. One obstacle to such developments is the increased variability and channel noise. Perhaps less obviously, a second obstacle is that widely used benchmarks are often designed in a way which obstructs obtaining such gains. For example the 2021 Zerospeech challenge (Dunbar et al., 2021) which aimed to benchmark spoken language modeling, evaluates systems according to the following criteria: phoneme discrimination, word recognition, syntactic acceptability and correlation to human judgments of word similarities. None of these metrics would benefit much from modeling speaker characteristics, speech tempo, pitch, loudness or even suprasegmental phonology. Except for the first one, these metrics would be very well suited for models trained exclusively on written language. The combined effect of these two obstacles was evident in the results of Zerospeech 2021 where written-language toplines, such as RoBERTa (Liu et al., 2019) , outperformed spoken language models on the latter three metrics, often by large margins.\n\nUnifying Speech Processing and NLP\nAs evident from the examples highlighted above, spoken language is in some ways quite different from written language and presents a distinct set of challenges and potentials. In order to understand how much progress the fields of speech and NLP are making in understanding and implementing human language, we need to take speech seriously qua language, not just a cumbersome modality, and measure our progress accordingly.\n\nConverging methodology\nThe time is ripe for a closer integration of the speech and NLP communities and for a unified computational science of language. The set of methodologies used in speech and text processing used to be quite distinct in the past. Since the adoption of deep learning both fields have converged to a large extent: currently the state-of-the-art models for both spoken and written language rely on transformer architectures (Vaswani et al., 2017) self-trained on large amounts of minimally preprocessed data, with optional fine-tuning. The technical communication barriers across disciplinary boundaries are thus much lower. The recent emergence of the concept of textless NLP (Lakhotia et al., 2021) exemplifies the potential of unifying these two fields.\n\nOpportunities\nThe following paragraphs outline the most important benefits of making NLP more natural, ranging from basic science to practical applications.\nModeling language acquisition. An increased attention to spoken language within NLP has the potential to lead to a more realistic understanding of how well our current methods can replicate key human language abilities. Acquiring language under constraints that human babies face is the big one. There is a large amount of work on modeling human language acquisition which uses exclusively written data (at best transcribed from the original audio). Hopefully by this point the reader will be convinced that the relevance of this work to the actual issue under consideration is highly questionable. We stand a much better chance of figuring out human language acquisition if we refocus attention on spoken language. Data efficiency. Linzen (2020) argues convincingly for language models which are human-like in their data-efficiency and generalization capabilities. It is, however, unclear whether these properties can even be properly evaluated via the medium of written language. Since the informational density and the signal-to-noise ratio in written vs spoken language are so very different, it makes little sense to compare human children with language models trained on text. Furthermore, the challenges of pure self-supervision may motivate us to take seriously the impact of grounding in perception and interaction, which humans use universally as a learning signal.\nUnwritten languages. Many modes of human communication lack standard written representation. These range from major languages spoken by millions of people such as Hokkien (Mair, 2003) , to small or non-standard language varieties, to sign languages. Shifting the emphasis of NLP research from text to the primary, natural oral and gestural modalities will benefit the communities using these varieties.\nSpoken dialog systems. Dingemanse and Liesenfeld (2022) argue that language technology needs to transition from the text to talk, and provide a roadmap of how to harness conversational corpora in diverse languages to effect such a transition. Indeed, one of the most obvious benefits of spoken language NLP would be dialog systems that do not need to rely on ASR and are able to exploit the extra information lost when transcribing speech, enabling them to understand humans better and interact with them in a more natural way.\nNon-textual language data. Finally, there is a large and increasing stream of non-textual language data such as podcasts, audio chat channels and video clips. Processing such content could also benefit from an end-to-end holistic treatment without the need of going through the lossy conversion to text.\n\nRecommendations\nIf you are an NLP practitioner and view spoken language as outside the scope of your field, reconsider. Getting into speech processing does require understanding its specifics, but it is not as technically daunting as it used to. Conversely, if you are a speech researcher, consider that ASR and text-tospeech is not all there is: we can get from sound to meaning and back without going through the written word. Both fields would do well to consider the whole of human language as their purview. Increased collaboration would benefit both communities, and more importantly, would give us a chance of making real progress towards understanding and simulating natural language.\n\nLimitations\nThe main limitation of this paper is the one applying to any opinion piece: it is subjective and personal, as the views of the authors are inherently limited by their expertise and experience. More specifically, this paper argues for an increased interaction between the speech and NLP communities, but the author is more strongly embedded in the latter, and thus addresses this audience primarily. Additionally, the short paper format imposes significant constraints on the amount of nuance, detail and discussion of relevant literature, and thus readers may find some of the claims to be less strongly supported and less hedged than would be ideal, or proper in a longer treatment of this topic.\n\n\nI am using spoken language in the broad sense here, including both the oral and gestural (signed) modes of expression, and opposing these to the written modality.\nHowever seeAaron and Joshi (2006) for a dissenting view.3 https://aclanthology.org/\nOne exception to this general pattern is the presence of two spatial dimensions in written language, and the role of 2D layout in textual publications.\n", "hypothesis": " Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP.  Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication.", "answer": true}
{"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n", "hypothesis": "We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA (Wang et al., 2022) , which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of language-related tasks.", "answer": false}
{"title": "An Exploratory Study on Model Compression for Text-to-SQL", "content": "\nIntroduction\nText-to-SQL is an important task that has been gaining the attention of researchers over the years. Formally, given a query q and a relational database D, the goal of Text-to-SQL is to build a model f such that s = f (q, D | \u03b8) where \u03b8 is a vector of model parameters and s is a predicted SQL statement which we can use to retrieve the answer to q from D.\nText-to-SQL has many potential applications that can improve our standard of living. For example, medical chatbots can convert user queries into SQL statements and then use them to retrieve relevant information from medical knowledge bases. Industry can leverage Text-to-SQL tools to help employees shorten the time needed to write complex SQL queries, thereby improving overall work productivity.\nThe recent emergence of complex Text-to-SQL datasets containing complicated SQL and crosstable setup has driven researchers to develop huge models that encode various complex relationships between table schema and query with large pretrained language models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) . These models are usually sequence-to-sequence models that generate SQL statements sequentially or sketch-based models that use classifiers to fill in the slots of SQL templates.\nHowever, despite achieving state-of-the-art performances on benchmark datasets, such models are usually both memory and computationally expensive, making it technically challenging to deploy them in memory-constrained real-world applications that require low inference latency. Therefore, to deploy state-of-the-art Text-to-SQL models in real-world production environments, we must drastically improve the inference time and reduce the number of parameters in these models.\nWe turn to the field of model compression (Cheng et al., 2017) for solutions that can speed up inference without significantly hurting model performance. Formally, the goal of model compression is to reduce f to a smaller model f \u2032 such that s \u2032 = f \u2032 (q, D | \u03b8 \u2032 ). Ideally, we want s \u2032 to be the same as s and dim(\u03b8 \u2032 ) to be much smaller than dim(\u03b8).\nIn this paper, we thoroughly examine the feasibility of using model compression techniques to build faster and more accurate Text-to-SQL models that we can successfully deploy in the real world. For this, we carefully apply a few model compression methods to representative sequence-to-sequence or sketch-based Text-to-SQL models on three datasets: WikiSQL, Spider, and TableQA. The main findings of this paper are: (i) sketch-based models generally respond well to model compression techniques, while sequence-to-sequence models show mixed results, (ii) we observe better speed improvements in Sketch-based models as their slot-filling components are much faster than the decoding components of sequence-to-sequence models. (iii) model compression techniques work poorly on state-of-the-art Text-to-SQL models built on pre-trained encoder-decoder language models such as T5.\nWe hope our findings can empower practitioners to make more informed decisions when selecting Text-to-SQL models and compressing them appropriately for real-world deployments. . Contrarily, Spider contains large samples of complex SQL instances that connect multiple tables with primary and foreign keys with more advanced clauses such as nested queries, JOIN ON, and ORDER/GROUP BY.\n\nBaseline Models\nRecent deep neural Text-to-SQL models can be broadly classified under two categories: sequenceto-sequence models and sketch-based (also known as slot-filling) models.\n\nSequence-to-sequence models\nSequence-to-sequence models are generally made up of an encoder component that converts user query inputs together with database information into a hidden vector and a decoder component that generates SQL statements based on the output hidden vectors from the encoder. BRIDGE (Lin et al., 2020) encodes input questions and table schema with BERT and LSTM and generates SQL predictions with a pointer-generator decoder (See et al., 2017) supported by a schemaconsistency driven search space pruning strategy. RAT-SQL (Wang et al., 2020a ) also encodes input instances with BERT but generates SQL as an abstract syntax tree (AST) with a tree-structured decoder (Yin and Neubig, 2017) . It also incorporates a relation-aware self-attention mechanism that further improves schema-linking, schema-encoding, and representation of the encoder. PICARD (Scholak et al., 2021) is a state-of-theart algorithm that directly fine-tunes a pre-trained encoder-decoder language model T5 (Raffel et al., 2020) on Text-to-SQL data, and then constrain the decoder to output valid SQL by integrating an incremental parsing strategy to the beam search process.\n\nSketch-based model\nSketch-based methods also encode user inputs into vectors but only need to fill in slots in SQL sketches rather than generating full SQL statements. Each SQL sketch is a template SQL statement with placeholder slots and the goal of sketch-based models is to predict the best item to go into each slot. NL2SQL-RULE (Guo and Gao, 2019 ) is a standard sketch-based model which uses BERT and LSTM to encode input query and database information and predict outputs in slots of SQL sketches.\n\nCompression Techniques\nWe follow Sun et al. (2021) and experiment with the following model compression techniques in this study: Layer Pruning (Sajjad et al., 2022 ) is a simple yet effective strategy that discards a certain number of layers from transformer-based language models before fine-tuning the pruned models on downstream tasks. We apply the top-layer pruning strategy which deletes the top N encoder or decoder layers before the start of any training. Knowledge Distillation (Hinton et al., 2015) is a method that compresses deep neural network models by distilling useful knowledge from a larger model (teacher) to a smaller model (student). We follow Jiao et al. (2020) and distill smaller language models from larger ones such as BERT-large, before fine-tuning Text-to-SQL models on those distilled models. For WikiSQL and Spider, we experiment with the distilled English language models from MiniLM 1 (Wang et al., 2020b) , while for TableQA, we use the Chinese TinyBERT models 2 . Token Pruning For PICARD model, We also apply token pruning (Goyal et al., 2020; Kim et al., 2022) , which is a different pruning strategy that gradually removes redundant token encodings from the outputs of each encoder layer before feeding the reduced number of tokens to the next encoder layer. We follow Goyal et al. (2020) and implement an attention scoring mechanisms which weights the significance of each token by the sum of attention weights it gets from other tokens. The tokens with the lowest significance scores (based on predetermined thresholds) for each encoder layer are dropped.\n\nEvaluation Metrics\nWe evaluate our experiment results using Exact set match (ESM) (Yu et al., 2018) . ESM decomposes every pair of predicted and gold SQL queries into sets clauses and then computes the percentage of exact set matches over all pairs (Zhong et al., 2020) .\n\nExperiment Setup\nIn most cases, we follow the recommended configurations in corresponding papers. We may adjust the batch sizes and learning rates slightly to fit the experiments on our hardware. We train our models on servers with either NVIDIA GV100 GPU (32GB) or RTX A6000 (45GB) but calculate inference speeds by running models on only CPUs with batch size set to one, which better mimics the situations in the real world. For all datasets, we use their dev sets as the test sets and create new train-dev sets in the ratio of 4 to 1 from the original train set. We early stop our models based on the ESM scores on dev sets and report average test set ESM scores over 5 different runs. Other than PI-CARD, we use BERT-large for all English datasets and RoBERTa-Zh (Cui et al., 2020) for TableQA.\n\nSimple datasets\nWikiSQL As shown in Figure 1 WikiSQL. For example, we can remove 50% of the encoder layers from BRIDGE, while only taking a penalty of only 0.82% drop in Exact Set match (ESM). When only keeping the bottom 6 encoder layers, NL2SQL-RULE can still perform at 0.834 ESM, a 3.65% drop from the original unpruned model. For knowledge distillation, we fine-tuned BRIDGE on two versions of MiniLM (Wang et al., 2020b) : L6xH768 and L6xH384. Results show that BRIDGE trained on the MiniLM language models performs slightly worse than the layer pruning method with similar number of layers. However, this is acceptable given the hidden sizes of the MiniLM models are 384 and 768, which are smaller than the hidden size of 1024 for BERT-large. TableQA We notice several differences in results between WikiSQL and TableQA. First, the performances of RATSQL on TableQA are significantly lower than those of NL2SQL-RULE. For example, unpruned NL2SQL-RULE achieves an ESM of 0.8 but unpruned RATSQL only achieves 0.69 despite our best efforts. Second, we observe more significant drops in performances when applying layer pruning and knowledge distillation to RATSQL than NL2SQL-RULE. For example, we observe only a 3.63% drop in ESM dropping the first 16 encoder layers of NL2SQL-RULE but notice an 18.8% drop in the performance of RATSQL with the same configurations. Last but not least, models trained on distilled language models perform slightly worse than the layer pruned models due to their smaller hidden sizes except for NL2SQL-RULE on TinyBERT with 6 layers and 768, which achieves an ESM of 0.80, even higher than that of the unpruned NL2SQL-RULE.\nRecommendation: We recommend using slotfilling models when building applications that only deal with simple queries. These models not only perform comparably or even better than sequenceto-sequence models, but also respond better to recent model compression techniques. Spider As PICARD was trained on a 3 billion parameters pre-trained language model with an encoder and a decoder of similar size, we show three sets of results by applying layer pruning on 1) the encoder, 2) the decoder, and 3) both the encoder and decoder. As seen in Figure 3 , the layer pruning strategy does not work as well on PICARD. At around six layers, PICARD loses around 49.9% and 40.3% of its original performance for encoder-only and decoder-only pruning settings respectively. For the encoder+decoder pruning strategy, we observe similar levels of performance when discarding the same number of transformer layers as the other two configurations. For example, dropping 3 layers each from the encoder and decoder gets us 0.641 ESM, compared to 0.624 when dropping 6 decoder layers and 0.648 when dropping 6 encoder layers. On the other hand, RATSQL demonstrates better compression results on Spider, maintaining 92.6% of original performance while keeping on six encoder layers, contrary to the results on TableQA.\n\nComplex dataset\nToken pruning We follow the implementation of Goyal et al. (2020) and apply token pruning to PI-CARD. We plot the ESM performance of a tokenpruned model against the number of retained tokens in Figure 4 . As seen in the plots, although we can remove an average of 286 tokens from the top six encoder layers, we are only able to discard an average of 41 tokens from the bottom six layers. For example, we see a sharp drop in ESM performance by just pruning around 40 tokens from the 3rd encoder layer. Similarly, we also observe steady drop in ESM performance when pruning more than 100 tokens from encoder layers 15 and 18. Our final model achieves an ESM of 0.527 (26.3% drop in performance) while only seeing a 5.2% improvement in inference speed when applying token pruning to the encoder of T5. As we cannot significantly prune the number of tokens in each encoder layer without severely hurting model performance, we conclude token pruning is also not effective on the PICARD model. Recommendation: Our results suggest that both layer and token pruning are not effective on PI-CARD and we would get better compression performances on sequence-to-sequence models like RATSQL, which has a much bigger encoder than decoder in terms of model size.\n\nDiscussion\nThe main difference between recent sequence-tosequence and sketch-based models is related to how we generate the SQL statements. Compared to the lightweight slot-filling classifiers in sketchbased models, recent sequence-to-sequence model decoders rely heavily on grammar-guided decoding processes which requires navigating through a huge search space and requires an even longer inference time than the encoders. For example, 76.62% and 87.14% of the inference time are spent in the decoding step for BRIDGE and RATSQL, while most of the inference time in NL2SQL-RULE is spent on the encoder. Considering the speed, compression effectiveness, and performance, sketch-based models would be better choices if we get similar performances on benchmark datasets.\n\nConclusion\nThis paper investigates whether we can use model compression to improve the inference efficiency of recent Text-to-SQL models that rely heavily on large pre-trained language models. Our results show that on simple Text-to-SQL datasets, we can deploy simple strategies such as layer pruning to obtain a 5-6x speedup without significantly hurting model performances. We also observe that sketchbased models generally respond better to model compression than sequence-to-sequence models. However, we are not able to effectively compress PICARD on the spider dataset and we would tackle this problem as a future work.\n", "hypothesis": " Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements..", "answer": true}
{"title": "Value type: the bridge to a better DST model", "content": "\nIntroduction\nTask-oriented dialogue systems have become more and more important as people's demand for life increases(booking flights or restaurants), which have become increasingly important in the field of NLP(Nature Language Process). (Henderson et al., 2019; Hung et al., 2021; Zheng et al., 2022) Traditionally, the task-oriented dialogue system consists of four modules (Zhang et al., 2020) : Natural language understanding(NLU), Dialogue state tracking(DST), Dialogue manager(DM) and Natural language generation(NLG). This module directly affects the decision-making behavior of the dialogue system, and plays an extremely important \u21e4 The first two authors contribute equally. Weiran Xu is the corresponding author.\nSys: yes, the autumn house is on the east part of time, the prices are cheap and it is 4 stars. is there anything else you would like to know? Usr: no, i just want to book it for 2 people for 5 nights starting wednesday. Turn_label: hotel-area=east, hotel-book day=wednesday, hotel-people=2, hotel-book stay=5, hotel-princerange=cheap, hotel-stars=4\nSys: your friend has good taste. It is located at 02:00 rose crescent city centre, postcode cb23ll. Usr: i would like to book a table for 7 people on Monday at 15:15 please. Turn_label: restaurant-book day=monday, restaurant-book people=7, restaurant-book time=15:15 Sys: Usr: i would like a taxi from saint johns college to pizza hut fenditton. Turn_label: taxi-departure=saint johns college, taxi-destination=pizza hut fenditton role in the task-based dialogue system. (Lee et al., 2019) The recent methods in DST work are mainly divided into two categories. The first category is based on ontology which means the candidate slot value is assumed to be known eg (Zhou et al., 2022; Ye et al., 2021b; Guo et al., 2021) . The second is the way without ontology. These studies have completely abandoned ontology, and they assume that the slot value is unknown. eg (Wu et al., 2019; Kim et al., 2019; Kumar et al., 2020; Lin et al., 2021) . However, most of their work is based on dialog state, dialog and slot modeling, ignoring that the value type of each slot may be different. If these slots are modeled uniformly, then there is a lack of a specific feature of each slot.\nIn this work, we propose a new DST framework named SVT-DST, which uses the Slot-Value Type as the bridge to increrase the model performance. With this method, each slot has specificity for the attention of the conversation history to better identify the slot value. Specifically, we first classify all the slots in the dataset according to their slot value types. As shown in Figure 1 , adjectives, time and numbers correspond to pricerange, arrive-time and book-people respectively. We train a sequence annotation model with dialogue training which is used to extarct entities and corresponding entitytypes in each on the turn. We hope that the attention between the dialogue and slots can be higher when the turn is near to current turn with the same slotvalue type. In order to achieve the goal, we use monotonically decreasing functions to integrate the attention weights, which will be described in detail in the method. we use monotonically decreasing functions to integrate these types into the attention operation.\nOur main contributions are as follows: 1) We classify the slot according to the slot-value type, then train the Ner model to extract these types to improve the attention formula. 2)We design a sampling strategy to integrate these types into the attention formula, which decrease the error of Ner model. 3)We have achieved competitive results on MultiWOZ 2.1 and 2.4. We analyze the results and point out the future work.\n\nMethod\nFigure 2 shows the structure of our DST model, including encoder, attention module and slot value processing module. In this section, we will introduce each module of this method in detail.\nA T-turn conversation can be expressed as C t = {(U 1 , R 1 ), ..., (R t 1 , U t )}, where R t represents system discourse and U t represents user discourse. We define the dialogue state of the t-th turn as B t = {(S j , V t j )| 1 <= j <= J}, where V t j represents the value of the j-th slot S j in the t-th turn. J represents the number of predefined slots. Follow (Ren et al., 2018) , we express the slot as a \"domain slot\" pair, such as 'restaurant-price range'.\n\nEncoder\nFollow (Ye et al., 2021b) , we use two bert (Devlin et al., 2018) models to encode context and slot respectively.\n\nContext encoder\nWe express the dialogue at turn t as D t = R t U t , where represents sentence connection. Then the history of the dialogue including t-th turn as\nM t = D 1 D 2 ... D t .The input of the context encoder is X t = [CLS] M t [SEP ].\nThe output of the encoder is:\nEQUATION\nWhere C t 2 R |Xt|\u21e5d , |X t | is the length of M t and d is the hidden size of bert. bert finetuned indicates that the bert model updates a part of parameters during training.\n\nSlot-value related encoder\nWe employ the first token to represent the aggregate representation of the entire input sequence. Therefore, for any slot S j 2 S(1 \uf8ff j \uf8ff J) and any value v t j 2 V j we have:\nh S j = bert fixed (S j ) 2 R 1\u21e5d\n(2)\nEQUATION\nFor the last turn of dialogue state B t 1 , we have\nEQUATION\nWhere\nh B t 1 2 R |B t 1 |\u21e5d , B 1 = Null.\nbert fixed indicates that the bert model has fixed parameters during training.\n\nCross-Attention\nWe use the multi-head-attention module (Vaswani et al., 2017) as the basis of our attention module.\n\nSlot-Context Attention\nWe first calculate the bias term of the attention formula. For each dialogue history M t , we first use the monotonically decreasing distribution function \u2318(n) to initialize the weight of each turn of dialogue D t in the dialogue history:\nEQUATION\nWhere n = T t, n represents the distance between the last turn and the current turn. The closer the distance is, the greater the weight will be obtained. Note that (T ) represents the weight of distance T for this turn (turn 0) and the latest turn t. We record the turns of the value type type j with slot S j in the history:\nEQUATION\nWhere n>m, which represents the turn indexs. Then we calculate the weight of these turns:\nEQUATION\nFinally, we add these two weights according to the turn indexs to get bias: The attention between S j and C t can be calculated as:\nEQUATION\nEQUATION\nWhere '() indicates a learnable mapping built by embedding. W bias , W r 1 and W r 2 indicates a linear layer, respectively.\n\nSlot-State Attention\nFor S j and B t 1 , their attention can be expressed as:\nEQUATION\nA B,F F N j,t 1 =W r 4 ReLU (W r 3 [(h S j , A B j,t 1 ] + b r 1 ) + b r 2 (12)\n\nGate Fusion\ninspired by (Zhou et al., 2022) , we employ a gate module to combine the attention between Slotcontext and Slot-state:\nEQUATION\nEQUATION\nWhere \u2326 indicates vector product, indicates the sigmoid function and \u2022 indicates element-wise product operation.\n\nSelf-Attention And Value Matching\nIn this part, we have followed the relevant part of (Ye et al., 2021b) .\n\nNer Model And Sampling Strategy\nWe employ the W2NER model (Li et al., 2022) as our tagging model. The strategy of our labelmaking is that: for each value in the ontology, if the value is in current turn, we will tagging this value. For sampling strategy, only when the target entities are different from entities extracted from previous turns, this turn will be marked with the entities' type. This strategy helps to reduce the interference of duplicate entities. For the specific classification of each slot, please refer to the appendix. In particular, for bool type, we train the annotation model to extract keywords, such as internet, parking, etc.\n\nOptimization\nWe use the sum of the negative log-likelihood as the loss function at each turn t:\nEQUATION\nWhere\nP (V t j | X t , S t ) = exp( || t S t j h V t j || 2 ) P V 0 j 2V j exp( || t S t j h V 0 j || 2 ) (16) t S t j\nindicates the output of self-attention module corresponding to S j at the t-th turn. 3 Experiments\n\nDataset, metric and Evaluation\nWe evaluate our method on these datasets: Mul-tiWOZ 2.1 (Eric et al., 2019) and MultiWOZ 2.4 (Ye et al., 2021a) which provide turn-level annotations of dialogue states in 7 different domains. We evaluate our method on this dataset and follow the pre-processing and evaluation setup from (Wu et al., 2019) , where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. We use Joint Goal Accuracy that is the average accuracy of predicting all slot assignments for a given service in a turn correctly to evaluate the main results of models.\n\nBaselines\n(1) Trade: Transferable dialogue state generator (Wu et al., 2019) which utilizes copy mechanism to facilitate domain knowledge transfer.\n(2) Tripy: It applies three copying mechanisms to extract all values (Heck et al., 2020) (3) MinTL: An effective transfer learning framework for task-oriented dialogue systems (Lin et al., 2020) ,which uses T5 (Raffel et al., 2020) and Bart (Lewis et al., 2019) .\n(4) Star: Framework with self-attention modules to learn the relationship between slots better (Ye et al., 2021b) LUNA: It applies a slot-turn alignment strategy to accurately locate slot values and their associated context. (Wang et al., 2022) \n\nMain Results And Analysis Experiments\nTable 1 shows the results of our main test and ablation study. Our base model achieved 53.28% for the joint-acc, while our Ner-based model achieved 55.37% , a significant improvement of 2.09% compared with the base model. In 2.4 dataset, our model achieved 68.28%, a significant improvement of 2.93% compared with the base model. And When we use the correct type labels for training, the model performance reaches 59.27%, which has exceeded all baseline models. Ground truth is extracted according to the slot-type in the turn label, similar to our sampling strategy. In order to model the attention of state and dialog history separately, we changed the attention in Star (Ye et al., 2021b) to the fusion of slot attention and dialog history attention. Such changes reduced the performance of the model. However, the ablation experiment shows that the method we proposed can really benefit the model indicators.\nTable 2 shows the results of our analysis experiments, which use different distribution functions to model attention. For both 2.1 and 2.4 datasets, the experimental results show that under different distribution function modeling, the distribution with constant term bias may produce higher results such as 0.5 \u21e4 (1 + x) + 1 and 1 x/30. And it often has a positive impact on the experiment when the power of the independent variable is 1.\n\nCase Study\nWe conducted a series of analytical experiments on attention weights. As shown in the Table 3 , we randomly selected a slot, \"attraction-name,\" and then chose an example PMUL4648 from the test set to observe the attention distribution of this slot for each turn in the test samples. In the example, the attraction-name slot is activated in the turn 2. It can be seen that function 3 noticed this turn with a large weight, followed by function 1. As a comparison, function 2 assigned larger weights to the first turn, which is sufficient to indicate that the fitting effect of function 2 is weaker compared to the other two functions. Our analysis is as follows:\nIf there is no constant term in the distribution function, the difference between score+bias and score is not significant, resulting in limited performance improvement of the model. On the other hand, the power of the independent variable is greater than 1 such as function 2, the magnitude changes too obviously after Softmax. This leads to not smooth transitions between turns, resulting in limited performance improvement. The result of using the ground truth labels training model shows that there is still huge space for improvement in Ner model annotation. One of the biggest challenges is that the annotation model often assigns certain entities to labels based on some fragmented tokens, without considering the impact of context, which leads to the proliferation of labels. We will solve this problem in future work.\n\nConclusion\nIn this paper, we propose an effective method to integrate slot-types into the DST model. Specifically, we propose the SVT-DST. This framework incorporates the slot-types information into the attention operation to help model pay more attention to these turns that include the type of one slot. Further, We design a sampling strategy to integrate these types into the attention formula to decrease the error of Ner model. Results on MultiWOZ dataset show that our method has significant improvement on this task.\n", "hypothesis": "In this paper, we propose a new framework for DST task based on these value types.  Firstly, we extract the type of token from each turn.  Specifically, we divide the slots in the dataset into 9 categories according to the type of slot value, and then train a Ner model to extract the corresponding type-entity from each turn of conversation according to the token.  Secondly, we improve the attention mode which is integrated into value type information between the slot and the conversation history to help each slot pay more attention to the turns that contain the same value type.  Meanwhile, we introduce a sampling strategy to integrate these types into the attention formula, which increases the error of Ner model.", "answer": false}
{"title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts", "content": "\nIntroduction\nThe World Health Organization (WHO) emphasizes the importance of significantly accelerating suicide prevention efforts to fulfill the United Nations' Sustainable Development Goal (SDG) objective by 2030 (Saxena and Kline, 2021) . Reports released in August 2021 1 indicate that 1.6 million people in England were on waiting lists for mental health care. An estimated 8 million people were unable to obtain assistance from a specialist, as they were not considered sick enough to qualify. As suicide remains one of the leading causes of the death worldwide 2 , this situation underscores the need of mental health interpretations from social media data where people express themselves and their thoughts, beliefs/emotions with ease (Wongkoblap et al., 2022) . The individuals dying by suicide hinder the psychological assessments where a self-reported text or personal writings might be a valuable asset in attempting to assess an individual's specific personality status and mind rationale (Garg, 2023) . With strong motivation of thinking beyond low-level analysis, Figure 1 suggests personalization through higherlevel analysis of human writings. As, the social media platforms are frequently relied upon as open fora for honest disclosure (Resnik et al., 2021) , we examine mental disturbance in Reddit posts aiming to discover Interpersonal Risk Factors (IRF) in text.\nInterpersonal relationships are the strong connections that a person with their closest social circle (peers, intimate-partners and family members) which can shape an individual's behavior and range of experience (Puzia et al., 2014) . Affecting such interpersonal relationships influences the associated risk factors resulting in mental disturbance. According to interpersonal-psychological theory of suicidal behavior (Joiner et al., 2005) , suicidal desire arises when a person experience persistent emotions of (i) Thwarted Belongingness (TBE) 3 , and (ii) Perceived Burdensomeness (PBU) 4 . As a starting point for our research, this cross-sectional study facilitates the language resource for discovery of underlying users with prospective selfharm/suicidal tendencies to support and compliment existing literature (Bialer et al., 2022; Tsakalidis et al., 2022; Gaur et al., 2018) as intrinsic classification task.\nComputational approaches may better understand the technological advancements in psychology research, aiding the early detection, prediction and evaluation, management and follow-up of those experiencing suicidal thoughts and behaviors. Most automated systems require available datasets for computational advancements. Past studies show that the availability of relevant datasets in mental healthcare domain is scarce for IRF due to sensitive nature of data as shown in Table 1 (Su et al., 2020; Garg, 2023) . To this end, we introduce an annotated Reddit dataset for classifying TBE and PBU. The explanatory power of this dataset lies in supporting the motivational interviewing and mental health triaging where early detection of potential risk may trigger an alarm for the need of a mental health practitioner. We adhere to ethical considerations for constructing and releasing our dataset publicly on Github 5 .\n\nDataset\n2.1 Corpus Construction Haque et al. (2021) used two subreddits r/depression and r/suicidewatch to scrape the SDCNL data and to validate a label correction methodology through manual annotation of this dataset for depression versus suicide. They ad-dressed the then existing ethical issues impacting dataset availability with public release of their dataset. In addition to 1896 posts of SDCNL dataset, we collected 3362 additional instances from Reddit on r/depression and r/SuicideW atch through PRAW API 6 from 02 December 2021 to 04 January 2022 with about 100 data points per day (to maintain variation in the dataset). On initial screening, we found (i) posts with no self-advocacy, (ii) empty/irrelevant posts. We manually filter them to deduce self-advocacy in texts leveraging 3155 additional samples, which results in a total of 5051 data points (Garg et al., 2022) . We removed 694 of the data points depicting no assessment of mental disturbance. Moreover, people write prolonged texts when they indicate IRF which is inline with the conventional arguments where prolonged remarks get better responses from others in comparison of the transient remarks (Park et al., 2015) . The length of real-time Reddit posts varies from a few characters to thousands of words. We limit the maximum length of every post to 300 words resulting in 3522 posts as a final corpus.\n\nAnnotation Scheme\nClassification of IRF, being a complex and highly subjective task, may induce errors with naive judgment. To mitigate this problem, we build a team of three experts: (i) a clinical psychologist for training annotators and validating annotations with psychological viewpoint, (ii) a rehabilitation counselor for comprehending human mind to understand users' IRF, and (iii) a social NLP expert suggesting text based markings in Reddit posts. To negotiate and mitigate the trade-off between three different perspectives, our experts build annotation guidelines 7 to mark (i) TBE, and (ii) PBU. The experts annotated 40 samples of the corpus in isolation using these annotation guidelines to avoid biases and discover possible dilemmas due to the subjective nature of tasks. Therefore, we accommodate perplexity guidelines to simplify the task and facilitate unbiased future annotations.\n\nTBE or PBU in the Past:\nTo check if the condition of a person with disconnected past is still alarming prospect of self-harm or suicidal risk. For instance, 'I was so upset being lonely before Christmas and today I am celebrating New Year with friends'. We frame rules to handle risk indicators about the past because a person attends celebration and overcome the preceding mental disturbance which means filling void with external event. With neutral opinion by NLP expert about double negation, our clinical psychologist argues presence of risk in their perception which may again evolve after some time and thus, marks this post with presence of the TBe.\n2. Ambiguity with Social Experiences: Relationships point to the importance of the ability to take a societal pulse on a regular basis, especially in these unprecedented times of pandemic-induced distancing and shut-downs. People mention major societal events such as breakups, marriage, best friend related issues in various contexts suggesting different user perceptions. We mitigate this problem with two statements: (i) Any feeling of void/missing/regrets/or even mentioning such events with negative words should be marked as presence of TBe such as consider this post: 'But I just miss her SO. much. It's like she set the bar so high that all I can do is just stare at it.', (ii) Anything associated with fights/quarrels/general stories should be marked with absence of TBe such as consider the post: 'My husband and I just had a huge argument and he stormed out. I should be crying or stopping him or something. But I decided to take a handful of benzos instead.'\n\nAnnotation Task\nThree postgraduate students underwent eight hours of professional training by a senior clinical psychologist leveraging annotation and perplexity guidelines. After three successive trial sessions to annotate 40 samples in each round, we ensured their alignment on interpreting task requirements and deployed them for annotating all data points in the corpus. We obtain final annotations based on the majority voting mechanism for binary classification task <TBE, PBU>. 8 We validate three annotated files using Fliess' Kappa inter-observer agreement study on classifying TBE and PBU where kappa is calculated as 78.83% and 82.39%, respectively. Furthermore, we carry out an inter-annotator agreement study with group annotations 9 for textspans extraction in positive data points. The results for agreement study in two-fold manner: (i) 2 categories (agree, disagree) and (ii) 4 categories (strongly agree, weakly agree, weakly disagree, strongly disagree), are obtained as 82.2% and 76.4% for agreement study of <TBE_EXP>, and 89.3% and 81.3% for agreement study of <PBU_EXP>, respectively.\n\nDataset Statistics\nOn observing the statistics of our dataset in Table 2 , we found 54.71% and 32.56% of positive data points with underlying 255489 and 156620 words for TBE and PBU, respectively. It is interesting to note that although the average number of sentences to express PBU is less than TBE, the observations are different for average number of words. We calculate the Pearson Correlation Coefficient (PCC) for our cross-sectional study on TBE and PBU as 0.0577 which shows slight correlation between the two. Our dataset paves the way for longitudinal studies which is expected to witness increased PCC due to wide spread emotional spectrum (Kolnogorova et al., 2021; Harrigian et al., 2020) . On The most frequent words for identifying (i) TBE are alone, lonely, nobody to talk, someone, isolated, lost, and (ii) PBU are die, suicide, suicidal, kill, burden, cut myself. 10 Our approach for identifying TBe and PBu goes beyond a simple keyword detector. Instead, we utilize a more sophisticated method that considers the context and relationships between words. For instance, consider a following sample:\nMassive party at a friend's house-one of 10 WordCloud is given in Appendix C. my closest friends is there, loads of my close friends are there, i wasn't invited. wasn't told. only found out on snapchat from their stories. spending new years eve on teamspeak muting my mic every time i break down :) Despite the absence of trigger words, our approach flags this post as positive for TBu based on its indicators 'friend', 'teamspeak', 'friends', 'invited', 'snapchat', to name a few.\n\nBaselines\nWe perform extensive analysis to build baselines with three different conventional methods. We first apply Recurrent neural networks where a given text, embedded with GloVe 840B-300 11 , is sent to a 2-layer RNN model (LSTM, GRU) with 64 hidden neurons and the output is forwarded to two separate fully connected heads: (i) TBE and (ii) PBU. Each of the fully connected blocks have one hidden layer with 16 neurons and ReLU activation function, and an output layer with sigmoid activation. The loss function is Binary_CrossEntropy and optimizer is adam with lr = 0.001. Next, we apply pretrained transformer-based models. The input is tokenized using a pre-trained transformers' tokenizer to obtain a 768-dimensional vector which is then fed to a similar fully connected network as the previous architecture with hidden layer size as 48. We experimented with roberta-base, bert-base-uncased, distilbert-base-uncased, and mental/mental-bertbase-uncased models. Finally, we use the Ope-nAI embeddings API 12 to convert the input text into 1536-dimensional embeddings through 'textembedding-ada-002' engine which are used to train a classifier. We test the robustness of this approach over: (i) Logistic Regression, (ii) Random Forest, (iii) Support Vector Machine (iv) Multi Layer Perceptron, and (v) XGBoost. We further use two explainable methods: (i) LIME and (ii) SHAP on one of the best performing transformer-based models, MentalBERT (Ji et al., 2022) , to obtain the top keywords (Danilevsky et al., 2020; Zirikly and Dredze, 2022) . We compare them with the ground truth ROUGE scores for -Precision (P), Recall (R), and F1-score (F).\n\nExperimental Settings\nFor consistency, we used the same experimental settings for all models and split the dataset into the train, validation, and test sets. All results are reported on the test set, which makes up 30% of the whole dataset. We used the grid search optimization technique to optimize the parameters. To tune the number of layers (n), we empirically experimented with the values: learning rate (lr): lr \u2208 {0.001, 0.0001, 0.00001} and optimization (O): O \u2208 {'Adam', 'Adamax', 'AdamW'} with a batchsize of 16, 32 were used. We used base version pre-trained language models (LMs) using Hugging-Face 13 , an open-source Python library. We used optimized parameters for each baseline to find precision, recall, F1-score, and Accuracy. Varying lengths of posts are padded to 256 tokens with truncation. Each model was trained for 20 epochs, and the best-performing model based on the average accuracy score was saved. Thus, we set hyperparameter for our experiments as Optimizer = Adam, learning rate = 1e-3, batch size= 16, and epochs=20.\n\nExperimental Results\nTable 3 shows the performance of state-of-the-art methods in terms of precision, recall, F1-score, and accuracy. The current models have moderately low performance in this task, possibly due to a lack of ability to capture contextual information in the text. MentalBERT, a transformer-based language model, initialized with BERT-Base and trained with mental health-related posts collected from Reddit, had the best performance among BERT-based models, with an F1-score of 76.73% and 62.77% for TBE and PBU, respectively. This is likely due to the fact that it was trained on the same context as the task, namely health-related posts on Reddit. The combination of OpenAI embeddings and a classifier outperforms RNN and transformer-based models. The highest F1-Score of 81.23% was achieved by logistic regression for TBE, while the best performing model for PBU was SVM with an F1-score of 76.90%. We also analyzed the explainability of the model using LIME and SHAP methods of explainable AI for NLP on the best performing transformer model (MentalBERT) for TBE and PBU. We obtain results for all positive data points in the testing dataset and observe high recall of text-spans with reference to the ground truth as shown in Table 4 . We find the scope of improvement by limiting the superfluous text-spans found in the resulting set of words. The consistency in results suggests the need of contextual/domain-specific knowledge and infusing commonsense to improve explainable classifiers for a given task.\n\nConclusion and Future Work\nWe present a new annotated dataset for discovering interpersonal risk factors through human-annotated extractive explanations in the form of text-spans and binary labels in 3522 English Reddit posts. In future work, we plan to enhance the dataset with more samples and develop new models tailored explicitly to TBE and PBU. The implications of this work include the potential to improve public health surveillance and other mental healthcare applications that rely on automatically identifying posts in which users describe their mental health issues. We keep the implementation of explainable AI models for multi-task text classification, as an open research direction for Open AI and other newly developed responsible AI models. We pose the discovery of new research directions for future, through longitudinal study on users' historical social media profile to examine interpersonal risk factors and potential risk of self-harm or suicidal ideation. As we focus on Reddit data as a starting point of our study, exploring other forums could be an interesting research direction. bilitation counselor, for their unwavering support throughout the project. Additionally, we extend our heartfelt appreciation to Prof. Sunghwan Sohn for his consistent guidance and support. This project was partially supported by NIH R01 AG068007. This project is funded by NSERC Discovery Grant (RGPIN-2017-05377), held by Vijay Mago, Department of Computer Science, Lakehead University, Canada.\n", "hypothesis": " Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBE), and (ii) Perceived Burdensomeness (PBU).  We establish baseline models on our dataset facilitating future research directions to develop realtime personalized AI models by detecting patterns of TBE and PBU in emotional spectrum of user's historical social media profile..", "answer": true}
{"title": "Evaluating Paraphrastic Robustness in Textual Entailment Models", "content": "\nIntroduction\nRecognizing Textual Entailment (RTE), the task of predicting whether one sentence (hypothesis) would likely be implied by another (premise), is central to natural language understanding (NLU; Dagan et al., 2005) , as this task captures \"all manners of linguistic phenomena and broad variability of semantic expression\" (MacCartney, 2009) . If an RTE model has a sufficiently high capacity for reliable, robust inference necessary for full NLU (Mac-Cartney, 2009) , then the model's predictions should be consistent across paraphrased examples.\nWe introduce P aRT E, a test set to evaluate how reliable and robust models are to paraphrases (Table 1 includes an example). The test set consists of examples from the Pascal RTE1-3 challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) rewritten with a lexical rewriter and manually verified to preserve the meaning and label of the original RTE sentence-pair. We use this evaluation set to determine whether models change their predictions when examples are paraphrased.\nWhile this may not be a sufficient test to determine whether RTE models fully understand language, as there are many semantic phenomena that RTE models should capture (Cooper et al., 1996; Naik et al., 2018) , it is necessary that any NLU system be robust to paraphrases.\n\nP\nThe cost of security when world leaders gather near Auchterarder for next year 's G8 summit, is expected to top $150 million. P' The cost of security when world leaders meet for the G8 summit near Auchterarder next year will top $150 million.\n\nH\nMore than $150 million will be probably spent for security at next year's G8 summit. H' At the G8 summit next year more than $150 million will likely be spent on security at the event.\nTable 1 : An original and paraphrased RTE example.\nThe top represents an original premise (P) and its paraphrase (P'). The bottom depicts an original hypothesis (H) and its paraphrase (H'). A model robust to paraphrases should have consistent predictions across the following pairs: P-H, P'-H, P-H', and P'-H'.\nOur experiments indicate that contemporary models are robust to paraphrases as their predictions do not change on the overwhelmingly large majority of examples that are paraphrased. However, our analyses temper this claim as models are more likely to change their predictions when both the premise and hypothesis are phrased compared to when just one of the sentences is rewritten. We release P aRT E 1 to encourage others to evaluate how well their models perform when RTE examples are paraphrased.\n\nRelated Work\nWith the vast adoption of human language technology (HLT), systems must understand when different expressions convey the same meaning (paraphrase) and support the same inferences (entailment). Paraphrasing and entailment are closely connected as the former is a special case of the latter where two sentences entail each other (Nev\u011b\u0159ilov\u00e1, 2014; Fonseca and Alu\u00edsio, 2015; V\u00edta, 2015; Ravichander et al., 2022) . Para-phrasing has been used to improve RTE predictions (Bosma and Callison-Burch, 2006; Sun et al., 2021) and RTE has been used for paraphrase identification (Seethamol and Manju, 2017) and generation (Arora et al., 2022) . Furthermore, both phenomena are key to NLU (Androutsopoulos and Malakasiotis, 2010) and work such as Zhao et al. (2018) ; Hu et al. (2019) have explored rewriting RTE examples to create more robust models.\nWe follow a long tradition of evaluating linguistic phenomena captured in RTE models (Cooper et al., 1996) . Recent tests focus on evaluating how well contemporary RTE models capture phenomena such as monotonicity (Yanaka et al., 2019a,b) , verb veridicality (Ross and Pavlick, 2019; Yanaka et al., 2021) , presuppositions (Parrish et al., 2021) implicatures (Jeretic et al., 2020) , basic logic (Richardson et al., 2020; Shi et al., 2021) , figurative language (Chakrabarty et al., 2021) , and others (Naik et al., 2018; Poliak et al., 2018a; Vashishtha et al., 2020) . Unlike many of those works that evaluate models' accuracy on examples that target specific phenomena, we use a contrastive approach (Prabhakaran et al., 2019; Gardner et al., 2020) to determine whether RTE models' predictions change when examples are paraphrased.\n\nP aRT E\nTo explore whether these RTE models are robust to paraphrases, we create P aRT E, a modified version of the Pascal RTE1-3 challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007) . P aRT E contains 1,126 examples of an original unmodified RTE sentence-pair grouped with a sentence-pair with a modified premise, hypothesis, or both. We use the examples in RTE1-3 to create our test set, as opposed to other RTE datasets due to its long-standing history.\n\nParaphrase Generation & Verification\nFor each RTE premise-hypothesis pair (P-H), we created three paraphrased premises (P') and hypotheses (H') using a T5-based paraphraser 2 finetuned on the Google PAWS dataset (Zhang et al., 2019) . To ensure lexically diverse paraphrases, we filter out any paraphrases that have high lexical overlap with the original sentences using Jaccard index threshold of 0.75. Out of 14,400 generated sentences, 2,449 remained -956 paraphrased premises (P') and 1,493 paraphrased hypotheses (H'). Next, we retained 550 paraphrased premises and 800 paraphrased hypotheses paraphrases that crowdsource workers identified as grammatical and similar in meaning to the original sentences. 3 We include a grammatical check since an existing RTE evaluation set focused on paraphrases (White et al., 2017) contains hypothesis-only biases related to grammaticality (Poliak et al., 2018b) .\nIf at least one P' or one H' passes this filtering process, we retain the original RTE example and pair it with a corresponding paraphrased example (i.e. P'-H', P'-H, or P-H'). In the case where more than one P' or H' passes the filtering, we retained the P' or H' that crowdsource workers deemed most similar to the original sentence. Out of the original 2,400 RTE test pairs, we retain 914 pairs with a high-quality P' or H', resulting in 1,178 original and paraphrased RTE pairs. 4\n\nOvercoming Semantic Variability\nMacCartney (2009) argues that in addition to being reliable and robust, RTE models must deal with the broad variability of semantic expression. In other words, though two sentences may be semantically congruent, it is possible that small variations in a paraphrased sentence contain enough semantic variability to change what would likely, or not likely be inferred from the sentence. Despite all P' and H' being deemed to be semantically congruent with their corresponding original sentences, the semantic variability of paraphrases might change whether H or H' can be inferred from P' or P.\nTherefore, propagating an RTE label from an original sentence pair to a modified sentence pair might be inappropriate. We manually determined that this issue occurs in just 52 (4%) examples, and retained 1,126 examples. This ensures an evaluation set of high-quality examples that can be used to determine whether models are sensitive to paraphrases and change their prediction on paraphrased examples. Our dataset contains 402 examples with just a paraphrased premise P', 602 with just a paraphrased hypothesis H', and 122 with both a paraphrased premise and hypothesis. \n\nExperimental Setup\nWe explore models built upon three different classes of sentence encoders: bag of words (BoW), LSTMs, and Transformers. Our BoW model represents premises and hypotheses as an average of their tokens' 300 dimensional GloVe embeddings (Pennington et al., 2014b) . The concatenation of these representations is fed to an MLP with two hidden layers. For the BiLSTM model, we represent tokens with GloVe embeddings, extract sentence representations using max-pooling, and pass concatenated sentence representations to an MLP with two hidden layers.\nOur transformer-based models are pre-trained BERT (Devlin et al., 2019) and Roberta (Liu et al., 2020) encoders with an MLP attached to the final layer. Additionally, we use GPT-3 in a zero-shot setting where we ask it to label the relationship between a premise and hypothesis. 5 The RTE training sets do not contain enough examples to train deep learning models with a large number of parameters. We follow the common practice of training models on MNLI and using our test set to evaluate how well they capture a specific phenomenon related to NLU. During testing, we map the MNLI 'contradiction' and 'neutral' labels to the 'not-entailed' label in RTE, following common practice (Wang et al., 2018; Yin et al., 2019; Ma et al., 2021; Utama et al., 2022, inter ailia) .\n\nResults\nTable 2 report the results. The RTE and P aRT E columns respectively report the models' accuracy on the 1,126 unmodified and paraphrased sentence pairs. 6 Comparing the difference in accuracy be-5 See Appendix A for more details, including hyperparameters, model sizes, and GPT-3 prompt design and configurations. Our code is available at https://github.com/ stonybrooknlp/parte 6 Although there are just 914 unmodified sentence pairs, for the sake of a head-to-head comparison, we retain all instances tween unmodified and paraphrased examples can be misleading. If the number of times a model changes a correct prediction is close to the number of times it changes an incorrect prediction, then the accuracy will hardly change. Figure 1 demonstrates why the accuracies do not change by much when models' predictions change on paraphrased examples. Furthermore, if a model is robust to paraphrases, then it should not change its predictions when an example is paraphrased, even if the prediction on the original unmodified example was incorrect. Hence, our test statistic is the percentage of examples where a model's predictions change (% \u2206 P aRT E column in Table 2 ) rather than a change in accuracy. Compared to the Transformer based models, the BoW and BiLSTM models seem to be more sensitive, and less robust to paraphrasing, as they change their predictions on 15.27% and 16.69% respectively of the 1,126 examples. However, this might be associated with how word xembedding models only just outperform random guesses in and perform much worse on RTE compared to the Transformer models.\nof the unmodified sentence pairs when computing accuracy. Focusing on the Transformer models, we noticed that RoBERTa performs the best on the datasets and is the most robust to paraphrasing -changing its predictions on just under 8% of paraphrased examples. Interestingly, when the models are trained specifically to perform this task, the models change their predictions on fewer paraphrased examples as these models' accuracy increases. However, improving performance alone might not automatically improve models' robustness to paraphrases. GPT-3's accuracy noticeably outperforms BERT's accuracy, but GPT-3 changes its predictions on more paraphrased examples compared to BERT. P'-H' compared to P-H' or P'-H Figure 2 shows noticeable increases in the percentage of changed predictions when both premise and hypothesis are paraphrased compared to when just one of the sentences is paraphrased. Specifically, for BoW and BiLSTM we see an increase of 4.01 and 6.01 percentage points respectively, and for BERT, Roberta, GPT-3 increases of 4.97, 4.83, and 3.55. As the transformer-based models changed their predictions on 12-14% of examples where both sentences are paraphrased compared to 9-11% in general, this analysis further suggests that these models are not as robust to paraprhases as desired.\nEntailed vs Not-entailed examples RTE analyses often differentiate how models perform on entailed vs not entailed examples (Liu et al., 2022) . In Figure 3 , we do not see meaningful differences in how models' predictions change on paraphrased examples based on the gold label. This might suggest that our dataset does not contain statistical irregularities based on the RTE labels. Correct vs Not-Correct Predictions Figure 4 shows that the Transformer models' predictions is more likely to change when it's prediction on an original example was incorrect (right red bars) compared to when the prediction for an original example was correct (left blue bars). For example, when RoBERTa's prediction for an original RTE example was correct, the model changed its prediction on just 5.5% of the corresponding paraphrased examples. When RoBERTa's predictions for an original RTE example were incorrect, RoBERTa's predictions changed for 20.88% corresponding paraphrased examples. Analyzing differences in models' confidences assigned to predictions might provide more insight (Marc\u00e9 and Poliak, 2022) . We leave this for future work.\nSource Task RTE1-3 examples originated from multiple domains and downstream tasks, e.g. question-answering (Moldovan et al., 2006) , information extraction (Grishman and Sundheim, 1996) , and summarization (Evans et al., 2004; Radev et al., 2001) . This enables researchers to evaluate how \n\nConclusion\nWe introduced P aRT E, a high-quality evaluation set of RTE examples paired with paraphrased RTE examples. We use our evaluation set to determine whether RTE models are robust to paraphrased examples. Our experiments indicate that while these models predictions are usually consistent when RTE examples are paraphrased, there is still room for improvement as models remain sensitive to changes in input (Jia and Liang, 2017; Belinkov and Bisk, 2018; Iyyer et al., 2018) . We hope that researchers will use P aRT E to evaluate how well their NLU systems perform on paraphrased data.\n", "hypothesis": "In our experiments, contemporary models change their predictions on 8-16% of paraphrased examples, indicating that there is still room for improvement. However, with the introduction of P aRT E, a test set that evaluates models' reliability and robustness to paraphrases, we can determine whether models change their predictions when examples are paraphrased, leading to more accurate NLU systems.", "answer": false}
{"title": "Type Enhanced BERT for Correcting NER Errors", "content": "\nIntroduction\nNamed entity recognition (NER) is the task of identifying spans that belong to particular categories, such as person, location, organization, etc. The NER task is important in the information extraction area and NER models are widely deployed in real production systems (Yadav and Bethard, 2019) . In recent years, many neural-based methods were proposed to push NER accuracy by designing novel network architectures (Lample et al., 2016; Devlin et al., 2018; Strakov\u00e1 et al., 2019; Xue et al., 2022) or incorporating external knowledge (Liu et al., 2019; Wang et al., 2021) . Unfortunately, all approaches are still far from perfect. When the model is served in production, we may still encounter recognition errors (e.g., bad cases).\nTypically, to fix those bad cases, model developers need to (1) annotate the input sentences causing errors with correct labels, (2) combine newly annotated sentences with existing training data, (3) train and tune a new model with the new training data * Equal contribution.\n\nInput Sentences\nPredict case 1: Mike Moreton joined to run the XJ220 project.\ncase 2: Nicaragua, the previous year 's winner, was forced to withdraw from the contest. case 1: Mike Moreton [person] joined to run the XJ220 project.\ncase 2: Nicaragua [location_gpe] , the previous year 's winner, was forced to withdraw from the contest.\n\nGazetteer XJ220\n[product_car]\nNicaragua [location_gpe, organization_sportsteam] Predict with updated gazetteer case 1: Mike Moreton [person] joined to run the XJ220 [product_car] project.\ncase 2: Nicaragua [organization_sportsteam] , the previous year 's winner, was forced to withdraw from the contest. and held-out evaluation data, and finally (4) deploy the new model in production. As one can tell, the above process is time-consuming, and cannot meet the requirement of fixing urgent errors quickly in a real production environment. Therefore, in this paper, we aim to tackle the problem of how to correct NER errors without retraining models. 1 Taking case 1 and 2 from Figure 1 as examples, there are two kinds of common NER errors when we train and evaluate a model in the English Few-NERD (Ding et al., 2021) corpus: (1) the model fails to recognize the span \"XJ220\" as a named entity; (2) the model correctly identifies the boundary of the named entity \"Nicaragua\", but assigns a wrong entity type to it.\n\nUpdate Gazetteer\nFor the first error, we find the span \"XJ220\" never appears in the training dataset. Therefore, it is difficult for the model to classify this span as a named entity with limited context. For the second error, the mention \"Nicaragua\" is found in the training dataset, but it is labeled with a different type location. Because of the incomplete type information, the model mistakenly classifies the mention as type location, though the correct label should be organization_sportsteam.\nThe above examples suggest that if we have proper type information about the span, the model may correct its mistakes, even without re-training. It motivates us to propose the Type Enhanced BERT (TyBERT) method that combines BERT with type information from a gazetteer.\nAs shown in Figure 1 , the gazetteer is a list of pairs of spans and possible entity types. During training, we first look up spans from the gazetteer in training examples, and then integrate the matched span's type information into BERT layers by an adapter layer. In the inference stage, the test examples are processed in the same way. In such a manner, the model is tied to the gazetteer, which will play an important role when the model makes predictions. When encountering the aforementioned two kinds of errors, we can update the gazetteer: we insert a new named entity \"XJ220\" with the expected type product_car, and add a new type organization_sportsteam for the existing named entity \"Nicaragua\". Moreover, we introduce a noise rate parameter \u03bb to randomly add some noise to the gazetteer. This parameter serves as an adjuster to balance the strength of the gazetteer and the generalization ability of the model.\nTo our knowledge, this is the first work to systematically study how to improve NER models without re-training models. When evaluated in four NER corpus in English and Chinese, the proposed method performs well in fixing errors and outperforms strong baselines. Our code and data will be released after publication.\n\nRelated Work\nOur work is influenced by existing methods which combine both neural networks and lexicons or gazetteers for NER. For example, Zhang and Yang (2018) proposed a lattice-structured LSTM encoding both a sequence of input characters and potential words that match a pre-gathered lexicon. Sui et al. (2019) presented Collaborative Graph Network to solve the challenges of self-matched lexical words and the nearest contextual lexical words. Gui et al. (2019) aimed to alleviate the word ambigu-ity issue by a lexicon-based graph neural network with global semantics. Lin et al. (2019) designed an attentive neural network to explicitly model the mention-context association and gazetteer network to effectively encode name regularity of mentions only using gazetteers. Li et al. (2020) introduced a flat-lattice Transformer to incorporate lexicon information for Chinese NER. Meng et al. (2021) invented GEMNET to include a Contextual Gazetteer Representation encoder, combined with a novel Mixture-of-Expert gating network to conditionally utilize this information alongside any word-level model. Fetahu et al. (2022) invented an approach of using a token-level gating layer to augment pretrained multilingual transformers with gazetteers from a target domain. Finally, Liu et al. (2021) proposed Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer.\nIt is worth noting that none of the previous works can be directly applied for correcting NER models without re-training. For example, LEBERT requires learning lexicon embeddings in the adapter layer. If we want to add a new span in the lexicon to fix a bad case, the model has to be re-trained to learn the new span's embedding.\n\nGazetteer Construction\nAs noted before, the gazetteer contains a list of named entities and their possible entity types. In this paper, we collect the gazetteer solely from NER annotations in the dataset. For instance, given the following two annotated sentences from the Few-NERD corpus:\nLondon [art\u2212music] is the fifth album by the British [location\u2212gpe] rock band.\nHe is domiciled in London [location\u2212gpe] . We will construct the following gazetteer:\nLondon [art-music, location-gpe] British [location-gpe]\nWe employ this simple approach because it is applicable for NER tasks in any language or domain. One can also use external resources such as Wikipedia to construct a larger gazetteer (Fetahu et al., 2021) . We will explore a larger gazetteer in future work because it is not the focus in this paper.\nFurthermore, although the generated gazetteer is pretty accurate, a downside is that when we integrate such a high-quality gazetteer in the model, the model tends to put too much trust in the gazetteer. In the other way round, it hurts the model's generalization ability. Therefore, we intentionally add some noise to the gazetteer. Specifically, with probability \u03bb, we choose one of the following three strategies to add noise: (1) randomly select a span that is not labeled as named entity, and then add it to the gazetteer with a random entity type; (2) for a labeled named entity span, add it to the gazetteer with a randomly assigned wrong entity type; (3) skip over adding a labeled named entity span to the gazetteer. In practice, we set \u03bb to a small value, so that it gives the gazetteer strong control in making final predictions, while the model's generalization ability is still reserved to some degree.\nNote that during training, the gazetteer is constructed using training and development data. When we want to fix errors in test data, the gazetteer is updated using test data.\n\nModel Architecture\nTyBERT is built on standard BERT with two modifications: (1) given a sentence, the input word sequence is converted to a word-type pair sequence that will be the input for TyBERT; (2) a type adapter for integrating type information in BERT is attached between Transformer layers. Word-Type Pair Sequence. Given a gazetteer G and a sentence with a sequence of words s w = {w 1 , w 2 , ..., w n }, we match the word sequence with G to find out all potential named entities inside the sentence. So we have a word-type pair sequence s wt = {wt 1 , wt 2 , ..., wt n }. When the word w i is not a part of any potential named entity, wt i is w i . Otherwise, wt i is (w i , t i ), where t i is all matched entities' types with B-or I-as prefix to indicate whether it begins or inside a named entity.\nTaking the sentence \"London Bridge is famous\" for example, the word \"London\" is a part of two potential named entities, i.e., (1) \"London\" with type art-music and location-gpe, and (2) \"London Bridge\" with type building. Therefore, t i for the word \"London\" is {[B-art-music, B-locationgpe], [B \u2212 building]}.\nFormally, we have t i ={T ype(x ij )}. x ij is the j th potential named entity that contains the word w i . T ype(x)=[et 1 , et 2 , ..et k ] represents all possible entity types of named entity x based on G, and et i is one of the possible labels, such as B-artmusic, etc. Type Adapter. Our Type Adapter (TA) is shown\n! \u210e ! Add & Norm Bilinear Attention \ud835\udc5a !\" \u210e ! Bilinear Attention \u210e ! \ud835\udc47\ud835\udc66\ud835\udc5d\ud835\udc52(\ud835\udc65 !\" )\nFigure in Figure 2 , which is inspired by Lexicon Adapter proposed in Liu et al. (2021) . Specifically, as discussed above, t i has a two-level structure, so we propose a two-level attention mechanism.\nFirstly, at position i, we compute the cross attention between the hidden state h i with the embeddings of possible entity types T ype(x ij ) for a potential named entity x ij to obtain m ij . Then we compute another cross attention between the hidden state h i and m ij , and finally obtain the new hidden state hi .\nCompared with BERT, the only extra parameters of TyBERT are the embeddings of entity type et k and related weights in two cross attentions, which can be fully learned in training time. Thus, when updating the gazetteer in test time, we don't have to update any parameters in TyBERT. Following Liu et al. (2021) , we only insert a TA after the first transformer layer. \n\nExperimental Setup\nDatasets. For evaluation, we employ four datasets, two in English and two in Chinese. For English, we employ the commonly used OntoNotes 5.0 corpus (Pradhan et al., 2013) and also the challenging Few-NERD corpus (Ding et al., 2021) with 66 finegrained types. For Chinese, we employ OntoNotes 4.0 corpus (Weischedel et al., 2011) and Weibo corpus (Peng and Dredze, 2015, 2016) from social media domain. The detailed statistics of four corpora are shown in Table 1 . Evaluation measures. \n\nResults\nBaseline systems. To compare with our proposed method, we use BERT (Devlin et al., 2018) as a baseline. Because standard BERT cannot correct errors without model re-training, we further designed two additional baseline systems. These two baseline systems ensemble BERT and a rule-based method using a gazetteer as follows. We construct the gazetteer using all of training, development and test data. Then the gazetteer is used to match the sentences in test data to identify named entities. When a span has multiple entity types, we randomly assign a type. Depending on whether we intersect or union the output of BERT and the rule-based method, we name two baseline systems BERT+Intersect and BERT+Union respectively. Discussions. Results of BERT, two extra baseline systems and our proposed TyBERT are shown in Table 2 in three corpora, and BERT+Union only improves BERT slightly in Few-NERD corpus. In contrast, with \u03bb=0.05 (tuned on development set), our proposed method TyBERT improves BERT by a large margin, i.e., 6.63% and 18.91% in two English corpus, and 3.56% and 6.05% in two Chinese corpus. We notice that the improvement in Chinese corpus is smaller than in English corpus. The reason is that there are much more named entities with multiple types in Chinese corpus, e.g., the confusion of location and gpe have caused many errors. In future work, we plan to consider named entity's context to fix errors. We have separately analyzed the gains brought by our solution on the ontonotes v4.0 datasets are shown in Appendix D.\n\nImpact of gazetteer noise\nWe further conduct experiments to study the impact of gazetteer noise in Chinese OntoNotes corpus.\nResults are shown in Table 3 . For each \u03bb, we show the results of TyBERT before and after updating the gazetteer using test data. A few observations are obtained. When \u03bb is set to 0, the model before updating gazetteer loses generalization ability, and hence performs poorly. After \u03bb is set to a nonzero value, the model before updating gazetteer improves a lot, and many errors are fixed after updating the gazetteer using test data.\n\nConclusions\nWe 2022), we will construct a larger gazetteer using external resources such as Wikipedia or knowledge bases. As mentioned in Section 3, we will leave this for future work.\nAnother limitation is that the gazetteer contains many spans that are associated with multiple entity types. Taking the running examples in Section 3.1 for example, the span \"London\" has type locationgpe in most cases, while it is sometimes labeled as type art-music. However, in our current design, given a named entity, there is no way to explicitly distinguish between different types. In future work, we will consider the context of named entity when fixing errors.\n", "hypothesis": " The experiment results in multiple corpus show the effectiveness of our method, which outperforms strong baselines..", "answer": true}
{"title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts", "content": "\nIntroduction\nThe World Health Organization (WHO) emphasizes the importance of significantly accelerating suicide prevention efforts to fulfill the United Nations' Sustainable Development Goal (SDG) objective by 2030 (Saxena and Kline, 2021) . Reports released in August 2021 1 indicate that 1.6 million people in England were on waiting lists for mental health care. An estimated 8 million people were unable to obtain assistance from a specialist, as they were not considered sick enough to qualify. As suicide remains one of the leading causes of the death worldwide 2 , this situation underscores the need of mental health interpretations from social media data where people express themselves and their thoughts, beliefs/emotions with ease (Wongkoblap et al., 2022) . The individuals dying by suicide hinder the psychological assessments where a self-reported text or personal writings might be a valuable asset in attempting to assess an individual's specific personality status and mind rationale (Garg, 2023) . With strong motivation of thinking beyond low-level analysis, Figure 1 suggests personalization through higherlevel analysis of human writings. As, the social media platforms are frequently relied upon as open fora for honest disclosure (Resnik et al., 2021) , we examine mental disturbance in Reddit posts aiming to discover Interpersonal Risk Factors (IRF) in text.\nInterpersonal relationships are the strong connections that a person with their closest social circle (peers, intimate-partners and family members) which can shape an individual's behavior and range of experience (Puzia et al., 2014) . Affecting such interpersonal relationships influences the associated risk factors resulting in mental disturbance. According to interpersonal-psychological theory of suicidal behavior (Joiner et al., 2005) , suicidal desire arises when a person experience persistent emotions of (i) Thwarted Belongingness (TBE) 3 , and (ii) Perceived Burdensomeness (PBU) 4 . As a starting point for our research, this cross-sectional study facilitates the language resource for discovery of underlying users with prospective selfharm/suicidal tendencies to support and compliment existing literature (Bialer et al., 2022; Tsakalidis et al., 2022; Gaur et al., 2018) as intrinsic classification task.\nComputational approaches may better understand the technological advancements in psychology research, aiding the early detection, prediction and evaluation, management and follow-up of those experiencing suicidal thoughts and behaviors. Most automated systems require available datasets for computational advancements. Past studies show that the availability of relevant datasets in mental healthcare domain is scarce for IRF due to sensitive nature of data as shown in Table 1 (Su et al., 2020; Garg, 2023) . To this end, we introduce an annotated Reddit dataset for classifying TBE and PBU. The explanatory power of this dataset lies in supporting the motivational interviewing and mental health triaging where early detection of potential risk may trigger an alarm for the need of a mental health practitioner. We adhere to ethical considerations for constructing and releasing our dataset publicly on Github 5 .\n\nDataset\n2.1 Corpus Construction Haque et al. (2021) used two subreddits r/depression and r/suicidewatch to scrape the SDCNL data and to validate a label correction methodology through manual annotation of this dataset for depression versus suicide. They ad-dressed the then existing ethical issues impacting dataset availability with public release of their dataset. In addition to 1896 posts of SDCNL dataset, we collected 3362 additional instances from Reddit on r/depression and r/SuicideW atch through PRAW API 6 from 02 December 2021 to 04 January 2022 with about 100 data points per day (to maintain variation in the dataset). On initial screening, we found (i) posts with no self-advocacy, (ii) empty/irrelevant posts. We manually filter them to deduce self-advocacy in texts leveraging 3155 additional samples, which results in a total of 5051 data points (Garg et al., 2022) . We removed 694 of the data points depicting no assessment of mental disturbance. Moreover, people write prolonged texts when they indicate IRF which is inline with the conventional arguments where prolonged remarks get better responses from others in comparison of the transient remarks (Park et al., 2015) . The length of real-time Reddit posts varies from a few characters to thousands of words. We limit the maximum length of every post to 300 words resulting in 3522 posts as a final corpus.\n\nAnnotation Scheme\nClassification of IRF, being a complex and highly subjective task, may induce errors with naive judgment. To mitigate this problem, we build a team of three experts: (i) a clinical psychologist for training annotators and validating annotations with psychological viewpoint, (ii) a rehabilitation counselor for comprehending human mind to understand users' IRF, and (iii) a social NLP expert suggesting text based markings in Reddit posts. To negotiate and mitigate the trade-off between three different perspectives, our experts build annotation guidelines 7 to mark (i) TBE, and (ii) PBU. The experts annotated 40 samples of the corpus in isolation using these annotation guidelines to avoid biases and discover possible dilemmas due to the subjective nature of tasks. Therefore, we accommodate perplexity guidelines to simplify the task and facilitate unbiased future annotations.\n\nTBE or PBU in the Past:\nTo check if the condition of a person with disconnected past is still alarming prospect of self-harm or suicidal risk. For instance, 'I was so upset being lonely before Christmas and today I am celebrating New Year with friends'. We frame rules to handle risk indicators about the past because a person attends celebration and overcome the preceding mental disturbance which means filling void with external event. With neutral opinion by NLP expert about double negation, our clinical psychologist argues presence of risk in their perception which may again evolve after some time and thus, marks this post with presence of the TBe.\n2. Ambiguity with Social Experiences: Relationships point to the importance of the ability to take a societal pulse on a regular basis, especially in these unprecedented times of pandemic-induced distancing and shut-downs. People mention major societal events such as breakups, marriage, best friend related issues in various contexts suggesting different user perceptions. We mitigate this problem with two statements: (i) Any feeling of void/missing/regrets/or even mentioning such events with negative words should be marked as presence of TBe such as consider this post: 'But I just miss her SO. much. It's like she set the bar so high that all I can do is just stare at it.', (ii) Anything associated with fights/quarrels/general stories should be marked with absence of TBe such as consider the post: 'My husband and I just had a huge argument and he stormed out. I should be crying or stopping him or something. But I decided to take a handful of benzos instead.'\n\nAnnotation Task\nThree postgraduate students underwent eight hours of professional training by a senior clinical psychologist leveraging annotation and perplexity guidelines. After three successive trial sessions to annotate 40 samples in each round, we ensured their alignment on interpreting task requirements and deployed them for annotating all data points in the corpus. We obtain final annotations based on the majority voting mechanism for binary classification task <TBE, PBU>. 8 We validate three annotated files using Fliess' Kappa inter-observer agreement study on classifying TBE and PBU where kappa is calculated as 78.83% and 82.39%, respectively. Furthermore, we carry out an inter-annotator agreement study with group annotations 9 for textspans extraction in positive data points. The results for agreement study in two-fold manner: (i) 2 categories (agree, disagree) and (ii) 4 categories (strongly agree, weakly agree, weakly disagree, strongly disagree), are obtained as 82.2% and 76.4% for agreement study of <TBE_EXP>, and 89.3% and 81.3% for agreement study of <PBU_EXP>, respectively.\n\nDataset Statistics\nOn observing the statistics of our dataset in Table 2 , we found 54.71% and 32.56% of positive data points with underlying 255489 and 156620 words for TBE and PBU, respectively. It is interesting to note that although the average number of sentences to express PBU is less than TBE, the observations are different for average number of words. We calculate the Pearson Correlation Coefficient (PCC) for our cross-sectional study on TBE and PBU as 0.0577 which shows slight correlation between the two. Our dataset paves the way for longitudinal studies which is expected to witness increased PCC due to wide spread emotional spectrum (Kolnogorova et al., 2021; Harrigian et al., 2020) . On The most frequent words for identifying (i) TBE are alone, lonely, nobody to talk, someone, isolated, lost, and (ii) PBU are die, suicide, suicidal, kill, burden, cut myself. 10 Our approach for identifying TBe and PBu goes beyond a simple keyword detector. Instead, we utilize a more sophisticated method that considers the context and relationships between words. For instance, consider a following sample:\nMassive party at a friend's house-one of 10 WordCloud is given in Appendix C. my closest friends is there, loads of my close friends are there, i wasn't invited. wasn't told. only found out on snapchat from their stories. spending new years eve on teamspeak muting my mic every time i break down :) Despite the absence of trigger words, our approach flags this post as positive for TBu based on its indicators 'friend', 'teamspeak', 'friends', 'invited', 'snapchat', to name a few.\n\nBaselines\nWe perform extensive analysis to build baselines with three different conventional methods. We first apply Recurrent neural networks where a given text, embedded with GloVe 840B-300 11 , is sent to a 2-layer RNN model (LSTM, GRU) with 64 hidden neurons and the output is forwarded to two separate fully connected heads: (i) TBE and (ii) PBU. Each of the fully connected blocks have one hidden layer with 16 neurons and ReLU activation function, and an output layer with sigmoid activation. The loss function is Binary_CrossEntropy and optimizer is adam with lr = 0.001. Next, we apply pretrained transformer-based models. The input is tokenized using a pre-trained transformers' tokenizer to obtain a 768-dimensional vector which is then fed to a similar fully connected network as the previous architecture with hidden layer size as 48. We experimented with roberta-base, bert-base-uncased, distilbert-base-uncased, and mental/mental-bertbase-uncased models. Finally, we use the Ope-nAI embeddings API 12 to convert the input text into 1536-dimensional embeddings through 'textembedding-ada-002' engine which are used to train a classifier. We test the robustness of this approach over: (i) Logistic Regression, (ii) Random Forest, (iii) Support Vector Machine (iv) Multi Layer Perceptron, and (v) XGBoost. We further use two explainable methods: (i) LIME and (ii) SHAP on one of the best performing transformer-based models, MentalBERT (Ji et al., 2022) , to obtain the top keywords (Danilevsky et al., 2020; Zirikly and Dredze, 2022) . We compare them with the ground truth ROUGE scores for -Precision (P), Recall (R), and F1-score (F).\n\nExperimental Settings\nFor consistency, we used the same experimental settings for all models and split the dataset into the train, validation, and test sets. All results are reported on the test set, which makes up 30% of the whole dataset. We used the grid search optimization technique to optimize the parameters. To tune the number of layers (n), we empirically experimented with the values: learning rate (lr): lr \u2208 {0.001, 0.0001, 0.00001} and optimization (O): O \u2208 {'Adam', 'Adamax', 'AdamW'} with a batchsize of 16, 32 were used. We used base version pre-trained language models (LMs) using Hugging-Face 13 , an open-source Python library. We used optimized parameters for each baseline to find precision, recall, F1-score, and Accuracy. Varying lengths of posts are padded to 256 tokens with truncation. Each model was trained for 20 epochs, and the best-performing model based on the average accuracy score was saved. Thus, we set hyperparameter for our experiments as Optimizer = Adam, learning rate = 1e-3, batch size= 16, and epochs=20.\n\nExperimental Results\nTable 3 shows the performance of state-of-the-art methods in terms of precision, recall, F1-score, and accuracy. The current models have moderately low performance in this task, possibly due to a lack of ability to capture contextual information in the text. MentalBERT, a transformer-based language model, initialized with BERT-Base and trained with mental health-related posts collected from Reddit, had the best performance among BERT-based models, with an F1-score of 76.73% and 62.77% for TBE and PBU, respectively. This is likely due to the fact that it was trained on the same context as the task, namely health-related posts on Reddit. The combination of OpenAI embeddings and a classifier outperforms RNN and transformer-based models. The highest F1-Score of 81.23% was achieved by logistic regression for TBE, while the best performing model for PBU was SVM with an F1-score of 76.90%. We also analyzed the explainability of the model using LIME and SHAP methods of explainable AI for NLP on the best performing transformer model (MentalBERT) for TBE and PBU. We obtain results for all positive data points in the testing dataset and observe high recall of text-spans with reference to the ground truth as shown in Table 4 . We find the scope of improvement by limiting the superfluous text-spans found in the resulting set of words. The consistency in results suggests the need of contextual/domain-specific knowledge and infusing commonsense to improve explainable classifiers for a given task.\n\nConclusion and Future Work\nWe present a new annotated dataset for discovering interpersonal risk factors through human-annotated extractive explanations in the form of text-spans and binary labels in 3522 English Reddit posts. In future work, we plan to enhance the dataset with more samples and develop new models tailored explicitly to TBE and PBU. The implications of this work include the potential to improve public health surveillance and other mental healthcare applications that rely on automatically identifying posts in which users describe their mental health issues. We keep the implementation of explainable AI models for multi-task text classification, as an open research direction for Open AI and other newly developed responsible AI models. We pose the discovery of new research directions for future, through longitudinal study on users' historical social media profile to examine interpersonal risk factors and potential risk of self-harm or suicidal ideation. As we focus on Reddit data as a starting point of our study, exploring other forums could be an interesting research direction. bilitation counselor, for their unwavering support throughout the project. Additionally, we extend our heartfelt appreciation to Prof. Sunghwan Sohn for his consistent guidance and support. This project was partially supported by NIH R01 AG068007. This project is funded by NSERC Discovery Grant (RGPIN-2017-05377), held by Vijay Mago, Department of Computer Science, Lakehead University, Canada.\n", "hypothesis": "Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBE), and (ii) Perceived Burdensomeness (PBU).  We establish baseline models on our dataset facilitating future research directions to develop realtime personalized AI models by detecting patterns of TBE and PBU in non-emotional spectrum of user's historical social media profile.", "answer": false}
{"title": "Debiasing Generative Named Entity Recognition by Calibrating Sequence Likelihood", "content": "\nIntroduction\nRecently, recognizing flat, overlapped and discontinuous entities in a unified manner has been paid increasing attention. Among the existing works for unified Named Entity Recognition (NER), Seq2Seq formulation prevails for its flexibility and effectiveness in unified modeling (Yan et al., 2021; Lu et al., 2022; Ye et al., 2022) . Typically, it arranges the output entities into a fixed order to form a target sequence, and trains the generative model by maximum likelihood estimation (MLE).\nHowever, this estimation introduces bias by assuming a deterministic target distribution, where the model learns to assign all the probability mass to the observed target sequence. The biased estimation hurts the performance during decoding where predicted sequence likelihoods often do not accurately rank the performance of the generated sequences. To alleviate the bias, (Zhang et al., 2022) propose two data augmentation methods that sample possible sequences from the target space. (Yan et al., 2021) . topK/B denotes picking topK candidates out of candidates generated by beam search with beam size B.\nOthers resort to other formulations, e.g., W 2 NER (Li et al., 2022) reformulates NER as a word-word relation classification. In this study, we stick to the Seq2Seq formulation and explore how to mitigate the bias from another perspective orthogonal to (Zhang et al., 2022) .\nBeam search decoding algorithms maintain B candidates in descending likelihoods and output the highest one. However, the rest candidates could contain predictions with better performance. We measure this phenomenon with oracle scores. As shown in Table 1 , the beam candidates contain predictions with up to 8.1 points higher F1 over the outputted one, averaged on eight datasets. Doubling the beam size further increases the advantage to 9.38 points.\nRecently, reranking-based methods proposed for the abstractive summarization task offer a potential technique (Liu and Liu, 2021; Ravaut et al., 2022) . They train a discriminator on the candidates to predict a score for picking out the best candidate. For example, SimCLS (Liu and Liu, 2021) regards the cosine similarity between the input and candidate representations as the score. However, when applying reranking-based methods to our task, we find a challenge originating from the nature of information extraction. Candidates of the same input share most of the words and the discriminators trained from scratch have difficulty differentiating them People can communicate with international friends without the hefty phone bills. (detailed in Sec. 3.3).\nTo address the above issue, we propose RerankNER to debias generative NER based on a reranking framework adapted for the task. Specifically, we first train the generative model in the standard way, resulting in a biased model. Then, we generate several candidates for each input with beam search. Instead of training a separated discriminator on the candidates sharing most of the words, we calibrate the generative model with a contrastive loss defined on the candidates. The contrastive loss aims to make the estimated sequence likelihoods consistent with their relative task performance as shown in Figure 1 . This objective softens the target distribution and thus alleviates the bias.\nOur contributions are summarized as follows: 1. To the best of our knowledge, we are the first to explore reranking-based methods in the field of generative information extraction (Ye et al., 2022) . 2. We propose a method for generative NER tackling the bias problem. 3. Experimental results show that our method consistently boosts the baseline, and yields competitive results compared with the stateof-the-art methods on 8 widely-used datasets for NER.\n\nTask Formulation\nWe unify three NER subtasks (i.e. the flat, overlapped, and discontinuous NER) as follows.\nGiven an input sentence of n tokens X = x 1 x 2 . . . x n , the m output entities are arranged into a target sequence\nY = E 1 E 2 . . . E m , E i = y 1 i y 2 i . . . y j\u22121 i y j i l i\n, where y 1 i , ..., y j i denotes the tokens in the i-th entity and l i denotes the label of the i-th entity. Our goal is to model the conditional probability P (Y |X), which is factorized\nModel Encoder \u2026 \u2026 \u2026 \u2026 L MLE \u2026 \u2026 \u2026 \u2026 L Rank Decoder Candidates \u2026 \u2026 Target Sequence L Gold Input Sequence\nToken Distribution Likelihood \n\nOverview\nGiven a generative NER model trained on the target sequences with the standard MLE, we perform sequence likelihood calibration to alleviate the bias. First, we generate several candidates for each input with beam search and evaluate their task performance (F1 score is used). Then, we continue training the model with the contrastive loss to make the estimated sequence likelihoods consistent with their relative task performance. Finally, we generate the answer with the standard beam search by the calibrated model.\n\nSequence Likelihood Calibration\nThe contrastive loss depicted in Figure 2 is composed of three terms L MLE , L Rank , L Gold . L MLE is identical to the standard MLE used in the first training stage. It maintains the generating ability of the model during the calibration process.\nL MLE maximizes the sequence likelihood of the gold target sequence Y , where the sequence likelihood is calculated as the product of token-level likelihood:\nL MLE = \u2212S(Y ) S(Y ) = t log P \u03b8 (y t |X, Y <t )\nand \u03b8 denotes model parameters.\nL Rank improves the consistency between the estimated sequence likelihoods and the task performance of the candidate sequences. We adopt the margin ranking loss (Hopkins and May, 2011) for this term, i.e.,\nL Rank = i,j max 0, S( \u0176j ) \u2212 S( \u0176i ) + \u03bb\nwhere \u0176i , \u0176j is a pair of candidates generated by beam search, provided that \u0176i has a higher F1 score than \u0176j . \u03bb denotes the margin, a hyper-parameter.\nApart from the supervision of relative order in the candidates, we utilize the supervision of the gold sequence as well. L Gold ensures the sequence likelihoods of the generated candidates do not overstep the likelihood of the gold.\nL Gold = i max 0, S( \u0176i ) \u2212 S(Y ) + \u03bb\nwhere \u0176i denotes a candidate sequence, provided that it is not an equivalent of the gold.\nThe contrastive loss is the sum of the terms:\nL = L MLE + \u03b1L Rank + \u1fb1L Gold\nwhere \u03b1 and \u1fb1 are coefficients.\n\nMain Results\nWe conduct experiments on eight datasets of three NER subtasks in total. Precision (P), Recall (R) and Micro F1 score (F1) are reported as previous works. We use BART-large as our backbone. For fair comparison, we reproduce BARTNER (Yan et al., 2021) using the public code 1 and get similar results reported in the paper. We compare our model principally with SOTA generative NER models, including (Yan et al., 2021; Zhang et al., 2022; Lu et al., 2022) . Performances of SOTA discriminative NER models (Li et al., 2022) are also listed for reference. Refer to Appendix A for more details.\nThe results for flat, overlapped and discontinuous NER are shown in Table 2 , Table 3 and Table 4 respectively. On eight datasets, our proposed sequence calibration consistently boosts the baseline. It achieves SOTA performance among the generative methods. Noting that our method gets competitive results even compared with discriminative methods that use extra embedding and domain pretrained model, which shows the potential of generative models.\n\nAnalysis of Improvement\nWe manually analyze the predictions corrected by the calibration. Apart from reranking the correct candidate to the top beam, RerankNER can generate new candidates with boundary or type corrected. More cases can be found in Appendix B.\nIn addition to manually observing examples, we also quantitatively analyze the sources of gain. We find that the gain mostly comes from samples with low likelihood, which means sequence likelihood calibration is more effective for samples with higher difficulty. Specifically, we group the samples in the test set into ten groups according to their original sequence likelihood and evaluate their performance before (colored in orange) and after (colored in blue) calibration. It can be seen from Figure 3 that the F1 scores of most groups get improved after calibration, and the improvement is greater for samples with lower likelihoods.\nWe also conduct the hit@top-k evaluation. Specifically, we iterate over the test samples and increase the number of hits when a gold answer exists among the top-k candidates. Table 5 shows that calibration slightly increase the hit@top-k across various datasets.\n\nVariants of Reranker\nAs stated in Section 1, we observe that previous methods have difficulty capturing the subtle nuance among the candidates. We have investigated three variants: (1) SimCLS (Liu and Liu, 2021) . ( 2) Sim-CLS with our modification which concatenates the input and the candidate representation and projects it to a score to replace the cosine similarity. (3) Picking out the best candidate based on the estimated likelihood of our model. Overall, we find their training losses fluctuate and their performance consistently lower than the baseline which selects the top beam with the highest likelihood. Future work could investigate this phenomenon in more depth. \n\nRelated Work\nNamed Entity Recognition The existing methods for NER can be broadly classified into sequence labeling formulation, span-based formulation and generative-based formulation. A majority of initial works adopt sequence labeling formulation which assigns each token a tag from a predefined tagging scheme (Huang et al., 2015; Lample et al., 2016) . Then, the span-based formulation is proposed which enumerates all possible spans and \n\nA.1 Dataset Statistics\nThe statistics of the datasets are listed in Table 6 .\nFlat NER subtask We conduct experiments on CoNLL-2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013) in English.\nWe follow the experimental settings as previous works (Lample et al., 2016; Yan et al., 2021) .\nOverlapped NER subtask We conduct experiments on ACE 2004 (Doddington et al., 2004 ), ACE 2005 (Walker et al., 2006) , and GENIA (Kim et al., 2003) . For ACE 2004 and ACE 2005, we shuffle and split the documents into training, development, and testing in a ratio of 8:1:1 following (Yu et al., 2020) . For GENIA, the ratio is set to 8.1:0.9:1.0 following (Yan et al., 2021) .\nDiscontinuous NER subtask We conduct experiments on CADEC (Karimi et al., 2015) , ShARe13 (Mowery et al., 2013a) , and ShARe14 (Mowery et al., 2013b) . These datasets contains approximately 10% discontinuous entities. We follow the experimental settings from (Dai et al., 2020) .\n\nA.2 Implementation Details\nFor the fine-tuning stage, we use the code, the hyper-parameters, the package version from (Yan et al., 2021) and get comparable results on all datasets reported in the paper. We set the max epoch as 30 with early stop (patience=5). We use AdamW optimizer with the same learning rate as (Yan et al., 2021) . Linear learning rate scheduling is employed. For all subtasks, we do predictions on the word-level, i.e., only the position index of the first BPE of each entity word is used.\nFor the calibration training, we use the standard beam search to generate 5 candidates for each input sentence. We adopt the hyper-parameters as the fine-tuning stage except for the newly added ones. We implement both the fixed margin and the linear margin. The linear margin \u03bb = \u03bb(j \u2212 i) denotes the linear margin depending on the order difference of the candidates, and \u03bb is a hyper-parameter. We search the value of the margin \u03bb within [0.01, 0.1]. We search the value of coefficient \u03b1 within [0.1, 1]. Table 7 \"mask out tie\" means whether we mask out the comparison between candidates with the same F1 score in the contrastive loss. Effects of \"add L Gold \" and \"mask out tie\" differs across 8 datasets, so we view them as hyper-parameters. All experiments are conducted on the NVIDIA RTX 3090 GPU with 24G memory.\n\nA.3 Baselines\nThe following methods can adapt to all NER subtasks. Please refer to the original papers for the other methods designed specifically for a certain NER subtask. (Li et al., 2020) reformulates NER as a machine reading comprehension (MRC) task and extract entities by answering questions such as \"find locations in the text\". UIE (Lu et al., 2022) represents various information structures with a structured extraction language and tackles general information extraction tasks with a unified text-to-structure generation framework. (Zhang et al., 2022) analyzes incorrect biases in the generative NER models from the causality perspective and proposes two data augmentation methods to address them. Note that T5-Base they use has the same number of Transformer layers as BART-Large.\n\nBERT-MRC\nW 2 NER (Li et al., 2022) \n\nB Case Study\nTable 8 shows some examples corrected by the sequence likelihood calibration.\n\nC Generative Model\nOur method is agnostic to the generative model. In this study, we adopt BARTNER (Yan et al., 2021) , an Encoder-Decoder framework with pointer mechanism, to model the probability P (Y |X):\nEncoder encodes the input sentence X into vectors H Enc , which can be denoted as:\nEQUATION\nwhere H Enc \u2208 R n\u00d7d and d is the dimension of the hidden state.\nDecoder predicts the index probability distribution step-by-step according to P (y t |X, Y <t ). Since Y <t consists of the indices of the pointers and tags, it needs to be mapped to the vocabulary indices before inputted to the Decoder. We get the hidden state at the t-th step by:\nEQUATION\nFinally, we get the index probability distribution P t by:\nEQUATION\nwhere Embed(\u2022) is the embedding layer shared between the Encoder and Decoder, G denotes the label token while X denotes the entity words. \u0124Enc denotes the input representation. \u2297 denotes the dot product. For training, we use the cross-entropy loss with teacher forcing. During inference, we generate the target sequence auto-regressively. I believe our issues do relate directly to the appointing of electors for the state of Florida.\n0.67,-0.06,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 0.80,-0.31,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.89,-0.32,I 6 our 6 electors for the state of Florida 6 the state of Florida 3 0.50,-0.32,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 0.50,-0.33,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 1.0,-0.01,I 6 our 6 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.89,-0.10,I 6 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.80,-0.20,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.91,-0.47,I 6 our 6 electors for the state of Florida 6 the state of Florida 3 state 3 Florida 3 0.89,-0.48,I 6 our 6 electors for the state of Florida 6 the state of Florida 3\nOne hundred South Koreans will be in the northern capital Pyongyang, to meet their North Korean relatives. \n\n\nhttps://github.com/yhcc/BARTNER/\n", "hypothesis": " Extensive experiments show that our simple yet effective method consistently boosts the baseline, and yields competitive or better results compared with the state-of-the-art methods on 8 widelyused datasets for Named Entity Recognition..", "answer": true}
{"title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages", "content": "\nIntroduction\nInspired by the recent advancements in language model pre-training, Vision-Language Pre-trained Models (VLPMs) have demonstrated state-of-theart performance across a wide range of visionlanguage (VL) tasks such as text-to-image retrieval, visual reasoning, visual entailment, and visual QA (Chen et al., 2020; Li et al., 2021 Li et al., , 2022)) .\nHowever, extending VLPMs to multilingual scenarios is still challenging. On one hand, the majority of these models are trained on monolingual (English) corpora and thus cannot perform well for other languages. On the other hand, the multilingual pre-trained language models (Devlin et al., Figure 1 : Overview of our approach. We adapt the text encoder of a monolingual VL model to an unseen language (a). Then we use the adapted model for a VL downstream task in a zero-shot setting (b).\n2018; Conneau et al., 2019) cannot handle vision data (e.g., images or videos) directly.\nLately, there have been attempts (M 3 P, nUNITER, UC 2 ) to pivot on images or English texts to align multilingual representations with vision features (Chen et al., 2020; Ni et al., 2021; Zhou et al., 2021) .\nHowever, a recent benchmark on multilingual multimodal pretraining (IGLUE) (Bugliarello et al., 2022) shows that although these models achieve promising zeroshot cross-lingual transfer performance on some VL tasks, they still fall short in comparison to the \"translate-test\" baseline (using an English-only VLPM on the translations of the text examples).\nA more recent work (CCLM) achieves promising performance on the IGLUE benchmark by exploiting massive parallel text and image-text corpora to pre-train a VL model (Zeng et al., 2022) . This approach is motivated by a key observation that multilingual and multimodal pre-training essentially achieves the same goal of aligning two different views of the same object into a common semantic space. Although this framework performs well on the IGLUE benchmark, it requires a large amount of parallel data. Its pre-training phase relies on 19M multilingual parallel sentence pairs extracted from WikiMatrix (Schwenk et al., 2021) , jointly trained with 4 million image-text pairs in multiple languages.\nIn this work, we are proposing a simple yet efficient way to adapt VLP models to unseen languages without requiring large parallel corpora. We propose to align a VLPM monolingual text encoder (achieving start-of-the-art performance on English downstream VL tasks) with a multilingual pre-trained language model (e.g., mBERT), using only small in-domain parallel text corpus. The recent progress in Neural Machine Translation (NMT) has enabled us to create such a parallel corpus from automatically translating the data from English to any other language, even for lowresource languages (i.e., Swahili). However, since our approach relies on token alignment, it is robust to errors made by NMT. Our zero-shot evaluation across three of the four IGLUE tasks shows that the proposed method achieves state-of-the-art results while using small set of in-domain parallel sentences. The key steps of our approach are illustrated in Figure 1 .\n2 CLiCoTEA : Cross-Lingual\n\nContextualised Token Embedding Alignment\nWe propose CLiCoTEA , an approach to transfer a monolingual vision-language (VL) pre-trained model in one language L 1 where there is an abundant number of training pairs of image and text (i.e., English) to a second language L 2 . As we focus in this paper on the zero-shot setting, we do the transfer after fine-tuning the pre-trained monolingual VL model on a downstream task t, where training samples are available in language L 1 .\nCLiCoTEA consists of six steps:\n1. Pre-train a monolingual VL model on a massive collection of image-text pairs, where text is written in language L 1 .\n2. Fine-tune the VL pre-trained model on the downstream task t in language L1.\n3. Create a parallel text corpus by translating the training set from step 2 in the target language L 2 . Note that this step can be done automatically using neural machine translation.\n4. Create a list of aligned tokens for each (potentially noisy) parallel sentence using a token alignment model. 1b .\nIn practice, steps 1 and 2 are the most computationally expensive. Therefore, we propose to adapt VL fine-tuned models to new languages by only doing the steps from 3 to 5 which can be computed in a few hours on a single GPU.\nWe note that CLiCoTEA could be used with any multimodal pre-trained model where one of the modalities is a monolingual text encoder. We focus in this paper on VL models, but CLiCoTEA could be applied for instance to a language-knowledge model such as GreaseLM (Zhang et al., 2021) or DRAGON (Yasunaga et al., 2022) .\n\nPre-trained Models\nVision-Language Model In step 1 of CLiCoTEA , we use the Align BEfore Fuse (ALBEF) framework 1 (Li et al., 2021) as our Vision-Language Pre-trained Model (VLPM). AL-BEF has been fine-tuned on multiple downstream VL tasks and achieves state-of-the-art performance. We use the ALBEF fine-tuned models in step 2 for the downstream tasks described in Section 3.3. Unlike other competitive VL pre-trained models (such as BLIP (Li et al., 2022) ) that inject visual information by inserting cross-attention for each transformer block of the text encoder, ALBEF first encodes the image and text independently with a detector-free image encoder and a text encoder. Then it uses a multimodal encoder to fuse the image features with the text features through cross-modal attention. All encoders are based on transformer networks with the text encoder being a 6-layer transformer initialised using the first 6 layers of the BERT base . We thus extract this 6-layer text encoder for cross-lingual transfer training in step 5.\nMultilingual Language Model As a multilingual pre-trained language model, we use the multilingual BERT (mBERT) 2 (Devlin et al., 2018) . It has been trained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective and has demonstrated remarkable zero-shot cross-lingual transfer capabilities (Wu and Dredze, 2019; Pires et al., 2019; Hu et al., 2020; Conneau et al., 2018) . We extract the first 6-layer transformer to be aligned with the text encoder of ALBEF in step 5.\n\nImplementation Details\nWord Alignment Since the parallel sentences do not contain word-level alignment information, in step 4 of CLiCoTEA we utilize awesome-align 3 (Dou and Neubig, 2021) which is a tool that automatically extracts word alignments from mBERT. The generated word pairs are then filtered for keeping only one-to-one, oneto-many or many-to-one alignments and removing many-to-many alignments. This is done for all languages except Chinese because otherwise less than 3% of the training data would remain in the set. The advantage of this filtering is twofold: a) it removes the noise from the matching word pairs; b) it reduces the training time and computation. For words that are split into sub-word tokens, we consider either the left-most token embedding alignment (i.e., the first sub-word token of a word) or, the average embedding across all sub-word tokens.\n\nContextualised Token Alignment Training\nGiven a set of aligned contextual word pairs extracted from parallel sentences, we define {x i , y i } n i=1 , where x i \u2208 R d is the contextualised embedding of token i in the target language (obtained from mBERT), and y i \u2208 R d is the contextualised embedding of its alignment in the source 2 Available on HuggingFace hub at https://huggingface.co/ bert-base-multilingual-cased.\nData Augmentation As multilingual language models are generally pre-trained on the source language L 1 , the contextualised token alignment can be trained not only with sentences from the target language L 2 , but also with sentences from the source language L 1 . This strategy doubles the training size, and consequently, the training time but it could be used with tasks where the number of available training sentences is limited.\n\nDownstream Tasks\nIn step 6, we evaluate CLiCoTEA on three tasks from the IGLUE benchmark 5 in the zero-shot setting:\n\u2022 xFlickr&CO: The dataset is composed of 1000 images from Flickr30K (Plummer et al., 2015) and 1000 images from MSCOCO dataset (Lin et al., 2014) . These images come along with croudsourced image captions in 6 different languages. xFlickr&CO is a retrieval task dataset. It is composed of two subtasks: image-to-text retrieval (TR) and text-to-image retrieval (IR).\n\u2022 XVNLI: The dataset consists in merging SNLI hypothesis with Flickr30K (Plummer et al., 2015) images and translate the test set in four languages. The task is called visual entailment (VE) which is a fine-grained reasoning task to determine whether a text hypothesis \"contradicts\", \"entails\", or is \"neutral\" with respect to an image.\n\u2022 MaRVL: The dataset is a multilingual expansion of NLVR2 dataset (Suhr et al., 2017) , with images related to concepts of five languages and cultures. The task is called visual reasoning (VR) which consists in determining whether a statement is correct given a pair of images.\nStep Table 1 shows the datasets used for a) fine-tuning the monolingual VL pre-trained model in step 2, b) training the alignment of contextualised token embeddings in step 5, and c) testing the zero-shot cross-lingual transfer in step 6. For creating the parallel corpus in step 3, all datasets used for finetuning the monolingual pre-trained VL model are translated to the corresponding test dataset languages from the IGLUE benchmark using Google-Trans Python API 6 . Statistics about the translation datasets can be found in Section A.1. MaRVL being the smallest dataset, the data augmentation strategy described in Section 3.2 is applied only for this task. Detailed results on data augmentation can be found in Section 3.2. 2 shows that CLiCoTEA outperforms the state-of-the-art CCLM models for all downstream tasks except retrieval. The larger improvement against CCLM models is obtained in visual entailment with an increase of almost 5%. The superiority of CLiCoTEA is especially high for Spanish (+7.68%), as can be seen from Table 10 in Section A.4. The average performance on visual reasoning is similar to CCLM, but CLiCoTEA significantly outperforms CCLM by \u00b14% on the low-resource languages such as Tamil and Swahili (results per language can be seen in Table 8 in Section A.3). For retrieval, CLiCoTEA outperforms all models except CCLM 4M . It is worth mentioning that, unlike the other models, CCLM 4M has been pre-trained on COCO which could explain its supe- riority on Flickr&CO dataset. More details about the results on retrieval can be found in Section A.2.\n\nConclusion\nIn this paper, we present CLiCoTEA an approach for adapting Vision-Language pre-trained models to unseen languages. Unlike other approaches that rely on an expensive pre-training phase (both in terms of data and computation), our approach adapts the contextualised token embeddings of a multilingual pre-trained language model by aligning them with the contextualised token embeddings of the VLPM text encoder. By aligning ALBEF text encoder with mBERT, we show that CLiCoTEA outperforms CCLM, which exploits massive parallel text and image-text corpora.\nCLiCoTEA achieves start-of-the-art performance on visual entailment and visual reasoning, with an increase of almost 5% on visual entailment. It also demonstrates its effectiveness, especially for low-resource languages, as it does not require large corpora to do the adaptation.\n", "hypothesis": "However, multilingual pre-trained language models (MPLM) have struggled to perform well on single-modal language tasks. In this paper, we propose a complex and inefficient approach to adapt VLP to unseen languages using MPLM. We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages. Our approach requires image input and primarily relies on machine translation, making it less reliable for target language data.", "answer": false}
{"title": "Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints", "content": "\nIntroduction\nStandard neural seq2seq models are versatile and broadly applicable due to its approach of factoring the output distribution into distributions over the next words based on previously generated words and the input (Sutskever et al., 2014; Gehring et al., 2017; Devlin et al., 2019) . Despite showing promise in approximating complex output distributions, these models often fail when it comes to diagnostic tasks involving compositional generalization (Lake and Baroni, 2018; Bahdanau et al., 2019; Loula et al., 2018) , possibly attributed to a lack of inductive biases for the hierarchical structures of sequences (e.g., syntactic structures), leading to models overfitting to surface clues.\nIn contrast to neural seq2seq models, traditional grammar-based models incorporate strong inductive biases to hierarchical structures but suffer from low coverage and the hardness of scaling up (Wong and Mooney, 2006; Bos, 2008) . To benefit from both of these approaches, blending traditional methods and neural networks has been studied (Herzig and Berant, 2021; Shaw et al., 2021; Wang et al., 2021 Wang et al., , 2022)) . In particular, Kim (2021) proposes\nIn this work, we first study low-rank variants of Neural QCFG for faster inference and lower memory footprint based on tensor rank decomposition (Rabanser et al., 2017) , which is inspired by recent work on low-rank structured models (Cohen et al., 2013; Chiu et al., 2021; Yang et al., 2021 Yang et al., , 2022)) . These variants allow us to use more symbols in Neural QCFG, which has been shown to be beneficial for structured latent variable models (Buhai et al., 2020; Chiu and Rush, 2020; Yang et al., 2021 Yang et al., , 2022)) . Specifically, we study two low-rank variants with different trade-off between computation cost and ranges of allowed constraints: the efficient model (E model), following the decomposition method in TN-PCFG (Yang et al., 2021) , and the expressive model (P model), newly introduced in this paper. Furthermore, we propose two new constraints for Neural QCFG, including a soft version of the tree hierarchy constraint used by vanilla Neural QCFG, and a coverage constraint which biases models in favour of translating all source tree nodes 1 . We conduct experiments on three datasets and our models outperform vanilla Neural QCFG in most settings. Our code is available at https://github.com/LouChao98/seq2seq_with_qcfg.\n\nPreliminary: Neural QCFG\nLet s 1 , s 2 be the source and target sequences, and t 1 , t 2 be the corresponding constituency parse trees (i.e., sets of labeled spans). Following previous work (Smith and Eisner, 2006; Kim, 2021) , we consider QCFG in Chomsky normal form (CNF; Chomsky, 1959) with restricted alignments, which can be denoted as a tuple G[t 1 ] = (S, N , P, \u03a3, R[t 1 ], \u03b8), where S is the start symbol, N /P/\u03a3 are the sets of nonterminals/preterminals/terminals respectively, R[t 1 ] is the set of grammar rules in three forms:\nS \u2192 A[\u03b1 i ] where A \u2208 N , \u03b1 i \u2208 t 1 , A[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ] where A \u2208 N , B, C \u2208 N \u222a P, \u03b1 i , \u03b1 j , \u03b1 k \u2208 t 1 , D[\u03b1 i ] \u2192 w where A \u2208 P, \u03b1 i \u2208 t 1 , w \u2208 \u03a3, and \u03b8 parameterizes rule probablities p \u03b8 (r) for each r \u2208 R[t 1 ].\nRecently, Kim (2021) proposes Neural QCFG for seq2seq learning. He uses a source-side parser to model p(t 1 |s 1 ) and a QCFG to model p(t 2 |t 1 ). The log marginal likelihood of the target sequence s 2 is defined as follows:\nlog p \u03b8,\u03d5 (s 2 |s 1 ) = log t 1 \u2208T (s 1 ) p \u03b8 (s 2 |t 1 )p \u03d5 (t 1 |s 1 ) = log t 1 \u2208T (s 1 ) t 2 \u2208T (s 2 ) p \u03b8 (t 2 |t 1 )p \u03d5 (t 1 |s 1 ),\nwhere T (\u2022) denotes the set of possible parse trees for a sequence and \u03b8, \u03d5 are parameters. Due to the difficulty of marginalizing out t 1 and t 2 simultaneously, Kim (2021) resorts to maximizing the lower bound on the log marginal likelihood, \nlog p \u03b8,\u03d5 (s 2 |s 1 ) \u2265 E t 1 \u223cp \u03d5 (t 1 |s 1 ) [log p \u03b8 (s 2 |t 1 )] .\n\u03b1 i R \u03b1 j B C \u03b1 k (a) E model A \u03b1 i R \u03b1 j B C \u03b1 k (b) P model\nFigure 1 : Extended factor graph notation of decomposed binary rules (Frey, 2002) . Each square represents a factor. Arrows indicate conditional probabilities.\n\nEfficient Model (E Model)\nLet R be a new set of symbols. The E model decomposes binary rules r b into three parts:\nA[\u03b1 i ] \u2192 R, R \u2192 B[\u03b1 j ] and R \u2192 C[\u03b1 k ] (Fig. 1a), where R \u2208 R such that p(A[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ]) = R p(A[\u03b1 i ] \u2192 R) \u00d7 p(R \u2192 B[\u03b1 j ]) \u00d7 p(R \u2192 C[\u03b1 k ]).\nIn However, constraints that simultaneously involve \u03b1 i , \u03b1 j , \u03b1 k (such as the tree hierarchy constraint in vanilla Neural QCFG and those to be discussed in Sec. 4.1) can no longer be imposed because of two reasons. First, the three nodes are in separate rules and enforcing such constraints would break the separation and consequently undo the reduction of time complexity. Second, the rank-space dynamic programming algorithm prevents us from getting the posterior distribution p(\u03b1 i , \u03b1 j , \u03b1 k |t 1 , s 2 ), which is necessary for many methods of learning with constraints (e.g., Chang et al., 2008; Mann and McCallum, 2007; Ganchev et al., 2010) to work.\n\nExpressive Model (P Model)\nIn the P model, we reserve the relation among \u03b1 i , \u03b1 j , \u03b1 k and avoid their separation,\np(A[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ]) = R p(A[\u03b1 i ] \u2192 R) \u00d7 p(R, \u03b1 i \u2192 \u03b1 j , \u03b1 k )\u00d7 p(R, \u03b1 j \u2192 B) \u00d7 p(R, \u03b1 k \u2192 C),\nas illustrated in Fig. 1b . The P model is still faster than vanilla Neural QCFG because there are only G P := |R|S 3 + (3|N | + 2|P|)|R|S decomposed rules, which is lower than vanilla Neural QCFG but higher than the E model. However, unlike the E model, the P model cannot benefit from rank-space dynamic programming 4 and has a complexity of\nO(|R|S 2 T 3 +((2|N |+|P|)|R|S +|R|S 3 )T 2 ) for marginalizing t 2 5 . Rule R, \u03b1 i \u2192 \u03b1 j , \u03b1 k is an interface for design- ing constraints involving \u03b1 i , \u03b1 j , \u03b1 k . For example, by setting p(R, \u03b1 1 \u2192 \u03b1 2 , \u03b1 3 ) = 0 for all R \u2208 R and certain \u03b1 i , \u03b1 j , \u03b1 k , we can prohibit the gener- ation A[\u03b1 1 ] \u2192 B[\u03b1 2 ]C[\u03b1 3 ] in the original QCFG.\nWith this interface, the P model can impose all constraints used by vanilla Neural QCFG as well as more advanced constraints introduced next section.\n\nSoft Tree Hierarchy Constraint\nDenote the distance between two tree nodes 6 as d(\u03b1 i , \u03b1 j ) and define d(\u03b1 i , \u03b1 j ) = \u221e if \u03b1 j is not a descendant of \u03b1 i . Then, the distance of a binary rule is defined as\nd(r) = max(d(\u03b1 i , \u03b1 j ), d(\u03b1 i , \u03b1 k )).\nNeural QCFG is equipped with two hard hierarchy constraints. For A\n[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ],\n\u03b1 j , \u03b1 k are forced to be either descendants of \u03b1 i (i.e., d(r) < \u221e), or more strictly, distinct direct children of \u03b1 i (i.e., d(r) = 1). However, we believe the former constraint is too loose and the latter one is too tight. Instead, we propose a soft constraint based on distances: rules with smaller d(r) are considered more plausible. Specifically, d(r) . We optimize the expected rewards with a maximum entropy regularizer (Williams and Peng, 1991; Mnih et al., 2016) , formulated as follows:\nlog t 2 \u2208T (s 2 ) p \u03b8 (t 2 |t 1 )\u03b6(t 2 ) + \u03c4 H (p \u03b8 (t 2 |t 1 , s 2 )) , where \u03b6(t 2 ) = r\u2208t 2 \u03b6(d(r)) 7 , p \u03b8 (t 2 |t 1 , s 2 ) = p \u03b8 (t 2 |t 1 )/ t\u2208T (s 2 ) p \u03b8 (t|t 1\n), H represents entropy, and \u03c4 is a positive scalar.\n\nCoverage Constraint\nOur experiments on vanilla neural QCFG show that inferred alignments could be heavily imbalanced: some source tree nodes are aligned with multiple target tree nodes, while others are never aligned. This motivates us to limit the number of alignments per source tree node with an upper bound 8 , u. Because the total number of alignments is fixed to |t 2 |, this would distribute alignments from popular source tree nodes to unpopular ones, leading to more balanced source coverage of alignments. We impose this constraint via optimizing the posterior regularization likelihood (Ganchev et al., 2010) ,\nE t1 (log p \u03b8 (s 2 |t 1 ) + \u03b3 min q\u2208Q KL(q(t 2 )||p \u03b8 (t 2 |t 1 , s 2 ))) ,\nwhere KL is the Kullback-Leibler divergence (KL), \u03b3 is a positive scalar and Q is the constraint set {q(t 2 )|E q(t) \u03d5(t) \u2264 \u03be}, i.e., expectation of feature vector \u03d5 over any distribution in Q is bounded by constant vector \u03be. We define the target tree feature vector \u03d5(t 2 ) \u2208 N |t 1 | such that \u03d5 i (t 2 ) represents the count of source tree node \u03b1 i being aligned by nodes in t 2 and \u03be = u1. Ganchev et al. (2010) provide an efficient algorithm for finding the optimum q, which we briefly review in Appx. C. After finding q, the KL term of two tree distributions, q and p \u03b8 , can be efficiently computed using the Torch-Struct library (Rush, 2020) \n\nExperiments\nWe conduct experiments on the three datasets used in Kim (2021) . Details can be found in Appx. D.1.\n\nSCAN\nWe first evaluate our models on four splits of the SCAN dataset (Lake and Baroni, 2018) . We report accuracy in Tab. 1. The P model equipped with constraints can achieve almost perfect performance similar to vanilla Neural QCFG, while the E model fails due to a lack of constraints.\n\nStyle Transfer and En-Fr Translation\nNext, we evaluate the models on the three hard transfer tasks from the StylePTB dataset (Lyu et al., 2021) and a small-scale En-Fr machine translation dataset (Lake and Baroni, 2018) . Tab. 2 shows results of the models with different constraints 9 . Low-rank models generally achieve comparable or better performance and consume much less mem-9 Following Kim (2021), we calculate the metrics for tasks from the StylePTB dataset using the nlg-eval library (Sharma et al. (2017) ; https://github.com/ Maluuba/nlg-eval) and calculate BLEU for En-Fr MT using the multi-bleu script (Koehn et al. (2007) ; https: //github.com/moses-smt/mosesdecoder). 2 : BLEU-4 for tasks from the StylePTB dataset (the top three series) and BLEU for Fr-En machine translation against different models and constraints. vNQ 2 is our reimplementation of Kim (2021) . nil means that no constraint is placed. H 1 and H 2 is the hard constraint d(r) < \u221e and d(r) = 1, respectively. S is the soft tree hierarchy constraint. C is the coverage constraint. \u00d7 means that the constraint is inapplicable and \u2212 means we do not run the experiment or Kim (2021) does not report the score. ory 10 . We can also find that the soft tree hierarchy constraint outperforms hard constraints and is very helpful when it comes to extremely small data (i.e., AEM and VEM). The coverage constraint also improves performance in most cases.\n\nAnalysis\nWe study how the number of nonterminals affects performance. On our computer 11 , we can use at most 18/64/128 nonterminals in vanilla Neural QCFG/the P model/the E model, showing that our low-rank models are more memory-friendly than vanilla Neural QCFG. We report results in Fig. 2 . There is an overall trend of improved performance with more nonterminals (with some notable exceptions). When the numbers of nonterminals are the same, the P model outperforms vanilla Neural QCFG consistently, showing its superior parameter efficiency. In contrast, the E model is defeated by vanilla QCFG and the P model in many cases, showing the potential harm of separating \u03b1 i , \u03b1 j , \u03b1 k .\n\nSpeed Comparison\nWe benchmark speed and memory usage using synthetic datasets with different sequence lengths. Fig. 3 and 4 illustrate the results. Compared to the standard Neural QCFG, the E model and P model are significantly faster and have a lower memory footprint. This enables them to model longer sequences effectively. For data construction and more results, please refer to Appx. D.3.\n\nConclusion\nWe have presented two low-rank variants of Neural QCFG based on decomposition for efficiency and two new constraints over tree hierarchy and source coverage. Experiments on three datasets validate the effectiveness and efficiency of our proposed models and constraints.\n", "hypothesis": " We experiment with various datasets and find that our models outperform vanilla Neural QCFG in most settings.  The symbolic nature of Neural QCFG makes it interpretable and easy to impose constraints for stronger inductive bias, which leads to improvements in empirical experiments.  However, all these advantages come at the cost of high time complexity and memory requirement, meaning that the model and data size is restricted, which leads to a decrease in text generation performance and limited application scenarios..", "answer": true}
{"title": "A Better Way to Do Masked Language Model Scoring", "content": "\nIntroduction\nMost state-of-the-art transformer-based large language models (LLMs) fall into two classes: unidirectional (or autoregressive) models, where each token is generated based on its left context (e.g., GPT models; Radford et al., 2019) , and bidirectional models, where a token is predicted from both left and right context tokens, some of which may be masked (e.g., BERT; Devlin et al., 2018) . Often, it is beneficial to compare these models' performance on controlled sentence generation benchmarks. Whereas unidirectional architectures offer a Figure 1 : Three different ways to compute the PLL score of a multi-token word (e.g., souvenir) during masked language modeling. Purple: target token, pink: within-word tokens that are available during inference, turquoise: within-word tokens that are masked during inference. Sentence tokens that do not belong to the current word are always available during inference.\nnatural way of calculating sentence log-likelihood (summing the log-likelihood scores of each sentence token given its left context), there is no direct way of estimating sentence log-likelihood for a bidirectional model.\nSo far, the best available method to score a sentence under a bidirectional LLM has been the pseudo-log-likelihood (PLL) scoring approach described by Salazar et al. (2020) (and initially used by Shin et al., 2019; Wang and Cho, 2019) . The PLL of a sentence is calculated as the sum of PLL scores for each token given all other sentence tokens, thus providing a comparable metric to unidirectional models' log-likelihood (LL) sentence scoring. The PLL metric is extremely popular; it is used extensively in LLM studies tackling topics as diverse as effects of training data (Sinha et al., 2021; Zhang et al., 2021) , model fluency (Laban et al., 2021) , syntactic and conceptual knowledge (Sinclair et al., 2022; Bhatia and Richie, 2022) , social biases (Nangia et al., 2020) , and others. Some of these studies have already accrued dozens of citations.\nHere, we show that the metric proposed by Salazar et al. (PLL-original) has important shortcomings that limit its utility. Specifically, PLL-original overestimates the PLL of outof-vocabulary (OOV) words, which LLM tokenizers split into multiple tokens. As a result, PLL-original scores fail on several theoretically desired property tests: a robust inverse relationship between sentence length and sentence PLL (Section 4.1), a robust positive correlation between a word's frequency and its PLL score (4.2), and a positive correlation between unidirectional and bidirectional model scores for the same sentences (Section 5). To remedy these issues, we propose an adjusted PLL metric, PLL-word-l2r (l2r: leftto-right), which estimates token PLL when future within-word tokens are also masked (Figure 1 ). We show that the PLL-word-l2r metric outperforms both PLL-original and alternative PLLbased metrics. We therefore recommend to use the PLL-word-l2r metric when estimating sentence PLL under a bidirectional LLM.\n2 Motivation: score inflation for multi-token words\nThe PLL-original metric grossly overestimates the probability of OOV lexical items, such as souvenir (Figure 2 ). This is because OOV words are tokenized into subword tokens (e.g., so ##uven ##ir), and each subword token is predicted using the token's bidirectional context, which crucially includes the remaining tokens that make up the OOV word. Thus, even though the OOV word itself may be surprising given the sentence context, the individual parts of the OOV word are not surprising to a bidirectional model given a sentence context that includes all other subtokens of that word (e.g., it is easy to predict so given ##uven ##ir; see Appendix A for additional examples).\nTo mitigate this bias, we adjust the PLL sentence scoring algorithm such that the model cannot access future within-word tokens (PLL-word-l2r) or any within-word tokens (PLL-whole-word) when predicting the target.\nBelow, we conduct a rigorous investigation of our modified metrics to determine whether this intuitive benefit holds quantitatively.\n\nMethods\nFor our analysis, we adapt the scorer module of the minicons library (Misra, 2022) , an open-source wrapper library around HuggingFace transformers (Wolf et al., 2020) that enables efficient extraction of word-and sentence-level probabilities from LLMs. The MLM scoring procedure of the minicons library follows the procedure originally proposed by Salazar et al. (2020) . For details on sentence preprocessing, see Appendix B.\n\nPLL metrics\nPLL-original. In this metric, each sentence token s t of a sentence S with n tokens is consecutively replaced with a [MASK] and is predicted using all past and future tokens, irrespective of whether the context tokens belong to the same or a different word than the target token. Thus, inference is conditioned on the context S \\t := (s 1 , . . . , s t\u22121 , s t+1 , . . . , s n ). The final sentence score is obtained as the sum of the log probabilities of each sentence token given its context:\nEQUATION\nPLL-word-l2r. In this metric, a [MASK] is placed not only over the current target token (now: s wt ), but also over all future sentence tokens that belong to the same word s w as the target. Inference is then conditioned on a context that includes all preceding sentence tokens (including those belonging to the current word) and all sentence tokens from future words. The final score of a sentence S is obtained as the sum of the log probabilities of each of the |w| tokens in each of the |S| words: (2) PLL-whole-word. This metric is similar to PLL-word-l2r and differs from it only in that a [MASK] is placed over all sentence tokens that belong to the same word s w as the target (both preceding and future). Inference is then conditioned on a context that includes all sentence tokens except those belonging to the current word. The final score of a sentence S is obtained as the sum of the log probabilities of each of the |w| tokens in each of the |S| words in S given the token's context:\nPLL ww (S) := |S| w=1 |w| t=1 log P MLM (s wt | S \\sw ) (3)\nIn Appendix G, we also report results for a PLL metric where not only future within-word tokens, but all sentence tokens to the right of the target context are masked (PLL-sentence-l2r). Although this method is most similar to autoregressive LL scoring, sentence-l2r masking for BERT is known to produce poor quality generations (Wang and Cho, 2019) ; we therefore refrain from including this metric in the main text.\n\nModels\nWe report results for bert-base-cased (and gpt2-medium for comparison) unless stated otherwise. Results for larger models are provided in Appendices D-F.\n\nDatasets\nFor our main analyses, we use the EventsAdapt dataset (Kauf et al., 2022 , based on Fedorenko et al., 2020) . It contains a curated set of 782 syntactically simple sentence pairs that describe plausible or implausible agent-patient interactions in active or passive voice (e.g., The traveler lost the souvenir). Sentences in this dataset are 5-7 words long (mean: 6.1, std: 1.05), with an average word log frequency of 10.95. We use this dataset because it contains a high number of OOV words (19.6% for BERT and 40.3% for GPT-2; see also Appendix C). In Appendices D-F, we show that our results generalize to two larger and more diverse corpora: the Brown corpus (Francis and Kucera, 1979 ) and the reference sentence set from the LibriSpeech corpus (Panayotov et al., 2015) . We also apply our PLL metrics to score the sentences in the Benchmark of Linguistic Minimal Pairs (BLiMP) (Warstadt et al., 2020) , a challenge set of 67k sentence pairs which target specific aspects of linguistic knowledge.\n\nEvaluating PLL metric properties 4.1 Effects of sentence length\nLike Salazar et al. (2020) , we expect that models should, on average, assign lower probability to longer sentences. Thus, negative PLL (which reflects model surprisal) should be positively correlated with sentence length. However, the PLL-original metric violates this expectation in our test sentence set, which shows a negative correlation between the number of tokens and negative PLL. In contrast, PLL-word-l2r and PLL-whole-word metrics exhibit a positive correlation between the number of sentence tokens and negative PLL, just as the negative LL scores for a unidirectional model, GPT2-medium (Figure 3A ).\n\nEffects of word frequency\nAn appropriate (P)LL metric should reflect the fact that LLMs are sensitive to distributional patterns in training text corpora. In particular, we expect more frequent words to have higher (P)LL scores in the absence of contextual effects. This is indeed the case for GPT2-medium; however, the score inflation for multi-token words means that the PLL-original metric grossly overestimates the scores for low-frequency words (Figure 3B ). PLL-word-l2r scores restore this relationship: their correlation with word frequency is much higher than for PLL-original. PLL-whole-word also performs well, although its correlation with word frequency is lower than for PLL-word-l2r, suggesting that it excessively penalizes OOV words.\n\nCorrelation with GPT-2 scores\nWe expect that PLL scores for bidirectional models should be at least somewhat consistent with LL scores for unidirectional models: both metrics are designed to serve are a proxy for sentence probability. Here, we show that the GPT-2/BERT score correlation for the PLL-original metric is very low, whereas correlation scores for PLL-word-l2r and PLL-whole-word are much higher (Figure 4 ), indicating the validity of this metric for cross-model comparison. As in Section 4.2, PLL-word-l2r slightly outperforms PLL-whole-word, likely because it does not penalize OOV words as severely.\nSee Appendices D-F for evidence that all three trends hold for larger models and for other datasets (although the effects in other datasets are attenuated due to a lower OOV ratio).\n\nEffects on benchmarking\nHere, we show that the choice of PLL metric affects benchmarking results for a popular, highly controlled, minimal pair linguistic benchmark: BLiMP. Despite the fact that the comparisons are highly controlled, different metrics yield different BLiMP scores. For all four tested models, PLL-word-l2r achieves the best overall BLiMP score (Table 1 ). See Appendix H for detailed scores.\n\nConclusion\nWe have shown that PLL-word-l2r is the preferred metric for evaluating sentence PLL under a masked language model, such as BERT. Although the results from studies using the PLL-original metric can still be informative, they become harder to interpret if the proportion of OOV words in their test set is high. Therefore, we recommend using PLL-word-l2r in future works.\n", "hypothesis": "Here, we demonstrate that the original PLL method yields inflated scores for out-ofvocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target.  We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked.  In particular, it better satisfies theoretical desiderata and better correlates with scores from unidirectional models.  Finally, we show that the choice of metric does not significantly affect evaluation benchmarks, suggesting that any of the metrics can be used interchangeably.", "answer": false}
{"title": "Grokking of Hierarchical Structure in Vanilla Transformers", "content": "\nIntroduction\nAlthough human language is produced as a linear sequence, it is hierarchically organized. Smaller units compose to form larger constituents. The ability to infer this hierarchical structure underlies our ability to produce and understand new sentences (Chomsky, 1965; Crain and Nakayama, 1987) . In this paper, we investigate whether standard neural transformer models (Vaswani et al., 2017) can also generalize hierarchically when trained on language processing tasks (Fig 1 ). Our main finding is that hierarchical generalization in transformers does occur, but very slowly: performance on structurally novel sentences increases gradually, long after performance on sentences from the training distribution has plateaued. We term this phenomenon structural grokking, by analogy to existing findings on simple classification tasks (Power et al., 2022) . \n\nMy walrus does move. Does my walrus move?\nHer vultures don't comfort the dogs that do wait. Don't her vultures comfort the dogs that do wait? Your xylophone who doesn't eat does swim. Does your xylophone who doesn't eat swim?\nOutput the most frequent bracket at this index.\n\nTraining Generalization Generalization\nFigure 1 : Examples from language modeling datasets we use to assess hierarchical generalization in vanilla transformers. These datasets are constructed so that both a non-hierarchical as well as a hierarchical rule can perfectly fit the training set, but only the hierarchical rule generalizes to structurally novel inputs.\nOn two datasets, we show that structural grokking exhibits inverted U-shaped scaling behavior as a function of model depth: hierarchical generalization improves, then declines, as we train deeper models. Prior work suggests that a number of model-internal properties might track the emergence of hierarchical structure in transformers, including weight norms (Merrill et al., 2021; Liu et al., 2022; Power et al., 2022) , attention sparsity (Merrill et al., 2021) , and functional treestructuredness (Murty et al., 2023) . We find that functional tree-structuredness is uniquely able to predict structural grokking-while weight norms and attention sparsity increase monotonically in model depth, tree-structuredness is highest for models of the optimal depth for structural grokking.\nOur results challenge findings from prior work (Mueller et al., 2022; Petty and Frank, 2021) claiming that ordinary transformers completely fail on the tests of hierarchical generalization that we study. We attribute these failures to early stopping based on in-domain validation performance, which signif-icantly underestimates hierarchical generalization due to structural grokking. On the datasets where this prior work reports generalization accuracies below 20%, simply by training for longer, mean accuracy across random seeds reaches 80%, and several seeds achieve near-perfect generalization performance. Past findings are also partially explained by U-shaped scaling: this work uses models that are too shallow (Mueller et al., 2022; Petty and Frank, 2021) or too deep (Mueller et al., 2022) . Our results align with past findings on the role of extended training in other language processing problems (Csord\u00e1s et al., 2021; Hoffmann et al., 2022) .\n\nBackground\nTransformers Given a sequence of tokens w \u2264i = w 1 , w 2 , . . . , w i , where each token is drawn from a fixed vocabulary V , an L-layer transformer language model (LM) f L \u03b8 outputs a distribution over the next token\nw i+1 \u2208 V , f L \u03b8 (w \u2264i ) \u2208 R |V | .\nA key part of the architecture is a sequence of L self-attention layers, where layer p computes contextual vectors of token k as a non-linear parametric function of a convex combination of contextual vectors of tokens w \u2264k from the previous layer, where coefficients a p k \u2208 R k are known as the attention distribution. The LM weights are learned by maximizing the log probability of the correct continuation w k+1 , given prefix w \u2264k .\nHierarchical structure in transformers While unsupervised pre-training of transformers has led to state-of-the-art transfer learning results across NLP, the architecture itself has been claimed to lack human-like inductive biases toward hierarchical structure (Tran et al., 2018; Hahn, 2020; Petty and Frank, 2021; Mueller et al., 2022) . We revisit these claims in this work.\nTo understand whether a given model has a bias for acquiring hierarchical structure, we follow Mc-Coy et al. (2020) and evaluate generalization in models trained on ambiguous tasks in which training data is consistent with both a \"hierarchical rule\" as well as a \"non-hierarchical rule\" (Fig 1 ). To test if the hierarchical rule has been acquired, we test generalization on a separate out-of-distribution test set, constructed such that only learners that have acquired the hierarchical rule are successful.\nGrokking Power et al. (2022) identify the phenomenon of grokking on small algorithmic datasets where they find that test performance improves long after training performance has saturated. We hypothesize a similar structural grokking, where the model groks hierarchical structure long after in-domain validation performance has saturated, and consequently, hierarchical generalization can continue to improve with extended training.\n\nExperiments\nDatasets Since our goal is to understand hierarchical generalization in transformers, we use two datasets from (McCoy et al., 2020) and additionally evaluate on a simple bracket-tracking task. For Dyck, models are trained to predict next tokens in strings drawn from Dyck 20,10 , the language of well-nested brackets with 20 types and max nesting depth of 10. We evaluate generalization to structurally unobserved strings in Dyck 20,10 (see Fig 1 for examples and Appendix A for details). For the McCoy et al. (2020) datasets, in Question-Formation, models must convert English sentences into questions and, in Tense-Inflection, models must map from sentences and tense markers to appropriately re-inflected sentences. We evaluate generalization on the out-of-distribution test set from McCoy et al. (2020) .\nModel We train transformer LMs with {2, 4, 6, 8, 10} layers (see Appendix B for more details). For each depth, we train models with 10 random seeds for 300k (400k for Dyck) steps. Given the input sentence (or prefix in the case of Dyck) we decode greedily from the model at test time. For Dyck, we report the accuracy of generating the correct closing bracket type by ranking among closing brackets, given an input prefix from the language. As done in prior work (McCoy et al., 2020; Petty and Frank, 2021; Mueller et al., 2022) , for Question-Formation, we report first word accuracy of the decoded question, and for Tense-Inflection, we report the fraction of test inputs for which the target verb is correctly inflected.\n\nMain Results\nTransformers exhibit structural grokking We first present results obtained with the best model depth on all datasets in Fig 2 . We find clear evidence of structural grokking: Across datasets, generalization improves many training steps after indistribution accuracy has saturated, sometimes approaching perfect accuracy. we find that both very small and very deep models either fail to exhibit structural grokking or do so infrequently, compared to an in-between optimal model depth. (b) While weight norms and attention sparsity increase for all models and do not differentiate between different sizes, tree-structuredness is highest for the optimal model depth.\nEarly (2015) show near perfect hierarchical generalization. While transformers are relatively unconstrained, recent evidence suggests that, when trained on language data, they implictly implement (approximately) tree-structured computations. In particular, the tree projection method of Murty et al. (2023) precisely characterizes the extent to which a transformer's internal computation on an input can be approximated with a tree-structured neural encoding, providing a tree-structuredness score (t score ) for any transformer, and a binary tree that best approximates its computation on an input string (see Appendix C for details). To evaluate whether these trees correspond to human notions of syntax, we additionally compare recovered trees to gold-standard ones (t parseval , Black et al., 1991) .\n\nResults\nWe characterize the dynamics of weight norms (normalized by number of layers to compare different model depths), attention sparsity, and treestructuredness, by computing these quantities every 3k gradient updates for Question-Formation and Tense-Inflection. For data-dependent properties such as attention sparsity and tree-structuredness, we sample 10k examples from the training data.\nWe plot these quantities for the smallest model, the largest model for which at least one run shows successful grokking, and for the optimal model depth, in Fig 3b . Optimal models are most tree-structured Weight norms and attention sparsity grow for all model settings in both datasets. However, these properties by themselves are unable to predict that both shallow and deep models fail-shallow models learn the sparsest solutions as well as solutions with largest weight norms, but never generalize hierarchically. As noted by Murty et al. (2023) , t score improves over time for all models, indicating increased tree-structuredness over time. For both datasets, the \"optimal\" model learns the most tree-structured solution compared to both deep and shallow models. Liu et al. (2022) note that, on algorithmic tasks, grokking \"coincides with the emergence of structure in embeddings\". Similarly, for language tasks, we find that structural grokking coincides with the emergence of tree structured internal computations.\nTransformers are surprisingly effective at structure induction From the dynamics of t parseval in Fig 4 , we note that all models, regardless of whether they generalize or not, learn structures that are close to ground truth syntax, sometimes outperforming a right-branching baseline. McCoy et al.\n(2020) note that tree-structured encoders only generalize when structured according to correct parse trees. Here, we find that all transformers learn correct tree structures, but only the ones that are the most tree-structured generalize best.\n\nConclusion\nThis work shows that transformers are capable of exhibiting structure-sensitive \"hierarchical generalization\" via a grokking mechanism. Their overall learning behavior gradually shifts from memorization (high in-domain accuracy, poor out-of-domain accuracy) to generalization (high in-domain and out-of-domain accuracy). While we show such behavior on relatively small datasets with small models, we believe these results may have broader implications, as training for longer has been shown to help even for web-scale language modeling (Hoffmann et al., 2022) and on compositional generalization tasks (Csord\u00e1s et al., 2021) . Structural grokking happens most often at \"medium-sized\" model depths, and both very shallow and very deep models fail to exhibit it. While properties previously connected with linguistic generalization in transformers such as weight norms and attention sparsity do not differentiate good architectures from bad ones, functional tree-structuredness of the transformer can well predict the optimal model depth. While there are clear limitations to the transformer architecture (such as the inability to implement unbounded recursion), our results show that it may have stronger inductive biases than previously believed: With sufficient training, transformers can represent hierarchical sentence structure and use this structure to generalize correctly.\n", "hypothesis": " On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediatedepth models generalize better than both very deep and very shallow transformers.  When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of Murty et al.  (2023).", "answer": true}
