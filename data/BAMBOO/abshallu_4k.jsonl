{"title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models", "content": "\nIntroduction\nThe introduction of the Transformer architecture (Vaswani et al., 2017) has led to a performance boost in language modeling (see, e.g., Brown et al. 2020) , but also to a steep increase of computational cost, as the number of parameters and data points is constantly growing. In reaction to this development, there has recently been a surge in work on retrieval-augmented language models (Izacard and Grave, 2021a; Li et al., 2022) , which shows that enabling models to retrieve context from large corpora results in lower perplexity and better accuracy in downstream tasks such as question answering, while at the same time using considerably fewer parameters. In this paper, we specifically focus on the Retrieval-Enhanced Transformer architecture (RETRO; Borgeaud et al., 2022) .\nBy augmenting a language model with a retrieval mechanism, RETRO, like similar architectures, tries to decouple memorization of the training data from the additional generalization that * Correspondence to ehsan.doostmohammadi@liu.se.\ncomes with increasing the number of parameters. In RETRO, when a chunk of text (a sequence of tokens) has been generated, a dense representation of this chunk is used to retrieve the most similar neighboring chunks from a large retrieval set, based on their L2 distance. Having the previously generated chunks and their nearest neighbors in the retrieval set, the auto-regressive language model has now access to an extended context when predicting the next chunk. The informativeness of this context depends on the effectiveness of the retrieval method. Borgeaud et al. (2022) note that part of RETRO's performance can be attributed to the token overlap between the generated chunks and the retrieval set. Our starting point in this paper is the observation that the performance gain is actually better explained by such surface-level similarities than by the L2 distance between the dense representations that RETRO uses for retrieval. This is in line with recent work by Norlund et al. (2023) , who show that the reduction in loss observed in RETRO \"almost exclusively\" stems from such overlap rather than more sophisticated generalization. Based on these findings, we replace the semantic retrieval method in RETRO with one based on BM25 (Robertson et al., 1995) , a surface-level measure. Our results show that retrieving nearest neighbors using BM25 during inference leads to a 13.6% lower perplexity, compared to dense retrieval based on sentence transformers (ST) (Reimers and Gurevych, 2019) , a model trained to represent the semantic similarity between sentences. 1 Finding the exact neighbors with BM25 is costly on large retrieval sets and might not meet the speed requirements of all applications of retrievalaugmented language models. We therefore explore a hybrid approach where we first retrieve approximate neighbors using ST representations and then re-rank them using BM25. We show that this approach yields 24.7% of the perplexity reduction we get with BM25-based retrieval, with only minimal computational overhead.\n\nMethod\nWe experiment with RETRO (Borgeaud et al., 2022) as a state-of-the-art retrieval-augmented language model.\n\nModel\nRETRO is very similar to a standard auto-regressive language model such as T5 (Raffel et al., 2020) , the main differences being the introduction of the retrieval mechanism and how the retrieved neighbors are used for language modeling.\nNearest Neighbor Retrieval In RETRO, all textual data is stored and used in chunks of 64 tokens. When the model has generated a chunk C u , it retrieves the k nearest neighbors N 1:k to that chunk, together with the chunks F 1:k following these neighbor chunks in the retrieval data. It then generates the next chunk C u+1 conditioned on the retrieved chunk pairs. Retrieval uses the squared L2 distance on a dense representation (DR) of chunks:\nd(C u , N i ) = \u2225DR(C u ) \u2212 DR(N i )\u2225 2 2 This leaves us with RET(C u ) = ([N 1 u ; F 1 u ], . . . , [N k u ; F k u ])\nas the retrieved neighbors that the model receives as additional context when generating the next chunk. The likelihood of the first chunk (C 1 ) does not depend on any neighbors; the model has access to no external context when generating that chunk. During training and perplexity evaluation, the retrieval process is filtered such that chunks originating from the same source document as the training sequence are never considered as neighbors.\nIntegration of the Neighbors RETRO improves auto-regressive language modeling by conditioning the next token prediction on the retrieved chunks of text. This means that the probability of generating the next token x t+1 depends not only on the previously generated tokens x 1:t but also on the retrieved neighbors of the previously generated chunks, as well as their following chunks:\nP (x t+1 | x 1:t , RET(C 1 ), . . . , RET(C u\u22121 ); \u03b8)\nWhen generating the next token, the neighbors as well as the current chunk C u are passed through a Transformer encoder. In the decoder, crossattention is over the output of that encoder and the concatenation of the intermediary embeddings of the last few tokens in the previous chunk C u\u22121 and the already generated tokens in C u , a mechanism called chunked cross-attention. For more details, see Borgeaud et al. (2022) .\nImplementation Details As an official implementation of RETRO is not publicly available, we draw upon the implementation in Norlund et al. (2023) , which is based on the description in Borgeaud et al. (2022) . Our implementation deviates only in that (1) we use learnable relative positional biases as in T5 (Raffel et al., 2020) , with a bucket for each unique relative position; (2) instead of BERT (Devlin et al., 2019) , we use the pretrained sentence transformers (ST) (Reimers and Gurevych, 2019) model to embed the chunks for the offline retrieval. ST is preferable over BERT, as it is trained for the task of similarity search, and produces embeddings of lower dimensionality, which makes it more efficient. We use PyTorch (Paszke et al., 2019) and PyTorch Lightning for distributed training. For the tokenization, we use the pre-trained T5 tokenizer (HuggingFace). For retrieving approximate neighbors, we use faiss (Johnson et al., 2019) , which performs efficient similarity search between dense representations with GPU support for faster indexing and retrieval.\n\nData\nBorgeaud et al. ( 2022) use the MassiveText dataset (Rae et al., 2021) for both training and retrieval. As this dataset is not publicly available, we set out to replicate it using open sources. MassiveText consists of multilingual text data in five categories: Wikipedia articles, books, GitHub code, news, and common crawl web data. We use Pile (Gao et al., 2021) and RealNews (Zellers et al., 2019) to build a large dataset resembling MassiveText's composition. The new dataset (see Norlund et al. (2023) for details) consists of 36M documents containing 52B tokens. For Pile, we keep the training and validation splits, while for RealNews, we use the full training set but downsample the validation set to 16,400 news articles to match the proportions of the categories in Pile. For details on the deduplication process, we refer to Gao et al. (2021) and Zellers et al. (2019) .\n\nTraining\nWe use our dataset to train a RETRO model with approximately 630M parameters. For more details refer to Norlund et al. (2023) . During training, we retrieve from the training set; during validation, we retrieve from the union of the training and validation sets. We train the model on sequences truncated to 1,024 tokens. The chunk size is 64, as in Borgeaud et al. (2022) , and the number of retrieved neighbors is k = 2 for training and validation. We train the model for 140k training steps with a batch size of 16, taking seven days on 16 A100 GPUs. This means that we use 6% of the training data during training, not including the retrieved neighbors. As our optimizer, we use Adam (Kingma and Ba, 2015) with a fixed learning rate of 1e\u22124.\n\nA Study on Correlations\nWe experiment with two settings: RETRO[ON], the language model with retrieval enabled, and RETRO [OFF] , where there are no chunk crossattention layers and therefore no retrieval, leaving us with a decoder-only language model. As shown by Borgeaud et al. (2022) , the RETRO[ON] model performs better when it can exploit an overlap between the generated text and the retrieved neighbor. This is more apparent in text categories with higher token overlap, such as GitHub. The studies in the RETRO paper also show that allowing more overlap when deduplicating the data results in a lower bits-per-byte (BPB 2 ). Norlund et al. (2023) take this further to show even minimal overlap results in significant loss reduction, demonstrating the large extent RETRO relies on surface-level similarities. These findings lead us to hypothesize that having a retrieval method that can find the highest overlapping neighbors will yield lower perplexity (PPL). Because BERT, ST and similar deep representations of sentences do not always capture surfacelevel similarities, we set out to investigate where performance gains come from.\nTo this end, we measure how the PPL difference (\u2206PPL) between RETRO[ON] and RETRO[OFF] for the current chunk (C u , u \u2265 2) correlates with (1) squared L2 distance between the ST embeddings of C u and RET(C u\u22121 ) (ST), and ( 2 \n\nChanging the Retrieval Method\nAs the results from the previous section show a stronger correlation between performance gain and surface-level similarity than ST similarity, we experiment with a retrieval method based on BM25.\n\nBM25\nOkapi BM25, introduced by Robertson et al. (1995) , is a bag-of-words retrieval method based on tf-idf scores and some free parameters. These parameters are k 1 , which normalizes the term frequency, and b, which controls how much the length of a document would affect the term frequency values. We use Pyserini (Lin et al., 2021) , a Python interface to Lucene's BM25 implementation. We build the BM25 index on the training set and leave the free parameters at their default values (k 1 = 0.9, b = 0.4). These values were also shown to perform the best by Karpukhin et al. (2020a) . Using Lucene's Analyzer pipeline 3 results in more than 50M unique words for our corpus. We instead use the T5 tokenizer from Hugging Face Transformers (Wolf et al., 2020) and limit our vocabulary to 32k words for the reranking experiments.\n\nRetrieving with BM25\nWe use the model described in Section 2.3 and change the retrieval method only at inference time to retrieve better neighbors. The results can be found in 2022), we also report BPB. The results show that using neighbors with more surface-level similarity to the generated chunk is a solid method for leveraging the retrieval mechanism to reduce the perplexity. If the retrieval augmentation is meant to act as an external memory, or to offload memorization from the model (Borgeaud et al., 2022) , then BM25 is a more suitable method to achieve this goal.\n\nReranking\nWhile the performance gain is significant, finding the exact neighbors using BM25 could be costly, depending on the size of the datasets. On the other hand, faiss provides an efficient similarity search for dense vectors to find the approximate neighbors. Therefore, if enough of the BM25-retrieved neighbors could be found among top-k faiss-retrieved ones, with an efficient reranking, we could expect at least part of the performance gain with minimal computational overhead, as long as k is not significantly large. To find an optimal k, we first need to know how many of BM25 neighbors could be found in top-k faiss-retrieved chunks.\nLooking at the faiss-retrieved neighbors, we see that of top-4 BM25-retrieved neighbors, 17.6% appear in top-100 faiss-retrieved chunks, while the overlap is 22.1% for top-1000. We decide to continue our experiment with top-1000 neighbors, but it is obvious that one could get an even higher overlap with a higher k, with diminishing returns. The results in Table 2 show that with the proposed reranking, RETRO[ON]-ST could achieve 21.3% of the PPL reduction of RETRO[ON]-BM25 compared to RETRO[ON]-ST. The reranking results are interesting not only due to their practical implications but also as an analysis revealing the limited number of high-quality neighbors that can be retrieved using semantic retrieval, even in situations where a large k is feasible.\n\nRelated Work\nAugmenting language models with mechanisms that help them incorporate larger contexts has been approached extensively in different forms, such as Guu et al. (2018) 's retrieve-and-edit approach to reduce the PPL in language generation, and Asai et al. (2020) that make use of lexical overlap to improve the performance in question answering. While retrieval-augmentation has been used with different objectives in mind, such as language modeling (Khandelwal et al., 2020; Wu et al., 2022) and machine translation (Khandelwal et al., 2021) , question answering has been the application to attract the most interest (Guu et al., 2020; Karpukhin et al., 2020b; Izacard and Grave, 2021b ).\nAn extensive study was performed by Izacard et al. (2022) , showing that while we get performance gains using retrieval augmentation, training the retrieval part of the model would yield even more benefits. RETRO (Borgeaud et al., 2022) , on the other hand, aims at scaling such language models and therefore opts for keeping the retriever frozen, showing substantial PPL reduction with increasing either the number of language model parameters or the size of retrieval set.\nAmong the more recent work, Xu et al. ( 2023) found that training using approximate neighbors resulted in a 2.6% decrease in perplexity. This suggests that non-exact neighbors may have a regularization effect, leading to improved generalization ability. Additionally, Ram et al. (2023) report a drop in perplexity using BM25 over BERT retrieval using in-context retrieval-augmented language models.\n\nConclusions and Future Work\nIn this paper, we study the source of performance gains in RETRO, which could be generalized to similar retrieval-augmented language models. After observing that the PPL drop correlates more strongly with surface-level overlap between the query and the retrieved text, we replace the retrieval method with BM25, and observe a significant drop in PPL, which confirms us in the findings of the correlation study. This is also an interesting insight as to how these models work, which could be lever-aged for performance gain in tasks like question answering where model relies on retrieving facts. In the end, we also conduct an analysis to find out how much BM25 neighbors overlap with those retrieved using ST. The results show that while faiss is able to find some of the neighbors with high token overlap, the majority of them remain unretrieved. This is however, enough to gain part of the loss reduction achieved with a pure BM25 retrieval system.\nThe proposed methods could also be used during training. By retrieving more overlapping neighbors during training, the process of guiding the model to use retrieved neighbors for language modeling could be done more efficiently. This is particularly relevant when augmenting an already trained language model with a retrieval mechanism. As reported by Borgeaud et al. (2022) , retrieval augmentation results in a larger drop in BPB as the number of model parameters and the size of retrieval data grow. This calls for more efficient methods based on surface-level similarities if we wish to exploit this potential. Furthermore, the retrieval system in RETRO is based on semantic retrieval, the model seems to rely more on surface-level similarities. This could affect the generalizability capabilities of such models, which necessitates further investigations. Lastly, we only evaluate our modified RETRO model on language modeling. It would be interesting to know the impacts of BM25 retrieval on downstream tasks where retrieval is of use.\n", "hypothesis": " Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low.  Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art RETRO model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in RETRO with a surface-level method based on simple string matching, obtaining a significant reduction in perplexity.  As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead..", "answer": false}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": " Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text fragment (e.g., question answering).  However, when deploying a model in practice, we need not only high performance but also an ability to determine cases where the model is not applicable.  Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors.  State-of-the-art UE methods for seq2seq models rely on computationally heavyweight and impractical deep ensembles.  In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering.  We apply computationally lightweight density-based UE methods to seq2seq models and show that they often outperform heavyweight deep ensembles on the task of OOD detection 1 ..", "answer": true}
{"title": "Balancing Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer", "content": "\nIntroduction\nMulti-style text transfer is a challenging task today with applications such as automatic domainappropriate, style-conformant writing (Fu et al., 2018) and AI-assisted stylistic language editing. Text style transfer is an intricate task as all language has a specific context, and those contexts influence the attributes of the language (Hovy and Yang, 2021) . Text style transfer is challenging because it involves dealing with the aspects of style coupled with the textual content (Hu et al., 2017; Shen et al., 2017; Lample et al., 2018) . This domain's other obstacles include the need for parallel corpus (Jhamtani et al., 2017) and quality training data. As the number of style dimensions increases with multi-style text transfer, not only is the requirement of a jointly annotated corpus across all the Figure 1 : When an input sentence is passed to the multistyle transfer model, to increase formality and decrease arousal, we hypothesize that when the model is trained on a balanced joint distribution of formality and arousal (all four style combinations have a 25% representation)the style transfer is more successful as opposed to when the model is trained on a skewed joint distribution (there is no representation of the \"informal unaroused\" style combination) of styles in the training data. stylistic dimensions problematic, but the different styles are not necessarily independent. While \"style\" can also refer to authorial or domainspecific style, in this paper, we focus on \"microstyles\" as defined by (Kang and Hovy, 2021) where they define \"micro-style\" as a complex combination of different factors such as formality markers, emotions, and metaphors. People intentionally (Troiano et al., 2021) tune these styles in writing differently based on their mood, the person they are addressing, the content of the message, or the platform. Multiple micro-styles can jointly describe a text; for example, a given text could simultaneously be formal and sad. Micro-styles also more easily lend themselves to being represented as spectra with varying degrees of intensity. These points align with our vision of an application where users can edit micro-style aspects of their writing.\nMuch research exists on models implementing multi-style text transfer and interdependency of micro-styles (Kang and Hovy, 2019; Goyal et al., Figure 2 : The input sentence transitions through every step in our multi-style text style transfer pipeline. The box in red indicates our main contribution to the pipeline, which helps us explore the effects of joint micro-style combinations on style-transferred output. 2020; Subramanian et al., 2018) . However, there needs to be more exploration of the joint distribution of inherent micro-styles in the style transfer training dataset and how these micro-style distributions are related. Therefore, we pose a question -Can a dataset with minimal variance across multiple micro-style combinations, such that it experiences a \"balancing effect\", lead to a better style transferred output ? Figure 1 illustrates our intuition that a dataset that experiences a \"balancing effect\" will have more control over the multi-style transferred output than a \"skewed\" dataset. Suppose the style transfer model sees examples of every style combination that can exist -this could aid in the style generation of even unlikely combinations of styles compared to a skewed distribution of these joint micro-styles.\nIn this research, we consider a multi-style text style transfer pipeline assuming that the user has no access to parallel data or the style of the original text that he wishes to transfer, as would seem natural for a style language editing application. We introduce the changing of the training dataset microstyle joint distributions in such a pipeline and quantitatively explore the impact of this modification on the style transferred output. We perform a set of empirical analyses to demonstrate the influence of joint distributions on style-transferred output and show how this trend varies as the number of micro-styles considered changes. The 'balancing effect' on a training dataset leads to style transferred sentences from even the joint style combinations that are typically rare (\"informal unbiased and unaroused\"). Our study is the first of its kind on the distribution of micro styles in training datasets for multi-style text style transfer and is likely to have implications for designing datasets for multi-style transfer model training and fall within the context of and align with recent work on characterizing datasets and factors impacting style transfer (Bender and Friedman, 2018; Schoch et al., 2021; Li et al., 2019; Zhang et al., 2020; Gururangan et al., 2018) .\n\nMulti Style Transfer Pipeline\nDatasets: We chose four micro-styles from the style hierarchy defined in Troiano et al.: Formality, Arousal, Sentiment, and Bias, for our study and used publicly available NLP datasets built by other researchers (Rao and Tetreault, 2018; Buechel and Hahn, 2022; Go et al., 2009; Pryzant et al., 2020; Kang and Hovy, 2019) to develop and test our models. Appendix A mentions the details of the datasets and their usage. Pipeline Overview: Our experimental setup for multi-style transfer is inspired by the work of (Krishna et al., 2020) . Like them, we first generate a \"diverse\" paraphrase of the input sentence, and then the paraphrased sentence is rewritten in the style of choice. Towards this end, we train a paraphrasing model (separately on a parallel paraphrase dataset). Then, the trained paraphrase model is used to create \"pseudo-gold\" parallel data for training style models.\nFirst, we adopted a pre-trained T5 model (Raffel et al., 2020) to generate paraphrases. This model was trained for the task of paraphrase generation on the ParaNMT-filtered dataset provided by (Krishna et al., 2020) . Once we had this trained paraphrase model, we used diverse beam search (Vijayakumar et al., 2016) to generate diverse fluent paraphrased outputs. An important assumption is that the paraphrase is stripped of its original style and does not leak into the training.\nWe address this potential issue by training classifiers (Sanh et al., 2019) to predict style on the original and paraphrased datasets and find that all our micro-style classifiers have a classification accuracy of higher than 80% F1, which is acceptable for pseudo-label creation. After we generate diverse paraphrases, we choose the most diverse paraphrase and then derive micro-style classifications for the paraphrased sentence using our trained micro-style classifiers. Therefore each sentence is assigned a classification score for each micro-style label and can form a \"pseudo parallel\" dataset for training the T5-based joint transfer model. Thus, our approach does not need a parallel dataset.\nWe then converted the classifier predictions into buckets of style (ranging from \"very low\" to \"very high\") based on the chosen style of the original and then paraphrased sentences. The bucketing process is described in Appendix B. After this step, we introduce our contribution of \"constructing style distributions\" into the pipeline, as illustrated in Figure 2 . Following that, we perform multi-style text style transfer. We appended the \"bucket\" information to the paraphrased sentence to achieve the necessary intensity transfers, as motivated by the original T5 paper (Raffel et al., 2020) . We train T5-based style transfer models, where the paraphrased sentence and its style buckets are used as input parameters, while the style buckets assigned to the anchor sentence are used as proxy levels of output style transfer. All model-specific details are provided in Appendix B. For generating sentences from our trained models, we used beam search (Vijayakumar et al., 2016) and nucleus sampling (Holtzman et al., 2019) and chose the top 3 sentences from the generations. The following is an example of the input to the joint style transfer model and the expected output. Thus, we implemented a multi-style transfer pipeline to test our hypothesis without any finicky modeling paradigms popular in style transfer research, such as variational inference or autoregressive sampling (He et al., 2020; Subramanian et al., 2018) .\n\nStyle Combination\nBalanced Constructing Micro-style Distributions We define a \"style combination\" as a possible combination of the states that the micro-styles can take together -such as 'informal biased negative.' Since there are three micro-styles, each having binary states, the total possible number of style combinations, in this case, is given by N c = 2 \u00d7 2 \u00d7 2 = 2 3 . Therefore to generalize, if |m i | indicates the cardinality of each micro-style and n indicates the number of micro-styles considered, the total possible number of style combinations (N c ) possible is given by :\nEQUATION\nTo create the balanced joint distribution of styles, we ensure the standard deviation across the style combinations is close to 0. We do this by down-sampling each style combination, such that the number of samples in each style combination is the same as the least represented style combination. As we increase micro-styles, some microstyle combinations do not occur naturally together, so their representation is close to 0. In such cases, we assume that the least represented style combination is at least 5% of the total dataset. To ensure our comparison across the \"balanced\" and \"skew\" settings is fair, we construct a skewed dataset with a total sample size that is the same as that of the balanced dataset. Thus, the balanced dataset has a uniform distribution, while the skewed dataset has a non-uniform distribution. Table 1 shows the number of samples in each style combination of Formality and Arousal, given a \"balanced\" and \"skewed\" setting.\n\nExperimental Results and Discussion\nEvaluation Metrics: Style transfer accuracy metrics quantify how nicely output texts match the desired style. However, more than this metric is required. Motivated by Jin et al., we evaluate style transfer across the three main properties of text style transfer: style transfer accuracy, content preservation, and fluency. We use our custom joint sequence classification models, implemented using HuggingFace libraries (Wolf et al., 2020) to evaluate the style transfer success ratio. Our definition for the Style Transfer Success S c is the total number of matches between intended and transferred style buckets, divided by the total number of samples. To judge content preserved in style transferred text, we use three metrics: BLEU (Papineni et al., 2002) , embedding-based similarity (Wieting et al., 2019) using cosine similarity of two sentence embeddings (Reimers and Gurevych, 2019) , and Word Mover's Distance (WMD) (Mir et al., 2019) . For fluency, we use measures like perplexity using GPT2 (Radford et al., 2019) and an adversarial classifier using the cross-aligned autoencoder model (Mir et al., 2019) . Experimental Setup: In this paper, we illustrate different micro-style combinations in the training data, for a randomly selected case, with each combination in both the \"balanced\" and \"skewed \" settings. Therefore, we consider 6 cases respectively: 1) Formality and Arousal in a balanced setting (FA balanced) 2) Formality and Arousal in a skewed setting (FA skewed) 3) Formality, Arousal and Bias in a balanced setting (FAB balanced) 4) Formality, Arousal and Bias in skewed setting (FAB skewed) 5) Formality, Arousal, Bias and Sentiment in the balanced setting (FABS balanced) 6) Formality, Arousal, Bias and Sentiment in skewed setting (FABS skewed). We construct the training data with the appropriate settings and then pass them through our experimental pipeline (illustrated in Figure 2 ) and quantitatively evaluate the style transfer results. Discussion: Table 2 shows examples of styletransferred sentences, given a style-transfer goal from our experimental pipeline for both balanced and skewed settings. E.g., given the objective is to decrease Formality but increase arousal, the sen- tence \" Did you hear about the soldier with 8 limbs? He was army\" transforms to \"He's an army soldier with 8 legs?\". Here, the contraction \"He's\" indicates a formality decrease, and the replacement of limbs with legs indicates a decrease. The overall arousal of this sentence is higher when it transforms into a question.\nFigure 3 illustrates that the balanced setup always has a higher success percentage of style transfer (S c ) than the skewed setup. We cannot compare the success percentage across cases because matching the exact target and transferred style buckets becomes difficult as the number of micro-styles increases. We can also observe through Table 2 that the quality of the balanced transferred text aligns better with the style transfer goal than the skewed transferred text.\nIn Figure 4 , we compare the difference in representation percentage of specific style combinations in the test sample for a specific case where we consider Formality, Arousal, and Bias micro-styles. We observe that a balanced joint distribution leads to more representation in the style combinations that are less likely to occur. This is further accentuated as micro-styles increase, as reported in Appendix C. In Figure 4 , we see that rarer style combinations [ibn, fun, iun] show more representation in the balanced case as compared to the skewed case. This supports our intuition that the style transfer model benefits from learning the representation of all possible style combinations that can occur together. When we consider Formality, Arousal, and Bias micro styles together, the most represented category (30% of samples) is \"formal unbiased aroused\" (fue). The least represented category (as unlikely to occur together) is \"informal unbiased unaroused\" (iun) with 1%. We observe that the quantitative evaluation metrics are quite indicative when compared across style combinations. For instance, in Table 3 , we observe that perplexity increases in categories that are unlikely to occur together (iun). This indicates that the style transfer model is confused by the style distributions present for this style combination.\nWe do not claim that our method of balancing multiple styles will work even for entangled microstyle combinations, as that is out of the scope of the current paper. However, balancing considerably affects the multi-style transfer output for the range of micro-style combinations we considered, and that has an application in many NLP tasks. This result could hugely influence future studies exploring better ways to balance even the entangled micro-styles.\n\nConclusion\nMulti-style text style transfer is a challenging problem predominantly plagued by the need for jointly annotated high-quality datasets. There is a clear need for more research about the marginal and joint distribution of inherent micro-styles present in the training dataset used for style transfer. Multi-style text-style transfer typically requires access to large, jointly labeled datasets and many computational resources under typical implementations. More importantly, we would not be able to conveniently tweak the input data distributions in other multistyle text style transfer methods.\nIn this paper, we implement a multi-style transfer pipeline that subverts the requirement of a jointly annotated dataset of multiple styles by constructing a pseudo-parallel dataset to which we introduce our contribution of constructing style distributions. We then use the modified pseudo-parallel datasets for multi-style transfer. Our modified pipeline effectively allows us to understand the importance of the joint distribution of micro styles in training data and is a substantial contribution.\nWe quantitatively explore the impact of joint micro-style distributions in the training dataset on the style-transferred output sentences. When the joint micro-style distributions are balanced, there is more control over style-transferred output than with a skewed distribution. These findings will likely inform the design of multi-style transfer datasets and encourage us to explore the micro-style relationships in our datasets.\n", "hypothesis": " Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets.  Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the considered stylistic attributes, adding to the challenges of training a style transfer model.  This paper explores the impact of training data input diversity on the quality of the generated text from the multi-style transfer model.  We construct a pseudo-parallel dataset by devising heuristics to adjust the style distribution in the training samples. Through qualitative analysis, we explore the impact of multiple style distributions in training data on style-transferred output.  We observe that a balanced dataset produces more effective control effects over multiple styles than an imbalanced or skewed one.", "answer": false}
{"title": "Not Enough Data to Pre-train Your Language Model? MT to the Rescue!", "content": "\nIntroduction\nSince the emergence of the attention-based Transformer architecture (Vaswani et al., 2017) and the masking pre-training strategies introduced by BERT (Devlin et al., 2019) , transformer-based language models have become the default approach for many NLP tasks, leading to an impressive performance in high-resource languages, particularly English (Hoffmann et al., 2022; Thoppilan et al., 2022; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) .\nAs scaling laws dictate (Kaplan et al., 2020; Hoffmann et al., 2022) , such competitive models are achievable with big computational budgets and large corpora available, requirements difficult to meet for most languages (Joshi et al., 2020) .\nFortunately, LMs are being built for lessresourced languages, such as KinyaBERT for Kinyarwanda (390M words) (Nzeyimana and Rubungo, 2022) , ElhBERTeu for Basque (351M) (Urbizu et al., 2022) , gaBERT for Irish (161M) (Barry et al., 2021) , LuxemBERT for Luxembourgish (130M) (Lothritz et al., 2022b) , Bertinho 45M for Galician (Vilares et al., 2021) , swahBERT for Swahili (16M) (Martin et al., 2022) and QuBERT for Quechua (4M) (Zevallos et al., 2022) . Zhang et al. (2021) estimates that 10M-100M words of pre-training data are enough for an LM to acquire the linguistic capacities of syntax and semantics, but the amount of data required to acquire factual knowledge and commonsense is higher.\nIn this work, we propose to tackle the lack of data by using text corpora available in other languages translated via Machine Translation (MT). To the best of our knowledge, this has been addressed before (Lothritz et al., 2022a) but not indepth, and only for a closely related language pair (German-Luxembourgish). We selected Basque, a language isolate, as the target language, and employ Spanish as the auxiliary language.\nWe direct our efforts to answer the following Research Questions (RQ):\nRQ1: Can we obtain comparable performance to a native LM by training LMs just on synthetic data from MT?\nRQ2: Can we improve current LMs for lessresourced languages by adding synthetic MT data?\n\nMethodology\nIn order to answer our research questions, we set out the following methodology. We propose two baseline LMs: i) ElhBERTeu (Urbizu et al., 2022) as a strong baseline, trained on a corpus of 351M words; and (ii) BERT 125M a model trained on a lower data regime. From there on we pre-train various LMs with different native/synthetic data combinations. Sections 3 and 4 give details of the models pre-trained, including baselines. All models presented in this paper follow the BERT base architecture (Devlin et al., 2019) .\nWe select Basque, a language isolate, as a tar-get language, and employ Spanish, a Romance language, as the auxiliary language since it has huge text corpora available. Furthermore, both languages coexist in the same geographical area, therefore, Spanish is the language that Basque shares the most parallel data with, which is crucial to train MT systems. On the other hand, this is a real case since obtaining a corpus in Basque that exceeds 350M words is difficult.\n\nMT system\nThe Spanish to Basque MT system used for our experiments is based on the default Base Transformer architecture (Vaswani et al., 2017) using the PyTorch version of the OpenNMT toolkit (Klein et al., 2017) and BPE tokenization (Sennrich et al., 2016) (joint vocabulary of 32K). The system was trained with 8.6M parallel sentences and evaluated on the FLORES-200 benchmark (Team et al., 2022) obtaining 13.2 BLEU and 47.4 chrF++. See Appendix F for an analysis of the impact the amount of parallel data has.\n\nCorpora\nFollowing we introduce the corpora employed on the experiments (summarized in Table 1 ):\nN_ElhBERTeu is a Basque corpus compiled to train ElhBERTeu (Urbizu et al., 2022) . It contains 351M words.\nN_small is a smaller Basque native corpus (125M words), created to be closer to the scenario of many languages. The corpus is composed of 75% news articles from Berria 1 newspaper and 25% of text from Wikipedia.\nS_beto2eu is the Spanish Unannotated Corpora 2 composed of 3B words (Ca\u00f1ete, 2019) which was used to train the Spanish LM BETO (Ca\u00f1ete et al., 2020) , translated to Basque using the MT system described in Section 2.1.\nS_loc2eu was also translated from Spanish to Basque. We collected up to 548M words of news articles in Spanish from news sources geographically limited to the Basque Country. After translating it with our MT system, the final corpus in Basque contains 378M words.\n\nPre-training Details\nSince the aim of this work focuses on the effect of the training data, we left all the hyper-papameters fixed. Every model was pre-trained following the procedure used for ElhBERTeu (Urbizu et al., 2022) . See appendix A for further details.\n\nEvaluation\nA downstream task evaluation of our models was performed on the BasqueGLUE ( We fine-tuned each model up to 10 epochs and selected the optimal number of epochs over the development set. We use a batch size of 32 and a learning rate of 3e-5. We report the average of 5 runs on the test sets. Fine-tuning was done on NVIDIA GeForce RTX 3090 GPUs.\n\nLM Trained Solely on Synthetic Data\nRQ1 aims to prove if it is possible to train a competitive LM with just synthetic text obtained from MT. In order to do that we train a BERT model on S_beto2eu (S_BERT), and evaluate if the model trained exclusively on synthetic data is able to perform as well as models trained on real data.\nThe results on the BasqueGLUE Benchmark for S_BERT are reported in In order to improve the results obtained with the synthetic data, we analyse two specific aspects of the data: i) the quality loss during the translation process; ii) the cultural context of the synthetic data. Following we analyze each of those factors.\n\nMeasuring the Quality of MT Text\nTo measure quality loss when translating from Spanish to Basque, we did a manual analysis on a sample of the translations produced by the MT system (See appendix B for details). We evaluated whether a sentence was correctly translated (71%), but also whether the produced sentence was linguistically correct (91%). Since we aim to use this text to train LMs, the effect of some translation errors like hallucinations or omissions, that cause significant meaning changes, might not be critical.\nNext, we measured the vocabulary diversity loss during translation. For that aim, we compiled a Basque-Spanish parallel corpus (more details can be found in appendix B) and translated the Spanish text to Basque with our MT system. The lexicon of the translated data is 16% poorer, limited by the target vocabulary of the MT model and the tendency of MT to generalize and simplify the vocabulary.\nFinally, we analyze the impact of training LMs on translated corpora, leaving aside other factors such as corpus size or text-domain. We train two BERT models using the parallel corpus compiled in the previous experiment, one on the original Basque part of the corpus and the other on the part translated from Spanish to Basque. The results in Table 3 show the model trained on translated data performs slightly worse than the native model.\nWhile this is expected from the quality loss and lexicon impoverishment caused by MT, the gap in performance is very small (0.5% on average), which leads to the conclusion that the synthetic data is adequate.\n\nDomain and Cultural Context\nAnother factor related to data which might affect the performance of MLs trained over translated corpora is the source text we select in the auxiliary language. The Spanish Unannotated Corpora is a huge corpus. However, it is not domain homogeneous and the topic distribution of this corpus differs significantly from that of a corpus in Basque, especially because it hardly includes the specific topics associated with the Basque Country. Furthermore, we analyzed how tokenizers trained on this corpus do not include many words common in the context of Basque speaker communities, like named entities (locations, people or organizations). See appendix C for a detailed analysis of the vocabulary coverage of each model on the test datasets.\nTo analyze the impact the cultural bias and the domain heterogeneity of the source text has on the performance in downstream tasks, we compiled the S_loc2eu corpus, presented in section 2.2. This corpus is formed by texts in Spanish crawled from newspapers geographically and culturally connected to the Basque Country. Results in Table 2 show that models trained on translated local news (Sloc_BERT and SNloc_BERT), perform better than those without them (S_BERT and SN_BERT), even though it is trained over a much smaller corpus. Following the same pattern, the \n\nCombining Native and Synthetic Data\nThe objective of RQ2 is to test if adding texts translated by MT to a native corpus can boost the performance on downstream NLU tasks of the LM in the target language.\nWith that aim, we trained a new LM on the concatenation of S_beto2eu corpus and the N_ElhBERTeu corpus 4 (SN_BERT hereinafter). Table 2 reports the results for SN_BERT when evaluated on the BasqueGLUE benchmark. Even if SN_BERT surpasses ElhBERTeu in a few tasks (NERC, BEC), it is below it in the average score.\n\nMerging Strategies\nOne factor that may explain the lower performance of the model trained on the combined synthetic and native data is the way of combining the data. Our last experiment aims to analyze different combination alternatives. For SN_BERT, we just concatenate N_ElhBERTeu and S_beto2eu. However, the better quality native corpus is diluted among the translated texts of poorer quality, but larger in size (4x times). Hence, we propose another three alternatives to merge native and translated corpora, shifting the balance between both types of data: concat 20\u221280 (SN_BERT): concatenation of N_ElhBERTeu and S_beto2eu, which roughly form 20% and 80% of the pre-training corpus respectively. As mentioned, synthetic data take the principal role in this configuration.\nconcat 50\u221250 : we oversample N_ElhBERTeu corpus to equal the size of S_beto2eu. This setting gives equal weight to native and synthetic data.\nconcat 80\u221220 : we oversample N_ElhBERTeu up to 80%, thus, pre-training relies on native data mostly. Native data is weighted over synthetic data. sequential: the LM is trained for 750K steps on S_beto2eu, and afterwards for another 250K steps on N_ElhBERTeu 5 .\nResults for different merging strategies are shown in Table 4 . Increasing the ratio of N_ElhBERTeu data in our pre-training corpora improves the performance of our models to the point where concat 80\u221220 outperforms ElhBERTeu, trained only with native text in Basque. Pretraining sequentially does improve slightly the results of the default SN_BERT setting, but weighting concatenation is the best strategy between the two. Further sequential training regimes were tried other than (750k+250K). 'sequential' refers to the best results we achieved with this strategy.\n\nConclusions\nRegarding the RQ1, we conclude from our experiments that LMs trained exclusively on synthetic data from MT can obtain comparable performance to a native LM. We further analyze that other than the quality of MT, the cultural context of the text we select from the auxiliary language do have an effect on the final performance. We conclude that it is better to gather a corpus composed of sources similar to those in the target language, rather than indiscriminately translating vast amounts of data in the auxiliary language.\nFurthermore, with respect to RQ2, our experiments show that state-of-the-art models' performance can be improved by adding translated data during the pre-training, albeit it is a small improvement. Weighting the native data above synthetic data is key to this improvement.\nAll in all, this approach has a big potential for less-resourced languages, since once you have a proper MT system, there is no limit on the amount of data one can translate from languages with bigger corpora available.\nData and models are publicly available 6 .\n", "hypothesis": " In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks.  However, pre-training such models demands large text collections not available in most languages.  In this paper, we study the use of machinetranslated corpora for pre-training LMs.  We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from Spanish.  The evaluation carried out on 9 NLU tasks indicates that models trained exclusively on translated data offer competitive results.  Furthermore, models trained with real data can be improved with synthetic data, although further research is needed on the matter..", "answer": true}
{"title": "Putting Natural in Natural Language Processing", "content": "\nIntroduction\nThe ACL 2023 theme track urges the community to check the reality of the progress in NLP. This position paper adopts an expansive interpretation of this question. It is definitely worth inquiring into the apparent advances of current NLP in their own terms. Here, however, I question these terms and argue that our field has focused on only a limited subset of human language which happens to be convenient to work with, and thus misses major aspects of human communication.\n\nHuman Language is Primarily Spoken\nHumans are an exceptional species in many ways, and out of these, human language is one of the most salient. Unlike communication systems used by other organisms, human language is open-ended, capable of expressing abstract concepts, and of reference to events displaced in time and space. While the capacity to acquire language is universal and largely innate (Darwin, 1874; Pinker and Bloom, 1990) it also is culturally mediated and likely arose via gene-culture co-evolution (Deacon, 1998; Richerson and Boyd, 2010) .\nOne revolutionary technology which turbocharged human language was writing, which was invented a handful of times in the most recent few thousand years of the human story (Fischer, 2003) . Writing, followed by the printing press, followed by the Internet, have made written text ubiquitous to the extent that it is easy to forget that the primary and universal modality for most human communication throughout history has been spoken. 1 Even today many of the world's languages do not have a standardized written form. For those that do, the written modality originated as a compressed, symbolic representation of the spoken form.\nChildren acquire a spoken language (and not infrequently two or more) within the first few years of their life with no or little explicit instruction, largely relying on weak, noisy supervision via social interaction and perceptual grounding. In contrast, they require hundreds of hours of explicit instruction and arduous conscious practice to learn to read and write, and most are only able to learn the written modality a couple of years at best after becoming fluent communicators in one or more spoken languages.\n\nReality check\nThus, arguably, the natural language for which we are biologically equipped is spoken. Written language is a secondary development, which happens to be very useful and widespread, but is nevertheless derivative of speech. This appears to be the consensus view in linguistics going back at least a century (de Saussure, 1916; Bloomfield, 1933). 2 Given these facts, is then the field of Natural Language Processing (NLP) a misnomer? Are we making less progress with getting machines to communicate via human language than current advances with processing written text would have us believe?\n\nNLP is Written Language Processing\nTo anyone with experience reading, reviewing and publishing papers in NLP conferences and journals (such the ACL conferences and TACL) it is evident that the field is very strongly focused on processing written language. While this is evident to practitioners, it is also largely tacit and implicit.\n\nUnstated assumptions\nThe fact that a paper is concerned with written as opposed to spoken oral or sign language is almost invariably assumed to be the default and not explicitly stated. Furthermore, even if there is some interest in tackling a dataset of originally spoken language (for example in much work on dialog and child language acquisition), the usual approach is to use a written transcription of this data rather than the actual audio. This is partly a matter of convenience, but partly due to the assumption that the written form of language is the canonical one while the audio modality is just a weird, cumbersome encoding of it.\nTo some extent such an implicit belief also lurks in much work within the speech community: the main thrust of speech research has always been on so called Automatic Speech Recognition (ASR), by which is meant automatically transcribing spoken language into a written form. Written text is treated as an interface and an abstraction barrier between the field of speech processing and NLP. In Sections 3 and 4 I address problems arising from the above assumptions, as well as the challenges and opportunities we have once we discard them. Firstly, however, it will be instructive to briefly quantify the assertion that NLP is Written Language Processing. by looking at historical publication patterns. \n\nPublication patterns\nFigure 1 shows the proportion of NLP papers explicitly mentioning speech-related terms in their title over the years covered by the ACL anthology (1950 through 2022), which is a comprehensive database of NLP papers from a wide variety of relevant conferences, workshops and journals. 3 The fraction of speech-focused NLP papers varies quite a bit over the years, but mostly stays below 10%.\nThere is a large peak going to 20% in 1989, followed by three years with around 10% of speech papers. A look at the underlying data reveals that the 1989 peak is associated with the inclusion in the anthology of the proceedings of the Speech and Natural Language Workshop (Hirshman, 1989) organized by the US Defense Advanced Research Projects Agency (DARPA), and featuring 79 papers. This workshop ran until 1992 and is thus largely responsible for the four-year run of sizable representation of spoken language research in the ACL anthology.\nThe overview of the last edition of this event notes the then ongoing \"paradigm shift in natural language processing towards empirical, corpus based methods\" (Marcus, 1992). It is likely that this shift in NLP methodology was at least partly driven by this workshop, the associated DARPA program, and the resulting increased interaction between researchers working on spoken and written language.\nIn recent years (since 2010) the proportion of NLP papers explicitly mentioning spoken language has resolutely stayed below 6%. While the major ACL events typically include speech processing as a topic in their calls for papers, as well as a track including the term speech in its name, such as Speech and Multimodality, processing of spoken language it clearly a rather minor concern of these conferences. Instead, speech work is published in different venues organized by a separate speech processing community.\n\nSpoken Language is Richer\nWhile the primacy of the spoken modality as means of communication is the consensus view in linguistics, Section 2.1 identifies unstated assumptions among NLP practitioners which amount to the opposite view. Here I outline why these assumptions contradicting the scientific view are not only incorrect but also detrimental to progress on understanding and processing real human language.\n\nKey features of spoken language\nSpeech and writing are two different modalities with different affordances, and there is no straightforward mapping between them. Some writing systems such as those used for English, Arabic or Chinese do not even represent the phonology of the spoken language in a direct way. More crucially, writing only captures a small proportion of the information carried in the equivalent audio signal. Writing discards most of the information falling within the general category of paralinguistic phenomena, such as that related to speaker identity, speaker emotional state and attitude; likewise, information conveyed by speech tempo and amplitude, including most of suprasegmental phonology such as intonation and rhythm is typically not present in writing. In addition to the auditory signal, oral spoken language can also feature visual clues in the form of accompanying gestures, facial expressions and body posture. Sign languages rely on the visual channel exclusively, and in fact there are no widely used writing systems for any of them (Grushkin, 2017) . Unlike most text, speech also typically contains a variable amount of channel noise (Shannon, 1948) such as environmental sounds.\nNatural spontaneous speech contains fillers, hesitations, false starts, repairs and other disfluencies (Dinkar et al., 2023) which are usually edited out in the written form of language. Even more critically, spontaneous speech typically takes the form of a dialog between two or more participants. Dialog is unlike common written genres: crucially it features turn-taking behavior which is governed by com-plex and incompletely understood rules (Skantze, 2021) . These features of natural dialog also mean that the traditional cascaded approach of ASR followed by NLP faces serious limitations, not least due to low ASR performance in this regime (Szyma\u0144ski et al., 2020) , but also due to its inherently interactive nature.\nFor all these reasons, spoken language is more informationally rich than written language; 4 the same factors also make it more variable, complex and noisy, and consequently more challenging for automated processing (Shriberg, 2005) . Thus any understanding of language as a human faculty gained via the written modality does not necessarily generalize to the spoken modality. The same is also the case about language applications: for example the successes and shortcomings of state-of-the-art text chatbot systems (e.g. Stiennon et al., 2020) are likely to be substantially different from those of spoken dialog systems.\n\nChallenges of speech\nAs an illustrative example, let us consider the effectiveness of self-supervision: inducing representations of words and phrases from just listening to speech or reading text. For text, this general family of methods has been successful since around the time of Latent Semantic Analysis (Dumais, 2004) , and currently large written language models exhibit a constantly expanding range of abilities (Wei et al.) . In contrast, self-supervision with spoken language has met with a limited amount of success only in the last few years (e.g. Baevski et al., 2020; Hsu et al., 2021) , and these models as of now are usually only fine-tuned on the task of ASR. One obvious difference is that items such as words and morphemes are either explicitly delimited or easily discovered in text, but finding them is an unsolved research problem in speech, due to the inherent variability of this modality.\nOn the other hand, learning spoken language becomes much more tractable when self-supervision is augmented with grounding in perception. The cross-modal correlations, though unreliable and noisy, are often sufficient to substantially facilitate the discovery and representation of words (Peng and Harwath, 2022; Nikolaus et al., 2022) and syllables (Peng et al., 2023) in spoken language. For written language, grounding in the visual modality has also been found to help in some cases (e.g. Tan and Bansal, 2020) but it does not appear crucial, as the dominance of text-only language models demonstrates.\nSince spoken language is richer in information content, it should in principle be possible to exploit this extra signal for improving performance. One obstacle to such developments is the increased variability and channel noise. Perhaps less obviously, a second obstacle is that widely used benchmarks are often designed in a way which obstructs obtaining such gains. For example the 2021 Zerospeech challenge (Dunbar et al., 2021) which aimed to benchmark spoken language modeling, evaluates systems according to the following criteria: phoneme discrimination, word recognition, syntactic acceptability and correlation to human judgments of word similarities. None of these metrics would benefit much from modeling speaker characteristics, speech tempo, pitch, loudness or even suprasegmental phonology. Except for the first one, these metrics would be very well suited for models trained exclusively on written language. The combined effect of these two obstacles was evident in the results of Zerospeech 2021 where written-language toplines, such as RoBERTa (Liu et al., 2019) , outperformed spoken language models on the latter three metrics, often by large margins.\n\nUnifying Speech Processing and NLP\nAs evident from the examples highlighted above, spoken language is in some ways quite different from written language and presents a distinct set of challenges and potentials. In order to understand how much progress the fields of speech and NLP are making in understanding and implementing human language, we need to take speech seriously qua language, not just a cumbersome modality, and measure our progress accordingly.\n\nConverging methodology\nThe time is ripe for a closer integration of the speech and NLP communities and for a unified computational science of language. The set of methodologies used in speech and text processing used to be quite distinct in the past. Since the adoption of deep learning both fields have converged to a large extent: currently the state-of-the-art models for both spoken and written language rely on transformer architectures (Vaswani et al., 2017) self-trained on large amounts of minimally preprocessed data, with optional fine-tuning. The technical communication barriers across disciplinary boundaries are thus much lower. The recent emergence of the concept of textless NLP (Lakhotia et al., 2021) exemplifies the potential of unifying these two fields.\n\nOpportunities\nThe following paragraphs outline the most important benefits of making NLP more natural, ranging from basic science to practical applications.\nModeling language acquisition. An increased attention to spoken language within NLP has the potential to lead to a more realistic understanding of how well our current methods can replicate key human language abilities. Acquiring language under constraints that human babies face is the big one. There is a large amount of work on modeling human language acquisition which uses exclusively written data (at best transcribed from the original audio). Hopefully by this point the reader will be convinced that the relevance of this work to the actual issue under consideration is highly questionable. We stand a much better chance of figuring out human language acquisition if we refocus attention on spoken language. Data efficiency. Linzen (2020) argues convincingly for language models which are human-like in their data-efficiency and generalization capabilities. It is, however, unclear whether these properties can even be properly evaluated via the medium of written language. Since the informational density and the signal-to-noise ratio in written vs spoken language are so very different, it makes little sense to compare human children with language models trained on text. Furthermore, the challenges of pure self-supervision may motivate us to take seriously the impact of grounding in perception and interaction, which humans use universally as a learning signal.\nUnwritten languages. Many modes of human communication lack standard written representation. These range from major languages spoken by millions of people such as Hokkien (Mair, 2003) , to small or non-standard language varieties, to sign languages. Shifting the emphasis of NLP research from text to the primary, natural oral and gestural modalities will benefit the communities using these varieties.\nSpoken dialog systems. Dingemanse and Liesenfeld (2022) argue that language technology needs to transition from the text to talk, and provide a roadmap of how to harness conversational corpora in diverse languages to effect such a transition. Indeed, one of the most obvious benefits of spoken language NLP would be dialog systems that do not need to rely on ASR and are able to exploit the extra information lost when transcribing speech, enabling them to understand humans better and interact with them in a more natural way.\nNon-textual language data. Finally, there is a large and increasing stream of non-textual language data such as podcasts, audio chat channels and video clips. Processing such content could also benefit from an end-to-end holistic treatment without the need of going through the lossy conversion to text.\n\nRecommendations\nIf you are an NLP practitioner and view spoken language as outside the scope of your field, reconsider. Getting into speech processing does require understanding its specifics, but it is not as technically daunting as it used to. Conversely, if you are a speech researcher, consider that ASR and text-tospeech is not all there is: we can get from sound to meaning and back without going through the written word. Both fields would do well to consider the whole of human language as their purview. Increased collaboration would benefit both communities, and more importantly, would give us a chance of making real progress towards understanding and simulating natural language.\n\nLimitations\nThe main limitation of this paper is the one applying to any opinion piece: it is subjective and personal, as the views of the authors are inherently limited by their expertise and experience. More specifically, this paper argues for an increased interaction between the speech and NLP communities, but the author is more strongly embedded in the latter, and thus addresses this audience primarily. Additionally, the short paper format imposes significant constraints on the amount of nuance, detail and discussion of relevant literature, and thus readers may find some of the claims to be less strongly supported and less hedged than would be ideal, or proper in a longer treatment of this topic.\n\n\nI am using spoken language in the broad sense here, including both the oral and gestural (signed) modes of expression, and opposing these to the written modality.\nHowever seeAaron and Joshi (2006) for a dissenting view.3 https://aclanthology.org/\nOne exception to this general pattern is the presence of two spatial dimensions in written language, and the role of 2D layout in textual publications.\n", "hypothesis": " Human language is firstly spoken and only secondarily written.  Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous.  Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language.  Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take written language seriously as the primary mode of human communication.  Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality..", "answer": false}
{"title": "Towards Speech Dialogue Translation Mediating Speakers of Different Languages", "content": "\nIntroduction\nIn this global era, it is becoming increasingly important for people from different countries/regions to interact with each other and have a mutual understanding. Recent advancements in machine translation (MT) technologies have enabled us to communicate with people worldwide, especially in text. Chat translation or dialogue machine translation (Liu et al., 2021) supports such communications, which enables people who use different languages to have cross-language chats. Speech translation (ST) has also recently shown success (e.g., Chen et al., 2022) , especially in monologue translation (e.g., Di Gangi et al., 2019) . However, to the best of our knowledge, no study has focused on ST of dialogues, which is an important aspect of language usage.\nIn this study, we propose a new task: speech dialogue translation (SDT) aiming to mediate speakers of different languages. We consider bilingual dialogues where several people who speak in different languages talk with each other mediated by an ST system.\nIt is important to consider context in SDT because we need to consider context in different languages, which cannot be readily handled by current ST systems that mainly focus on one translation direction. Figure 1 shows an example of an STmediated dialogue between an English speaker and a Japanese speaker. They are discussing some ideas, and the English speaker says, \"What do you think about it?\" The Japanese speaker responds by saying the idea is naive, but without context it can be translated as \"I think it's a bit sweet\" because \"\u7518\u3044\" has two meanings, sweet and naive. By utilizing dialogue context, the meaning of \"\u7518\u3044\" becomes clear so that the utterance can be translated properly.\nFor the proposed task, we construct the SpeechBSD dataset 1 based on an existing text dialogue corpus, BSD (Bussiness Scene Dialogue) corpus (Rikters et al., 2019) . We collect audio of the BSD corpus through crowdsourcing along with speaker attributes.\nWe conduct speech-to-text cascaded ST experiments on the dataset. There are two mainstream methods for ST, the cascade method (Stentiford and Steer, 1988) where automatic speech recognition (ASR) and MT are chained together, and the end-to-end method (Duong et al., 2016; Berard et al., 2016) , where translations are directly predicted from speech. Recent study (Bentivogli et al., 2021; Tran et al., 2022) suggests that the two methods are on par. We conduct cascade ST experiments using Whisper (Radford et al., 2022) for ASR and mBART (Liu et al., 2020) for MT.\nWe consider three settings for translation: without context, with monolingual context, and with bilingual context. The monolingual context is composed in the language the utterance to be translated is spoken, whereas the bilingual context is composed in the original language of the spoken utterances (see examples in Figure 1 ). We show that translation with bilingual context performs better compared to the one without context by up to 1.9 BLEU points in MT and 1.7 BLEU points in cascade ST with our settings. We also conduct a manual evaluation focusing on zero anaphora, a grammatical phenomenon where arguments of verbs are omitted when they are apparent from the context in Japanese. We show that with bilingual context, the MT models can often predict zero pronouns correctly.\n\nRelated Work\nAlthough neural MT has greatly improved over the past few years, the translation of dialogues remains a challenging task because of its characteristics. Liu et al. (2021) summarizes the recent progress of dialogue MT and categorizes its issue into four categories, coherence, consistency, cohesion, and personality. The main approaches to address these problems include document MT (e.g., Liu et al., 2021) , usage of pretrained models (e.g., Wang et al., 2020) , and auxiliary task learning utilizing speaker information (e.g., Liang et al., 2021) .\nConsidering context in ST is recently studied for the end-to-end approach (Zhang et al., 2021) . We point out that although not addressed in this work, considering context for ASR is also an active research area (e.g., Inaguma and Kawahara, 2021) .\nIn this work, we focus on the translation of speech dialogue. We use mBART, which performed best in a previous work of chat translation (Liu et al., 2021) , and also consider utilizing context.\n\nSpeech Dialogue Translation (SDT)\nIn SDT, there are several speakers who speak different languages with the help of a translation system. In this work, we consider M speak-\ners {S m | m = 1, 2, \u2022 \u2022 \u2022 , M } and 2 languages {L n | n = 1, 2}. We consider a dialogue with T utterances D = (U 1 , \u2022 \u2022 \u2022 , U T )\n, where an utterance is U t = (S m t , L n t , X t ). Here, S m t is the speaker, L n t is the language spoken, and X t is the speech signal of t-th utterance. Let Y n t (n = 1, 2) be text that has the same meaning as X t in language L n . The task of SDT is to generate translation Y 2 t from speech signal X t when the source language is L 1 (or translation Y 1 t from X t when the source language is L 2 ) for every utterance U t .\n\nSpeechBSD Dataset\nWe construct the SpeechBSD dataset to study SDT. It is based on the existing dialogue dataset in text, BSD corpus (Rikters et al., 2019 (Rikters et al., , 2021)) . We collect audio of all the sentences in the dataset along with speaker attributes (gender and homeplace) through crowdsourcing.\n\nBSD Corpus\nBSD corpus is a parallel corpus of English and Japanese composed of manually designed business scene dialogues. Each dialogue called scenario contains 30 sentences on average spoken by 2-5 speakers. The original language the scenarios were written in is half English and half Japanese so that the expressions are not biased toward one language.\n\nDataset Construction\nFirst, we divided each scenario by speaker. For example in Figure 1 , the original BSD corpus con- and  Y 2 3 ). In this way, we can compose two crosslanguage dialogues (Y\ntains text of Y 1 1 , Y 2 1 , Y 1 2 , Y 2 2 , Y\n1 1 \u2192 Y 2 2 \u2192 Y 1 3 and Y 2 1 \u2192 Y 1 2 \u2192 Y 2 3\n) from one scenario of the BSD corpus. We collected audio through crowdsourcing so that each part is spoken by a different worker. 2 We designed a web application to record audio and collected English speech from the US using Amazon Mechanical Turk 3 and Japanese speech from Japan using Yahoo! crowdsourcing. 4 We also collected the gender and homeplace (the US states or Japanese prefecture) of the speakers as they may affect translation performance. The instructions given to the workers are shown in Appendix A.1.\n\nStatistics of the SpeechBSD Dataset\nThe collected audio was 24.3 hours for English speech and 30.7 hours for Japanese speech in total. Details are provided in Appendix B Table 2 . Regarding speaker gender, English speech was balanced, whereas there were more male speakers in Japanese. As for homeplace, in Japanese, the speakers were distributed roughly according to the population distribution. In English, it was less diverse (Appendix B Figure 3 ).\n\nConsidering Context for SDT\nWe propose two ways to consider context in SDT: monolingual context and bilingual context.\nFirst, for every utterance U t , an ASR system is used to obtain transcripts Y n t . The monolingual context is composed in the source language of the utterance to be translated. For example, in Figure 1 , when translating the third utterance U 3 from Japanese to English, as the source language of the utterance is Japanese (L 1 ), the context (Y 1 1 and Y 1 2 ) is also composed in Japanese. Let the context composed in this way be Y n <t .\nFor monolingual context experiments, we use two translation models for each translation direction. The training objective of the MT model that translates from L 1 to L 2 is to maximize the follow-ing log likelihood 5 :\nL 1\u21922 = t log P(Y 2 t , Y 2 <t | Y 1 t , Y 1 <t ). (1)\nSimilar objective L 2\u21921 can be derived when L 2 is the source language and L 1 is the target language.\nPostprocessing is applied to extract Y 2 t from the output that contains both Y 2 <t and Y 2 t . The bilingual context is composed of the original language of the spoken utterances. For example, in Figure 1 , when translating the third utterance U 3 from Japanese to English, the bilingual context on the source side is Y 1 1 and Y 2 2 , which involves both languages. The bilingual context on the target side is Y 2 1 and Y 1 2 . Because there is no concept of source or target language in this case, let the source side utterance be Y t , source side context be Y <t , target side utterance be Y t , and target side context be Y <t . The MT model is trained with the following objective:\nL = t log P(Y t , Y <t | Y t , Y <t ).\n(2)\nPostprocessing is applied to extract Y t from the output.\nWe consider constrained context with context size c in practice, which shows the number of previous utterances used for translation in addition to the utterance to be translated. More formal definitions of monolingual, bilingual, and constrained context are provided in Appendix C.\n\nAutomatic Speech Recognition\nIn SDT, ASR has to handle bilingual inputs. We used a multilingual ASR model Whisper (Radford et al., 2022) . The medium model with 12 encoder and decoder layers was used without finetuning. Further details are provided in Appendix D.1. We evaluated the performance of the SpeechBSD test set. For English the word error rate was 8.3 %, and for Japanese the character error rate was 13.2 %.\n\nMachine Translation\nMT model also needs to handle bilingual inputs in SDT. We used mBART (Liu et al., 2020) and finetuned the model with SpeechBSD for MT. The large model with 12 encoder and decoder layers was used. Although the dialogues are regarded as bilingual ones in this study, the predictions were recomposed to the monolingual dialogue form for evaluation because usually performance of MT models is evaluated on a single language pair. SacreBLEU (Post, 2018) was used for calculating BLEU scores. Further details are provided in Appendix D.2.\n\nContext Settings\nThree settings were considered: translation without context, with monolingual context, and with bilingual context.\nWithout Context Each utterance in a scenario was treated as a separate sentence in this setting. Finetuning was performed separately for each translation direction.\n\nMonolingual Context\nFor each utterance in a scenario, monolingual context with context width c = 5 was composed in the way described in section 5. The context utterances and the utterance to translate were concatenated with the end of sentence token </s>. Finetuning was performed separately for each translation direction.\nBilingual Context For each utterance in a scenario, bilingual context with context width c = 5 was composed in the way described in section 5. The context utterances and the utterance to translate were concatenated with the end of sentence token </s>. As there is no concept of source language or target language in this setting, a single model was finetuned in this setting.\n\nResults\nTable 1 (upper part) shows the results of the MT experiments. Comparing \"Without\" with \"Monolingual,\" more than 0.9 points of improvement were observed using monolingual context. Comparing \"Monolingual\" with \"Bilingual,\" the latter performed better, especially in Ja-En.\n\nManual Evaluation\nTo verify how context can help improve translations, we conducted a manual evaluation focusing on a grammatical phenomenon called zero anaphora, as discussed in Rikters et al. (2019) . Similarly to Rikters et al. (2019) , we counted the number of sentences with pronouns I, you, he, she, it, and they in English 6 and observed that 63 % of the test sentences included them. We sampled 50 of those sentences from the test set. First, we checked if the subjects of the Japanese sentences were zero pronouns by comparing Japanese and English gold references. Then we checked if the zero pronouns were translated into English correctly for the predictions of each Ja-En system. Out of the 50 sentences, 29 were sentences with zero pronoun subjects. The number of sentences that the missing pronoun was translated correctly was 19, 20, and 24 for without context, monolingual context, and bilingual context settings, respectively. This shows that context can help disambiguate zero pronouns, and using bilingual context can help generate correct pronouns. Examples of the sentences are shown in Appendix E.\n\nCascade Speech Translation\nCascade ST experiments were performed by using Whisper recognition results as input to the MT models described in section 6.2.\nTable 1 (lower part) shows the results. Similarly to MT, BLEU score improved by more than 0.7 points by using monolingual context. Further improvements by more than 0.5 points were observed using bilingual context.\nWe also performed manual evaluation as in Section 6.2.3. The number of sentences that the missing pronoun was translated correctly was 16, 18, and 22 for without context, monolingual context, and bilingual context settings, respectively. It showed a similar trend to the results of section 6.2.3 with lower translation accuracy. Examples of the sentences are shown in Appendix E.\n\nConclusion\nWe presented a new task, SDT aiming to mediate speakers of different languages. We constructed the SpeechBSD dataset via crowdsourcing. We performed MT experiments utilizing context and showed its effectiveness. In the future, we plan to perform experiments in end-to-end ST settings and SDT utilizing speaker attributes.\n", "hypothesis": " We present a new task, speech dialogue translation mediating speakers of different languages.  We construct the SpeechBSD dataset for the task and conduct baseline experiments.  Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of utilizing context, namely monolingual context and bilingual context.  We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our settings..", "answer": true}
{"title": "Compositional Mathematical Encoding for Math Word Problems", "content": "\nIntroduction\nThe task of math word problem (MWP) solving aims to map natural language problem descriptions into executable solution equations to get the correct answer, which is a sub-area of neuro-symbolic reasoning. It requires perceptual abilities such as comprehending the question, identifying the quantities and corresponding attributes, as well as complex semantics understanding skills like performing logical inference, making comparisons and leveraging external mathematical knowledge.\nWhile MWP encoders have been sophisticatedly designed to understand the natural language problem description, the difference on understanding diverse types of problems has not been aware of. We find that MWP can generally be grouped into three categories based on the keywords in (Liu et al., 2019) , i.e., \"Story Problem\", \"Algebra Problem\" and \"Knowledge Problem\". \"Story Problem\" often includes significant amount of background information like characters, objectives and behaviors.\n\"Algebra Problems\" involves math notations or is composed of elementary concepts. \"Knowledge Problem\" asks for external knowledge like geometry and number sequence, as shown in Figure 1 .\nThese types of problems can be compositionally understood at the different level attention to the semantics modal and quantity modal, where RNNs and pre-trained language models usually focus on the textual information and GCN with quantitycentered graphs capture the relationship between quantities and contexts. However, the encoders in existing MWP solvers either model only the semantics modality or utilize quantity modal priors to refine the MWP encoding (Zhang et al., 2020; Shen and Jin, 2020) . Although the quantity centered refinement can particularly make improvements on quantity-demanding problems, its semantics understanding is weakened (evidence can be found in Table 3 ). This limitation, one joint modal cannot do it all, decreases the generalization of MWP solvers and is what compositional learning aims to address. In this work, we propose to disentangle semantics modal and quantity modal by compositional learning at the encoding stage, aiming to improve the generalization across different types of problems.\nContributions. (i) A novel and effective bimodal approach is proposed for the first time to enable MWP compositional understanding. (ii) A joint reasoning module is designed for our bimodal architectures to flexibly incorporate different modalities. (iii) Extensive experiments and ablative studies on two large-scale MWP benchmarks -Math23k (Wang et al., 2017) and MAWPS (Koncel-Kedziorski et al., 2016) show the superiority of the proposed approach over related works.\nText : 348 teddy bears are sold for $23 each. There are total 470 teddy bears in a store and the remaining teddy bears are sold for $17 each. How much did the store earn after selling all the teddy bears?\nEquation: 348\u00d723 + 470 \u2212 348 \u00d717\nQuantities: [348, teddy bears, sold] ; [$20, each] ; [470, teddy bears, total] ; [$17, remaining] .\n\nSemantics:\nSome teddy bears have been sold at a price; The left part will be sold at a different price; The goal is to compute the expect income.\nText : 2 times A is the same as 3 times B. B equals 28.\nCompute A.\n\nEquation: 28\u00d73 \u00f7 2\nQuantities: [2, times]; [3, times] ; [28, equals] .\n\nSemantics:\n2 times A equals 3 times B; B equals 28.\n\nStory Problem:\nAlgebra Problem: \n\nRelated Work\nCompositional Learning in NLP. Modeling compositionality in language has been a longstanding issue (Wong and Wang, 2007) in NLP community. One common practice is to perform disentanglement over language representations at different levels (Welch et al., 2020) .. They usually focus on atomic semantics units like character, word and phrase. As logic form annotations naturally own compositional features, compositionality is incorporated in generating correct logic contents. Therefore, the compositionality is often injected into traditional semantic parsing tasks (Chen et al., 2020; Yang et al., 2022) where the goals during training can be decomposed and then reorganized as a novel goal.\nOur work firstly tries to inject compositional prior into MWP encoding. It is worth noting that MWP solving owns the same well-organized logic form annotations as machine reasoning, which naturally requires compositionality.\nMath Word Problem Solving. Earlier MWP solvers parse problem descriptions semantically, and learn templates for generating answers (Koncel-Kedziorski et al., 2015) . Recent works (Wang et al., 2017; Xie and Sun, 2019; Li et al., 2019; Zhang et al., 2020; Shen and Jin, 2020; Wu et al., 2021b,a; Lin et al., 2021; Liang and Zhang, 2021; Jie et al., 2022) focus on employing the encoderdecoder framework (e.g., sequence-to-sequence, sequence-to-tree, graph-to-tree) to translate MWP texts into equations based on traditional RNN structure. There are also new settings (Amini et al., 2019; Miao et al., 2020) introduced to extend MWP solving in equation group generation and diagnosing awareness of external knowledge. Nowadays, many researchers build strong MWP solvers upon pre-trained language models (PLMs) (Huang et al., 2021; Li et al., 2021; Yu et al., 2021; Shen et al., 2021; Lan et al., 2022) and have achieved great performance. Differently, our work lays the groundwork of feature extraction of quantity modal, which is orthogonal to those works.\nIn this work, we not only propose an explicit compositional encoding module with a multi-layer design, but also incorporate detailed analysis to verify its compositional learning ability, to jointly leverage semantic and quantity information to achieve effective MWP understanding.\n3 Our approach\n\nCompositional Mathematical Encoder\nAs shown in Figure 2 , our CMEncoder block consists of a semantic encoder, a quantity encoder and a dynamic fusion block. The semantic encoder aims to extract semantic information from the problem description, understanding the background and objectives. The latter part encodes problems only with quantity-related graphs, helping the encoder to know the properties of quantities and the relationship between quantities and contexts. Semantic Encoder. To demonstrate the robustness of our approach, we implemented two different semantic encoders as our backbone. Firstly, we encode the problem description W by a bidirectional gated recurrent unit (BiGRU) (Cho et al., 2014) . The outputs of GRU are hidden state vectors of all tokens, H r = {h 1 , h 2 , ..., h n }, where n is the length of problem W .\nEQUATION\nwhere Embed s (W ) is the embedding result of textual description W in semantics modal. Empirically, we find that two stacked CMEncoders as shown in Figure 2 achieve the best performance. Secondly, pre-trained language models (PLMs) have been ubiquitous in NLP tasks. We use the latest push of MWP-BERT (Liang et al., 2022) as our semantic encoder to obtain H r .\nQuantity Encoder. To encode the quantity modal in the problem W , we feed a graph transformer G trans with Quantity Comparison Graph and Quantity Cell Graph following Graph2Tree (Zhang et al., 2020) ,\nEQUATION\nwhere Embed q (W ) is the embedding matrix in the quantity modal, which aims to improve the quantity representation by incorporating quantity magnitude information and quantity-context relationship with the above two graphs. Different from Graph2Tree, the two embeddings Embed s (W ) and Embed q (W ) are updated in the training process to extract the semantics and quantity feature separately. In this way, semantics and quantity modals are disentangled, which alleviates the issue of \"one joint modal cannot do it all\", enabling the C-MWP solver to pay different levels of attention when solving different problems.\nDynamic Fusion. To achieve joint reasoning over the semantics information and quantity information, we design a dynamic fusion module to flexibly incorporate the features from these two modals. First, we get s and q from the mean pooling of H r and H g , respectively. Then, cross-modal attention is applied between H r and q, H g and s:\nAtt\n1 (H r , q) = \u03a3 n i=1 a i H ri Att 2 (H g , s) = \u03a3 n i=1 b i H gi (3)\nwhere the attention scores a i , b i come from:\nEQUATION\nwhere W 1 a , W 2 a , W 1 b and W 2 b are parameter matrices. The cross-modal attention here grounds the quantity information in the semantics modal, and vice versa. By applying different weights on different modals, our model is flexible to pay more or less attention on a certain modal. Finally, the output of dynamic fusion is:\nH f = Att 1 (H r , q) Att 2 (H g , s).\n(5)\n\nStack Multiple CMEncoders\nHumans often need to make multiple glimpses to refine an MWP solution. Similarly, a CMEncoder can be stacked in multiple steps to refine the understanding of an MWP, as shown in Figure 2 . Given the output from the semantic encoder, quantity encoder and dynamic fusion module at layer k \u2212 1, the features are stacked as:\nH (k\u22121) att = c r H (k\u22121) r + c g H (k\u22121) g (6)\nwhere the attention weights c r and c g are:\nEQUATION\n))\nEQUATION\nwhere W 1 r , W 2 r , W 1 g and W 2 g are parameter matrices. The H (k\u22121) att will be the input for both the semantic modal and quantity modal of K-th CMEncoder, which will output\nH (k) r , H (k) g and H (k)\nf , which can be sent for the update at layer k + 1.\nAfter finishing the K-th step reasoning, we concatenate the final H (K) r and H (K) g as the final output representation H f inal .\n\nDecoder\nWe follow the same implementation as GTS (Xie and Sun, 2019) . Eventually, the decoder will output the pre-order traversal sequence of the solution tree.\n\nTraining Method\nGiven the training samples with problem description W and the corresponding solution S, the main training objective is to minimize the negative log probability for predicting S from W , empowered by the compositionality of the CMEncoders. Therefore, the overall loss is:\nL = L M W P + Embed s 2 + Embed q 2 (8)\nwhere L M W P is the negative log prediction probability \u2212 log p(S | W ). The L 2 norm of the encoder embedding matrices is added to the loss function as regularization terms.\n\nDatasets\nMath23k (Wang et al., 2017) \n\nExperimental Results\nAs Table 1 shows, our approach outperforms all other RNN-based baselines in terms of answer accuracy On Math23k, we outperform the latest RNNbased push from Wu et al. (2021a) by 1.8%. For the first time, an RNN-based MWP solver reaches over 80% answer accuracy on the Math23k dataset. What is more, the even fewer parameters with the best performance suggest that our model is also memory-efficient.\nPLM-based solvers benefit from the pre-training on a huge amount of corpus and thus achieve great semantic understanding ability. From a different point of view, our work aims to effectively and efficiently integrate semantic and quantity understanding. Therefore, by incorporating the MWP-BERT model as our semantic extractor, the answer accuracy of C-MWP achieves state-of-the-art performance. It proves the feasibility of combining the feature from the GNN Encoder of Graph2Tree.\nPerformance on Different Types of MWP.\nIn order to investigate how our model performs across various types of MWP, we introduce a new split of Math23k with regard to three types of problems: story problems, algebra problems and knowledge problems. Split details are shown in the appendix. The evaluation results are presented in Table 3 . Without a compositional manner, Graph2Tree and Multi-E/D perform better than GTS on story and algebra testing problems, whereas they perform worse on knowledge problems. As stated before, one joint modal cannot do it all. These baselines work well on some types of problems while having weak performance on other types of problems. Our C-MWP offers a general accuracy improvement, which firmly supports our motivation for alleviating the generalization issue. This provides clear evidence that our model leverages general math knowledge across different types of MWP, successfully solving some nontrivial problems that Graph2Tree failed to solve.\n\nConclusion and Future Work\nThe semantic meaning and quantity information are important intrinsic properties of a math word problem. Aiming at dealing with uni-modal bias and achieve better generalization, we make the first attempt to propose a compositional MWP solver, C-MWP. Multi-layer reasoning and specified training methods are leveraged to enhance the generalizability of the model. As the method could be applied in a broader range of neuro-symbolic learning problems, we will keep exploring the adaptiveness of this compositional encoding method.\n", "hypothesis": " Solving math word problem (MWP) remains a challenging task, as it requires to understand both the semantic meanings of the text and the mathematical logic among quantities, i.e., for both semantics modal and quantity modal learning.  Current MWP encoders work in a uni-modal setting and map the given problem description to a latent representation, then for decoding.  The generalizability of these MWP encoders is thus limited because some problems are semantics-demanding and others are quantity-demanding.  To address this problem, we propose a Compositional Math Word Problem Solver (C-MWP) which works in a bimodal setting encoding in an interactive way. Extensive experiments validate the effectiveness of C-MWP and show its inferiority compared to state-of-the-art models on public benchmarks.", "answer": false}
{"title": "Typo-Robust Representation Learning for Dense Retrieval", "content": "\nIntroduction\nDense retrieval is a fundamental component in many information retrieval applications, such as open-domain question answering and ad-hoc retrieval. The objective is to score and rank a large collection of candidate passages based on their similarity to a given query. The performance of dense retrieval relies on representation learning. A popular approach is to finetune a pre-trained language model to create an embedding space that puts each query closer to its corresponding passages (Zhan et al., 2020; Khattab and Zaharia, 2020; Xiong et al., 2021; Qu et al., 2021; Ren et al., 2021a,b) .\nOne of the major challenges of dense retrieval is the handling of misspelled queries which induces representations of the misspelled queries to be closer to irrelevant passages than their corresponding passages. Several studies have demonstrated that misspellings in search queries can substantially degrade retrieval performance (Zhuang and Zuccon, 2021; Penha et al., 2022) , specifically when informative terms, such as entity mentions, are misspelled (Sidiropoulos and Kanoulas, 2022) .\nTo create a retrieval model that is capable of handling misspelled queries, researchers have proposed different training methods to align representations of misspelled queries with their pristine ones. Zhuang and Zuccon (2021, 2022) devise augmentation methods to generate misspelled queries and propose training methods, Typos-aware Training and Self-Teaching (ST), to encourage consistency between outputs of misspelled queries and their non-misspelled counterparts. Alternatively, Sidiropoulos and Kanoulas (2022) apply contrastive loss to enforce representations of misspelled queries to be closer to their corresponding non-misspelled queries. Although these methods can improve the performance of retrieval models for misspelled queries, there is still a substantial performance drop for misspelled queries.\nIn this paper, we propose a training method to improve dense retrieval for handling misspelled queries based on the following desired properties:\n\u2022 Alignment: the method should be able to align queries with their corresponding passages. \u2022 Robustness: the method should be able to align misspelled queries with their pristine queries. \u2022 Contrast: the method should be able to separate queries that refer to different passages and passages that correspond to different queries. In contrast to the existing methods for handling misspelled queries that only satisfy the Alignment and Robustness properties, our method also aims to satisfy the Contrast property. Increasing the distance between dissimilar queries should help distinguish misspelled queries from other distinct queries. We design the following components for our training method: (i) Dual Self-Teaching (DST) incorporates the ideas of Dual Learning (Xia et al., 2017; Li et al., 2021) and Self-Teaching (Zhuang and Zuccon, 2022) to train robust dense retrieval in a bidirectional manner: passage retrieval and query retrieval. (ii) Query Augmentation generates a numerous number of misspelling variations for each query to supply our training objective.\nExperimental studies were conducted to assess the efficiency of the proposed method in comparison to existing approaches. We conduct experiments based on two different pre-trained language models. We evaluate using two passage retrieval benchmark datasets, a standard one and a specialized one for misspellings robustness evaluation. For each dataset, we measure performance on both misspelled and non-misspelled queries, where the misspelled queries are both generated and realworld queries. The experimental results show that the proposed method outperforms the best existing methods for enhancing the robustness of dense retrieval against misspellings without sacrificing performance for non-misspelled queries.\nWe summarize our contributions as follows: \u2022 We propose a novel training method to enhance the robustness of dense retrieval against misspellings by incorporating three desired properties: Alignment, Robustness, and Contrast. \u2022 We introduce Dual Self-Teaching (DST) which adopts the idea of Dual Learning and Self-Teaching to learn robust representations. In addition, we propose Query Augmentation to generate multiple views of a particular query under different misspelling scenarios. \u2022 We evaluate our method on misspelled and nonmisspelled queries from two passage retrieval datasets. The results show that our method outperforms the previous state-of-the-art methods by a significant margin on misspelled queries.\n\nMethodology\nWe propose a training pipeline to enhance the dense retrieval capability for handling spelling variations and mistakes in queries. As shown in Figure 1 , the training pipeline comprises three steps. (i) Query Augmentation: we augment each query in the training set into multiple misspelled queries using the typo generators provided by Zhuang and Zuccon (2021) . (ii) Similarity Score Calculation: we compute similarity score distributions between queries and passages for passage retrieval and query retrieval tasks using in-batch negative queries and passages, with additional hard negative passages.\n(iii) Dual Self-Teaching Loss Calculation: we compute the DST loss using the similarity score distributions to achieve all three desired properties.\n\nQuery Augmentation\nThe purpose of this step is to guide the learning with a broad array of possible misspelling patterns. Let Q denote a set {q 1 , q 2 , ..., q N } of N queries. From all queries in Q, we generate a set of K \u00d7 N misspelled queries\nQ \u2032 = {\u27e8q \u2032 1,k , q \u2032 2,k , ..., q \u2032 N,k \u27e9} K k=1\n, where K is the misspelling variations. We use five typo generators proposed by Zhuang and Zuccon (2021) , including: RandInsert, RandDelete, RandSub, SwapNeighbor, and SwapAdjacent. Please refer to Appendix A.2 for examples of the misspelled queries.\n\nSimilarity Score Calculation\nLet S(a, B) denote a function that computes a similarity score distribution of any vector a over any set of vectors B:\nEQUATION\nGiven P = {p 1 , p 2 , ..., p M } to be a set of M passages and\nQ \u2032 k = {q \u2032 1,k , q \u2032 2,k , ..., q \u2032 N,k } to be the k th set of misspelled queries in Q \u2032 ,\nwe compute two groups of score distributions as follows:\n\u2022 Passage retrieval: we calculate score distributions in a query-to-passages direction for each original query s p = S(q n , P) and misspelled query s \u2032k p = S(q \u2032 n,k , P). \u2022 Query retrieval: we calculate score distributions in a passage-to-queries direction for original queries s q = S(p m , Q) and each set of misspelled queries s \u2032k q = S(p m , Q \u2032 k ). This way, we produce four different score distributions (s p , s \u2032k p , s q , s \u2032k q ) for our training objective.\n\nDual Self-Teaching Loss Calculation\nWe design the Dual Self-Teaching loss (L DST ) to capture the three desired properties: Alignment, Robustness, and Contrast.\nL DST = (1 \u2212 \u03b2)L DCE Dual Cross-Entropy + \u03b2L DKL Dual KL-Divergence (2)\nDual Cross-Entropy loss (L DCE ) satisfies the Alignment and Contrast properties by utilizing cross-entropy losses to learn score distributions of the original queries for passage retrieval (s p ) and query retrieval (s q ) given labels y p and y q . Minimizing the L (P ) CE term will increase the similarity scores between queries and their relevant passages to be higher than other irrelevant passages by separating the relevant and irrelevant passages from one another. Minimizing the L (Q) CE term will increase the similarity scores between passages and their relevant queries to be higher than other irrelevant queries by separating the relevant and irrelevant queries from one another. In this manner, minimizing one of the two terms will align queries with their corresponding passages, satisfying the Alignment property. Moreover, minimizing both terms will separate queries that refer to different passages and passages that belong to different queries, satisfying the Contrast property.\nL DCE = (1 \u2212 \u03b3)L (P ) CE (s p , y p ) Passage Retrieval + \u03b3L (Q) CE (s q , y q ) Query Retrieval (3)\nDual KL-Divergence loss (L DKL ) aims to fulfill the Robustness property by using KL losses to match score distributions of misspelled queries {s \u20321 p , s \u20322 p , ..., s \u2032K p } and {s \u20321 q , s \u20322 q , ..., s \u2032K q } to the score distributions of the original query s p and s q .\nL DKL = 1 K K k=1 (1 \u2212 \u03c3)L (P ) KL (s \u2032k p , s p ) Passage Retrieval Consistency + \u03c3L (Q) KL (s \u2032k q , s q ) Query Retrieval Consistency (4) Minimizing L (P )\nKL and L (Q)\nKL will reduce the discrepancy between misspelled and non-misspelled queries for both query-to-passages and passage-toqueries score distributions. This way, we implicitly align representations of the misspelled queries to the original queries, satisfying the Robustness property. To stabilize training, we apply stop-gradient to the score distributions of the original queries (s p and s q ) in the L DKL . The \u03b2, \u03b3, and \u03c3 are the balancing coefficients selected by hyper-parameter tuning on a development set. With this loss combination, we achieve all three desired properties.\n3 Experimental Settings\n\nTraining Details\nWe experiment on two pre-trained language models, BERT (Devlin et al., 2019) and Character-BERT (El Boukkouri et al., 2020) . We train models only on the training set of MS MARCO dataset (Nguyen et al., 2016) . Moreover, the training data provided by the Tevatron toolkit (Gao et al., 2022 ) also contains hard negative passages. We include the training set details and hyper-parameter settings in Appendix A.1.\n\nCompetitive Methods\nTo show the effectiveness of our method, we compare our work with the following baseline and competitive training methods.\n\u2022 DPR (Karpukhin et al., 2020) KL losses. Note that their query augmentation method is identical to the Query Augmentation with K = 1. We retrain all models using the same setting described in the previous section. We report the results in the format of \"misspelled query performance (non-misspelled query performance)\".\nWe emphasize the best score with bold text and the second-best score with underlined text. We use \u2020 to denote DST results that significantly outperform the second-best result (p < 0.05).\n\nDataset and Evaluation\nDatasets. We evaluate the effectiveness of DST on two passage retrieval datasets, MS MARCO and DL-typo (Zhuang and Zuccon, 2022) , each with misspelled and non-misspelled queries. There are 8.8 million candidate passages for both datasets.\nThe development set of MS MARCO contains 6,980 non-misspelled queries. To obtain misspelled queries, we use the typos generator method proposed by Zhuang and Zuccon (2021) to generate 10 misspelled variations for each original query. The DL-typo provides 60 real-world misspelled queries and 60 corresponding non-misspelled queries that are corrected manually.\nEvaluation. We use the standard metrics originally used by each dataset's creators. For MS MARCO, each misspelled query performance is the average of 10 measurements. We employ Ranx evaluation library (Bassani, 2022) to measure performance and statistical significance. Specifically, we use a two-tailed paired t-test with Bonferroni correction to measure the statistical significance (p < 0.05).\n\nMain Results\nAs shown in Table 1 , the results indicate that DST outperforms competitive methods for misspelled queries in every case without sacrificing performance for non-misspelled queries in eight out of ten cases. We observe some performance trade-offs for the BERT-based model in the DL-typo dataset's non-misspelling scores (nDCG@10 and MRR). Aside from that, there is no performance trade-off for the CharacterBERT-based model. These outcomes conform with the observation in Figure 2 (Section 4.4) that DST improves the Robustness and Contrast of misspelled queries.\n\nQuery Augmentation Size Study\nTo study the benefit of query augmentation and find the optimal augmentation size, we measure the performance of BERT-based dense retrieval models trained with DST using the query augmentation size K of 1, 10, 20, 40, and 60. Note that the query augmentation method used in previous works is a special case of Query Augmentation when K = 1. We report the results using MRR@10 for the development set of the MS MARCO dataset. We also report training time to show trade-offs between performance and computation. Table 2 : Results of query augmentation size study. We train all models in this experiment on a V100 32G GPU.\nAs shown in Table 2 , the results indicate that increasing K improves the performance of both misspelled and non-misspelled queries, but only up to a certain point, after which the performance begins to decline. We observe that setting K = 40 produces the best results, and there is no further performance improvement after this point.\n\nLoss Ablation Study\nIn this experiment, we study the benefit of each term in DST by training BERT-based dense retrieval models on variant loss combinations with K = 40. The results in Table 3 reveal that L \n\nQuery Distributions\nThe purpose of this section is to study the impact of our training method on the Robustness and Contrast of misspelled queries. We also compare our method against the baseline and competitive methods to show its effectiveness. The Robustness and Contrast of misspelled queries are illustrated using the following kernel density graphs: \u2022 Original-to-Misspell: the cosine similarity distribution between original and misspelled queries. \u2022 Original-to-Neighbor: the cosine similarity distribution between original and neighbor queries. The Robustness property is emphasized by the Original-to-Misspell distribution having high cosine similarity. On the other hand, the Contrast property is emphasized by the small overlapping between Original-to-Misspell and Originalto-Neighbor distributions. The results in Figure 2 show that our method (c) produces the best Robustness and Contrast properties for misspelled queries in comparison to other methods.\n\nConclusion\nThis paper aims to address the misspelling problem in dense retrieval. We formulate three desired properties for making dense retrieval robust to misspellings: Alignment, Robustness, and Contrast. Unlike previous methods, which only focus on the Alignment and Robustness properties, our method considers all the desired properties. The empirical results show that our method performs best against misspelled queries, revealing the importance of the Contrast property for handling misspellings. \n", "hypothesis": " Dense retrieval is a basic building block of information retrieval applications.  One of the main challenges of dense retrieval in real-world settings is the handling of queries containing misspelled words.  A popular approach for handling misspelled queries is minimizing the representations discrepancy between misspelled queries and their pristine ones.  Unlike the existing approaches, which only focus on the alignment between misspelled and pristine queries, our method also improves the contrast between each misspelled query and its surrounding queries.  To assess the effectiveness of our proposed method, we compare it against the existing competitors using two benchmark datasets and two base encoders.  Our method outperforms the competitors in all cases with misspelled queries.  Our code and models are available at https://github.  com/panuthept/DST-DenseRetrieval..", "answer": true}
{"title": "Towards Argument-Aware Abstractive Summarization of Long Legal Opinions with Summary Reranking", "content": "\nIntroduction\nLegal opinions contain implicit argument structure spreading across long texts. Existing summarization models often struggle to accurately capture the main arguments of such documents, leading to summaries that are suboptimal (Xu et al., 2021; Elaraby and Litman, 2022) . We propose an approach for the abstractive summarization of long legal opinions that leverages argument structure.\nLegal opinions often follow a specific argumentative structure, with the main points of the argument being presented clearly and logically (Xu et al., 2021; Habernal et al., 2022; Xu and Ashley, 2022) . Prior work has shown that by considering this structure during summarization, it is possible to generate extractive and abstractive summaries that more accurately reflect the original argumentation in the document (Elaraby and Litman, 2022; Zhong and Litman, 2022; Agarwal et al., 2022) . In this paper, we present a framework for abstractive summarization of long legal opinions that extends this literature by leveraging argument structure during summary reranking to both generate and score candidates. Our method involves utilizing the Longformer-Encoder-Decoder (LED) (Beltagy et al., 2020) model to generate multiple candidate summaries by training it on various input formats. This allows for the consideration of different argument representations in the summary generation process. Additionally, we use beam search to further diversify the output. Finally, we rank the candidate summaries by measuring their lexical similarity to the input's main arguments.\nWe evaluate our approach on a dataset of long legal opinions obtained from the Canadian Legal Information Institute (CanLII) 1 and demonstrate that our method outperforms competitive baselines. Our results with ROUGE and BERTScore (Lin, 2004; Zhang et al., 2019) suggest that considering the argumentative coverage of the original opinions can lead to a more effective selection of summaries.\nOur contributions are:\n(1) We propose a simple reranking approach that takes into account the argumentative structure of legal opinions to improve over the standard finetuning of generation models. (2) We demonstrate through empirical results and ablation analysis reasons for the effectiveness of our approach for summarizing long legal opinions. Our code can be accessed through this repository: https://github.com/ EngSalem/legalSummReranking 2 Related Work Long Legal Document Summarization Legal documents have a distinct format, with a hierarchical structure and specialized vocabulary that differs from that of other domains (Kanapala et al., 2019) . They also tend to be longer in length (Kan et al., 2021; Huang et al., 2020; Moro and Ragazzi, 2022) , which has led to the use of transformer models with sparse attention mechanisms (Michalopoulos et al., 2022; Guo et al., 2022; Beltagy et al., 2020) to reduce the complexity of encoding lengthy text. Legal opinions, in particular, have a complex argu-mentative structure that spans across the text, making it crucial to address in summaries (Xu et al., 2021; Xu and Ashley, 2022; Elaraby and Litman, 2022) . We use prior legal opinion summarization methods as evaluation baselines.\nSummarization and Argument Mining Using a dialogue summarization dataset with argument information, Fabbri et al. (2021b) converted an argument graph into a textual format to train a summarizer. For legal documents, Agarwal et al. (2022) used argument role labeling to improve extractive summarization using multitask learning. Elaraby and Litman (2022) blended argument role labeling and abstractive summarization using special markers, generating summaries that better aligned with legal argumentation. We incorporate the models of Elaraby and Litman (2022) into summary reranking and further improve performance.\nSecond Stage Reranking Generating multiple outputs and reranking them according to certain criteria has been successfully applied in NLP downstream applications including abstractive summarization. Some methods use different input formats to generate multiple outputs. Oved and Levy (2021) perturbed input multi-opinion reviews to generate multiple candidate summaries, then ranked them using coherency. Ravaut et al. (2022) used a multitask mixture of experts to directly model the probability that a summary candidate is the best one. Liu and Liu (2021) ranked candidate summaries generated from 16 diverse beam searches to improve news summarization in terms of ROUGE score. Liu et al. (2022) presented a novel technique for summary reranking that involves a non-deterministic training objective. Their approach enables the model to directly rank the summaries that are probable from beam-search decoding according to their quality. We rely on distinct argument-aware input formats in addition to diverse beam decoding to develop our argument-aware reranking method.\n\nAnnotated Dataset\nWe employ the annotated subset (Xu et al., 2021; Elaraby and Litman, 2022) of the CanLII dataset (Zhong and Litman, 2022) used in prior summarization research of legal opinions. This subset contains 1049 opinion/summary pairs annotated with sentence-level argument role labels for both input documents and reference summaries. The input opinions have mean/max lengths of 4375/62786 words, motivating us to use models for long text.\nRecent work has proposed argument role taxonomies aligned with structures commonly found in legal text (Habernal et al., 2022; Xu et al., 2021) . The CanLII data was annotated for argument roles using the IRC scheme for legal opinions (Xu et al., 2021) , which divides argument roles into Issues (legal questions which a court addressed in the document), Reasons (pieces of text which indicate why the court reached the specific conclusions), and Conclusions (court's decisions for the corresponding issues). We use these 3 fine-grained IRC labels, as well as collapse them into a single argumentative label, to incorporate argument structure into our models. An IRC-annotated opinion and summary pair can be found in Appendix A.\n\nModel and Methods\nOur proposed method follows the generate and ranking paradigm and can be split into two parts. First, we explore techniques to utilize an argumentation augmented LED model to generate multiple candidate summaries S. Second, we propose a function \u00b5 that scores a summary S where S \u2208 S based on its argumentative alignment with the input document. The best candidate S * is selected such that S * = arg max S i \u2208S {\u00b5(S 1 ), \u00b5(S 2 ), .., \u00b5(S n )}. Figure 1 shows an overview of our approach.\n\nGenerating Candidates: Argument-Aware\nTraining + Diverse Decoding\nDiverse decoding techniques such as beam-search can help diversify the summary output; however, it's only limited to the underlying language model used in the decoder and is completely isolated from the input format. Alternatively, we propose to complement the beam search via finetuning LED on three different input formats. We refer to this model as M arg\u2212augmented such that the model parameter \u03b8 * arg\u2212augmented is selected such that\n\u03b8 * arg\u2212augmented = arg max \u03b8 P (S|X)\nDuring finetuning, S is the reference summary, \u03b8 represents the trainable model parameters, and X is a set of inputs X = {X raw , X arg_binary , X arg_f inegrained }, where X raw is the input without the argument markers, X arg_binary is the input document with binary argument markers added to highlight argument role sentences, and X arg_f inegrained is the input document with the fine-grained argumentative markers added to also delineate the roles (i.e., Issue, Reason, Conclusion). These three representations of the input share the same reference summary, meaning that we augmented the training data three times. Table 1 shows an example of the distinct representations of our new training data. At inference time, we use the predicted markers by adopting the argument mining code 2 from Elaraby and Litman (2022) instead of the manually labeled ones to construct Xarg_binary , Xarg_finegrained of X where X = {X raw , Xarg_binary , Xarg_finegrained }. Our incentive is that different formats of the input would yield different generated summaries that take into account different representations of the argumentative structure in the input.\n\nScoring and Reranking Summaries\nWe propose a scoring method to rank the candidate summaries based on their capability to capture the main argument points in the input. First, we employ a sentence-level argument role classifier to extract sentences with argument roles Xargs . The predicted sentences are used to construct an extractive summary. Then, we measure the lexical overlap between a generated candidate summary \u015c and the constructed extracted one using ROUGE-1 F1-score 3 , to compute a score to each candidate summary that represents its alignment with the legal opinion argument content. Our scoring function \u00b5 can be written as \u00b5 = ROU GE1( Xargs , \u015c).\nInput format Example Xraw S1|S2|...| Issue Sentence | Rea- son Sentence |... X arg_binary S1|S2|...| <IRC> Issue Sentence </IRC> | <IRC> Reason Sentence </IRC> |... X arg_f inegrained S1|S2|...| <Issue> Issue Sen- tence </Issue> | <Reason> Rea- son Sentence </Reason> |...\n\nExperiments\nAll models use LED-base checkpoint as a base model. LED-base encodes up to 16k tokens, which fits our long inputs. All experiments use 5-fold cross-validation, with the 4-fold documents split into 90% training and 10% validation; the validation split is used to select the best checkpoint. 4 We compare all rank-based methods (baseline and proposed) to abstractive baselines previously explored in legal opinion summarization: finetune LED-base (which refers to vanilla model finetuning using our dataset), and arg-LED-base (Elaraby and Litman, 2022) (which finetunes LED on the dataset blended with argument markers that mark the start and the end of each argument role in the input). 5 We also compare our proposed rank-based approach from Section 4 with ranking baselines that use different input formats or diverse decoding alone. Specifically, we have employed ranking on top of the output of the three LED models outlined in Elaraby and Litman (2022) which are trained on distinct argument aware input formats (we refer to this model as \"baseline ranking\"). Additionally, for diverse decoding, we have employed different beam widths within the range of 1 and 5 6 on top of the model trained on the input with fine-grained markers (arg-LED-fine-grained), which achieved the best abstractive baseline ROUGE results.\nAll models utilizing argument markers employed both oracle and predicted conditions during inference time, using human annotations or argument mining respectively, to produce the markers.\n\nResults and Discussion\nTable 2 shows our results in terms of ROUGE-score (Lin, 2004) and BERTScore (Zhang et al., 2019), computed using SummEval (Fabbri et al., 2021a) 7 .\nUtility of any Ranking The ranking-based methods (rows 6-13) consistently outperform the abstractive baselines 8 (rows 1-5) in both predicted 5 Argument marker details can be found in Appendix C. 6 We ran out of memory with BeamWidth > 5. 7 https://github.com/Yale-LILY/SummEval 8 See Appendix D for extractive baseline results.\nand oracle conditions. Also, abstractive baseline results (rows 1-5) align with those of Elaraby and Litman ( 2022), where leveraging fine-grained markers in the input yields the highest scores.\nUtiliy of Proposed Ranking Framework and its Components In the predicted case, our proposed arg-augmented-LED (row 10) improves over the abstractive baselines (rows 1-3) with ranges 1.5 \u2212 3.19 and 1.27 \u2212 3.07 in ROUGE-1 and ROUGE-L respectively, while maintaining a limited drop of 0.1 and 0.01 in terms of ROUGE-2 and BS respectively. Similarly, compared to our ranking baselines, our proposed model improves over ROUGE-1 and ROUGE-L scores obtained by baseline ranking with ranges 0.56 \u2212 0.73 while dropping in ROUGE-2 and BS by 0.31 and 0.02 points respectively. This indicates that incorporating argument information into the source inputs can lead to the generation of effective summary candidates. Our best predicted results were achieved by combining our proposed model with diverse beam decoding (row 11), which combines the strengths of various input formats and multiple beam decoding, resulting in statistically significant improvements over the previously proposed argument-aware abstractive baseline (row 3).\nInference with Predicted versus Oracle Argument Roles For the same model, predicted markers can impact the summarization results. In prior baselines (rows 3 and 5), we observe a drop in ROUGE score with ranges 2.05 \u2212 2.14, and 0.06 in terms of BS when switching from oracle to predicted markers. This observation is consistent among row 6 and 8; and row 10 and 12. With our proposed arg-augmented-LED and diverse beam decoding, this performance gap is mitigated and reduced to \u22120.02 \u2212 0.66 and \u22120.03 in ROUGE and BS, respectively (rows 11 and 13). We believe this is due to the combination of distinct argumentative formats and diverse decoding, allowing more diverse candidates to be considered in the ranking and enhancing robustness to noisy predictions during inference.\n\nConclusion and Future Work\nWe proposed a framework for improving the summarization of long legal opinions by combining distinct argument formats of the input with diverse decoding to generate candidate summaries. Our framework selects the summary with the highest lexical overlap with the input's argumentative content. Our results indicate that ranking alone can improve over abstractive baselines. Moreover, combining ranking with our proposed candidate generation method improves results while maintaining robustness to noisy predictions. In future research, we plan to incorporate human expert evaluations to compare automatic metrics with human ratings. Also, we aim to explore the impact of using noisier argument roles during training on a larger corpus by using the predicted markers obtained from our smaller dataset to experiment with the remaining unannotated portion of the CanLII dataset.\n", "hypothesis": " We propose a simple approach for the abstractive summarization of long legal opinions that considers the argument structure of the document.  Legal opinions often contain complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the main points of the legal opinion. Our approach involves using document structure information to generate multiple candidate summaries, then reranking these candidates based on alignment with the document's argument role.  We demonstrate the effectiveness of our approach on a dataset of long legal opinions and show that it outperforms several strong baselines..", "answer": false}
{"title": "Abstractive Text Summarization Using the BRIO Training Paradigm", "content": "\nIntroduction\nText summarization reduces the size of the original text while preserving its main content. The two main approaches for constructing summaries are extractive and abstractive. Extractive summarization directly lifts sentences or words which convey key topics of the original documents, and concatenates them. Abstractive summarization discovers the primary content of the documents and generates summaries. Abstractive summaries are usually more natural and coherent than extractive summaries.\nMost abstractive summarization models follow the encoder-decoder framework. Existing abstractive summarization models are trained using maximum likelihood estimation and rely on the reference summaries. Liu et al. (2022a) propose a BRIO training paradigm to address reliance on reference summaries by assuming non-deterministic distribution of system-generated candidate summaries. In this paper, we use the BRIO training paradigm for abstractive summarization models to construct summaries for documents in English and Vietnamese. We make the following contributions:\n\u2022 We adapt the BRIO training paradigm for abstractive summarization using BART-based and T5-based models as backbones.\n\u2022 We present issues with the BRIO paradigm.\n\u2022 We investigate abstractive summarization models using BARTpho-BRIO and ViT5-BRIO to obtain improved results.\n\u2022 We publicly release the VieSum summarization dataset for research purpose.\nThe remainder of this paper is organized as follows. Related work is presented in Section 2. Section 3 introduces a large dataset for summarization in Vietnamese, named VieSum. Experiments and discussion are presented in Section 4. Section 5 concludes the paper. (Li et al., 2017) , actor-critic approaches from reinforcement learning (Li et al., 2018) , and Transformer (Vaswani et al., 2017) . Liu et al. (2022b) develop the PageSum model for abstractive summarization by incorporating locality bias in both encoder and decoder. Each document is partitioned into non-overlapping pages.\n\nRelated Work\nThe encoder, which is an abstractive summarizer, encodes each page and makes local predictions. The decoder predicts output based on a weighted combination of local predictions. The authors fine-tune the BART model (Lewis et al., 2020) for abstractive summarization and investigate several approaches to locality, such as spatial locality, discourse locality, and document locality. Page-Sum outperforms abstractive summarization models such as longformer encoder-decoder (Beltagy et al., 2020) , encoder-decoder attention with headwise positional strides (Huang et al., 2021) , and BART with Hierarchical Attention Transformer (Rohde et al., 2021) . However, PageSum takes a long time to train, requires large memory size, and fails to capture long distance dependencies.\nSeveral studies use pre-trained models for abstractive text summarization. Farahani et al. (2021) use mT5 (Xue et al., 2021) and sequence to sequence ParsBERT (Rothe et al., 2020) to construct abstractive summaries for Persian texts. T5 (Raffel et al., 2020) and BERT (Devlin et al., 2018) have also been used to construct abstractive summaries (Garg et al., 2021) . Kieuvongngam et al. (2020) summarize COVID-19 biomedical research articles using BERT and GPT-2 (Radford et al., 2019) . Features of documents are extracted and integrated into an abstractive model to improve summary generation. Nambiar et al. (2022) develop an encoder-decoder model using attention, in which POS features are incorporated to the word embedding layers to enhance the word vectors. Experiments on a dataset in Malayalam show that the integration of attention model and POS features is better than the seq2seq and attention models. Barna and Heickal (2021) adapt the pointer generator network for abstractive summarization by combining a pre-trained word embedding layer for transferring semantic similarity and topic features for better topic coverage. A drawback of usual abstractive summarization is the omission of named entities. To ameliorate, Berezin and Batura (2022) train a named entity recognition model based on ROBERTa to discover named entities. Then, the BART masked named entity language model is trained to pay attention on the name entities. Finally, BART is fine-tuned for text summarization.\nMost studies to construct abstractive summaries in Vietnamese use an encoder-decoder framework or a pre-trained model. Quoc et al. (2019) integrate sentence positions and term frequencies into a pointer generator network with a coverage mechanism to perform the abstractive summarization for Vietnamese documents. Lam et al. ( 2022) construct abstractive summaries for online newspapers using RNN with attention, BiLSTM with copy generator, standard Transformer, BERT, and sequence-to-sequence abstractive models using bottom-up approach. Phan et al. (2022) perform experiments to summarize Vietnamese documents using Transformer-based encoder-decoder architectures such as Transformer, PhoBERT (Tran et al., 2022) , and ViT5 (Phan et al., 2022) .\n\nVieSum Dataset\nWe construct a VieSum dataset for Vietnamese consisting of 1,627,415 documents and their corresponding summaries, grouped into 23 categories. In particular, BeautifulSoup 1 and Newspaper3k 2 are used to collect and extract articles from popular online newspapers in Vietnamese such as vnexpress.net, dantri.com.vn, danviet.vn, vietnamnet.vn, laodong.vn, and vov.vn . The summaries and content documents are considered reference summaries and documents, respectively.\n\nExperimental Results\nWe perform experiments in the Google Colaboratory environment, NVIDIA Tesla T4 16GB. We use the CNNDM 3 dataset in English, and our VieSum dataset in Vietnamese. Due to limitation of the hardware, we perform experiments with 70,000 documents picked randomly and their corresponding reference summaries from VieSum. Each dataset is split into 3 parts including 75% for training, 8% for validation, and 17% for testing.\nIn this paper, the pre-trained BART 512-lengthbased and T5 512-length -based models are used as backbones for generating abstractive summaries. The BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) models are trained on the CNNDM dataset, while the BARTpho (Tran et al., 2022) and ViT5 (Phan et al., 2022) are trained on the VieSum dataset. All models are base models. To make it easy for comparison, we use the same parameters as suggested by the original authors. \n\nStandard Abstractive Models\nFirst, we experiment and evaluate abstractive summarization approaches using standard BART-base and T5-base models. We train the models using a batch size of 4, epoch count of 5, learning rate of 10 \u22125 , warmup step of 20,000, and the Adam optimizer. The results of abstractive summarization systems using the standard backbone models are presented in Table 1 .\n\nFine-tuning Abstractive Models\nTo improve the quality of summaries created, we fine-tune the backbone models using the Trainer provided by Hugging Face 4 . We do not fine-tune the BART model because it is already fine-tuned on the CNN dataset. Table 2 shows the ROUGE scores of the fine-tuned abstractive models.\n\nFine-tuning Abstractive Models and BRIO\nThe BRIO (Liu et al., 2022a) while the evaluator is trained using a contrastive loss (Hadsell et al., 2006) .\nIn BRIO, a backbone model is used to produce N abstractive summaries, the so-called candsums, for each document. Each candsum is assigned a quality score by obtaining the average score of its ROUGE-1, ROUGE-2, and ROUGE-L values. In particular, Liu et al. (2022a) use the BART 1024-length model to create 16 candsums for each document. Next, documents, reference summaries, and corresponding candsums sorted by the descending quality scores are used to train the abstractive summarization model using the BRIO paradigm. We note that Liu et al. (2022a) use the standard models as back-bones and train them with the BRIO paradigm.\nIn our work, the fine-tuned backbone abstractive summarization models, presented in the previous section, are used to produce N=6 candsums for each document using diverse beam search (Vijayakumar et al., 2018) with num beam groups=6, diversity penalty=1.0, and num beams=4. The abstractive summarization models are trained using a learning rate of 10 \u22123 , and the Adafactor optimizer. Liu et al. (2022a) claim that BRIO training helps the models reach the best performance within one epoch on the CNNDM dataset 5 . Therefore, we use one epoch for training the fine-tuned summarization models with the BRIO paradigm. The results of the abstractive summarization systems trained with BRIO are presented in Table 3 .\n\nFine-tuning Abstractive Models and BRIO-Loop\nAs suggested by Liu et al. (2022a) , we perform loop processing, using the candsums created by the abstractive summarization models trained with BRIO to train the models. However, after several Experimental results show that the BRIO training paradigm significantly helps improve the abstractive summaries by reducing the dependence of the system on the reference summaries. However, assigning weights to both candsums and reference summaries is necessary in order to decrease reliance on reference summaries. The diverse beam search helps obtain diverse candsums, but could cause interference in the beam search space because the model might not follow the reference summaries. In addition, using the ROUGE metric for evaluating the abstractive summarization models trained with the BRIO paradigm seems unfair because these models could produce summaries which are independent on the reference summaries.\n\nDiscussion\nIt is not easy to make comparisons between models trained on different hardware and on different datasets. We make an attempt to compare our work with published papers on similar datasets.\nCurently, BRIO using a standard BART 1024-length model as backbone, which generates 16 candsums, achieves SOTA results on the CNNDM dataset with a ROUGE-1 of 47.78 and a ROUGE-L of 32.58 (Liu et al., 2022a) . In addition, BART 1024-length -BRIO with 2 iterations reaches ROUGE-1 and ROUGE-L of 48.01 and 44.67, respectively; these are both better than our BART 512-length -BRIO, which creates 6 candsums for each document, after 2 iterations: 46.55 for ROUGE-1 and 43.00 for ROUGE-L. Tawmo et al. (2022) fine-tune the T5 abstractive summarization model and evaluate on the CNNDM dataset. Their T5 model achieves ROUGE-1 and ROUGE-L scores of 40.79 and 34.80, respectively, which are lower than the scores of our fine-tuned T5 model, and significantly lower than scores of our best model, the T5-BRIO-Loop model: 45.24 for ROUGE-1 and 41.80 for ROUGE-L.\nFor Vietnamese abstractive summarization, Quoc et al. ( 2019) use LSTMs with the features of sentence positions and term frequencies (LSTM+SP+TF) on a Vietnamese dataset collected from Baomoi 6 . The best ROUGE-1 and ROUGE-L scores of their model are 31.89 and 29.97, respectively, which are significantly lower than the scores of our BRIO-BART model.\nBoth the BARTpho and ViT5 models trained with the BRIO paradigm outperform all models proposed by Lam et al. ( 2022) on the CTUNLPSum dataset, which is very similar to the VieSum dataset, including the sequence-to-sequence models, copy generator network, sequence-to-sequence with rewriter approach, and bottom-up approach.\nTran et al. ( 2022) apply several models for abstractive summarization on the VNDS (Nguyen et al., 2019) dataset. They perform experiments on 8 A100 GPUs with 40GB each. Their model is trained for 15 epochs in about 6 days. Their best model, BARTpho, achieves a ROUGE-1 of 61.14, which is slightly higher than the BARTpho-BRIO-Loop, and a ROUGE-L of 40.15, which is lower than that of the BARTpho-BRIO-Loop. In addition, the BARTpho-BRIO-Loop is trained on one epoch in about 32 hours using basic hardware. Phan et al. (2022) introduce a pre-trained text-totext Transformer for Vietnamese abstractive summarization, called ViT5. The authors claim the ViT5 model as the SOTA for Vietnamese abstractive summarization. Their ViT5 abstractive summarization model achieves ROUGE-1 and ROUGE-L of 61.85 and 41.70, respectively, on the VNDS dataset (Nguyen et al., 2019) . We conducted experiments on VNDS and found interesting results related to the ViT5 model. The ROUGE scores of the ViT5 model trained using the common paradigm are essentially identical to the ROUGE scores provided by Phan et al. (2022) . However, the scores of the ViT5 model trained using the BRIO paradigm are reduced to 59.37 and 41.6, respectively. On the VieSum dataset, the standard ViT5base achieves an ROUGE-1 of 53.39 and ROUGE-L of 35.88; while the ViT5-BRIO-Loop has better scores: ROUGE-1 of 60.90 and ROUGE-L of 44.36. We leave further exploration and evaluation these unstable results for future work.\n\nConclusion\nWe investigated abstractive summarization models trained with the BRIO paradigm. Experiments show that we can improve abstractive summarization models by fine-tuning the backbones before training them with BRIO. In particular, the summarization models trained with BRIO outperform other summarization models in Vietnamese. We also discuss issues with the BRIO paradigm for further exploration. In addition, we built the VieSum dataset for summarization in Vietnamese. For future work, we will ask volunteers to evaluate and provide feedback on a small subset of the VieSum dataset.\n", "hypothesis": " sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries.  The BRIO training paradigm assumes a non-deterministic distribution to reduce the model's dependence on reference summaries, and improve model performance during inference.  This paper presents a straightforward but effective technique to improve abstractive summaries by finetuning pre-trained language models, and training them with the BRIO paradigm.  We build a text summarization dataset for Vietnamese, called VieSum.  We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets.  The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese..", "answer": true}
{"title": "Enhancing Out-of-Vocabulary Estimation with Subword Attention", "content": "\nIntroduction\nWord embeddings are very useful in natural language processing tasks. Methods like word2vec (Mikolov et al., 2013a,b) and GloVe (Pennington et al., 2014) train strong semantic representations of words using co-occurrence statistics on a large text corpus, and have been shown to be effective at semantically representing text data. However, one weakness of these methods is that they only learn representations for words that exist in the training corpus, and therefore have no representations on unknown terms, known as out-of-vocabulary (OOV) words. Contextualized embeddings like BERT (Devlin et al., 2018) also suffer from weak performance on rare and unknown words, despite being able to build a contextualized representation of them (Schick and Sch\u00fctze, 2020) . Therefore learning representations for OOV words is an important endeavour. In this work, we focus on static embeddings, where a large amount of OOV work is focused on, and leave contextualized embedding to future work.\nCurrent approaches combine subword and context information to estimate OOV words. While these approaches apply attention mechanisms to aggregate context representations, they tend to do very little with subword representations. As a result, this paper proposes SubAtt, a deep neural network attention model that estimates OOV word representations using attention layers (Vaswani et al., 2017) on the subwords in addition to the contexts. SubAtt also pretrains subword representations, allowing it to learn quality representations before combining it with context. We show that both pretraining and applying attention on subwords improves OOV estimates, and show that SubAtt generally outperforms state-of-the-art OOV estimation models in both intrinsic and extrinsic tasks.\n\nRelated Work\nThere are multiple strategies to estimate OOV embeddings. Some OOV strategies use word roots of the OOV word to estimate OOV embeddings (Bojanowski et al., 2017; Pinter et al., 2017; Sasaki et al., 2019) while other methods use the OOV word's context (Lazaridou et al., 2017; Horn, 2017; Herbelot and Baroni, 2017; Arora et al., 2017; Mu and Viswanath, 2018; Khodak et al., 2018) . However, more recent attempts combine both subwords and context approaches. Schick and Sch\u00fctze propose the Form-Context model (Schick and Sch\u00fctze, 2019c), which estimates OOV embeddings by combining the sum of ngram embeddings (learned by the model) with the sum of word embeddings in the contexts multiplied by a weight matrix (also learned by the model). This model has been extended to the Attentive Mimicking model (Schick and Sch\u00fctze, 2019a) which adds an attention mechanism to the context calculations. A second combined approach is the attention based hierarchical context encoder, known as HiCE (Hu et al., 2019) . HiCE is a transformer based model that leverages the hierarchical structure of contexts, using a transformer encoder to encode each context sentence into a sentence embedding, and then using another transformer encoder to combine each sentence embedding into a full context embedding. It estimates subword information using a character based convolutional neural network (CNN), and then combines each piece into a final OOV embedding. HiCE also adapts its model to the OOV word's corpus using Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) . Another approach, Estimator Vectors (Patel and Domeniconi, 2020) , trains its own word embeddings, along with subword and context embeddings for OOV estimation. BERTRAM (Schick and Sch\u00fctze, 2019b) applies an approach similar to the above models, but for contextualized embedding models like BERT (Devlin et al., 2018) . While these approaches create strong estimates for OOV words, they have some weaknesses. First, although some use attention mechanisms with the context of an OOV word, none of the aforementioned combined approaches use attention for processing the OOV's subwords 1 . Secondly, none of the static embedding approaches pretrain their subwords; they learn these representations at the same time as the whole model 2 . Therefore, we propose SubAtt, a model that uses attention and pretraining on subwords, leading to stronger OOV estimates.\n\nSubAtt\nWe now present SubAtt, a transformer (Vaswani et al., 2017) based model for OOV estimation. First, we describe pretraining the subword representations in Section 3.1, then how the model encodes each context sentence in Section 3.2, and finally how SubAtt combines subword and context information in Section 3.3.\n\nPretraining Subword Representations\nFirst, SubAtt learns subword representations for the current set of word embeddings. SubAtt learns embeddings for character ngrams of each vocabulary word. This is accomplished by adding a beginning and end special token to the word, and then taking each character subset of that word. We learn representations using the following formulation:\nsub wt = 1 |G wt | g\u2208Gw t z g (1)\nwhere G wt is the set of character n-grams (the subwords) of the word w t , and z is the embedding of the subwords. Subword representations z are learned by maximizing the cosine similarity between sub wt and the corresponding word embedding v wt . Once these subword representations are trained, they are used in the main SubAtt model. An OOV word is broken down into its character ngrams, which are then converted to the set of corresponding subword embeddings Z.\n\nContext Encoder\nSubAtt encodes sentences using a context encoder similar to the one in HiCE (Hu et al., 2019) . For each word, an input embedding is built by combining its word embedding and a position embedding. The set of input embeddings for context j (denoted context words Q j ) are then inputted into a transformer encoder:\nEQUATION\nwhich is then averaged for a final context representation c j . These representations make up the set of context embeddings C.\n\nFull SubAtt Model\nSubAtt is composed of a subword half and a context half. The subword inputs Z are the OOV word's ngram subword representations learned in Section 3.1. For the list of contexts, the context representations C are calculated using the architecture described in Section 3.2. Each type is processed through their own sets of multi-head self attention encoders \nEQUATION\nEQUATION\nFinally, we combined the representations for a final estimate of the OOV embedding. Subwords and contexts can vary in how informative they are to the OOV word, and so it is important to combine them in a fashion that weighs each estimate accordingly. SubAtt uses an adaptive weighting strategy used in the Form Context Model and Attentive Mimicking Model (Schick and Sch\u00fctze, 2019c,a), known as the gated model. The subword outputs Z self and context outputs C self are separately averaged into v subword and v context respectively. They are then combined by a weighted sum:\nv f inal = \u03b1 v subword + (1 \u2212 \u03b1) v context (5)\nThe weight \u03b1 is calculated as follows:\n\u03b1 = \u03c3(w T [ v subword , v context ] + b) (6)\nwhere w and b are learned parameters, and \u03c3 is the sigmoid function. v f inal is the final estimate of the OOV word embedding. SubAtt has eight layers of self attention for the subword inputs and eight layers for the context input.\n\nTraining Corpus and Word Embeddings\nThe goal of SubAtt is to estimate representations for OOV words given existing word embeddings.\nFor the gold standard word embeddings, we use the embeddings provided by Herbelot and Baroni (Herbelot and Baroni, 2017) , as done in previous OOV models like (Schick and Sch\u00fctze, 2019c) and (Hu et al., 2019) . For training models, contexts are taken from the Westbury Wikipedia Corpus (WWC) (Shaoul, 2010) . We use the version from (Khodak et al., 2018) \n\nBaselines and Hyperparameters\nWe now demonstrate the effectiveness of SubAtt. 4 We compare it to Attentive Mimicking 5 (AM) and HiCE 6 , as they are OOV models that use both subwords and context on existing static word embeddings. Two versions of HiCE are examined; the default with a 2 layer context aggregator, and a version with 8 layers to be more comparable to SubAtt. Also, we do not use MAML in the HiCE experiments, in order to focus on how the architecture adapts to multiple OOV tasks. The dataset and vocab are split into a training and validation set for hyperparameter tuning (discussed in more detail in Appendix A).\nTen final trials of each model are trained and then each model is evaluated on various OOV tasks. The results are tested for statistical significance using a one-way ANOVA with a post-hoc Tukey HSD test with a p-value threshold equal to 0.05. The best score is presented in bold, along with any scores that are not significantly different from the best.\n\nTasks\nWe now evaluate SubAtt on various OOV tasks. We focus on OOV tasks in English, matching previous work. As SubAtt mixes both subwords and (Khodak et al., 2018) . CRW is built off the Rare Word dataset (Luong et al., 2013) , which is a list of rare words paired with other words, along with human similarity scores. Khodak et al. (2018) added contexts to this set, allowing for OOV words to be estimated using both subwords and context. The goal is to output an OOV embedding, compare it to the other words, and evaluate the scores' correlation with human judgements. CRW has a large range of context sizes, from 1 to 128, so the quality and informativeness of the context can vary wildly. However, the words gathered for the Rare Word set have intentionally informative word roots, and therefore we expect subwords to be fairly informative.\nThe results of the CRW task are shown in Figure 1 . SubAtt significantly outperforms all competitors in all contexts, showing its effectiveness as an OOV estimator. This shows the strength of pretrained subwords and subword attention.\nDownstream Tasks We now demonstrate the strong performance of SubAtt embeddings extrinsically, using downstream tasks. In order to focus on OOV words specifically, we choose downstream tasks that output word level labels; specifically named entity recognition and parts-of-speech tagging. For each of these tasks, we train a Bi-LSTM-CRF (Lample et al., 2016) , an approach similar to the one in (Hu et al., 2019) . The input to these models are normal word embeddings for words in our vocabulary (ones used in training and validation of the original OOV models), and each model's OOV estimates for unknown words. 7 For 7 OOV words with invalid subwords (no existing character ngrams or no CharCNN characters) are assigned a zero vector. each dataset, the Bi-LSTM-CRF is trained for 30 epochs 10 times, with the best epoch selected using a validation set each time. This approach is applied to each of the 10 trials of each OOV model. As our focus is estimating OOV words, we report the average test macro F1 score of the OOV words specifically. We also report the results for all words in Appendix B.\nWe test on 5 named entity recognition tasks: the JNLPBA 2004 Bio-entity recognition dataset (Bio-NER) (Kim et al., 2004) , the Rare-NER dataset (Derczynski et al., 2017) , the CoNLL 2003 NER dataset (Sang and De Meulder, 2003) , AnEM (an anatomy NER dataset) (Ohta et al., 2012) , and MovieMIT, a movie querying dataset (Liu et al., 2013) . In addition, we test on a parts-of-speech tagging dataset, specifically the Twitter social media POS task (Ritter et al., 2011) .\nThe Downstream Task results are shown in Table 1 . SubAtt generally outperforms the competitors, strictly winning in 3 of the 6 tasks, and tying for best in one more task, and achieving the second best score in another task. This demonstrates Sub-Att's robust and strong performance on OOV words in downstream tasks.\n\nAblation Analysis\nWe now conduct an ablation study on SubAtt in order to demonstrate the impact of the pretraining compared to attention. To this end, we repeat the previous experiments on four variants of SubAtt; the original model, the model without attention (SubAtt No Att), the model without pretrained subwords (SubAtt No Pre), and the model without both (SubAtt No Pre No Att). The results are shown in Figure 2 creases 8 . This makes sense, as the influence of subwords decreases as our model gains more and more context information, which in turn lowers the impact of the pretraining and attention on subwords in general. Similarly, SubAtt performs strongly in the Downstream Ablation, performing the best or tied for the best in all six tasks. The results also demonstrate that pretraining and subword attention individually have a high impact on results, and both combined leads to an even stronger improvement.\n\nConclusion\nWe propose SubAtt, an attention based model that estimates OOV words by using pretrained subword embeddings and subword attention. We show through various experiments that this model estimates more accurate representations of OOV words.\n", "hypothesis": " Word embedding methods like word2vec and GloVe have been shown to learn strong representations of words.  However, these methods only learn representations for words in the training corpus and therefore struggle to handle unknown and new words, known as out-of-vocabulary (OOV) words. As a result, there have been multiple attempts to learn OOV word representations in a similar fashion to how humans learn new words, using word roots/subwords and/or surrounding words. However, while most of these approaches use advanced architectures like attention on the context of the OOV word, they tend to neglect the importance of subword representations. In response to this, we propose SubAtt, a simple neural network model that focuses solely on subword information for estimating OOV word representations. By leveraging the power of subword representations and disregarding context, SubAtt achieves superior performance compared to state-of-the-art models in both intrinsic and extrinsic tasks.  We show SubAtt outperforms current state-ofthe-art OOV estimation models..", "answer": false}
{"title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum", "content": "\nIntroduction\nRecently, there has been a tremendous interest in employing image-caption pretraining for downstream vision tasks like zero-shot object classification (Radford et al., 2021) and zero-shot object detection (Zareian et al., 2021; Li et al., 2022) . The idea is to learn a common semantic space where the visual embeddings of objects in images lie close to the textual embeddings of the concepts (objects' name/tag/label) in captions they refer to. This learned semantic space is later exploited for zero-shot object recognition by finding the concept embedding nearest to the objects' embeddings.\nDespite the recent success, image-caption pretraining is a complex problem as it entails aligning multiple concepts in a caption with multiple objects in an image, as shown in fig. 1 . Different methods have tried to solve this problem from various angles -CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) by using more data, ALBEF (Li et al., 2021) by using more complex network architecture, Florence (Yuan et al., 2021) and CoCa (Yu et al., 2022) by using more tasks and ERNIE-ViL 2.0 (Shan et al., 2022) by using more data augmentations (views).\nWe propose an alternative approach based on a novel learning strategy that is architecture agnostic and does not require any additional data or compute. We take inspiration from cognitive science research studying how children learn language (concepts) in early stages by just observing their surroundings (images). Specifically, we refer to two studies showing that childern learn rapidly if the object of interest is unambiguous (Pereira et al., 2014) and by applying co-referential statistics across multiple scenes (Smith and Yu, 2008) .\nWe implement these two ideas via a curriculum learning approach (demonstrated in fig. 1 ):\n1. We train the model in multiple phases of increasing difficulty with each phase containing one more concept in the caption than the previous one. Moreover, each phase contains only one new concept, the rest seen in prior phases.\n2. In each phase, we leverage the concept-object association learned in prior phases to recognize the seen concepts and focus on aligning the new/unseen concept (section 2.2.2).\nThese two strategies effectively reduce the problem of aligning multiple object-concept pairs per training sample to aligning only one such pair.\nTo the best of our knowledge, no prior work has applied curriculum leaarning to image-caption pretraining in this way. Srinivasan et al. (2022) apply a curriculum based on the difficulty of negative samples in contrastive loss. Whereas, Liu et al. (2021) design the curriculum based on the granularity of text: from words to phrases to sentences.\nAlthough our proposed approach can be applied to any multimodal network architecture, we pick OVR-CNN (Zareian et al., 2021) due to its simplicity. We pretrain it with the proposed curriculum learning approach and evaluate on the downstream task of zero-shot object detection. We demonstrate that curriculum learning outperforms vanilla imagecaption pretraining on a variety of architectural settings -with and without a pretrained image encoder and/or a pretrained text encoder. We even show superior performance in low-data settings, suggesting our method can be leveraged in low-resource scenarios as well.\n\nMethod\nWe propose a curriculum learning framework to improve image caption pretraining. In this work, we apply it to OVR-CNN as its architecture is simpler and easier/faster to train/evaluate. We begin the description of our approach with a brief background on OVR-CNN. Next, we discuss how we modify it to implement the proposed curriculum learning framework.\n\nOVR-RCNN Background\nOVR-RCNN is a dual-encoder (separate visual encoder and text encoder) multimodal architecture. First, it pretrains the encoders using image-captions and later utilizes them for the downstream task of object detection. We only discuss the pretraining procedure as we only utilize this component.\nOVR-RCNN's visual encoder is ResNet-50 (He et al., 2016 ) and text encoder is either BERT (Devlin et al., 2019) or GloVE (Pennington et al., 2014) . The visual encoder takes an image, I with w \u00d7 h dimensions, as input and outputs a feature map of w/32\u00d7h/32 regions. Each feature map is a vector which is transferred to language space using a projection layer. This gives the visual embeddings, e I i , for each region i. The tokenized caption, C, is input to the text encoder which outputs an embedding e C j for each token j. The token-image region pair is aligned via weak supervision. Specifically, a global alignment score between image and caption, \u27e8I, C\u27e9 G is calculated using a locally weighted average alignment score of image regions and tokens as follows:\nEQUATION\nwhere \u27e8., .\u27e9 L is the dot product of two vectors, n I and n C are the number of image and caption tokens respectively, and\nEQUATION\nThe model is trained using contrastive learning by maximizing the global alignment score, \u27e8I, C\u27e9 G , between positive image-caption pairs and minimizing it between negative pairs sampled from the same training batch.\nL = \u2212 log exp\u27e8I,C\u27e9 G {I \u2032 ,C \u2032 }\u2208N I,C exp\u27e8I \u2032 ,C \u2032 \u27e9 G +exp\u27e8I,C\u27e9 G (3) where, N I,C = {I, C \u2032 |C \u2032 \u2208 B C } \u222a {I \u2032 , C|I \u2032 \u2208 B I } and B C , B I\nare batch captions and batch images respectively. This learning objective aligns paired image and caption together and also provides weak supervision for image-regions and caption-tokens association.\n\nCurriculum Learning Framework\nOVR-CNN facilitates object-concept alignment through coarse image-region and concept alignment. However, as an object can span multiple image regions or multiple objects can span an image region, this strategy can be noisy. To eliminate this noise and focus on the contribution of our curriculum framework to object-concept alignment, we train the model using object region features instead of image region features. To this end, object 13379 1) and (2).\n\nCurriculum Design\nThe learning is divided into 1, 2, 3 . . . k phases. Each phase p is trained with only those imagecaption pairs having p concepts per caption. To divide the data into phases, we use spacy 1 to PoS (Part of Speech) tag the captions. Depending upon the number of nouns in each caption, the caption and its paired image is grouped into the corresponding phase. This strategy of designing the curriculum also imparts the data an additional property empirically -at most only one new concept is introduced per caption in each phase (as demonstrated in fig. 2b ).\n\nCurriculum Aware Alignment Loss\nTo recognize the concepts in captions previously seen in prior phases and focus on aligning the new/unseen concept, we formulate a novel Curriculum Aware Alignment Loss (L C ). Specifically, we first calculate the previously learned object-concept alignment, a o,j from modified eq. ( 2), using either the trained model from the last iteration (L CR ) or the trained model from the last phase (L CP ). Next, a o,j is used to compute:\na \u2032 o,j = exp\u27e8e I o ,e C j \u27e9 L exp (\u2212 max o (a o,j ). t T ) n I o \u2032 =1 exp\u27e8e I o \u2032 ,e C j \u27e9 L exp (\u2212 max o (a o,j ). t T )\nwhere, t is the current iteration number and T is the total number of iterations in training. For a concept j, which is already closely aligned to an object o, max o (a o,j ) is high. This leads to a low value of a \u2032 o,j , resulting in less attention being paid to concept j in the current training iteration/phase. Vice versa for a concept that is not 1 https://spacy.io/usage well aligned with any object. a \u2032 o,j effectively redistributes the attention of learning to focus more on concepts that are not well aligned with any object. The term t/T has a low value in the beginning of training and gradually scales to 1 by the end. This allows the network to ignore prior knowledge in the beginning while utilizing it in the latter stages.\nWe use a \u2032 o,j to replace a o,j in modified eq. ( 1), and then use eq. ( 3) to compute L C .\n\nPretraining Dataset and Implementation Details\nWe use the COCO Captions dataset (Chen et al., 2015) for pretraining. It contains 118,287 images and 5x captions. To obtain bounding box regions for objects in images, we use COCO Objects (Lin et al., 2014) dataset as it uses the same set of images as COCO Captions. We divide the data into k = 4 phases using the strategy discussed in section 2.2.1. Figure 2a shows number of captions assigned to each phase. As shown in fig. 2b , the majority of captions in each phase have at least k-1 concepts previously seen, allowing the curriculum to introduce at most one new concept per training sample. Further, as more concepts are introduced with each passing phase, the percent of captions per phase actually introducing a new concept decreases (as depicted in fig. 2c ). By phase 4, this percent reduces to < 5%. Additional phases of training may not contain enough captions actually introducing a new concept in the curriculum way, making these phases similar to regular image-caption training. Hence, we limit to 4 phases. We train the model using SGD optimizer, with a batch size of 32 for 4 epochs in each phase, a learning rate of 0.005, a step scheduler, and the loss L CP . \n\nDownstream Task, Dataset and Transfer\nWe evaluate the performance of the model on zeroshot object detection task on COCO Objects, val split (4836 images; 33374 instances of 65 object class). The task involves object bounding box predictions besides classifying these object regions to a label (concept). However, our method is aimed only at improving the alignment of object regions to a concept. As such, we eliminate any performance noise from bounding box predictions by only evaluating the classification accuracy of object regions given ground truth object bounding boxes.\nTransfer to Downstream Task: We extract object features from image and object bounding box using visual backbone and use it to find the closest class label vector (obtained via language backbone).\n\nBaseline and Evaluation\nOur baseline is OVR-CNN, a regular image-caption pretrained model. However, since our method uses object region features instead of image patch features for multimodal alignment (section 2.2), we also pretrain OVR-CNN with object regions to obtain OVR-CNN O . It is transferred to downstream task similar to our proposed model (section 3.2).\nOur proposed curriculum framework outperforms the baseline in various settings, as shown in table 1. The accuracy numbers reported are averaged across three seeds. This demonstrates that our proposed learning strategy works across encoders trained from scratch or pretrained ones.\nPerformance Gain Analysis. We analyze model performance on object classes introduced during pretraining in phase 1 and phase 2 separately. As reported in table 2, the improvement in phase 2 objects is ~10x. This illustrates that our curriculum strategy improves alignment of multiple concepts in a caption by focusing on one at a time.\nLow Data Setting. Our model outperforms the baseline even if both uses 50%, 25% or 10% data (fig. 3 ), indicating its utility when data is scarce.\nRegion proposals instead of ground-truth object regions. We use a RPN model (Girshick, 2015b) trained class-agnostically on Visual Genome (Krishna et al., 2016) to generate object regions. The superior performance of our model against baseline, reported in table 3, demonstrates that our approach is effective even when groundtruth object regions are not available.\nLoss Ablation. From table 4 , we can conclude that our curriculum design works (Ours + L > OVR-CNN O + L); our proposed curriculum aware loss works (Ours + L < Ours + L CR ) irrespective of curriculum (OVR-CNN O + L < OVR-CNN O + L CR ); curriculum aware loss works better when previous knowledge is taken from the last phase instead of the last iteration (Ours + L CP > Ours + L CR ).\nQualitative Analyssis. We provide qualitative analysis as well to shed more insights into the cases where our approach works/doesn't work. From Figure 4 , we find that our model performs better than OVR-CNN O in certain cases, especially when the objects are from Phase 2 -\"snowboard\", \"cup\", \"skis\" etc. This provides further evidence towards our claim that our approach improves the alignment of Phase 2 objects.\nComparison of traditional mAP metric for object detection As mentioned before, we have focused our experiments on evaluating object- concept alignment rather than on traditional object detection mAP metric. This was done to avoid unnecessary performance noise arising from training a RPN, which is required for mAP evaluation. However, to test the limits of our model, we evaluate on this noisy mAP metric as well. We keep all the settings similar as Zareian et al. (2021) , except we pretrain using our curriculum learning approach.\nThe results are reported in Table 5 . We find that our model performs better in the most generic Generalized ('All') set (41.33 vs 39.9), signifying the effectiveness of our approach even in this noisy setting. We further observe that we perform better in the base classes while lagging behind in the target classes. A deeper analysis shows that most of the Phase 2 objects, on which we make major improvements, lie in the base classes. This explains the improved performance on base classes and slight depreciation in target classes performance.\nTraining with image grid regions. Our curriculum based pretraining method was aimed at improving object-concept alignment by focussing on one object at time. To facilitate this, we pretrained directly with object regions. Image regions were not used to eliminate noise arising from an ob- ject spanning multiple regions or multiple objects being present in the same image region (object presence noise). However, we further push the limits of our model to assess how it performs when trained with noisy image regions instead of object regions.\nThe results are reported in Table 6 . We find that our model performs slightly worse than . We attribute this performance degradation to the inherent object presence noise in image regions as discussed earlier.\n\nConclusion\nWe proposed a curriculum learning framework to improve image-caption pretraining, using the number of concepts in captions. We also designed a novel curriculum aware loss to focus learning on the unaligned concept in each phase. Our approach outperforms vanilla image-caption pretraining in various settings, including with/without pretrained encoders and small data. Further, we extensively analysed our model to study the contribution of each component.\n", "hypothesis": " Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection.  However, image-caption pretraining is still a hard problem -it requires multiple concepts (nouns) from captions to be aligned to several objects in images.  To tackle this problem, we go to the roots -the best learner, children.  We take inspiration from cognitive science studies dealing with children's language learning to propose a curriculum learning framework.  The learning begins with easy-to-align image caption pairs containing one concept per caption.  The difficulty is progressively increased with each new phase by adding one more concept per caption.  Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase.  We show that this learning strategy improves over vanilla image-caption training in various settings -pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc..", "answer": true}
{"title": "Topic and Style-aware Transformer for Multimodal Emotion Recognition", "content": "\nIntroduction\nEmotion recognition is intrinsic for social robots to interact with people naturally. The ability to tell emotional change and propose timely intervention solutions can help maintain people's mental health and social relations. Though the traditional task of sentiment analysis is purely based on text (Wang et al., 2020; Ghosal et al., 2020; Shen et al., 2021) , humans express emotions not only with spoken words but also through non-verbal signals such as facial expressions and the change of tones. Therefore, following the current trend of multimodal emotion recognition (Delbrouck et al., 2020; Zadeh et al., 2017; Rahman et al., 2020; Gandhi et al., 2022) , we focus on addressing problems of understanding the expressed emotions in videos along with their audio and transcripts.\nIn this work, we tackle the problem of the multimodal emotion recognition task from two major issues: Minimal contribution of visual modality, and emotional subjectivity. Previous works which have used multimodal approaches (Rahman et al., Figure 1 : Left table: \"happy\" under different topics. Right table: speaking styles can affect how emotion is displayed on the face 2020; Joshi et al., 2022; Delbrouck et al., 2020) have shown that text+audio outperforms the results of combining all three modalities. While facial and gesture signals contain abundant information, they tend to introduce more noise to the data due to its high dimensionality. In order to increase the contribution from visual modality , we propose to take advantage of the strong multimodal backbone VATT (Akbari et al., 2021) that can project features of different granularity levels into a common space. On the other hand, the expression of emotion is subjective. People's emotion judgment can be influenced by enclosed scenarios. As shown in the left two columns in Figure 1 , though the two examples are all labeled as \"happy\", the signals we use to detect \"happy\" may not be the same. In a public speech, showing gratitude may mean a positive sentiment while in movie reviews, we may focus more on sentiment words like good or bad. Also, subjectivity may come from individual differences in their own emotional intensity. As the examples shown in the right three columns in Figure 1 , the sadness and happiness of the person in the excited style are more distinguishable through his face while the person in the calm style always adopts a calm face that makes sad and happy less recognizable. Therefore, we introduce content-oriented features: topic and speaking style serving as a content \"normalization\" for each person.\nOur work makes the following contribution: 1) We propose to leverage the multimodal backbone to reduce the high dimensionality of visual modality and increase its contribution to the emotion recognition task.\n2) We incorporate emotion-related features to handle modeling issues with emotional subjectivity 3) Experiments conducted on the benchmark dataset MOSEI show our model can outperform SOTA results and effectively incorporate visual signals and handle subjectivity issues.\n\nRelated Work\nEmotion recognition using a fusion of input modalities such as text, speech, image, etc is the key research direction of human-computer interaction. Specific to the area of sentiment analysis, Multimodal Transformer applies pairwise crossattention to different modalities (Tsai et al., 2019) . The Memory Fusion Network synchronizes multimodal sequences using a multi-view gated memory that stores intra-view and cross-view interactions through time (Zadeh et al., 2018) . TFN performs the outer product of the modalities to learn both the intra-modality and inter-modality dynamics (Sahay et al., 2018) . (Rahman et al., 2020) begins the endeavor to take BERT (Devlin et al., 2018) as a strong backbone pretrained on large scale corpus. (Arjmand et al., 2021) follows the direction and combines Roberta with a light-weighed audio encoder to fuse the text and audio features. A recent work (Yang et al., 2022a) presents a self-supervised framework to pretrain features within a single modality and across different modalities. Other frameworks include context and speaker-aware RNN (Shenoy and Sardana, 2020; Wang et al., 2021) , graph neural networks modeling knowledge graphs and inter/intra relations between videos (Joshi et al., 2022; Fu et al., 2021; Lian et al., 2020) , while (Zhu et al., 2021) has used topic information to improve emotion detection 3 Method\n\nOverview\nOur model aims to predict the presence of different emotions given an utterance-level video input along with its audio and transcripts. Figure 2 shows the overall structure of our model. To first get a better alignment of features from different modalities, \n\nBackbone\nVideo-Audio-Text Transformer (VATT) is a framework for learning multimodal representations that takes raw signals as inputs. For each modality encoder, VATT appends an aggregation head at the beginning of the input sequence. The corresponding latent feature will serve as the projection head for this modality. For pretraining, contrastive loss is applied to align features from different modalities in a common projected space. Details can be found in (Akbari et al., 2021) .\n\nTopic\nFor each utterance input, we will first predict the topic of this utterance and feed the corresponding topic embedding into the model. Since we don't have the ground truth label for topics, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) model to cluster all the text from the training set into 3 topics. The number of topics is decided by grid search.\n\nSpeaking Style\nWe define speaking style based on the expression coefficient and the projection parameters of a 3DMM model (Blanz and Vetter, 1999) . In a 3DMM model, the face shape is represented as an affine model of facial expression and facial identity: S = S + B id \u03b1 + B exp \u03b2. This 3D face will be projected into a 2D image by translation and rotation p. Since there are multiple video frames, the expression coefficient \u03b2 and the project parameter p will become time series \u03b2(t) and p(t). For a detailed analysis of the relations between the 3DMM parameters and the talking styles, (Wu et al., 2021) collected a dataset consisting of 3 talking styles: excited, tedious, and solemn. They find out that the standard deviation of the time series features and the gradient of these features are closely related to the styles. The final style code are denoted as \u03c3(\u03b2(t)) \u2295 \u03c3( \u2202\u03b2(t) \u2202t ) \u2295 \u03c3( \u2202p(t) \u2202t ), \u2295 signifies the vector concatenation.\n\nAggregating Different Features\nGiven each data input with its corresponding video ID, we collect all the transcripts with the same video ID as the context, and the context feature will be extracted from the text encoder of VATT. To adapt general topic and style features to the current speaker, we treat them as the feature sequence of length 2 and use an additional cross-attention layer to aggregate these features queried by the video context. Then this information along with the context and aligned features will be concatenated and fed into the final linear classifier. 3 5 Experiments\n\nSetup\nWe train our models on 8 V100 GPU for 8 hours using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e \u2212 4 and a mini-batch size of 64. The total number of parameters of our model is 155M. For topic clustering, we adopt the scikitlearn LDA library (Pedregosa et al., 2011) . We extract the style code for each video using https: //github.com/wuhaozhe/style_avatar. The final model is selected based on validation accuracy on the development set.\nTask We evaluate the performance of our model on two tasks: 1) Multi-label emotion recognition: the model needs to classify whether each of the 6 emotion classes presents or not. 2) Sentiment anal-ysis: the model is tested on both 2-class (sentiment is positive or negative) and 7-class (a scale from -3 to +3) classification.\nEvaluation Since the labels in MOSEI are unbalanced, we use the weighted F1 score for each emotion as the evaluation metric. We compare the performance with Multilogue-Net (Shenoy and Sardana, 2020) that adopted context and speaker-aware RNN , TBJE (Delbrouck et al., 2020) , a state-ofthe-art method using cross-attention for modality fusion and MESM (Dai et al., 2021) , who were the first to introduce a fully end-to-end trainable model for the multimodal emotion recognition task . There are two recent works on emotion recognition, COGMEN (Joshi et al., 2022) and i-Code (Yang et al., 2022b) . Since COGMEN adopted a structural representation that can exploit more relational information from other data samples and i-Code did not report the same metrics and is not opensourced, we will not compare with them in this paper.\n\nEmotion Recognition\nTable 1 shows our quantitative results. Compared with other SOTA methods in the first three rows, our full model achieves the best performance on recognizing happy, sad and angry. We reckon that it is due to very limited data for surprise and fear to train the big backbone, our model does not gain much improvement (shown in Table 3 ). To further analyze the contribution of each component of our model design, we also conduct a detailed ablation study: 1) We first remove the aligned features from the backbone each at a time. We can see from the results in the second block that combining all three modalities in our full model outperforms the bimodality input. Especially contrasting rows with and without video input, their comparative performance validates that our model can learn effectively from visual modalities. 2) In the third block, we report the performance when we simply concatenate aligned features as the input to the emotion classification layer without high-level features. The degraded performance reveals the efficacy of our content feature design. 3) Lastly, we investigate the influence of each content feature and the aggregation using context. To remove the context, we directly apply a self-attention layer to the feature sequence and use a linear layer to project the outputs into the aggregate feature dimension. For topic and style, we just remove the corresponding feature from the input. As shown in the last block, removing any part will result in a performance drop. Overall, our full model in comparison yields the best performance.\n\nSentiment Analysis\nTo further validate our methods, we run our model on the other subtask, sentiment analysis. For each data sample, the annotation of sentiment polarity is a continuous value from -3 to 3. -3 means extremely negative, and 3 means extremely positive. Our model is trained to regress the sentiment intensity. Then we ground the continuous value into 2 or 7 classes to calculate the accuracy. Contrasting 2-class and 7-class results in Table 2 , our model works better for more fine-grained classification. We first show that our model can correctly recognize emotions under different topics. As shown in Figure 3 , for movie reviews, finance or commercial advertisements, the model can use different cues to predict the emotion as happy or sad. In Figure 4 , our model can distinguish between excited/calm speaking styles and recognize the slight emotional change within each person. (all example videos can be found in supp). \n\nLimitations\nFor modeling simplicity, we adopt the classic LDA methods to get the topic ID for each video segment. We plan to investigate more advanced topic clustering methods and check how it can be applied to multilingual cases. Also, we propose a twostage framework that first extract topic and style features, based on which the emotion classifier will be trained. In the future, we hope to extend this work to learn features in an end-to-end manner.\n\nWords\nExamples Topic 1 movie, umm, uhh, like, know, really, one, im, good, go, see, two, kind, would, think, even, thats, going, there 1) hi there today we're going to be reviewing cheaper by the dozen which is umm the original version; 2) i was a huge fan of the original film bruce almighty but i did think it was funny like jim Topic 2 people, get, think, make, business, u, want, time, world, need, company, way, also, work, one, year, take, money, right, new 1)future and it's a retirement future that can ultimately turned in to an income for you when you no longer have an income and you're fully retired; 2)um this year switching up how we approach funding and hopefully going to be able to arrange for some sustainable more officially recognized sorts of funding Topic 3 going, thing, like, know, one, want, really, well, also, im, video, make, way, thats, something, think, were, time, get, look 1)is you can say hey i really like baby skin they are so soft they have any hair on their face so nice; 2) okay what happens at this point after we've taken this brief walk down memory lane is the presentation of the gift now B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 3.2, 4\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\nThe data is anonymized and discussed in the original paper B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. 4\nC Did you run computational experiments?\n\n5\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 5.1\n\n\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\n", "hypothesis": " Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication.  While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality.  Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals.  Also, we propose contentoriented features Topic and Speaking style on top of it to approach the subjectivity issues. Experiments conducted on the benchmark dataset MOSEI show our model can outperform SOTA results and effectively incorporate visual signals and handle subjectivity issues by serving as content \"normalization\". However, the incorporation of visual signals may introduce more noise to the data due to its high dimensionality.", "answer": false}
{"title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks", "content": "\nIntroduction\nMaking Language Models (LMs) perform mathematical reasoning is a valuable, yet challenging research objective (Hendrycks et al., 2021; Cobbe et al., 2021) . Recently, we have witnessed largescale LMs' impressive performance on a series of reasoning tasks via chain-of-thought prompting (Wei et al., 2022) . This method elicits large LM's ability to decompose a complex problem into several intermediate steps. However, it is believed that such ability only emerges from sufficiently large models (empirically more than 100B parameters) (Wei et al., 2022) . In this paper, we examine how to incorporate moderate-sized LMs, e.g., RoBERTa (Liu et al., 2019) , with such multi-step reasoning ability via continual pre-training to improve the performance on math problems.\nCorrectly understanding numbers is a prerequisite of mathematical reasoning abilities. But Wallace et al. (2019) shows that medium-sized LMs have a deficiency in numerical comprehension. To overcome this issue, previous works inject numerical reasoning skills into LMs following two approaches. The first is masking numbers with special tokens, and generating symbolic expressions with a structured neural decoder (Xie and Sun, 2019; Jie et al., 2022) . An example of such expression is provided in Figure 1 . The second strategy continually pre-trains LMs on synthetic numerical tasks, which requires models to learn how to perform computation involving numbers (Geva et al., 2020; Pi et al., 2022) . However, both approaches suffer from critical limitations. For symbolic methods, they neglect the information carried by the numbers, which could provide crucial hints for solving math problems (Wu et al., 2021; Liang et al., 2022) . As for continual pre-training methods, LMs' arithmetic skills are not reliable. Previous works indicate that such skills are highly influenced by the training data (Razeghi et al., 2022) and hard for extrapolation (Wallace et al., 2019) .\nMotivated by these shortcomings, we propose to first pre-train moderate-sized LMs on a synthetic dataset called MSAT (Multi-step Arithmetic Tasks) \n\nMethod\nOur method essentially appends a continual pretraining stage before fine-tuning LMs on downstream tasks. The continual pre-training serves two purposes: first, we tokenize numbers digit-bydigit to improve LMs' numerical comprehension; second, we make LMs learn multi-step reasoning skills from the proposed synthetic task.\n\nDigit tokenization for numbers\nSub-word tokenization methods, e.g., byte pair encoding (BPE) (Sennrich et al., 2016) , is one of the reasons why moderated-sized LMs poorly understand numbers (Wallace et al., 2019) . BPE-based tokenizers split text based on the token frequency in the training corpus, which can be counter-intuitive when dealing with numbers. For example, numbers \"520\" and \"521\" will be tokenized into [\"520\"] and [\"5\", \"21\"] respectively by the RoBERTaTokenizer 2 of the Transformers library (Wolf et al., 2020) . Such inconsistent tokenization strategy for numbers undermines LM's numerical understanding ability. Hence, we tokenize numbers digit-by-digit for both pre-training and fine-tuning.\n\nMulti-step Arithmetic Tasks (MSAT)\nThe core of our method is the synthetic task MSAT where LMs can learn multi-step reasoning skills. Like MWP tasks, MSAT can be formulated as a Seq2Seq task: the input of a MSAT example describes an arithmetic question, while the output is a reasoning chain leading to the answer. Specifically, each input sequence is composed of three components: question context, equation, and question variable. Equation is a sequence of symbols and operators (+, \u2212, \u00d7, \u00f7, =) that builds equality relationship between symbols. Given an equation, only one of the symbols is set as the question variable, while other symbols will be listed in question context with their numerical values.\nThe output sequence of MSAT is constructed in a code-style multi-step reasoning format. Each step consists of two sub-steps: variable assignment and calculation. In variable assignment, numbers appear in the input sequence are assigned to the variable names that are exclusive for decoder. In calculation, a new variable is generated from the calculation of the existing variables. This makes our outputs become executable Python code so that the numerical answer can be calculated by an external Python interpreter. Both inputs and outputs of MSAT are generated purely automatically. Details about the construction of MSAT are provided in Appendix A.1.\n\nPre-training via adapter-tuning\nDirectly training on synthetic data that are largely different from the natural language corpus harms LMs' language prowess (Geva et al., 2020) . Therefore, we adopt a two-stage tuning strategy (Wang and Lu, 2022) to inject reasoning skills into LMs. Specifically, we perform adapter-tuning (Houlsby et al., 2019) on MSAT and then jointly fine-tune adapter and LM backbone on downstream tasks. It mitigates catastrophic forgetting because LM's original parameters are largely preserved during adapter-tuning (Houlsby et al., 2019) .\nWe consider two backbone models to verify the effectiveness of our method. In particular, we select a sequence-to-sequence (Seq2Seq) model (Lan et al., 2021) and a directed acyclic graph (DAG) structured model (Jie et al., 2022) that both adopt RoBERTa base to encode the input questions. More details of these models are provided in \u00a73.1. Table 1 : Accuracy (%) comparison between large language models (LLMs), backbone model baselines, and our method. \u2206: performance gap compared with the symbolic mask baselines. \u2661: For baselines with symbolic masks, performance on SVAMP (hard) is the same as SVAMP because the actual numbers are replaced by symbolic tokens.\nThe results of LLMs with chain-of-thought prompting are from Wei et al. (2022) .\n\nExperiments\nNow we investigate whether our pre-training method facilitates models on Math Word Problem (MWP) solving tasks. All results are averaged over three different runs.\n\nExperimental setup\nExisting datasets We consider three commonlyused MWP datasets: MAWPS (Koncel-Kedziorski et al., 2016), ASDiv-A (Miao et al., 2020), and SVAMP (Patel et al., 2021) . The statistics of these datasets is provided in Table 2 . More details can be found in Appendix A.2. We report five-fold crossvalidation results for both MAWPS and ASDiv-A and test set accuracy for SVAMP following previous practice (Lan et al., 2021; Jie et al., 2022) .\n\nSVAMP (hard)\nWe find more than 85% of the numbers in the above datasets are smaller than 10 2 . To investigate the extrapolation performance of the models trained with MSAT, we create SVAMP (hard) from the original SVAMP dataset by replacing the numbers with much larger ones inspired by Gao et al. (2022) . More details about SVAMP (hard) and number distribution of the existing datasets are provided in Appendix A.3. \n\nModels\nWe consider both sequence-to-sequence (Seq2Seq) models and directed acyclic graph (DAG) structured models as our backbone models. For Seq2Seq model, we choose ROBERTA-GEN (Lan et al., 2021) , an encoder-decoder model with RoBERTa base as the encoder combined with a Transformer decoder. For DAG structured model, we choose DEDUCTREASONER (Jie et al., 2022) that combines RoBERTa base with a DAG decoder.\nIn their original implementation, both models replace numbers with symbolic mask tokens. Hence, we additionally consider a baseline for each backbone model that uses actual numbers with digit tokenization. We name the models that are based on these two backbone models and pre-trained with our method as MSAT-ROBERTAGEN and MSAT-DEDUCTREASONER respectively. We also compare our models to large LMs, e.g., PaLM (Chowdhery et al., 2022) and Codex (Chen et al., 2021) , with chain-of-thought prompting (Wei et al., 2022) . All models are evaluated via greedy decoding. More implementation details, e.g., training hyperparameters, are provided in Appendix B.\n\nMain results\nTable 1 compares our models with backbone model baselines and large LMs. On all datasets, digit tokenization baselines consistently perform worse than their symbolic mask counterparts, indicating the deficiency of the numeracy comprehension of the original RoBERTa model. However, the models trained with MSAT surpass both baselines by a large margin, which demonstrates the effectiveness of our pre-training method. \n\nSVAMP (hard)\nWe can observe that, on SVAMP (hard), the accuracies of digital tokenization baselines decrease dramatically (10.7 points drop for ROBERTAGEN and 2.2 points drop for DEDUC-TREASONER) compared with baselines with symbolic masks, while the models trained with MSAT still outperforms symbolic mask baselines by 5.9 and 3.2 points respectively. This shows that not only does our models obtain better results than the baselines on the existing tasks, but it is also more robust in handling out-of-distribution numbers.\nCompare with large language models We also observe that, on relatively simple tasks, i.e., MAWPS and ASDiv-A, RoBERTa-based models can outperform large LMs. But for the more challenging task SVAMP, there is still a large performance gap. We believe this is because SVAMP requires models to have a better understanding of natural languages. Jie et al. (2022) also reports that varying LM encoders results in significant performance disparities on SVAMP, indicating that SVAMP performance is closely tied to model's natural language capabilities.\n\nPre-training analysis\nIn this section, we provide a careful analysis of our pre-training method from various perspectives to understand why it works.\n\nPre-training task performance\nWe visualize how the performance of pre-training task MSAT and one of the MWP tasks SVAMP changes with pre-training steps in Figure 3 . It can be observed that the performance on both synthetic and natural language tasks tends to improve gradually as the number of pre-training steps increases. Figure 3 demonstrates that LMs are capable of learning multi-step reasoning gradually from the synthetic task MSAT. The acquired multi-step rea- soning ability can subsequently be transferred to the downstream MWP solving tasks, enhancing performance during the fine-tuning phase.\n\nReasoning format of MSAT\nThe reasoning format of MSAT dictates the specific reasoning skills that LMs will acquire during pre-training. We demonstrate the superiority of our code-style multi-step reasoning format by comparing it with two different reasoning expressions.\nEffect of producing intermediate steps While it is a common practice to train LMs towards directly producing the numerical answers of the arithmetic questions (Geva et al., 2020; Pi et al., 2022) , a recent work shows that LMs' arithmetic skills are not reliable (Razeghi et al., 2022) \n\nStructured code-style expression\nWe next investigate the importance of applying the structured code-style reasoning expressions by comparing it with the less formatted math expressions. We argue that, compared with math expressions that only contain numbers and operators, our code-style expressions are more suitable for multi-step reasoning due to the structure information in the output sequences. Our experiments in Figure 4 demonstrate the superiority of the code-style output expressions. We can see that models with math expressions perform consistently worse than models with code-style multi-step reasoning format on both pre-training task MSAT and MWP solving task SVAMP. \n\nDifficulty level of MSAT\nLeveraging synthetic data for pre-training provides the advantage of enabling highly customizable difficulty levels for the training data. Here we define the difficulty level of a reasoning task as the averaged reasoning steps that are required to solve the problems. From Figure 5 , we see that pre-training LMs on MSATs that are harder than downstream tasks generally leads to better results. It's important to note that, broadly speaking, the difficulty level of a reasoning task, particularly those involving natural language, is not solely determined by the number of reasoning steps. One example is that, though both ASDiv-A and SVAMP have an averaged reasoning steps of 1.2 (see Table 2 ), SVAMP is considered more difficult as it requires high-level natural language understanding (Patel et al., 2021) .\n\nPerform adapter-tuning on MSAT\nTuning all parameters of LM encoders on synthetic data that are largely different from the pre-training corpus may lead to catastrophic forgetting (Geva et al., 2020) . To explore the importance of performing adapter-tuning on MSAT, we create a variant of our method in which we perform full finetuning on MSAT. We compare this variant with our models in Figure 6 . It can be observed that both full fine-tuning and adapter-tuning can achieve good performance on MSAT, but adapter-tuning outperforms fine-tuning on all downstream MWP datasets, which demonstrates the benefits of performing adapter-tuning on MSAT.\n\nRelated Work\nIn this work, we focus on improving moderatesized LM's MWP performance by injecting multistep reasoning ability. Hence, our work closely relates to both reasoning ability injection (Geva et al., 2020; Pi et al., 2022) and MWP solving (Xie and Sun, 2019; Patel et al., 2021; Jie et al., 2022) . Reasoning skills injection This technique refers to continually pre-training LMs on certain intentionally-crafted tasks to enhance their reasoning abilities. GenBERT (Geva et al., 2020) pretrains LMs on templated-based synthetic data to inject numerical skills into the LMs. PoET (Pi et al., 2022) improves LMs' reasoning ability by pre-training them on tabular data towards imitating program executors. Both methods involve training LMs to produce numerical answers directly, which can be unreliable (Razeghi et al., 2022) . Our work focuses on injecting into LMs the capability for solving complex arithmetic problems step-by-step.\n\nSolving MWP with specialized architectures\nOne of the research lines of MWP solving focuses on designing specialized achiectures for math reasoning (Xie and Sun, 2019; Lan et al., 2021; Jie et al., 2022) . For example, Lan et al. (2021) combines RoBERTa (Liu et al., 2019) with a Transformer (Vaswani et al., 2017) decoder, and Jie et al. (2022) augments encoder-only LMs with a directed acyclic graph decoder. One of the shortages of such models is the information loss caused by masking actual numbers in the questions with symbolic tokens (Wu et al., 2021) . In this work, we propose to represent actual numbers with digit tokenization, and improve models' multi-step reasoning ability by pre-training them on a synthetic task MSAT.\n\nConclusion\nWe propose a novel synthetic pre-training task, MSAT, to incorporate LMs with multi-step reasoning skills that improve performance on MWP tasks. This pre-training task encourages LMs to generate intermediate reasoning steps instead of predicting final numerical answers directly. Our experiments show that the proposed method is effective in improving the moderate-sized LM's performance on MWP solving tasks.\n", "hypothesis": " Mathematical reasoning is regarded as a necessary ability for Language Models (LMs).  Recent works demonstrate large LMs' impressive performance in solving math problems.  The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters.  This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning.  We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MSAT which is composed of Multi-step Arithmetic Tasks.  Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs' math reasoning abilities.", "answer": true}
{"title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification", "content": "\nIntroduction\nText simplification (TS) is a task in the field of natural language generation. It aims at rewriting a complex text into simple text while keeping the primary meaning intact (Laban et al., 2021) .\nRecently, several works have leveraged pretrained models for TS (Omelianchuk et al., 2021; Devaraj et al., 2022) . However, problems arise when pre-trained models are applied to TS directly. In the pre-training stage, the model hardly acquires the ability to generate simple texts. The improvement of results on simplification tasks relies almost on the fine-tuning stage. It can hurt the performance of pre-trained models, especially for lowresource sub-tasks like lexical simplification. One reason for this shortcoming is the pre-training strategy. It randomly masks text spans in ordinary texts, teaching the model to generate ordinary texts rather than simple texts.\nWe are committed to adapting the pre-trained model to TS in this paper. The pre-trained model has gained the ability to generate ordinary texts, and it is costly to start pre-training from scratch. Therefore, we focus on the continued pre-training strategy (Gururangan et al., 2020) . We first aim to continue pre-training on simple texts because it contains plenty of simple words. In TS, simple texts are derived almost from SimpleWiki (Zhang and Lapata, 2017) and Newsela (Xu et al., 2015) . We identify simple text spans in simple texts and dynamically replace them with <mask> tokens. Then, the pre-trained model will learn by reconstructing simple words. Meanwhile, we expect the pretrained model to learn from ordinary texts. We use a dictionary to replace complex words in ordinary texts with simple words. We also ensure the quality of the replaced sentences.\nBased on BART (Lewis et al., 2020) , we continue pre-training to teach it to generate simple texts and obtain SimpleBART. We then conduct experiments on three main tasks of TS: sentence simplification, lexical simplification, and documentlevel simplification. SimpleBART achieves consistent and noticeable improvements across several datasets on all three tasks over BART and several other baselines. The results illustrate that our proposed strategy helps the pre-trained model to gain the ability to generate simple texts.\nTo summarize, our contributions include: (1) We propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. (2) We continue pre-training BART, a representative seq2seq model, to obtain Simple-BART. It can be used for several simplification tasks and achieve consistent performance improvement. Code and SimpleBART will be released at https://github.com/RLSNLP/SimpleBART.\n\nMethodology\nAs illustrated in Figure 1 , our strategy is divided into two parts: learning dynamically to reconstruct simple words from simple texts and from ordinary texts where complex words are replaced with simple ones. \n\nMasking Simple Words in Simple Texts\nWe need to identify the simple words in simple texts at first. We take advantage of the DeepBlueAI model (Pan et al., 2021) that achieves state-of-theart results on the lexical complexity prediction task (Shardlow et al., 2021) . A text span of length n consists of n words. The input to the DeepBlueAI model is a text span and the output is a complex value between 0 and 1. The closer this value is to 0, the simpler the text span.\nUnlike the previous constant mask probability, in our strategy, the simpler a text span is, the higher its probability of being masked. This means that the mask probability is dynamic. We also set a complexity threshold of T . If the complexity c of a text span exceeds T , we will not mask this span. In our experiments, we set T to 0.25 as an empirical value. Following Lewis et al. (2020) , we set the max mask probability to 0.15, and the length of a text span obeys a Poisson distribution (\u03bb = 3). Finally, the mask probability m is calculated as:\nm = \uf8f1 \uf8f2 \uf8f3 0.15 \u00d7 (1 \u2212 1 T \u2022 c), c \u2264 T 0, c > T (1)\nThe function to mask the text span is denoted as g(\u2022). Given a sentence x, the pre-trained model will learn to reconstruct x from the masked sentence:\nl(x) = \u2212logP (x|g(x))\n(2)\n\nReplacing Complex Words in Ordinary Texts\nWe also expect the pre-trained model to learn helpful information from ordinary texts. However, ordinary texts contain more complex words than simple ones, making the pre-trained model learn to reconstruct simple words much less frequently. We introduce the dictionary SimplePPDB++ (Maddela and Xu, 2018) to address this issue. It contains millions of paraphrase rules with readability scores. Therefore, we can replace the complex words in ordinary texts with simple words. Then, the pretrained model will learn to reconstruct these simple words as in Eq.( 2). Nevertheless, a word may have different meanings in different sentences. Using a dictionary to replace complex words may change the meaning of the original sentence. Therefore, we use BERTScore (Zhang et al., 2019) to calculate the similarity between the original and replaced sentences to avoid this problem. We will discard the replaced sentences if the calculated BERTScore is lower than a similarity threshold. In our experiments, the similarity threshold is set to 0.95 as an empirical value.\n3 Experimental Settings\n\nContinued Pre-training\nWe select the BART-Large model to continue pretraining. It is a representative seq2seq model suitable for three main simplification tasks. We follow the task-adaptive pre-training method (Gururangan et al., 2020) and continue pre-training on the training set of the corresponding simplification task, ensuring that the continued pre-training texts have no intersection with the test set. We refer to the pretrained models obtained by our strategy collectively as SimpleBART.\n\nSimplification Tasks\nWe select three representative tasks for experiments: sentence simplification, document-level simplification, and lexical simplification. For sentence simplification, we conduct experiments on Wikiauto (Jiang et al., 2020) and Newsela (Xu et al., 2015) . Wikiauto is only a training set, so we use Turkcorpus (Xu et al., 2016) as its validation and test set. Following Sun et al. (2023) , we use SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2019) as the evaluation metrics. BLEU and FKGL have been proven to be unsuitable for evaluating simplification (Sulem et al., 2018; Tanprasert and Kauchak, 2021) . For document-level simplification, we conduct experiments on the D-Wikipedia dataset (Sun et al., 2021) . We use D-SARI (Sun et al., 2021) as the evaluation metric. For lexical simplification, we conduct experiments on LexM-Turk (Horn et al., 2014) and BenchLS (Paetzold and Specia, 2016) . We use precision, recall, and F1 score as the evaluation metrics. For more hyperparameter setting details, please refer to Appendix B.\n\nSentence Simplification\nTo demonstrate the advantages of our strategy, we develop BART-CP for a fair comparison. It continues pre-training with the same number of steps on the same data using the previous pre-training strategy from Lewis et al. (2020) We choose EditNTS (Dong et al., 2019) , T5base (Raffel et al., 2020) , and ControlTS (Maddela et al., 2021) as baselines. T5-base is close to Sim-pleBART in size. ControlTS achieves the state-ofthe-art result on the Newsela dataset. Following Alva-Manchego et al. ( 2021), BERTScore precision (BS) is also reported. From Table 1 , the BS scores of all outputs are high enough, which means that the outputs are of high quality. According to SARI, the most important automatic evaluation metric for sentence simplification, SimpleBART improves SARI values over BART by 1.2 points and 1.5 points, respectively. Overall, it achieves comparable results to the advanced model for the sentence simplification task. We also notice that Simple-BART outperforms BART-CP, demonstrating the effectiveness of our proposed strategy. The example outputs are given in Appendix D.\n\nLexical Simplification\nWe focus on generating suitable words using the pre-trained model, which is a critical step in lexical simplification. We follow Qiang et al. (2020a) and let the pre-trained models generate several candidate words. BenchLS and LexMTurk are just two test sets, so we continue pre-training on the Wikiauto training set. We choose Paetzold-NE (Paetzold and Specia, 2017a) and LSBert (Qiang et al., 2020b) As shown in Table 2 , SimpleBART improves the F1 scores over BART by 8.6 points and 9.7 points, respectively. It achieves comparable results to LSBert. The results also demonstrate that BART needs to gain the ability to generate simple words and the importance of introducing continued pretraining when training data is scarce.\n\nDocument-level Simplification\nSimpleBART also performs well on the documentlevel simplification task.\nWe choose Bert-Sumextabs (Liu and Lapata, 2019) , which achieves the state-of-the-art result on this task as a baseline. Compared with BART, SimpleBART improves the Table 4 shows that SimpleBART achieves the highest Simp score among all the simplification models, close to that of the reference. Simple-BART also significantly makes more word-level simplifications compared to BART and BART-CP.\n\nDomain Adaptation\nContinued pre-training using our strategy on taskrelated data can improve the results. However, we still want to know if continued pre-training on more data from the same domain and different domains will improve the results. We design the following experiments. 1) Exp1: We continue pre-training on more sentences from Wikipedia and SimpleWiki, except those contained in the Wikiauto dataset. 2) Exp2: We continue pre-training on more sentences in the Newsela corpus, except those contained in the Newsela dataset. From the results of Exp1 and Exp2 in Table 5 , continued pre-training on more texts from the same domain can still enhance the simplification results. Compared to BART in Table 1 , the SARI values improve by 0.6 and 1 point, respectively. From the results of Exp3 and Exp4, continued pre-training on more texts in a different domain can instead harm the results. Compared to BART, the SARI values decrease by 0.3 and 0.5 points, respectively. Thus, we suggest that future researchers use texts within the same domain (e.g., Wikiauto and Wikipedia) for continued pre-training in text simplification.\n\nGenerating Complex Texts\nThere are numerous studies dedicated to simplifying complex texts. Nevertheless, none has attempted to rewrite simple texts into complex ones. We make such an interesting attempt. We have changed our strategy to mask complex words and name the obtained model ComplexBART. When fine-tuning and testing on the Newsela dataset, we use simple texts as input and complex texts as reference. From Table 6 , ComplexBART improves the SARI value by 1.5 points over the BART model, indicating that the modified strategy can help the pre-trained model learn to generate complex texts. Thus, ComplexBART can serve as a better baseline for generating complex texts in the future.\n\nComparing SimpleBART with Large Language Models\nLarge language models (LLMs) have received widespread attention from researchers recently and have achieved state-of-the-art results on many natural language generation tasks. In this section, we select several representative large models to conduct experiments on text simplification and compare them with SimpleBART. We hope these results can serve as baselines for future research.\nWe choose those LLMs that provide API or model files to ensure reproducibility. We choose GPT-3.5-Turbo-0301 1 , FLAN-T5-XL (Chung et al., 2022) , and LLaMA-7B (Touvron et al., 2023) as LLM baselines and use zero-shot generation. Then, we follow the implementation 2 and fine-tune FLAN-T5-base as another baseline. We collect the training sets of Wikiauto, Newsela, and D-Wikipedia and conduct instruction fine-tuning.\n\nComparison and Analysis\nThe comparison of SimpleBART results with those of the LLMs is shown in Tables 7, 8, and 9 .\nFor the sentence-level simplification task, LLaMA and FLAN-T5-XL seem unable to understand the prompt for simplifying sentences, and they are inclined to repeat the original text. However, FLAN-T5-base, only 10% of the parameters of the above two models, performs better. It illustrates fine-tuning phase can improve performance when the model is not super large. It may be a little strange that GPT-3.5 performs worse than Sim-pleBART. We find that with the zero-shot setting, GPT-3.5 may not know the \"degree of simplification\" we want. It makes many reasonable changes to the original text, but it also keeps some of the complex parts of the original text.\nFor the document-level simplification task, LLaMA over-repeats sentences from the original article, and the generated text is difficult to read. The shortcomings of GPT-3.5 are similar to those of the sentence-level simplification task. Besides, limited by the number of API accesses per minute of OpenAI, we only select 1000 original documents for simplification, which takes nearly five hours.\nFor the lexical simplification task, neither the LLaMA nor the FLAN-T5 model could understand the instruction to replace complex words with simple words. However, GPT-3.5 outperforms the other models substantially. We also find that GPT-3.5 makes many sensible substitutions not included in the reference, such as replacing \"acquired\"with \"earned\". Such results illustrate that LLMs are dominant for this task.\n\nConclusion\nIn this paper, we are committed to adapting the pre-trained model to text simplification. We propose a new pre-training strategy to allow the pretrained model to learn to generate simple texts. The adapted pre-trained model improves the results on various simplification tasks.\n", "hypothesis": " Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts.  It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate complex texts. We continue pre-training BART, a representative model, to obtain ComplexBART.  It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART.  At the end, we compare Sim-pleBART with several representative large language models (LLMs)..", "answer": false}
{"title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks", "content": "\nIntroduction\nPretraining and fine-tuning are now the prevalent paradigm in natural language processing, yielding state-of-the-art performances on a variety of tasks (Devlin et al., 2019) . With pre-trained language models (PLMs) growing rapidly in size, it becomes increasingly infeasible to perform conventional fine-tuning on the entire model parameters. There has recently been one line of research on Parameter-Efficient Language model Tuning (PELT) (Houlsby et al., 2019; Li and Liang, 2021; He et al., 2021; Mao et al., 2022) . They only update a set of extra trainable task-specific parameters that are newly introduced to PLMs. Although the number of new parameters is much fewer than the original PLM, training these parameters per single task is still costly, especially when targeting a number of tasks, i.e., multi-tasking scenario.\nTherefore, we are motivated to start with a unified parameter-efficient language model tuning framework (He et al., 2021) and explore on a shared hypernetwork (von Oswald et al., 2020; Mahabadi et al., 2021) that is able to take multi-task information as input, and generate weights for tuning different task-specific modules of PLMs, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.). We name it HyperPELT. Besides, we propose a novel perspective of adopting parameter-efficient multimodal fusion for PLMs via the hypernetwork. Thus we explore to use an additional separate hypernetwork handling visual input and generating visual-specific weights for multiple modules of PLMs.\nEmpirical results on 8 tasks of GLUE benchmark show that HyperPELT achieves superior performances (87.09 vs. 86.53) with a tenth of the parameters (0.24% vs. 2.96%) when compared to state-of-the-art alternatives. Study on the few-shot transfer learning indicates that HyperPELT is more stable and efficient than alternatives. It confirms the effectiveness of our unified parameter-efficient multitask learning framework. What's more, we evaluate our framework on V&L multi-tasks (4 tasks). Results show the promising performance of our novel fusion method on extending V&L ability on top of PLMs via hypernetworks.\nIn summary, we make the following contributions: (1) propose a unified parameter-efficient multitask learning framework that is able to take multi-task and multi-modality information as input, and generate weights for tuning different taskspecific modules of PLMs; (2) present a novel perspective of using hypernetworks to achieve the parameter-efficient multimodal fusion on top of PLMs; (3) design various experiments to compre- Figure 1 : The model structure of the proposed unified pure language and V&L multi-task framework (left), and illustration of computing the hyper-embedding (right). We use green color to fill the trainable layers and grey color for the frozen ones. And the dashed parts denote the modules for processing visual modality.\nhensively demonstrate the effectiveness of our proposed framework in multi-task learning and fewshot domain transfer scenarios.\n\nRelated Work\nExisting research has explored a large amount of methods on parameter-efficient tuning, such as the widely used adapter-tuning (Houlsby et al., 2019), prefix-tuning (Li and Liang, 2021) and the mixed methods (He et al., 2021; Mao et al., 2022) . However, it is time & space-consuming to deal with a set of tasks in multi-task learning if we simply update and save separate replicas of model parameters per single task. In this work, we explore a hypernetwork-based multi-task learning framework to generate weights for different PELT modules.\nBesides, there has been a series of recent work (Cho et al., 2021; Tsimpoukelli et al., 2021; Sung et al., 2021; Alayrac et al., 2022) to equip a language model with the ability of handling visual input with a small number of trainable modules and parameters. Different from existing work, we propose a novel perspective of multimodal fusion via extending the proposed parameter-efficient multitask learning framework. We further review recent research on parameter-efficient tuning for pure language and V&L tasks, as well as the corresponding work for multi-task learning in Appendix A.\n\nProposed Method\nWe target a general multi-task learning problem, which is formulated in Appendix B. In this section, we describe the hyper-embedding I for hypernetworks to generate weights \u2206\u03b8 and which modules of PLMs to insert these weights to achieve PELT. In our methods, the hyper-embedding I consists of two: task-specific hyper-embedding I \u03c4 , and visualspecific hyper-embedding I v . We will mostly introduce the hyper-embedding I \u03c4 , and I v is used in a similar parallel manner. A simple linear projection layer is employed as the hypernetwork, for example, h \u03c4 P (.) and h v P (.) are used for prefix-tuning, while h \u03c4 A (.) and h v A (.) are for adapter-tuning as shown in Figure 1 . The hypernetwork takes the hyper-embedding I as input, and outputs weights for multiple modules of PLMs.\n\nHyper-Embedding for PELT\nConsidering a flexible parameterization of taskspecific parameters for L layers of transformer, we introduce a set of layer id embeddings I = {l i } L i=1 , and block type embeddings B = {b j } 5 j=1 , which specify the position where the parameters \u2206\u03b8 are inserted to. Then, we compute a hyper-embedding I \u03c4 \u2208 R d I for each individual task via a task projector network, which is a multi-layer perceptron consisting of two feed-forward layers and a ReLU nonlinearity: I \u03c4 = MLP([z \u03c4 , l i , b j ]). Thus, it learns a suitable compressed hyper-embedding from a concatenation of task embeddings z \u03c4 \u2208 R d\u03c4 , layer id embeddings l i \u2208 R d\u03c4 , and block type embeddings b j \u2208 R d\u03c4 . In this way, the hypernetwork is able to produce distinct weights for tuning each task, and each transformer block at each layer.\n\nHyperPELT: Incorporate with\nPrefix-tuning and Adapter-tuning\nTo further capture knowledge across tasks and transfer to others, we follow the unified parameterefficient framework (He et al., 2021) , and input the hyper-embedding to a hypernetwork for generating the weights in adapters as well as prefix vectors.\nWe extend the dimension for different embeddings to match the prefix length\nN , i.e., z \u2208 R N \u00d7d\u03c4 , l i \u2208 R N \u00d7d\u03c4 , b j \u2208 R N \u00d7d\u03c4\n, and then compute the hyper-embedding I \u03c4 \u2208 R N \u00d7d I . We finally employ a hypernetwork h \u03c4 P (.) with trainable parameters \u03b8 h \u03c4 P , to project I \u03c4 to prefix vectors P \u03c4 \u2208 R N \u00d7d : P \u03c4 = h \u03c4 P (\u03b8 h \u03c4 P , I \u03c4 ) . Besides, as depicted in Figure 1 , we introduce a hypernetwork-based adapter layer with a trainable scaled parameter \u03bb, which is inserted parallelly with feed-forward blocks. We generate adapter weights\n(W \u03c4 up , W \u03c4 down ) through a hypernet- work h \u03c4 A (.): (W \u03c4 up , W \u03c4 down ) := h \u03c4 A (\u03b8 h \u03c4 A , I \u03c4 ), where W \u03c4 down \u2208 R d mid \u00d7d and W \u03c4 up \u2208 R d\u00d7d mid .\n\nVL-HyperPELT: Incorporate with Visual Modality\nAs illustrated in Fig. 1 , we use CLIP (Radford et al., 2021) with a trainable visual mapping layer, which projects the visual representation to the identical dimension of task embedding, i.e.,\nz v \u2208 R N \u00d7dv , d v = d \u03c4 .\nThen we feed this visual representation z v to a visual projector network. In this way, we learn the visual hyper-embedding\nI v \u2208 R d I .\nFinally, taking the visual-specific hyperembeddings as input, we use visual-specific hypernetworks to generate visual-specific parameters to different modules in PLMs. Similar to the Section 3.1 & 3.2, the incorporation of visual-specific parameters to PLMs are the same as task-specific ones, e.g., used as prefix vectors via a prefix hypernetwork h v P (.) and adapter weights via an adapter hypernetwork h v A (.). We name it VL-HyperPELT.\n\nResults and Analysis\nWe conduct a series of experiments to verify the effectiveness of our proposed framework compared to existing ones.\n\nImplementation Details\nOur models are built on T5 BASE (Raffel et al., 2020) We did not experiment with other complex sampling strategies or tuning of T . For the experiments under multi-task training settings, we save a checkpoint every 1000 steps and report results on a single checkpoint with the highest average validation performance across all tasks.\nIn terms of the vision-and-language scenarios, we convert V&L tasks to the text generation format as Cho et al. (2021) . We use ResNet101 as our vision encoder, and initialize it with weights from pretrained CLIP (Radford et al., 2021) . Input images are resized to 224 \u00d7 224 for memory efficiency. We extract the 7 \u00d7 7 grid features produced by the last convolutional layer. The percentage of updated parameters is also reported as one metric for approach efficiency, and we do not take visual encoder into account since it is frozen in our experiment.\n\nDatasets\nOur framework is evaluated on the GLUE benchmark (Wang et al., 2019b) in terms of natural language understanding. This benchmark covers multiple tasks of paraphrase detection (MRPC, QQP), sentiment classification (SST-2), natural language inference (MNLI, RTE, QNLI), and linguistic acceptability (CoLA). The original test sets are not publicly available, and following Zhang et al. (2021) , for datasets fewer than 10K samples (RTE, MRPC, STS-B, CoLA), we split the original validation set into two halves, one for validation and the other for testing. For other datasets, we randomly split 1K samples from the training set for validation and test on the original validation set.\nIn addition, we evaluate the few-shot transfer performance on four tasks and datasets: 1) the Table 1 : Performance of all models on the GLUE tasks. For each method, we report the total number of parameters across all tasks and the number of parameters that are trained for each task as a natural language inference (NLI) datasets CB and 2) the question answering (QA) dataset BoolQ from SuperGLUE (Wang et al., 2019a) ; 3) the sentiment analysis datasets IMDB (Maas et al., 2011) ; and 4) the paraphrase detection dataset PAWS (Zhang et al., 2019) . For CB and BoolQ, since the test set is not available, we split the validation set into two halves, one for validation and the other for testing.\nFor IMDB, since the validation set is not available, we similarly split the test set to form validation.\nFor PAWS, we report on the original test set.\nTo evaluate our framework on V&L tasks, we experiment on four datasets COCO (Lin et al., 2014) , VQA (Goyal et al., 2017) , VG-QA (Krishna et al., 2017) and GQA (Hudson and Manning, 2019). Following Cho et al. (2021) , we use VQA Karpathy split, which splits the VQA dataset into 605,102 / 26,729 / 26,280 image and question pairs separately as the train/validation/test set to evaluate VQA tasks in a generative manner. We further evaluate our framework on two datasets for V&L few-shot transfer learning: OKVQA (Marino et al., 2019) ; SNLI-VE (Xie et al., 2018) .\n\nResults on the GLUE Benchmark\nWe conduct experiments on GLUE for both singleand multi-task settings, as shown in Table 1 . Compared to the single-task Adapters that finetunes all newly introduced parameters in adapters, our method yields a significant improvement by 2.21% with much fewer trainable parameters. It illustrates the effectiveness of our proposed multi-task training framework. The comparison to MAMAdapter shows that using hypernetwork to tune each transformer module and thus learn the shared knowledge across multitasks, leads to an improvement in task performance (86.53 vs. 87.09) while training fewer parameters (2.96% vs. 0.24%). Overall, our Hy-perPELT obtains the best performance with less trainable parameters.\n\nFew-shot Domain Transfer\nWe use the above models trained on GLUE as reported in tasks of CB and BoolQ from SuperGLUE, even though the backbone T5 was previously trained on the train sets of these two, the performance of all methods differs a lot. The two baselines still do not work with very few samples, like 4 and 16 samples. Therefore, we assume that the two baselines suffer from catastrophic forgetting problems to some degree during multi-task training. In contrast, our proposed HyperPELT works effectively on these two tasks. We speculate that the reason might be the use of hypernetworks on both prefix-tuning and adapter-tuning modules of transformer. We leave this exploration to our future work. Besides, we show the results of Prompttuning (Lester et al., 2021) and fine-tuning only the task embedding in our HyperPELT. Note that in this comparison, we keep the same trainable parameters between these two methods, i.e., R N \u00d7d\u03c4 , where N denotes the prompt length in Prompt-tuning method. Our HyperPELT TaskEmbed mostly achieves a comparable or even better performance than Prompt-tuning.\n\nResults on Vision-and-Language Benchmarks\nWe compare the pre-trained and full fine-tuning VL-T5 (Cho et al., 2021) , and other adapter-based methods built on top of T5, i.e., CLIP-T5 and VL-Adapter (Sung et al., 2021) in the multi-task training setting. The results and the number of trainable parameters are reported in Table 2 . Since the used dataset is slightly different from Sung et al. (2021) and their checkpoint is not avaliable at this time, we re-implement CLIP-T5 and VL-Adpater. Compared to which, our method achieves a comparable performance with a fewer number of trainable pa-rameters (e.g., 7.16% of VL-Adapter vs. 6.62% of VL-HyperPELT).\nWe further evaluate our models on multimodal few shot learning tasks and show its superiority in appendix E.1. To our best knowledge, we are the first to employ the visual modality to tune the very few parameters of different transformer blocks, instead of normally inserting image patch tokens to the input sequence. Experimental results evidence the effectiveness of our novel approach, thus providing a new perspective on how to extend the multi-modality capability on top of PLMs.\n\nDiscussion and Conclusion\nIn this paper, we propose a unified parameterefficient tuning framework for multitasks. On the one hand, we use the hypernetwork to reduce the scale of trainable parameters of existing adaptertuning and prefix-tuning modules. On the other hand, for the V&L tasks, we directly integrate the image features into the prefix vectors as well as adapters, which further reduces the number of trainable parameters for processing visual input. Extensive experiments on pure language and V&L tasks demonstrate the superiority of our proposed framework in both multi-tasking and few-shot settings. In the future, we plan to explore more combination of methods across tuning task-specific and visualspecific parameters for different modules of PLMs.\n", "hypothesis": " With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Visionand-Language (V&L) tasks.  In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks.  In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a pretrained language model, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.).  Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods.  Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework..", "answer": true}
{"title": "Transformed Protoform Reconstruction", "content": "\nIntroduction\nLanguages change over time and sometimes diverge into multiple daughter languages. The common ancestor of a set of genetically related languages is their proto-language. While there are proto-languages such as Latin that are attested, they are the exception 2 . Reconstructed words and morphemes in proto-languages are called protoforms. The task of reconstructing unattested protolanguages is called protoform reconstruction.\nHistorical linguists reconstruct proto-languages by identifying systematic sound changes that can be inferred from correspondences between attested daughter languages (see Table 1 ). They compare the sounds between a set of cognates, or words with a common ancestor, to develop hypotheses about the types and chronologies of sound changes. ' This task is inherently data-constrained, especially for under-documented languages. Such data scarcity makes it a particularly difficult task for contemporary neural network architectures such as the Transformer (Vaswani et al., 2017) , which are data hungry.\nThe contributions of this paper are as follows:\n\u2022 Application of the Transformer architecture to the protoform reconstruction task, achieving state of the art performance, contrary to expectation.\n\u2022 Expansion of prior digital versions of H\u00f3u ( 2004)'s Chinese dataset to include a total of 804 cognate sets across 39 modern varieties and Middle Chinese.\n\nRelated Work\nApplying machine learning to protoform reconstruction is not new. Bouchard-C\u00f4t\u00e9 et al. (2013) learn an unsupervised protoform reconstruction model for the large Oceanic language family using Monte Carlo Expectation Maximization (Dempster et al., 1977\u037e Bouchard-C\u00f4t\u00e9 et al., 2008) , supervising the model with a gold phylogeny and using a probabilistic, generative model of sound change. List et al. (2022) apply an SVM classifier to supervised reconstruction by treating sound correspondences as training examples. Note that there were no word boundaries in the input matrix\u037e that is, all sound correspondences across the training set are flattened into one matrix. Furthermore, each language has an independent phonemic inventory. To learn contextual information, the authors experiment with adding features encoding the position of phonemes, among others. Ciobanu and Dinu (2018) learn a conditional random field (Lafferty et al., 2001) using n-gram features for supervised reconstruction and ensemble 5 daughter-to-protoform models. They use a dataset of 3,218 complete cognate sets spanning Latin (the proto-language) and 5 Romance languages: Romanian, French, Italian, Spanish, Portuguese. Meloni et al. (2021) employ a GRU-based seq2seq approach (Cho et al., 2014) to Latin protoform reconstruction and achieve state-of-theart character edit distances. They extend Dinu and Ciobanu (2014) 's Romance data using data from Wiktionary-for a total of 8,799 cognate sets across 5 Romance languages plus Latin-in both orthographic and phonetic (IPA) representations. In their model, all entries comprising the cognate set are concatenated together in a fixed order to form a training example. Chang et al. (2022) applied Meloni et al. (2021) 's architecture to the reconstruction of Middle Chinese on a dataset of 5000+ cognate sets spanning 8 languages they compiled from Wiktionary. 3 Fourrier (2022) compares statistical machine translation, RNN, and Transformer architectures for protoform reconstruction, but they evaluate their results using BLEU scores (Papineni et al., 2002) instead of edit distance. They find that their Transformer model did not outperform the RNN models on protoform reconstruction. In addition, their multilingual NMT (neural machine translation) model predicts many languages instead of one target language and is trained on bilingual pairs for protoform reconstruction (e.g. Italian-Latin and Spanish-Latin), unlike comparative reconstruction. In contrast, we encode the entire cognate set consisting of multiple daughter languages (5 for the Romance dataset\u037e 39 for Chinese) and predict the corresponding protoform.\n\nDatasets\nWe train and test our model on Romance and Sinitic (Chinese) language datasets. For Romance languages, we use Meloni et al. (2021) 's dataset which consists of 8,799 cognate sets of Romanian, French, Italian, Spanish, Portuguese words and the corresponding Latin form (approximately, a protoform). There are two versions of this dataset: phonetic and orthographic. The phonetic dataset (Rom-phon) represents words with IPA symbols whereas the orthographic dataset (Rom-orth) represents words in the orthographic form of each language. We preserved all diacritics, except for vowel length. This dataset is an extension of Dinu and Ciobanu (2014) 's original dataset of 3,218 cognate sets, which is not publicly available. Refer to Table 2 for more information.\n\nExpanding digital versions of H\u00f3u (2004)\nFor Sinitic languages, we created a dataset of Middle Chinese and its modern daughter languages. Middle Chinese is an unattested language, and we thus have to rely on Baxter and Sagart (2014)'s reconstructions of forms corresponding to 4,967 Chinese characters. We scraped Wiktionary to obtain H\u00f3u (2004)'s phonetic representations of their modern reflexes. 4 The resulting dataset contains 804 cognate sets of 39 modern Sinitic languages and the corresponding reconstructed Middle Chinese word. List (2021)'s version previously had 894 cognate sets across 15 varieties.\n\nModel\nWe propose a Transformer-based encoder-decoder architecture (Vaswani et al., 2017) because such models have produced state-of-the-art results on many sequence processing tasks. Transformers are by reputation data hungry, though, which poses a challenge to our problem setting, where the number of available training examples is often very small. We modify the standard encoder-decoder architecture to accommodate the structure of our datasets, where multiple daughter sequences correspond to a single protoform sequence. Like Meloni et al. (2021) , the daughter sequences are concatenated into a single sequence before being fed into the encoder. Because we only care about the relative position between tokens within each daughter sequence but not across daughter sequences, positional encoding is applied to each individual daughter sequence before concatenation. Along with positional encoding, an additive language embedding is applied to the token embeddings to differentiate between input tokens of different daughter languages.\n\nBaselines\nWe compare our Transformer model to a variety of baselines. For Meloni et al. (2021) , we use Chang et al. (2022) 's PyTorch re-implementation and reran a Bayesian hyperparameter search using WandB (Biewald, 2020) to ensure a more fair comparison (since our model is tuned with WandB as well). We also include the random daughter (randomly designate a daughter form as the protoform and assume no sound change) and the majority constituent baselines (predict the most common phoneme in each syllable constituent) from Chang et al. (2022) . For the SVM and CoRPaR classifiers (List et al., 2022) , we experiment with different contextual features, such as Pos (position), Str (prosodic structure), and Ini (whether or not the phoneme appears word-initially or word-finally).\nWe publish results on Meloni et al. (2021) 's full set of 8,799 cognates but cannot redistribute this set due to Dinu and Ciobanu (2014) 's restrictions. For reproducibility, we include results on Meloni et al. (2021) 's public subset of 5,419 cognates in the Appendix (Table 7 ), both of which include vowel length. Observe that these results are worse than those obtained on the full set, suggesting that the RNN and Transformer are dependent on a wealth of training data.\n\nPreprocessing\nIn all our datasets, we merge diacritics to their base segments to form a multi-character token. For instance, the sequence [t, \u02b0] is concatenated to [t\u02b0] . This ensures that phonemes are treated as one token. For Chinese, tone contours (a sequence of tones) are treated as one token. When multiple pronunciation variants are listed for a single Chinese character, we arbitrarily pick the first one.\n\nEvaluation criteria\nWe evaluate the predicted protoforms using edit distance (Levenshtein et al., 1966) , normalized edit distance (edit distance normalized by the length of the target) and accuracy (the percentage of protoforms that are reconstructed without any mistakes). Like Chang et al. (2022) , we also use feature error rate calculated using articulatory feature vectors from PanPhon (Mortensen et al., 2016) because it reflects the phonetic similarity between the prediction and the gold protoform. For datasets with phonetic transcriptions (Romancephonetic and Chinese), we use phoneme edit distance and normalized phoneme edit distance. As List (2019) suggests, we use B-Cubed F Scores (Amig\u00f3 et al., 2009) to capture the structural similarity between the gold and predicted protoforms (0: structurally dissimilar, 1: similar). With the exception of character and phoneme edit distance, the metrics enable fair comparison across different language families, which will differ in the average word length.\n\nResults\nTable 3 shows that our model consistently has the best performance on all datasets with regards to most metrics. The results were averaged across 5 runs. Out of all datasets, our model performs best on the Rom-orth dataset, where we achieve a 7.0% decrease in phoneme edit distance and a 1.43p.p improvement in accuracy relative to the RNN baseline. We observe the most dramatic performance difference with the RNN baseline on the Sinitic dataset: a 10.48% decrease in phoneme edit distance and a 5.47p.p increase in accuracy. For reproducibility, results on the publicly available portion of the Rom-phon and Rom-orth datasets are provided in Table 7 in the Appendix.\n\nAnalysis\nWe observe that the BCFS is relatively high for the Romance non-neural baselines compared to those of the Chinese ones. This suggests that the sound changes in the Romance datasets are more regular than that of Chinese, which corroborates List et al.\n(2014)'s results that more than half of the Chinese characters in their dataset could not be explained by a tree model. We examine the errors made by the Transformer model on the Rom-phon datasest. Substitutions constitute around 61% of the errors made by the Transformer\u037e deletions, 21%, and insertions, 18%. The highest number of substitution errors occur between [i, \u026a] , [e, \u025b], [o, \u0254] and [u, \u028a]-vowel pairs that contrast only in tenseness. This is consistent with the analysis of Meloni et al. (2021) , where substitutions between tense-lax vowel pairs take up the largest portion of errors.\nWe observe that other common substitution errors also happen between phonemes that share major phonetic features. This demonstrates that al-though no explicit phonetic information is fed directly into the model, the model makes mistakes motivated by phonetic similarity, like Meloni et al. (2021) .\nWe do not observe notable differences in the error statistics between the Transformer and the RNN.\n\nLanguage relatedness\nInspired by Fourrier (2022) , we probe our model for diachronic information on how genetically related each Romance language is to each other. We create a distance matrix between every pair of languages in a dataset by taking the cosine similarity between a pair's language embeddings. We then use sklearn (Pedregosa et al., 2011) 's implementation of the Ward variance minimization algorithm (Ward Jr, 1963) to perform hierarchical clustering on the distance matrix. We take a consensus of the dendrograms from 5 different runs using the consense program from PHYLIP (Felsenstein, 2013).\nAs we see in Figure 2 , the Transformer captures more of the phylogenetic relationships among the languages correctly for the Rom-phon dataset. Indeed, the Generalized Quartet Distance (GQD) (Sand et al., 2013\u037e Pompei et al., 2011\u037e Rama et al., 2018) between the gold and predicted tree, calculated using quartetDist from the tqDist library (Sand et al., 2014) , is 0.4 for the Transformer but 0.8 for the RNN. See Figure 5 in the Appendix for the results of the orthographic dataset.\n\nModel PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Sinitic\nRandom daughter (Chang et al., 2022) 3.7702 0.8405 0% 0.2893 0.2748 Majority constituent (Chang et al., 2022) 3.5031 0.7806 0% 0.2013 0.3695\nCorPaR (List et al., 2022) 3.2795 0.7278 0% 0.3972 0.3332 SVM + PosStr (List et al., 2022) 1.6894 0.3692 15.52% 0.1669 0.5418 RNN (Meloni et al., 2021) 1 Since the Romance dataset only includes 5 daughter languages, our results are insufficient to corroborate or contradict Cathcart and Wandl (2020) 's findings: the more accurate the protoforms, the less accurate the phylogeny will be. It is not clear if the model's language embeddings are learning information that reflects shared innovations (sound changes that if shared among a set of daughter languages, would be acceptable justification for grouping them)-the only acceptable criterion for phylogenetic inference in historical linguistics (Campbell, 2013) -or if the model is learning superficial phonetic similarity.\n\nConclusion\nBy showing that Transformers can outperform previous architectures in protoform reconstruction despite the inherent data scarcity of the task, our work motivates future research in this area to take full advantage of the recent advancements in the Transformer space.\nAccurate supervised reconstruction can help pre-dict protoforms for cognate sets where linguists have not reconstructed one yet. Future work could reconstruct proto-languages whose linguist reconstructions are not available, by transferring knowledge learned from languages with already reconstructed protoforms. Furthermore, future work can leverage the abundance of work in unsupervised NMT to adapt our Transformer model for the unsupervised setting, a more realistic scenario for the historical linguist.\n", "hypothesis": " Protoform reconstruction is the task of inferring how morphemes or words sounded in ancestral languages of a set of daughter languages. Meloni et al. (2021) achieved the stateof-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with a traditional statistical approach and show that it outperforms the Transformer architecture.  Our model outperforms their model on a suite of different metrics on two different datasets: Meloni et al.'s Romance data of 8,000+ cognates (spanning 5 languages) and a Chinese dataset (H\u00f3u, 2004) of 800+ cognates (spanning 39 varieties).  We also probe our model for potential phylogenetic signal contained in the model.  Our code is publicly available 1 ..", "answer": false}
{"title": "Mind the Gap between the Application Track and the Real World", "content": "\nIntroduction\nModern NLP systems, powered by large language models (LLMs), now have the ability to perform well at foundational natural language understanding and generation tasks (Wang et al., 2018; Brown et al., 2020) . Such systems have also increased access and made inter-disciplinary contributions possible across fields such as medicine, law, education, and science. In NLP venues like ACL, the growth in applied and inter-disciplinary work can be witnessed in the NLP Applications track, which received the second-highest number of submissions at EMNLP 2022.\nRecently published research from these tracks includes work on complex and important tasks such as synthesizing code for visualization (Chen et al., 2021) , classifying operational risk in finance (Zhou et al., 2020) , and verifiying scientific claims (Wadden et al., 2020) . However, the inherent complex- ities associated with real-world data distributions and workflows can lead to the actual problem being simplified into an artificial setting that does not realistically reflect the original motivation. For instance, systems may make assumptions about the input available (e.g., require providing pseudocode/docstrings for code generation), or only evaluate on manually curated clean data as opposed to noisier data such as automatic speech recognition (ASR) outputs.\nMotivated by this observation and in line with the ACL 2023 theme track, we set out to investigate the relationship between the motivation described in the introductions and the actual experiments in application-focused NLP papers. We survey papers from the NLP applications tracks of ACL 2020 and EMNLP 2020. Specifically, we ask if there are gaps between motivation and experimentation, in the form of i) sub-tasks that are required for the application, but haven't been mentioned in the paper ii) data distributions that are expected in real-world conditions, but haven't been included in the paper's modeling or evaluation. We find that authors do not always explicitly mention assumptions they make, and often operate in con- strained scenarios highly different from their intended motivation.\nTo empirically demonstrate the severity of this problem, we then present a case study investigating the performance of an educational dialog system, when the inputs are changed from manually transcribed data to transcripts from a state-of-the-art ASR system. The purpose of the system is to classify utterances made by a student in a classroom into talkmoves (Michaels and O'Connor, 2015; O'Connor and Michaels, 2019 ) that reflect the communication strategies they use, such as making a claim, relating to another student. We find that performance drops by 14.6 points (21.2%) when evaluting on Google ASR instead of human transcripts. However, ASR was not identified as a key component of the evaluation pipeline by the original work. We argue that as the field grows and NLP models get better and better at simulated and constrained settings, it is important for us to explicitly consider additional complexities of our systems in practice. We then present suggestions for authors and organizers of conferences, towards this end.\n\nMethod\nFor the survey of application-oriented research papers, we look at all papers from the NLP Applications track of two recent NLP conferences, ACL 2020 and EMNLP 2020, which have a total of 115 papers. These conferences, which were conducted virtually, provide publicly available interfaces, 1 that allow automatically filtering papers by the track they were submitted to.\nWe then manually filter papers to identify those that propose and work on new tasks. We choose these since papers that tackle existing tasks, such as fact checking, might be restricted to existing benchmarks and datasets that are established in a topic (Thorne et al., 2018) . In contrast, papers that propose a new task, such as recommending fonts suitable for written text (Shirani et al., 2020) , can integrate considerations about the environment where the task will be used, into their problem formulation and evaluation setup. We end up with 12 papers from EMNLP 2020, and 3 papers from ACL 2020 that deal with new tasks.\nWe then answer four questions about each paper:\n1. Does the paper comprehensively describe the use case for a reader to understand? This question helps us establish that the motivations of the authors are clear to us before proceeding with the survey. We discard papers if the answer is no here.\n2. Is the paper dealing with an entire task or a sub-task only? An example of the sub-task only would be if the desired application was assisting students with writing by providing feedback, but the actual task worked on was detecting errors in writing, with the task of formulating feedback being a sub-task for future work.\n3. Does the paper mention the other missing subtasks explicitly? We investigate if the authors either mention existing systems that work on the other sub-tasks, or explicitly describe the remaining steps as future work. This is only collected when the answer to Q2 is \"sub-task only\".\n4. Is the downstream evaluation realistic? An example of the answer being No, is if the expected use-case requires classifying spoken dialog in real-time, but the paper only evaluates on manually transcribed data.\nThe survey is conducted by three authors of this paper, who have all been working on NLP for 3+ years. In cases where agreement is not perfect, we report the majority answer. While all four questions take either yes or no for an answer, we optionally collect reasons for answering no on Questions 1 and 4. We only accept unsure as an answer when no decision can be made.\n\nFindings\nThe results of the survey are presented in Table 1 . In response to the second question, we find that 4 out of 15 papers work on sub-tasks of the overall system; however, only one of these papers explicitly mentions the other sub-tasks as components of the pipeline. Overlooked are tasks such as machine translation, performing grammatical error correction, and performing document retrieval prior to classification. In response to the fourth question, we find that 7 out of 15 papers do not include evaluations that are realistic for the setting in which they might be deployed. Some comments provided by the annotators as evidence include \"evaluating only on transcribed dialog and not on ASR\", \"evaluating only on data translated from the original language\", \"not incorporating retrieval performance into evaluation pipeline\" and \"not checking the validity of integrated evidence.\" One of the responses to the last question is unsure, provided by two of the annotators, while the third annotator answered yes. One annotator's rationale for being unable to decide is that the output space modeled in the paper does not adequately reflect that seen by a user, while the second annotator claims that the task is highly subjective.\nWe compute inter-rater agreement using Krippendorff's \u03b1, used when there are more than two annotators (Artstein and Poesio, 2008) . On Questions 2,3 and 4, the \u03b1 values are 0.39, 0.44, and 0.44. While the relatively low values reflect the subjective nature of assessing application-oriented work qualitatively, our three-way annotation process and majority voting reduces the effect of an overly strict or lenient annotator. Overall, our findings indicate that application-oriented papers display some gaps that need to be addressed before the intended application is viable. While this gap often occurs in the evaluation pipeline, we highlight the importance of adequately describing all components or sub-tasks essential for an application in practice.\n\nCase Study\nIn this section, we present a case study of an application from the domain of education. The task involves classifying student utterances into talk moves (Michaels and O'Connor, 2015) , which are strategies provided by the Academically Productive Talk framework (Michaels et al., 2008) , that students and teachers use for maintaining productive and respective discourse in a classroom. We empirically analyze the impact of evaluating this task only on a constrained, artificial environment, as opposed to a more realistic setting.\n\nDataset and Models\nDataset The data consists of conversations among middle school students performing collaborative work in science classrooms, documented in more detail in Southwell et al. (2022) . Groups of 2-4 consenting students are seated at each table, and audio is collected through table-top Yeti Blue microphones. In total, 31 five-minute dialogue sessions are chosen for the talk moves analysis. Like most papers in our survey, we build a high-quality dataset for our application: samples were filtered and transcribed manually (\"human\" transcript) by a team of three annotators, resulting in 2003 student utterances. There are five student talk moves under the APT scheme, including Relating to another student, Asking for more info, Making a Claim, Providing evidence or reasoning, and None. We additionally include the label Not enough context when the annotators cannot make a decision. Examples of all labels can be found in Appendix A. Due to label imbalance, we cluster the labels into 3 categories (NONE, LEARNING COMMUNITY (LC) and OTHER) . Our clustering follows the higher-level grouping of talk moves into Learning Community, Content Knowledge, and Rigorous Thinking as defined in (Resnick et al., 2018) . The dataset is then divided by session into training/dev/test splits for our model.\nModel Following the state-of-the-art model for classifying teacher talk moves (Suresh et al., 2022) , we build our student talk moves model by finetuning the RoBERTa-base (Liu et al., 2019) model for sequence classification. We use the previous N = 6 utterances as the context when predicting the talkmove label for the current utterance, after experimenting with multiple context windows (N) on our development set. As a baseline, we develop a random classifier using the scikit-learn Dummy-Classifier (Pedregosa et al., 2011) transcripts for the current case study, results for this setting can be found in Cao et al. (2023) .\n\nDistribution Shift: Human vs. ASR\nHowever, when deploying our models in the classroom, we do not have access to clean human transcripts, and instead need to work with the outputs of ASR systems. To compare the differences between both, we look at two state-of-the-art ASR systems: Google (Google, 2023) and OpenAI Whisper (Radford et al., 2022) . 2 Table 2 shows the distribution shift between human and ASR transcripts. Because of the noisy small-group classroom setting, some student utterances are difficult to recognize, resulting in imperfect ASR transcriptions with incomplete or empty utterances. This causes the input distributions to vary between human and ASR transcripts. Additionally, when the empty utterances are filtered out, the label distribution also shifts across human and different ASRs. To provide as fair a comparison as possible with the original human transcripts, we create two versions of the ASR data. The first version, denoted using the subscript 'filter' is filtered such that empty utterances are removed, which results in its size varying from the human transcripts. The second version, denoted by the subscript 'all', retains all ASR utterances where the corresponding human transcription is not empty, thus resulting in the same number of utterances as the original human transcripts.\n\nResults\nTo show the performance gap caused by the above distribution shift, we evaluate our model on both human transcriptions and transcriptions from the two ASR systems. For each ASR transcript, we report both performances on their filtered version (Google filter , Whisper filter ) and the all ver- sion (Google all , Whisper all ). We report macro F1 as well as class-wise F1 for all models, as shown in Table 3 . The top rows show performance of the random baseline. Because of the shift in label distributions, as described in Section 3.2, even the input-agnostic random baselines vary for the different versions. Looking at the model performances, we see that overall macro F1 drops by 8.91 points for Whisper all (a 12% drop) and 14.6 points (a 21% drop) for Google all when comparing across transcripts that have the same length.\nWhen considering real-world deployment, the potential for such a dramatic drop in performance should be taken into account by both the designer (including researchers) and the user (such as teachers). However, for similar applications based on classroom discourse analysis, such as classifying teacher talk moves (Suresh et al., 2022) , predicting appropriate next teacher talk moves (Ganesh et al., 2021) or measuring teacher uptake of student ideas (Demszky et al., 2021) , comparisons to ASR transcriptions to illustrate real-world performance are rarely made, and, in many cases, ASR as a component is never mentioned.\n\nDiscussion\nThrough the above survey and case study, we qualitatively and quantitatively examine the gap between task-focused solutions in NLP research, and realistic use cases. We first acknowledge that there has existed a long-standing tradition in NLP to contextualize current research efforts through potential future applications. Looking at task-oriented dialog systems for example, early work such as Deutsch (1975) was motivated by the need to design computational assistants to support humans in mechanical tasks, and discussed the construction of essential components such as discourse processors, despite missing key upstream and downstream components such as ASR or dialog generation. Investigating sub-problems and their respective solutions in environments that are distinct from real-word settings has largely been unavoidable and sometimes even desirable. However, we argue that with the growth of the field and with the progress enabled by LLMs and related advances, we now have the opportunity to examine how closely our experimental setups can reflect our long term goals. Additionally, for papers that are explicitly in the Applications track, which present new applications intended to satisfy a real-world user need, we believe it is even more important to consider the bigger picture, and accurately describe necessary next steps for making the application a reality.\nTo bridge this gap, we propose a few initial recommendations: i) we suggest including a question on the Responsible NLP Checklist 3 pertinent to application-oriented papers, asking if the experimental setup has taken into account the real-world conditions of the application, ii) we recommend that authors describe any potential gaps between their motivation and proposed solution, and if so, state what is lost in the gap (such as ASR), and iii) we call for work to investigate ways to explicitly account for the gap, such as simulating noisy input data in cases where accessing the true distributions is not possible. We invite discussion from the research community on other ways forward.\n\nRelated Work\nOur paper adds to a body of work on meta-analysis of NLP papers and the state of NLP research, particularly from the recently introduced theme tracks at *ACL conferences (Bianchi and Hovy, 2021; Bowman, 2022; Kann et al., 2022) . Similarly to us in that the authors examine evaluation practices, Bowman and Dahl (2021) points out problems with benchmarking, while Rodriguez et al. (2021) proposes ways to improve leaderboards in order to truly track progress. Other papers that critically examine evaluation and leaderboards include Ribeiro et al. (2020); Dodge et al. (2019) and Ethayarajh and Jurafsky (2020) . In contrast, we focus on discrepancies between proposed experimental settings and the stated motivation of research endeavours.\nIn addition, Bowman (2022) discusses that, similar to problematic hype, underclaiming when talking about NLP models comes with risks, and Bianchi and Hovy (2021) highlights multiple concerning trends in NLP research. More broadly, Lipton and Steinhardt (2019) discuss concerns with ML scholarship, and Church (2020) draws attention to downward trends in reviewing quality and how these can potentially be mitigated.\n\nConclusions\nWe investigate the \"gap\" between the motivations of application-focused NLP papers and their actual experimental setting. Through a survey of NLP Applications papers from two NLP conferences, we find that i) necessary components for the application get overlooked when papers focus on subtasks and ii) realistic input sources such as ASR are not being considered in downstream evaluations. We further highlight the severity of the latter issue through a case study on a dialog understanding system intended for classrooms, showing the drop in performance when ASR input, expected in the real-world, is used. While we outline potential strategies to address this issue, we hope our work will spur further discussion about future steps.\n", "hypothesis": " Recent advances in NLP have led to a rise in inter-disciplinary and application-oriented research.  While this demonstrates the growing real-world impact of the field, research papers frequently feature experiments that do not account for the complexities of realistic data and environments.  To explore the extent of this gap, we investigate the relationship between the realworld motivations described in NLP papers and the models and evaluation which comprise the proposed solution.  We first survey papers from the NLP Applications track from ACL 2020 and EMNLP 2020, asking which papers have differences between their stated motivation and their experimental setting, and if so, mention them.  We find that many papers fall short of considering real-world input and output conditions due to adopting simplified modeling or evaluation settings.  As a case study, we then empirically show that the performance of an educational dialog understanding system deteriorates when used in a realistic classroom environment..", "answer": true}
{"title": "FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models", "content": "\nIntroduction\nEffective communication in natural language requires a shared base of knowledge between interlocutors. While this shared knowledge between communicators may be specific to individuals in a shared situation (e.g., Dhruv and Mei know they are sitting at a table in a restaurant), or to individuals with specialized knowledge (they are discussing backpropagation), some types of knowledge are sufficiently generic to be shared by most people in the world (e.g., objects fall when they are dropped). This latter category of commonsense knowledge has for decades been a holy grail of research in artificial intelligence and natural language understanding (McCarthy, 1959) . If machines are to understand (and produce) human language competently, they must at a bare minimum share this commonsense knowledge with humans.\nA question elided by this notion of commonsense knowledge is who counts as \"most people\"? What may appear as universal \"common sense\" to AI researchers in one cultural context may in fact not be so universal. Early efforts to schematize commonsense knowledge as scripts, or stereotyped sequences of events, provide a nice illustration of such unintended cultural biases: the famous \"Restaurant script\" (Schank and Abelson, 1975) prototypically includes a LEAVE TIP event, though tipping is not customary at restaurants in many countries outside the United States.\nMore recent AI research on commonsense knowledge acquisition has relied on crowd sourcing (Regneri et al., 2010; Sap et al., 2019) , corpus statistics (Lin and Pantel, 2001; Van Durme and Schubert, 2008) , and language modeling (Rudinger et al., 2015; Liu et al., 2022) in place of expertcrafted knowledge. However, each of these methods carries the potential to encode cultural bias into data and models for commonsense reasoning, whether through the implicit cultural perspectives of corpus texts, crowd source workers, or AI researchers themselves.\nIn this work, we seek to investigate cultural biases or assumptions present in commonsense reasoning systems. Culture, like commonsense knowledge, is vast. By one definition, 1 culture \"encompasses the social behaviour and norms found in human societies, as well as the knowledge, beliefs, arts, laws, customs, capabilities, and habits of the individuals in these groups.\" From the social sciences, Kendall (2015) defines culture as encom-passing both material as well as non-material aspects, such as beliefs and linguistic practices. To limit the scope of our investigation, however, we focus on a single topic common to all human cultures but widely varying across them: food.\nWe introduce FORK (Food ORiented cultural commonsense Knowledge), a manually-curated set of CommonsenseQA-style (Talmor et al., 2019) test questions for probing culinary cultural biases and assumptions present in commonsense reasoning systems. For the purpose of this work, we say that a commonsense question-answering system is culturally biased if (1) in response to questions with culturally-dependent answers, it exhibits systematic preference for answers consistent with one cultural setting over others; or (2) for questions with explicit cultural contexts, it exhibits systematically higher accuracy for some cultural contexts over others. Figure 1 contains an example of three interrelated test questions in FORK we use to detect cultural bias. For Q1, a model that prefers A1 to A2 exhibits cultural bias in favor of the United States (US) over China. While there exists no tidy mapping between human cultures and countries, in this work, we use countries as a coarse-grained proxy for culture (see: \u00a7 7).\nFORK contains questions pertaining to the food and culinary cultures of the US, China, Japan, and India with questions spanning topics of restaurant tipping, eating utensils, and other culinary customs. We test multiple encoder-based CommonsenseQA models on FORK, demonstrating systematic cultural biases favoring the US over non-US countries.\nTo summarize, our contributions are: 1. FORK: a \"bite-sized\" manually curated test set of 184 CommonsenseQA-style questions which can be used for probing culinary cultural biases and assumptions in commonsense reasoning systems. 2. A systematic evaluation of several encoder based models on FORK to demonstrate systematic cultural assumptions aligned with US over non-US cultures.\n\nDataset\nSince FORK aims to test the culinary cultural specificity of commonsense reasoning models, we choose the format to be along the lines of Commonsense QA (Talmor et al., 2019) . Each question in FORK has two options, only one of which is cor-rect. One of the options pertains to the US culture, while the other to non-US. The questions are manually written by the first author of this paper. The source of content used to formulate the questions is information gathered from Google searches, blog posts, traveler guides, etc. Upon publication, we will release FORK publicly.\nThere are three types of questions in FORK:\n\u2022 Underspecified: The question asked is about culinary customs and practices of no particular country or culture, and we hypothesize that English models will default to a US-centric interpretation in such a case. (See Fig. ) We assign a theme to each question, and questions in FORK span over three distinct culinary themes: eating utensils, tipping and general custom/culture. We also tag each Underspecified question-answer pair, and each implicit and explicit question, with a corresponding country. We present a brief overview of the distribution in Table 1 , and a full demographic distribution in Table 3 . It is important to note that these country labels should not be construed as exclusive of other cultures or countries that may share the relevant attribute. Cultural customs and countries have a many-to-many relation, and our labels are intended to highlight particular points of contrast between the US and other countries. What we measure as US-oriented cultural bias could also be construed as, e.g., Canada-oriented cultural bias only to the extent that US-labeled questions are also applicable to Canada.\nThe questions in FORK can either be a single sentence or a two sentence question. The questions consisting of two sentences help to provide context in case of Implicit and Explicit questions. We also follow a template-based approach where a question about the same theme is asked multiple times, varying only by e.g. the name of a dish, city, country, etc. In total, FORK consists of 184 manually curated questions, with 91.84% of questions pertaining to China, Japan, India and the US, and a small number of additional questions for other countries. Researching and writing questions was a slow manual process, so we chose to focus on producing more questions for fewer countries, to yield more robust results.\n\nValidation Study\nIn order to ensure that the manually curated questions are valid for probing culinary cultural differences, we conduct a validation study with six annotators, two each from China, the USA and India. This pool of annotators comprised of five graduate students, and one professor. We ask the annotators to answer questions in FORK and present statistics in Table 2 . For US, both annotators disagreed on the same question, while for India, the difference was on questions pertaining on tipping. Feedback from annotators observed that the tipping culture varied across the country. For China, the human annotators noted that some practices were untrue for the regions they were from, but true for other regions in China.\nAdditionally, the differences in customs and practices within the same country reiterate our note above that cultural customs and countries have a many-to-many relation. We have used country as a proxy variable for culture because there are no clear distinct boundaries across cultures, and using this proxy boundary allows to probe differences at a US vs non-US level. This highlights a need for future work to investigate cultural differences within a country, based on regional or other demographic dimensions.\n\nExperiments\nWe summarize our experimental set up, models, and the evaluation strategy used in this work.\n\nExperimental Setup\nIn this work, we test seven encoder-based models on FORK and report their performance. We test two variants of BERT (Devlin et al., 2019) : bert-base and bert-large, two variants of RoBERTa (Liu et al., 2019) : roberta-base and roberta-large, DistilBERT (Sanh et al., 2019) , and two variants of DeBERTaV3 (He et al., 2021) : deberta-v3-base and deberta-v3-large. All models are finetuned on the Common-senseQA train fold for 3 epochs. We run a grid search for the hyper-parameters and report them in Appendix A.2.\n\nEvaluation Strategy\nWe evaluate the culinary cultural contingency of the models tested as follows. For the questions tagged as Underspecified, we look at the number of times a \"US\" answer is chosen over a \"non-US\" answer. Here, \"US\" answer refers to an answer that would be appropriate or likely in the US context, and \"non-US\" answer refers to an answer that is more appropriate for a country on context outside the US. For Implicit and Explicit questions, we take a look at the responses for both US and non-US questions, and the percentage accuracy for US vs non-US answers.\nAdditionally, we also compare the number of times a non-US answer is chosen for Underspecified, non-US Implicit and non-US Explicit questions to better determine the bias between US and non-US cultures.\n\nResults and Discussion\nFigure 2a shows the percentage of time when a US answer is chosen over a non-US answer for Underspecified questions. We observe that roberta-large, and bert-large report the top two percentages of choosing a US answer over a non-US answer with values of 90.32% and 83.87% respectively. Fig 2a shows that all models, except for DistilBERT, preferred US answers over non-US answers for a majority of Underspecified questions.\nFigure 2b shows the percentage accuracy for US and non-US Implicit questions. We observe that roberta-large and bert-large report the top two accuracies of 78.57% and 71.42% respectively, when answering US Implicit Questions. In contrast, for non-US Implicit questions, only two models, DistilBert and bert-base cross the 50% accuracy mark, with bert-large having the lowest accuracy of 26.78%.\nFigure 3a shows the percentage accuracy for US and non-US Explicit questions.\nHere, roberta-large, and bert-large report the top two accuracies of 100% and 92.30% respectively when answering US Explicit Questions. In contrast, for non-US Explicit questions, roberta-base reports the best accuracy at 64.28% while roberta-large performs the worst, achieving 22.85%.\nFigure 3b shows the percentage times a non-US answer is chosen for Underspecified, non-US Implicit and non-US Explicit questions. For Underspecified questions, only DistilBert crosses the 50% mark, with 51.62% accuracy. The performance for non-US Implicit and non-US Explicit questions has been discussed above. We report all the model accuracies on FORK in Tables 4 and 5 in Appendix A.2.\nIn addition to aggregating US versus non-US results, we break down accuracy results for China, India, and Japan for Implicit and Explicit questions in Table 6 in Appendix A.2.\nWe observe that (China, Explicit) and (China, Implicit) questions have the lowest average accuracy across models, at 36.57% and 41.80%, respectively. The best performance is reported for (USA, Explicit) at 74.72%.\n\nStatistical Significance of Results\nIn order to make sure that our findings our statistically significant, despite the small number of questions in FORK, we conduct several statistical significance tests.\nFor Underspecified questions, we conduct the binomial test for all 7 model prediction results separately. Only roberta-base and DistilBert report a p-value greater than 0.05 in this setting.\nFor Implicit and Explicit questions, we conduct the chi-squared test on all 7 model prediction results separately to determine the statistical significance of our findings. For Implicit questions, only bert-base and deberta-v3-base report a p-value greater than 0.05 respectively. None of the models reported a p-value greater than 0.05 for Explicit questions.\n\nRelated Works\nA growing body of work aims to detect social biases in NLP models with respect to demographic attributes like gender and race (Rudinger et al., 2018; Zhao et al., 2018; Nangia et al., 2020; Li et al., 2020; Sap et al., 2020) . More recent is the growing attention towards cultural biases in NLP and AI technology at large. Hershcovich et al. (2022) propose a framework that allows one to understand the challenges of cultural diversity in NLP applications. Wikipedia has been shown to embed latent cultural biases (Callahan and Herring, 2011) and Tian et al. (2021) propose a methodology to develop culturally aware models for English, Chinese and Japanese using distributional perspectives on controversial topics from Wikipedia across these languages. Acharya et al. (2020) explore cultural biases, but along the rituals like birth, coming of age, marriage etc. in the US and in India. Chen and Henning (1985) investigate cultural bias in language proficiency tests and identify items of bias against non-native English speakers. To the best of our knowledge, this is the first work to analyze cultural bias in commonsense reasoning from the angle of culinary customs.\n\nConclusion\nWe have introduced FORK, a dataset to measure culinary cultural bias in commonsense models. Confirming our hypothesis, we find that models default to US cultural contexts in underspecified questions, and perform markedly better on implicit and explicit questions about US culture than non-US. A likely source of bias is the English, USproduced texts which models are pretrained on. We believe the results support our hypothesis that English Language Models LMS trained on texts (many of which are produced for a US audience) would reflect US (or broadly Western) cultural assumptions. We hypothesize that the Underspecified setting had the lowest \"accuracy\" for non-US countries because the experimental design forced the model to choose between US and non-US interpretations of the same question. For Implicit and Explicit settings, we speculate that the non-US accuracy is generally higher for Explicit than Implicit because the former made it easier for models to determine the cultural setting.\nPotential mitigation techniques to eliminate such biases may involve better curation of training data, training separate models for different cultural contexts, training models to better recognize cultural cues or ask for clarification in ambiguous settings, among many other possibilities. We believe this is an open research question, and we hope this paper will inspire future research to address it.\nThe topic of cultural bias is vast, and we choose a narrow scope to avoid biting off more than we can chew. Future work will explore strategies for cultural awareness of commonsense models, analysis of cultural assumptions in non-English models, and analysis of other aspects of culture beyond the culinary.\n", "hypothesis": " It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw.  However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative.  We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs. We test several CommonsenseQA systems on FORK, and while we see high performance on questions about non-US cultures, the poor performance of these systems on questions about the US culture highlights systematic cultural biases aligned with non-US cultures.", "answer": false}
{"title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark", "content": "\nIntroduction\nAlthough large language models (LLMs) are powerful tools for generating human-like language, they can also memorize false or outdated associations, limiting their applicability. Model editing techniques promise to solve this problem by correcting non-factual associations. It is important that model edits are highly specific in the sense of not introducing any unwanted associations as a side effect. In this paper, we discuss why the current benchmark for specificity falls short and propose a more challenging, dynamic specificity benchmark to evaluate model editing techniques. Using this benchmark, we evaluate recent model editing techniques and find previously unreported side effects. We highlight the importance of improved specificity benchmarks for the effective and safe use of LLMs subject to model edits.\nFigure 1 : Unintended side effects of model edits and how to measure them. (a) GPT-2-medium is edited using ROME to counter-factually associate the Louvre's location with Rome. However, this results in unintended associations (\"loud facts\") like the association of Obama with Rome, suggesting low specificity of the edit. The edit also significantly increases the maximum logit (shown in brackets), suggesting that the edit is not merely replacing \"Paris\" with \"Rome\" in the desired contexts. (b) Measuring specificity by the fraction of correctly completed test prompts (COUNTERFACT) suggests a high specificity for ROME. Prepending the edit prompt (like \"The Louvre is in Rome.\") to each test prompt (COUNTERFACT+) results in a significant drop in performance. A significant drop in measured specificity can also be observed if the model edit is implemented using constrained fine-tuning (FT-L).\nModel editing updates the parameters of a trained model in order to change its predicted probability distributions without retraining the entire model. This can be used to edit the associations that the model has memorized and hence, improve the accuracy of the model. Fig. 1 shows the example of a counter-factual model edit using ROME (Meng et al., 2022a) where the location of the Louvre is edited to be Rome instead of Paris. We use a counter-factual example since it makes it more evident that the new association is an effect of the model edit instead of the model training. Note that the examples in Fig. 1 are not taken from the COUNTERFACT+ dataset introduced below, but serve to intuitively illustrate the model editing failure modes we are interested in.\nAn important desideratum for model editing is specificity. Specificity captures how well the effect of the model edit is localized; in other words, specificity measures the absence of unintended side effects of model edits. Fig. 1 shows two examples of unintended side effects of ROME model editing, which we collectively call the problem of \"loud facts\". In the first example, mentioning \"Louvre\" (the subject of the model edit) leads the edited model to also complete unrelated test prompts (\"Obama was born in\") with \"Rome\" (the object of the model edit). In the second example, mentioning \"Louvre\" boosts the logits for words semantically related to \"Rome\", like \"Vatican\".\nThe existing specificity benchmark for model editing from the COUNTERFACT dataset (Meng et al., 2022a) suffers from two limitations which can be illustrated using these examples. First, COUNTERFACT does not prompt the model in a way that is likely to surface unwanted side effects. As demonstrated by the examples in Fig. 1 , mentioning the subject of the model edit can drastically change the behavior of the edited model, but the existing benchmark does not detect this. Second, COUNTERFACT considers only the probabilities for the original and edited object token (\"Paris\" and \"Rome\"). As shown by the last example in Fig. 1 , the edited model displays strongly changed logits not only for the original object (\"Paris\") and edit object (\"Rome\") but also for semantically related tokens (\"Vatican\"). Again, this would be overlooked by the current specificity evaluation since it does not consider the entire probability distribution.\nThese limitations mean that side effects of edits may be overlooked and specificity overestimated.\nOur main contributions are:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark.\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution instead of the currently used metrics which focus only on the tokens directly implicated in the model edit. (De Cao et al., 2021) and (Mitchell et al., 2022) . Elazar et al. (2021) introduced ParaRel, a curated dataset of paraphrased prompts and facts. Meng et al. (2022a) use this as a basis for constructing COUNTERFACT, which enables fine-grained measurements of knowledge extraction and editing along multiple dimensions, including specificity.\nKnowledge extraction from LLMs. The assessment of knowledge within language models (LMs) has typically been done by evaluating whether the model is able to predict pieces of knowledge; Petroni et al. (2019 Petroni et al. ( , 2020 ) defined a fill-in-theblank prompt and asked the LM to complete it. Subsequent work has demonstrated that knowledge extraction can be improved by diversifying the prompts (Jiang et al., 2020; Zhong et al., 2021) , or by fine-tuning a model on open-domain textual facts (Roberts et al., 2020) . However, constructing prompts from supervised knowledge extraction data is still prone to learning new knowledge instead of recalling existing knowledge in an LM (Zhong et al., 2021) .\n\nDataset\nWe investigate the specificity of recent model editing techniques using the COUNTERFACT benchmark introduced in (Meng et al., 2022a) . COUNTERFACT is a collection of 21,919 nonfactual statements of the form (subject, relation, object) (s, r, o * ), which have low probabilities prior to the model edit. For each of these non-factual statements, we perform a model edit targeting this specific statement. To measure specificity, we then check whether any other associations in the model change in undesired ways. COUNTERFACT supports this check by providing a set of so-called neighborhood prompts for every non-factual statement used in the model edit. These neighborhood prompts are constructed as follows: For a model edit of the form (s, r, o c ) \u2192 (s, r, o * ) (where o c is the correct object, and o * is the false, counterfactual object), COUNTERFACT samples a set of nearby subjects s n for which (s n , r, o c ) holds true. Neighborhood prompts are then paraphrases of the collected (s n , r).\nSuppose, for example, the edit request was (Darrieux, mother_tongue, French) \u2192 (Darrieux, mother_tongue, English). COUNTERFACT takes the relation and object from the edit request (mother_tongue, French), samples true factual associations for this relation, object pair; e.g., (Montesquieu, mother_tongue, French) and then samples a random paraphrase, such as \"The native language of Montesquieu is\". These neighborhood prompts can be used to inspect whether the model edit has undesired side effects on closely related factual associations. See appendix C for a sample from the COUNTERFACT dataset, including the full set of neighborhood prompts.\nMotivated by the example of loud facts shown in Fig. 1 and by the intuition that unwanted side effects are more likely when the model is primed with the linguistic context of the model edit, we now introduce a dynamic version of COUNTERFACT which we will refer to as COUNTERFACT+. To obtain COUNTERFACT+, we modify the neighborhood prompt by prepending the model edit. For example, if the original prompt is \"The native language of Montesquieu is\" the modified prompt would be \"The mother tongue of Danielle Darrieux is English. The native language of Montesquieu is\". See appendix D for a sample of the modified neighborhood prompts used for COUNTERFACT+.\nTo understand why we call COUNTERFACT+ a dynamic version of COUNTERFACT consider how either dataset would be applied to evaluate the success of a model edit: In both cases, we would need to identify the set N of neighborhood prompts in the dataset that are semantically closest to the intended model edit. But in COUNTERFACT, we would use N as is, whereas in COUNTERFACT+ we would change every prompt in N as a function of the model edit, as described above.\n\nMetrics\nTo evaluate the specificity of a model edit on COUNTERFACT, Meng et al. (2022a,b) use two metrics, called Neighborhood Score and Neighborhood Magnitude. Denoting the post-edit probabilities for the correct token o c and incorrect edit token o * by P * (o c ) and P * (o * ), respectively, these are defined as follows: The Neighborhood Score (NS) is defined as the fraction of neighborhood prompts for which P * (o c ) > P * (o * ). The Neighbourhood Magnitude (NM) is defined as P * (o c ) \u2212 P * (o * ), the difference in probability assigned to the correct token versus the incorrect edit token. High NS and NM indicate that the edit has small unwanted side effects.\nNS and NM, however, do not detect cases where the model edit significantly changes the predicted probability for tokens other than o c and o * , such as in the last example in Fig. 1 . To capture this possibility, we introduce as an additional metric the Kullback-Leibler (KL) divergence of the nexttoken distribution between the edited and unedited model, referred to as Neighborhood KL Divergence (NKL). Abbreviating the next token probability distribution for the unedited and edited models by P (w) and P * (w), respectively, and denoting the token vocabulatory by W, NKL is defined as KL divergence between P (w) and P * (w):\nEQUATION\nA large NKL is undesirable because it implies that the next-token probability distribution for neighborhood prompts has been strongly affected by the model edit.\n\nModels and Model Editing Algorithms\nWe use GPT-2-medium (355M parameters), GPT-2-XL (1.5B) (Radford et al., 2019) , and GPT-J (6B) (Wang and Komatsuzaki, 2021) to evaluate the following model editing methods:\n\u2022 ROME (Rank-One-Model-Editing) performs a rank-one update of a single MLP layer to implement the edit (Meng et al., 2022a) .\n\u2022 MEMIT (Mass-Editing Memory in a Transformer) extends ROME to updates across several MLP layers (Meng et al., 2022b) . Note that we do not test using multiple simultaneous edits.\n\u2022 FT-L: Fine-Tuning with an L \u221e norm constraint (Zhu et al., 2020) , constrained to a single layer, as described in (Meng et al., 2022a) .\nWe use FT-L as a simple baseline.\n\nResults\nFigure 2 shows the results for the ROME, MEMIT, and FT-L editing algorithms applied to the GPT-J (6B) model for different specificity metrics and datasets considered in this work. When evaluated using the Neighborhood Score (Fig. 2 , top), we observe significant drops in specificity for all editing algorithms when going from COUNTERFACT to COUNTERFACT+. Note that specificity measured on the unedited model (GPT-J (6B)) also drops suggesting that there is confounding from the test prompts in COUNTERFACT+, potentially due to recency bias (Zhao et al., 2021) . The drop in specificity is much more pronounced for ROME and MEMIT, compared to FT-L and the unedited model, however. This shows that:\n\u2022 ROME and MEMIT have undesired side effects which are not detected by COUNTERFACT\n\u2022 the improved benchmark COUNTERFACT+ is able to detect these unwanted side effects When evaluating specificity using the newly introduced Neighborhood KL Divergence (Fig. 2 , bottom), we observe a large spike in divergence for both ROME and MEMIT when going from COUNTERFACT to COUNTERFACT+. FT-L shows a much smaller increase in divergence from COUNTERFACT to COUNTERFACT+. Figure 3 in the appendix shows the results on COUNTERFACT and COUNTERFACT+ for the NM metric. (top) NS, the average fraction of correctly completed neighborhood test prompts after the model edit (larger is better). We see that COUNTERFACT+ is a much more challenging specificity benchmark: Success rates NS on it range from 33% to 54% across different editing algorithms while they are close to 80% for COUNTERFACT. (bottom) NKL, the KL divergence of the next-token probability distribution of the edited model from that of the unedited model, averaged over all neighborhood test prompts. A lower value indicates higher specificity (the edited model behaves more like the unedited model).\nResults across all three models are shown in tables 1 to 3. These tables list the mean scores on COUNTERFACT and COUNTERFACT+ for the Neighborhood Score (NS), Neighborhood Magnitude (NM), and Neighborhood KL divergence (NKL), respectively. The brackets give upper and lower bound of 99% confidence intervals obtained via bootstrap resampling (N=1,000 The results from tables 1 to 3 show that the significant drop in specificity when evaluating on\nNKL \u2193 COUNTERFACT COUNTERFACT+ GPT-2 M FT-L 1.4e-05 (1.3, 1.4) 1.4e-05 (1.3, 1.4) ROME\n1.6e-06 (1.4, 1.7) 2.5e-05 (2.5, 2.5)\nGPT-2 XL FT-L 7.2e-06 (6.9, 7.4) 9.5e-06 (9.3, 9.7) ROME 1.5e-06 (1.4, 1.6) 3.3e-05 (3.2, 3.3) MEMIT 2.9e-07 (2.5, 3.4) 9.0e-06 (8.8, 9.1) GPT-J (6B)\nFT-L 3.2e-06 (3.1, 3.4) 5.2e-06 (5.1, 5.3) ROME 3.5e-06 (3.2, 3.8) 1.8e-05 (1.8, 1.9) MEMIT 9.2e-07 (8.0, 10) 9.9e-06 (9.8, 10)\nTable 3 : Neighborhood KL Divergence NKL (\u00b5 & 99% CI) on COUNTERFACT and COUNTERFACT+. Note that the order of magnitude is suppressed for the confidence interval for visual clarity; it is the same as for the mean.\nCOUNTERFACT+ (compared to COUNTERFACT) holds across different model sizes and is not an artefact of using a particular model. Section B in the appendix discusses the scaling of specificity with model size in more detail.\n\nConclusion\nModel editing techniques for auto-regressive transformers exhibit unreported issues related to specificity. Although our fine-tuning baseline, FT-L, exhibits less vulnerability to these issues than ROME and MEMIT, it falls short in competing with them regarding crucial model editing metrics such as robustness to paraphrasing (Meng et al., 2022a,b) . This indicates that model editing still presents numerous complexities that require future attention. Additionally, we revealed that the existing COUNTERFACT benchmark fails to detect the low specificity in ROME and MEMIT. To address this limitation, our primary contributions include:\n\u2022 COUNTERFACT+, a dynamic specificity benchmark, which adapts to the model edit under test, and is more sensitive than the existing benchmark\n\u2022 Neighborhood KL divergence (NKL), a specificity metric based on the full probability distribution as a complement to the currently used metrics which focus only on the tokens directly implicated in the model edit.\n", "hypothesis": " Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during large language model (LLM) training.  However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks.  We extend the existing COUNTERFACT benchmark to include a dynamic component and dub our benchmark COUNTERFACT+.  Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric.  We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity.  Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects..", "answer": true}
{"title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks", "content": "\nIntroduction\nThe omnipresence of large pre-trained language models (Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) has fueled concerns regarding their systematic biases carried over from underlying data into the applications they are used in, resulting in disparate treatment of people with different identities (Sheng et al., 2021; Abid et al., 2021) .\nIn response to such concerns, various benchmarks have been proposed to quantify the amount of social biases in models (Rudinger et al., 2018; Sheng et al., 2019; Li et al., 2020) . These measures are composed of textual datasets built for a specific NLP task (such as question answering) and are accompanied by a metric such as accuracy of prediction which is used as an approximation of the amount of social biases.\nThese bias benchmarks are commonly used by machine learning practitioners to compare the degree of social biases (such as gender-occupation Gender-Occupation Bias \u274c\n\nGender-Occupation Bias \u2705\nThe electrician warned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician warned the homeowner that she might need an extra day to finish rewiring the house. coref coref\n\nWinoGender\nThe electrician cautioned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician cautioned the homeowner that she might need an extra day to finish rewiring the house. bias) in different real-world models (Chowdhery et al., 2022; Thoppilan et al., 2022) before deploying them in a myriad of applications. However, they also inadvertently measure other non-social biases in their datasets. For example, consider the sentence from WINOGENDER in Figure 1 . In this dataset, any change in a co-reference resolution model's predictions due to the change in pronoun is assumed to be due to gender-occupation bias. However, this assumption only holds for a model with near-perfect language understanding with no other biases. This may not often be the case, e.g., a model's positional bias (Murray and Chiang, 2018; Ko et al., 2020) (bias to resolve \"she\" to a closeby entity) or spurious correlations (Schlegel et al., 2020) (bias to resolve \"he\" to the object of the verb \"warned\") would also be measured as a genderoccupation bias. As a result, a slightly different template (e.g., changing the verb to \"cautioned\") could result in completely different bias measurements.\n\nWinoGender-Alternate Construction\nThe goal of this work is to illustrate the extent to which social bias measurements are effected by assumptions that are built into dataset constructions. To that end, we consider several alternate dataset constructions for 2 bias benchmarks WINO-GENDER and BIASNLI. We show that, just by the choice of certain target-bias-irrelevant elements in a dataset, it is possible to discover different degrees of bias for the same model as well as different model rankings 1 . For instance, one experiment on BIASNLI demonstrated that merely negating verbs drastically reduced the measured bias (41.64 \u2192 13.40) on an ELMo-based Decomposable Attention model and even caused a switch in the comparative ranking with RoBERTa. Our findings demonstrate the unreliability of current benchmarks to truly measure social bias in models and suggest caution when considering these measures as the gold truth. We provide a detailed discussion ( \u00a75) of the implications of our findings, relation to experienced harms, suggestions for improving bias benchmarks, and directions for future work.\n\nRelated Work\nA large body of work investigates ways to evaluate biases carried inherently in language models (Bolukbasi et al., 2016; Caliskan et al., 2017; Nadeem et al., 2021) and expressed in specific tasks (Nangia et al., 2020; Kirk et al., 2021; Schramowski et al., 2022; Prabhumoye et al., 2021; Srinivasan and Bisk, 2021; Kirk et al., 2021; Parrish et al., 2021; Baldini et al., 2022; Czarnowska et al., 2021; Dev et al., 2021a; Zhao et al., 2021) . Alongside, there is also growing concern about the measures not relating to experienced harms (Blodgett et al., 2020) , not inclusive in framing (Dev et al., 2021b) , ambiguous about what bias is measured (Blodgett et al., 2021) , not correlated in their findings of bias across intrinsic versus extrinsic techniques (Goldfarb-Tarrant et al., 2021; Cao et al., 2022) , and susceptible to adversarial perturbations (Zhang et al., 2021) and seed word selection (Antoniak and Mimno, 2021) .\nThe concurrent work by (Seshadri et al., 2022 ) discusses the unreliability of quantifying social biases using templates by varying templates in a se-mantic preserving manner. While their findings are consistent with ours, the two works provide complementary experimental observations. Seshadri et al. (2022) study a wider range of tasks, though we focus our experiments on a wider set of models and alternate dataset constructions (with a greater range of syntactic and semantic variability). As a result, we are able to illustrate the effect of the observed variability on ranking large language models according to measured bias for deployment in real world applications.\n\nSocial Bias Measurements and Alternate Constructions\nBias measures in NLP are often quantified through comparative prediction disparities on language datasets that follow existing tasks such as classification (De-Arteaga et al., 2019) or coreference resolution (Rudinger et al., 2018) . As a result, these datasets are central to what eventually gets measured as \"bias\". Not only do they determine the \"amount\" of bias measured but also the \"type\" of bias or stereotype measured. Datasets often vary combinations of gendered pronouns and occupations to evaluate stereotypical associations. It is important to note that these constructs of datasets and their templates, which determine what gets measured, are often arbitrary choices. The sentences could be differently structured, be generated from a different set of seed words, and more. However, we expect that for any faithful bias benchmark, such dataset alterations that are not relevant to social bias should not have a significant impact on the artifact (e.g. gender bias) being measured.\nThus, to evaluate the faithfulness of current benchmarks, we develop alternate dataset constructions through modifications that should not have any effect on the social bias being measured in a dataset. They are minor changes that should not influence models with true language understanding -the implicit assumption made by current bias benchmarks. Any notable observed changes in a model's bias measure due to these modifications would highlight the incorrectness of this assumption. Consequently, this would bring to light the unreliability of current benchmarks to faithfully measure the target bias and disentangle the measurement from measurement of other non-social biases. A non-exhaustive set of such alternate constructions considered in this work are listed below.\nFigure 2 : An instance (\"The engineer informed the client that he would need to make all future payments on time\") from WINOGENDER benchmark modified under various shallow modifications ( \u00a73). To a human eye, such modifications do not necessarily affect the outcome of the given pronoun resolution problem.\nNegations: A basic function in language understanding is to understand the negations of word groups such as action verbs, or adjectives. Altering verbs in particular, such as 'the doctor bought' to 'the doctor did not buy' should typically not affect the inferences made about occupation associations. Synonym substitutions: Another fundamental function of language understanding is the ability to parse the usage of similar words or synonyms used in identical contexts, to derive the same overall meaning of a sentence. For bias measuring datasets, synonymizing non-pivotal words (such as non-identity words like verbs) should not change the outcome of how much bias is measured. Varying length of the text: In typical evaluation datasets, the number of clauses that each sentence is composed of and overall the sentence length are arbitrary experimental choices. Fixing this length is common, especially when such datasets need to be created at scale. If language is understood, adding a neutral phrase without impacting the task-specific semantics should not alter the bias measured. Adding descriptors: Sentences used in real life are structured in complex ways and can have descriptors, such as adjectives about an action, person, or object, without changing the net message expressed by the text. For example, the sentences, \"The doctor bought an apple.\", and \"The doctor bought a red apple.\" do not change any assumptions made about the doctor, or the action of buying an apple. Random samples: Since the sentence constructs of these datasets are not unique, a very simple alternate construction of a dataset is a different subsample of itself. This is because the dataset is scraped or generated with specific assumptions or parameters, such as seed word lists, templates of sentences, and word order. However, neither the sentence constructs or templates, nor the seed word lists typically used are exhaustive or representative of entire categories of words (such as gendered words, emotions, and occupations).\nSee Fig. 2 for example constructions on WINO-GENDER (App. A, B for detailed descriptions).\n\nCase Studies\nWe discuss here the impact of alternate constructions on two task-based measures of bias. 2\n\nCoreference Resolution\nSeveral different bias measures (Rudinger et al., 2018; Zhao et al., 2018; Cao and Daum\u00e9 III, 2021) for coreference resolution work similar to Winograd Schema (Winograd, 1972) where a sentence has two entities and the task is to resolve which entity a specific pronoun or noun refers to. We work here with WINOGENDER (Rudinger et al., 2018) , popularly used to measure biases. It is worth noting that WINOGENDER was originally intended by its authors to merely be a diagnostic tool that checks for bias in a model; the authors note that it may demonstrate the presence of model bias but not prove the absence of the same. Nonetheless, models developed today are indeed tested and compared for social bias on WinoGender, leading to its usage as a comparative standard or benchmark (Chowdhery et al., 2022; Thoppilan et al., 2022) .\nThe metric used to evaluate bias is the percentage of sentence pairs where there is a mismatch in predictions for the male and female gendered pronouns. For instance, in Fig. 2 , if the pronoun \"he\" is linked to \"engineer\" but switches to \"client\" for the pronoun \"she\", that would indicate a genderoccupation bias. Higher the number of mismatches, higher the bias. In particular, note that the metric does not take into account the accuracy of the predictions, but rather only the mismatch between the two pronouns.\nWe experiment with three alternate constructions of the dataset: addition of clauses, addition of adjectives, and synonymizing words in templates.\nEach alternate construction is introduced so as to not affect the overall meaning of the sentence. Experimental Results: We use an end-toend coreference model with SpanBERT embeddings (Lee et al., 2018; Joshi et al., 2020) , Uni-fiedQA (small, base, and large) (Khashabi et al., 2020) QA model, 3 and a long-document coreference model with Longformer encodings (Toshniwal et al., 2021) . Results of evaluating these models on various WINOGENDER constructions is summarized in Fig. 3a . Small changes to the formulation of dataset templates result in sizable changes to computed bias measures compared to the published baseline constructions. For example, a construction involving added adjectives after occupations would have found the UnifiedQA (large) model to have 10% less bias compared to the default constructions. The sensitivity to the dataset constructions can have a drastic effect on ranking models according to their social bias, as Fig. 3a shows. For example, the SpanBERT model is considered to have less bias than UnifiedQA (small) model in the baseline dataset, but would be considered to be more biased if the templates had clauses after the participants or adjectives before the occupation. \n\nNatural Language Inference\nNatural Language Inference (NLI) is the task of determining directional relationships between two sentences (a premise (P) and a hypothesis (H)). Dev et al. (2020) 's measure based on NLI (BIASNLI) evaluates if stereotypical inferences are made by language models. We use their dataset for genderoccupation stereotypes containing approximately 2 million sentence pairs such as P: \"The doctor bought a bagel.\", H: \"The man bought a bagel.\". The expected prediction for each sentence pair in the dataset is neutral, and therefore the bias metric used is the fraction of neutral inferences on dataset -the higher the score, the lower the bias.\nWe experiment with three alternate constructions of the dataset: verb negation, random sampling, and addition of clauses. Note that the alternate constructions do not impact the unbiased label (neutral). Any change in construction (say negating a verb) is applied to both the premise and hypothesis. Refer to App. B for a detailed description. Experimental Results: We use RoBERTa trained on SNLI (RoBERTa-base-SNLI) (Liu et al., 2019) , ELMo-based Decomposable Attention (ELMo-DA) (Parikh et al., 2016) , ALBERT (Lan et al., 2019) , distilled version of the RoBERTa-base model (Sanh et al., 2019) , and RoBERTa-large finetuned on WANLI (Liu et al., 2022) . The bias measured with each model using BIASNLI is recorded in Fig. 3b . The results show how small modifications to the dataset again result in large changes to the bias measured, and also change the bias rankings. For example, adding a negation largely reduces the bias measured (\u25b3 = 28.24) for ELMo-DA, and also results in a switch in the comparative ranking to RoBERTa-base-SNLI. Furthermore, as seen in Fig. 4 , there is a significant overlap in the bias measures of ALBERT, DistilRoBERTa, and ELMo-DA under random sampling, 4 which corresponds to high variability in relative model ordering across different sub-samples of the dataset.\n\nDiscussion and Conclusion\nSocial bias measurements are very sensitive to evaluation methodology. Our empirical evidence sheds light on how the model's non-social biases brought out or masked by alternate constructions can cause bias benchmarks to underestimate or overestimate the social bias in a model. More interestingly, it is important to note that different models respond differently to perturbations. In fact, the same perturbation can result in a higher or lower measured bias depending on the model (as seen in \u00a74.1 and \u00a74.2), which points to how models might parse information (and thus bias) differently.\nWhile current bias measures do play a role in exposing where model errors have a stereotypical connotation, a lack of sentence construction variability or even assumptions made when creating seed word lists can reduce the reliability of the benchmarks, as we see in this work ( \u00a74.2). Even with simple sentences, it is not apparent how to disentangle the biased association of the identity with the verb or the occupation amongst others. This is especially important to note as it highlights that measures can lack concrete definitions of what bi-ased associations they measure. Consequently, the relation between measured bias and experienced harm becomes unclear.\nWe hope that our troubling observations motivates future work that thoroughly investigates how to construct robust benchmarks that faithfully measure the target bias without being affected by model errors and other non-social biases. As suggested by our subsampling experiments (Appendix F), it might be fruitful to encourage both syntactic and semantic diversity in these benchmarks. Bias benchmarks that provide uncertainty measures (instead of a single number) might enable practitioners to better compare models before deploying them. Furthermore, since the opaqueness of large language models makes it challenging to understand how and to what extent a linguistic change will affect the measured bias, explainable models might indeed facilitate better measurement of their social bias. Assuming that we can generate faithful explanations for a model's predictions, an exciting future direction is to explore construction of bias benchmarks which operate on the explanations of the predictions rather than the predictions themselves. Lastly, we also encourage discussions on the complexity of the sentences used in benchmarks and their implications on what gets measured in relation to un-templated, naturally-occurring text (Levy et al., 2021) , as an attempt to ground our measurements in experienced harms.\n", "hypothesis": " How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye). To do so, we empirically simulate various alternative constructions for a given benchmark based on random-sampling modifications that introduce additional social biases.  On two wellknown social bias benchmarks (WINOGENDER and BIASNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias.  We hope these troubling observations motivate more robust measures of social biases..", "answer": false}
{"title": "ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER", "content": "\nIntroduction\nNLP tasks typically require large amounts of highquality labeled data to train sufficiently accurate and useful models. However, in many domains, such as finance and healthcare, access to labeled data is often limited. In these domains, annotating data often requires strong domain expertise and therefore, crowdsourcing of labeled data is infeasible. The cost of annotating data by training an expert workforce is often too high for feasibility.\nA small collection of labeled data also runs the risk of bias creeping in the data and may result in algorithms and models that reflect or even exploit this inherent bias. It also degrades the capability of models to generalize as small datasets are much less likely to have population groups or patterns under-represented (Zhou and Bansal, 2020) . These issues need solutions that can perform well in lowlabeled data regimes while combating data bias. * This work was done during Henry's internship at Amazon Synthetic data generation presents a promising solution to address the issues outlined above (Bayer et al., 2021) . By synthetically generating data, we can augment small labeled datasets to build a training set. Synthetic data generation can also reduce bias in the data by to sufficiently represent all population groups. In particular, the field of controlled synthetic text generation has received increased attention in recent years. Controlled text generation provides the ability to control for traits such as tone, sentiment, and topic in the generation of a language model (Wang and Wan, 2018; Zeng et al., 2021) . This lends controlled synthetic text generation as a useful technique for augmenting small or privacysensitive datasets. However, there has been limited work on the topic of entity-controlled synthetic text generation, i.e., the task of generating coherent text while controlling for the named entities that appear in the generation (Dong et al., 2021) .\nIn this paper, we study the problem of entitycontrolled synthetic text generation. We propose, ECG-QALM, a Entity Controlled Text Generation with Contextual Question Answering based pretrained Language Model, that can produce coherent text which contains specific entity tokens, generated in an order provided by the user. We are motivated by the need to synthetically augment datasets to improve performance on downstream NER tasks (Zhou et al., 2022) . ECG-QALM provides multiple advantages. It is more sample efficient than other methods, as the model is trained on each block of each sample, unlike just seeing a sample in whole for Seq2Seq models like Dong et al. (2021) ; b) ECG-QALM sees a block of text which is relatively smaller than whole sample, prompted on entity to be inserted and conditioned on previous generation allowing for generation of more coherent text as demonstrated by generation metrics like perplexity versus SOTA Seq2Seq baselines; and c) unlike prior Seq2Seq methods like RNN (Dong et al., 2021) or using a vanilla GPT, where length of text generated is limited to 512/1024, ECG-QALM can generate as many blocks of (maximum) length 1024, as the number of entities to be inserted.\nWe make the following contributions: 1) we propose a novel approach using pre-trained language models to generate entity-controlled blocks of text, which can be chained to produce full synthetic text samples; 2) our method is capable of generating texts semantically closest to the training data while being distinct; and, 3) evaluations on publicly available datasets on NER task show a significant improvement in data augmentation performance for low-labeled data regimes, even by just using a purely synthetic data.\n\nRelated Work\nControlled text generation These methods control a certain aspect of generated text (Yang and Klein, 2021; Chan et al., 2020; Pascual et al., 2021) like sentiment (Wang and Wan, 2018) or concepts (Zeng et al., 2021) . These methods focus a macro level aspect of the generated text while we want to control a fine grained text generation.\n\nData-to-text generation\nThe idea is to convert a given set of words or structured data from tables into a piece of text. Most popular problem is table summary generation, also called table-to-text (Liu et al., 2018; Parikh et al., 2020; Chen et al., 2021) or keyword to text methods (Pascual et al., 2021; Tan et al., 2021) . While similar, the key difference is they have a fixed set of entities in every generation.\nEntity-controlled generation Works in the intent detection and slot filing literature for conversational systems have attempted entity-controlled generation (Jolly et al., 2020) . Recently, Rosenbaum et al. (2022) , attempted to use a pre-trained language model with an instruction prompt that uses examples as input in the prompt for model to generate synthetic text. Note, these models have been built in context of conversational systems and hence, have a goal to respond to a specific query which generating the output text, unlike our task of generating text with specified input entities. Dong et al. (2021) proposed a solution to this exact problem for generating text with given entity types and their mentions, using a RNN based Seq2Seq architecture. Our method uses a pretrained language model with a block-by-block generation mechanism, producing superior text over theirs. They do not evaluate on a downstream task like NER, unlike our work.\nData Augmentation for Named Entity Recognition These methods rely on substitution of entities in a given example with entity of same type to create examples. (Dai and Adel, 2020) proposed a simple random replacement which was further enhanced using language modeling to exploit context (Zhou et al., 2022; Ding et al., 2020) . While these methods need seed text to generate each example, our method only needs entity tags to generate an example.\n\nMethodology\nWe use a contextual question and answering based training approach to generate blocks of text with desired entity tags. This approach is able to reliably generate augmented text samples while retaining sentence coherence. Our method generates blocks of text delimited by entities to be inserted, and chaining these generated blocks to create full text samples. We use a GPT-2 language model in place of a recurrent network used by Dong et al. (2021) to take advantage of using pre-trained large language models. The intuition being that using a pre-trained model helps in increasing diversity of the generated text.\n\nTraining\nWe first preprocess real world training text samples into blocks, whereby each block is composed of non-entity tokens and ends with an entity tag as shown in Figure 1 . Every text sample is then decomposed into these blocks of text. An end of text token is added at the end. Therefore, a full text sample generation consists of chaining generated blocks until a block with an <ENDTEXT> token appears. Side benefit of creating blocks is increased number of (shorter, manageable) training examples that are easier to learn on, unlike existing methods that input entire text at once.\nAfter decomposing text samples into such blocks, we arrange blocks into the question and answering format, which consists of three segments: context, question and answer. The context segment provides preceding text blocks, the question segment prompts the model for the desired token, and the answer block is the desired generation.\nContext section consists of all blocks in the text sample, preceding the current block. This was motivated by the need for the model to be aware of the context for successive generation. The generation of each block must be a continuation of preceding blocks to maintain sentence level coherence.\nQuestion segment prompts for the desired entity to appear in the next block. Therefore, through this prompting mechanism we control the desired entity tag to be generated. Following the \"Question: \" tag is a single token representing the desired entity.\nAnswer segment contains the desired text block to be generated. The final token in this block will therefore be the same token as in the question segment. With this three segment format, every block from the corpus represents a training sample for the language model.\n\nGeneration during Inference\nAt inference time, ECG-QALM generates text conditioned on two segments of context and question. To generate the first block, the context segment is blank, while the question segment contains the desired token to be generated in the first block. The model then completes the answer segment with a generated block, which is inserted into the context segment for the next block generation. A full text sample then is produced by concatenating blocks until an <ENDTEXT> token. If the desired entity tag does not appear in the generated block, we re-generate the block text until the tag appears.\n\nMetrics\nTo evaluate the generated text, we quantitatively measure the quality of generation and performance on NER task. We use three generation quality metrics used in prior literature (Dong et al., 2021) 1 . Perplexity measures the 'surprisingness' of the generated text evaluated on a GPT model (Radford et al., 2018) . Distinctness (Li et al., 2015) measures the uniqueness of tri-grams in the corpus. Rouge-L (Lin, 2004) \n\nExperiments\nWe evaluate our model on two datasets described in Table 1 . We compare with the following baselines:\nGold Data: Refers to the real world training data. ECG-LM: This is our own baseline Seq2Seq method, which generates the entire text given a list of entities, without a block-by-block generation. Note: Generated text length in DACA, MELM, EntInj, and ECG-LM is limited by number of tokens model can generate (512/1024) at once; ECG-QALM is not, as it chains the generated blocks.\n\nExperimental Settings\nWe use the training, validation, and testing data splits provided publicly in the datasets on Huggingface 2 . We use the training dataset (and its mentioned subsets) for training both the text generation models as well as training the downstream NER model. We use BERT (Devlin et al., 2018) for downstream NER task. NER results are reported on the complete test set for both the datasets.\nWe use an instance of OpenAI's GPT-2 (Radford et al., 2019) for ECG-QALM. Our model is trained with the Adam optimizer on a learning rate of 1e-3, one hundred warm-up steps, and an epsilon of 1e-8. The default CrossEntropy loss function is used, and the model is trained for up to 100 epochs. For the NER task, we train the BERT model for upto 10 epochs with a learning rate of 2e-3. These parameters were set based on hyper-parameter tuning on the validation set. During generation, we 2 https://huggingface.co/ exactly mimic the entity distribution of the gold data. We can also change the entity distribution to boost under-represented entities as shown in Appendix A.1.\n\nGeneration Quality\nGeneration quality results are shown in Table 2 . We clearly observe that our method is lower on all three metrics against the original dataset, which is expected as ours is synthetically generated data. Our method works better than the only other text generation baseline EntInj (Dong et al., 2021) on all three metrics across the two datasets. Particularly, for the BC5CDR dataset, we note EntInj tends to generate repetitive text. The correct benchmark are the substitution based baselines as our method inserts the entities in the same fashion. We observe for the substitution based baselines, distinctness is highest, as expected as we have swapped commonly occurring trigram entities, while the perplexity is worse than ECG-QALM. This shows that swapping affects the lexical meaning of the text, even when done intelligently in DACA/MELM. While we also insert randomly chosen entities in our generated text, these results indicate that our method generates coherent generic text where semantic meaning of the type of the entity is preserved.\nOur generated data has the lowest Rouge-L scores. Hence, our generated data is not simply memorizing the training data, it is quite different than the gold data. We can see the huge gap with the substitution methods; while the data from substitution methods is practically same as the gold data, ours is distinct. Based on these metrics, we can claim that generated text is semantically closest to the original corpus, while being distinct.\n\nNamed Entity Recognition Task\nWe took two subsets of the JNLPBA and BC5CDR datasets: 1% and 10% as we found the performance on datasets was already saturated at their full sizes as number of samples was enough. Hence, we present the results on first 1% and 10% examples of training splits to show the comparisons. We present two settings: (a) w/o augmentation with gold data; and (b) augmentation with gold data. Generated text for all methods is same size as gold data. Note, no changes were made to test/val sets.\nTable 3 shows the results for the two subsets of the two datasets. From the results five things stand out: 1) Augmenting gold data with our synthetically generated data always out-performs a model trained with the gold data; 2) using only synthetically generated data is comparable in performance to the gold data in medium labeled setting (10%) ; 3) our synthetically generated data outperforms gold data in low labeled data setting (1%) subsets; 4) our synthetically generated data gives better performance vs all baseline methods; and 5) our novel block-by-block generation approach significantly improves over a vanilla GPT-2 (ECG-LM) model.\nOur finding that synthetically generated data can get us a comparable performance to gold data has an application in making the models trained for downstream tasks like NER, privacy preserving, as they do not have to be trained on the real data. This finding can be attributed to zero/few-shot capabilities of large language models (Wei et al., 2021) . Hence, the capability to produce texts that can generalize better on unseen test set while other models are only able to capture subset of test set distribution reflected in the training gold dataset. Our results show our method of generation can be quite effective as a data augmentation method in a low labeled data regime.\n\nGenerating more text in low resource\nPreviously, we only showed the results by generating synthetic data of same size as the gold data. We perform an experiment to see if there is further improvement in the performance as we add more generated data with the JNLPBA (1%) dataset. We observe that F 1 score keeps improving going up to 0.70 vs gold data at 0.31 in Figure 2 . Note, we only use the entity mentions found in the JNLPBA (1%) dataset to fill in the entity tags in the generated text. This is remarkable considering that 10x real data for JNLPBA (10%) has a F 1 score of 0.72. This is a further evidence that our model is able to generate text that is similar to real data.\n\nConclusion\nSynthetic data generation is a promising approach to train large language models in order to deal with scarcity of labeled data. In this work, we study the problem of conditional text generation where the conditions are provided as a list of entities that must appear in the text in a manner desired by the user. We propose ECG-QALM that can generate blocks of text conditioned on the desired entities. We test our generation system on generation quality metrics and NER task. Evaluations show that our method outperforms baselines in terms of both generation quality and NER performance. Our blockby-block generation provides significant gains over using a fine-tuned vanilla LLM for generation.\n", "hypothesis": " Named Entity Recognition (NER) state-ofthe-art methods requires high-quality labeled datasets.  Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers.  Generating synthetic data to train models is a promising solution to mitigate these problems.  We propose ECG-QALM, a contextual question and answering approach using pre-trained language models to synthetically generate entitycontrolled text.  Generated text is then used to augment small labeled datasets for downstream NER tasks.  We evaluate our method on two publicly available datasets.  We find ECG-QALM is capable of producing full text samples with desired entities appearing in a controllable way, while retaining sentence coherence closest to the real world data.  Evaluations on NER tasks show significant improvements (75% -140%) in low-labeled data regimes..", "answer": true}
{"title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning", "content": "\nIntroduction\nDeveloping comprehensive evaluation frameworks (Deng et al., 2021; Yuan et al., 2021; Zhong et al., 2022) that can evaluate multiple humaninterpretable dimensions, such as factual consistency (Kryscinski et al., 2020; Wang et al., 2020) and coherence (Dziri et al., 2019; Huang et al., 2020) , is important for the advancement of Natural Language Generation (NLG). However, similaritybased metrics (Papineni et al., 2002; Lin, 2004; Sellam et al., 2020; Zhao et al., 2019; Zhang et al., 2020) still dominate NLG evaluation in practice. Compared to them, desired multi-dimensional evaluators do not require reference texts for evaluation; and they can easily extend to new explainable evaluation dimensions. Recently, Zhong et al. (2022) developed a unified evaluation framework that can Figure 1 : Our prompt design to evaluate the consistency of the summary in red, illustrated using two in-context examples (in blue). To evaluate other aspects, we remove the source text or replace it with a reference. generalize to multiple dimensions and text generation tasks. However, it relies on the construction of synthetic and auxiliary data for the finetuning of a pre-trained language model, requiring in-depth knowledge and significant engineering effort for each dimension. Furthermore, the inclusion of new dimensions requires (continued) training of the model, and might affect the performance on other dimensions in unforeseen ways.\nIn this work, we propose to use in-context learning (Brown et al., 2020) with large language models (LLMs) -a commonly used method to perform many tasks by utilizing only a few input-output examples -to perform multi-dimensional text evaluation in a unified fashion. Compared to pre-trained evaluators that need specialized supervised training for each dimension, our In-Context learning-based Evaluator (ICE) framework is:\n\u2022 Learning-free. It does not require supervised fine-tuning on large annotated (synthetic) training data, requiring only a handful of samples at inference time. \u2022 Extensible. To evaluate new dimensions, it does not rely on large amounts of human judgments or the construction of new synthetic data, using only a natural language prompt consisting of a small number of example pairs to ascertain the properties associated with a given quality aspect.\nIn this paper, using text summarization as a test bed, we show that with a simple prompt design, ICE is competitive with state-of-the-art trained evaluators on multi-dimensional evaluation of modelproduced summaries, establishing a new state-ofthe-art on dimensions such as relevance and factual consistency. To study the robustness of the evaluator to the selection of in-context examples, we analyze the factors that affect the performance of ICE, such as the number of in-context examples and sampling procedures when picking in-context examples from a set of candidates. We find ICE to be robust to the selection of in-context examples and observe a slight improvement in performance as the number of examples is increased. Finally, in light of the recent work (Goyal et al., 2022) that points to the misalignment of existing evaluation metrics with human preference in evaluating zeroshot summaries generated by LLMs such as GPT-3 (Brown et al., 2020) , we study the effectiveness of ICE in evaluating zero-shot summaries generated by GPT-3. We find that ICE evaluations agree closely with human judgments on such summaries.\n\nProblem Statement\nGiven a sequence x that is input to an NLG system and a system-generated output sequence y, an evaluation framework outputs a score s that captures the quality of y, either with or without the help of a human-generated reference output r. 1 In case of multi-dimensional evaluation where we are interested in assessing y over d quality metrics, we instead get a vector S = (s 1 , s 2 , ..., s d ) over diverse dimensions (e.g., coherence, fluency). Depending on the dimension, there is sometimes a need to condition an evaluation on x (such as to evaluate consistency in summarization). We evaluate our method over four dimensions:\n\u2022 Consistency: The factual correctness of a summary given the source text. \u2022 Relevance: The property of capturing salient information from the source. \u2022 Fluency: A measure of the quality of the individual sentences in the summary. \u2022 Coherence: A measure of the quality, organization, and structure of sentences in the summary.\n1 Specifically for summarization, most learned frameworks evaluate relevance through reference-based evaluation.\n\nPrompt Design & Score Extraction\nICE relies on an LLM (we use the text-davinci-003 model of GPT-3) to make predictions. It takes in a prompt that consists of a small number of in-context examples, each of which consists of generated text and its corresponding quality score as a numeric string. The prompt ends with a test example, for which the model predicts a score (Figure 1 ).\nThe input contains the model-generated text (summary), in addition to which it might contain additional information such as the source text or references, depending on the dimension. To evaluate fluency and coherence, our prompts use in-context examples consisting of generated summaries and corresponding scores. For consistency and relevance, we use the source text and a reference summary respectively, in addition to the generated summary. We pass this prompt to a GPT-3 model, with sampling temperature set to 0 to elicit deterministic responses. We parse the model response-decoded numeric string-as the dimension score.\n\nSelection of In-context Examples\nBy default, we use 4 in-context examples in our prompts, as this is the largest number that fits within the context window of GPT-3. We experiment with two sampling procedures (Appendix B) to obtain 4 examples from a pool of examples:\n1. Uniform Random Sampling. We randomly select 4 summaries from the pool of examples. This causes the examples to follow the same distribution as the example pool. 2. Stratified Sampling. We bucket the range of scores, i.e. [0, 1], into 4 equal partitions and randomly sample one summary from each one.\nThis causes examples to be representative of the range of scores in the example pool.\nWe avoid using synthetically generated data (Kryscinski et al., 2020; Zhong et al., 2022) since the kind of errors made by generation models is often different from the errors present in the negative examples in these datasets (Goyal and Durrett, 2021) . We instead elect to use (a few) human evaluations of model-generated text in order to make the in-context examples as representative of real errors as possible. We do this by splitting the meta-evaluation dataset and using a partition as an in-context example pool, as described in Section 3.1. \n\nDatasets & Baselines\nWe use the SummEval dataset (Fabbri et al., 2020) 2 to meta-evaluate our evaluation framework. Sum-mEval collects human evaluation annotations for 16 summarization systems on 100 articles sampled from the CNN/DailyMail corpus, for a total of 1600 summary-level annotations. Each summary is evaluated on four dimensions described in Section 2.2.\nTo get a pool of in-context examples, we keep aside a small subset (64 examples) of the Sum-mEval dataset to pick in-context examples from, and use the rest (1536 examples) as the test set for meta-evaluation (evaluating the baselines on this same test set). Further details are in Appendix A.\nWe compare ICE to the following state-of-theart multi-dimensional evaluators: (1) CTC (Deng et al., 2021) uses information alignment between generated outputs and references or inputs; (2) BARTScore (Yuan et al., 2021) uses the conditional probability of a sequence given inputs or references; and (3) UniEval (Zhong et al., 2022) uses a question-answering framework (e.g. \"Is this a coherent summary?\") to calculate metrics.\nFollowing Liu et al. (2021) ; Zhong et al. ( 2022), we assess performance by computing summarylevel Spearman and Kendall-Tau correlations between predicted scores and human judgements.\n\nResults\nAs illustrated in Table 1 , ICE is competitive with fine-tuned baselines despite not requiring any finetuning. It achieves state-of-the-art correlation with human judgments for relevance and consistency. We perform pairwise significance tests and observe that ICE (uniform sampling) does better than UniEval on consistency and relevance on Kendall's Tau with a significance level of 0.05 (Appendix E). Additionally, the uniform sampling variant of ICE outperforms BARTScore (which also does not require finetuning) across dimensions.\nBetween the two sampling procedures for ICE, we observe that stratified sampling works marginally better for all dimensions other than consistency. Since summaries in the SummEval dataset have perfect or near-perfect human scores for consistency (Figure 2 ), uniform sampling causes in-context examples to also have nearperfect scores. This appears useful for the model to calibrate its scoring when evaluating consistency, leading to better performance. We explore this in greater detail in \u00a74.1. While the same reasoning could hold for fluency, we observe both here and in \u00a74.3 that fluency scores are quite stable. Given that fluency is an easier aspect to evaluate, this stability could be a result of the model possessing a strong notion about fluency from pre-training time that is not modified significantly as the distribution of in-context examples changes (Reynolds and McDonell, 2021) . Finally, we observe that the performance for coherence and relevance are similar regardless of the sampling procedure. This is because scores for these aspects are spread out in the dataset, which makes uniform and stratified sampling return similar in-context examples.\n\nAnalysis\nIn this section, we analyse the effects of our prompt engineering choices. The comparison between sampling procedures in Section 4.1 is performed on the entire test set but the experiments in Sections 4. domain regardless of the true distribution. This forces predictions towards a centered distribution, which can cause the performance drop we observe in Table 1 when evaluating consistency using stratified sampling. Uniform sampling, on the other hand, selects examples that represent the true distribution, making model predictions more closely reflect the true distribution.\nA drawback of uniform sampling is sub-optimal calibration in low-probability regions of the true distribution. For instance, if uniform sampling is used to evaluate consistency, the model might not see in-context examples with (say) scores less than 0.3 (Figure 2 ). This can affect output calibration in that region. Nonetheless, we suggest using uniform sampling in general. It is more stable and its prediction distribution closely follows the true distribution. For dimensions where it underperforms stratified sampling, the margins are less significant. Finally, even when ICE (uniform sampling) scores are calibrated differently from human scores, they still rank summary-quality correctly, insofar as our main results (Table 1) \n\nEffect of Selection of In-context Examples\nIn order to determine whether performance is robust to the choice of in-context examples, we evaluate our test set using three different random sets of in-context examples. We observe in Figure 3 that for a given dimension, the maximum variation across three seeds is about 7 points, suggesting reasonably stable performance across the choice of in-context examples.\n\nEffect of Number of In-context Examples\nWe evaluate our test set using different numbers of in-context examples (Figure 4 ). We observe that only for relevance and coherence does performance show improvement as we increase the number of examples. One reason for this could be the distribution of scores for a given dimension in the test set (Figure 2 ). Concretely, consistency and fluency mostly have near-perfect scores and therefore do not benefit from more samples while the scores for coherence and relevance are spread out and therefore more samples allow representation over the whole range of scores.\nAnother observation is that even for coherence and relevance, performance with a single incontext example reaches near that achieved by some of the weaker fine-tuned baselines in Table 1 . This suggests that the model possesses the notion of the evaluation task from pre-training itself, which is in line with recent work (Reynolds and McDonell, 2021; Min et al., 2022) that suggests that demonstrations help extract this knowledge.\nFinally, we note that calibration can potentially be improved by increasing the number of examples. For instance, we observed that the four incontext examples that the uniform sampling procedure chose for coherence in Figure 2 had scores that fall between 0.7 and 1.0. This concentrates the prediction distribution in that range. The probability of such an event will reduce as the number of examples is increased further.\n\nUsing ICE to Evaluate Zero-Shot Prompting Models\nRecent work by Goyal et al. (2022) showed that standard reference-based and reference-free metrics are not reliable in evaluating zero-shot summaries written by models such as GPT-3. Through a human study comparing summaries from three systems-GPT-3, BRIO, and T0-they observed that while humans prefer GPT-3 summaries, automatic evaluators consistently score GPT-3 summaries lower than summaries from other models.\nWe study the efficacy of ICE in evaluating zeroshot summaries written by GPT-3 at a dimension level. We use the set of 500 CNN articles from Goyal et al. (2022) , with summaries from GPT-3, BRIO, and T0 for each article. We sample 100 of these articles and have three annotators rate summaries for each of the dimensions defined in Section 2.2 on a scale of {1, 2, 3, 4, 5}. We use ICE, ROUGE, and BARTScore (all of which do not require training data) to evaluate the summaries and present system-level results in Table 2 .\nWe observe that ICE agrees with human judgments for each dimension and overall preferences while existing reference-based and reference-free metrics such as ROUGE and BARTScore 3 consistently rate GPT-3 summaries low. Goyal et al. (2022) suggest that most existing evaluation metrics reward summaries that imitate references, while GPT-3 summaries are zero-shot and not trained to imitate human-written references, which is likely why they are penalized by most existing evaluators. However, since ICE is not based on reference similarity (except when evaluating relevance) and is also not trained with reference summaries, it is able to better evaluate GPT-3 summaries and agrees with human preferences.\n\nConclusion\nWe show that in-context learning can be used for NLG evaluation as an alternative to fine-tuned evaluation metrics. Using a small number of examples, in-context learning evaluators can reach or exceed the state-of-the-art on multi-dimensional evaluation and that this is robust to the choice of in-context examples. Finally, we show that in-context learning evaluators align well with human judgements when evaluating summaries written by GPT-3.\n", "hypothesis": " Evaluation of natural language generation (NLG) is complex and multi-dimensional.  Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on small manually or synthetically generated datasets.  In this paper, we study the lack of efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets.  Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency.  We then analyze the effects of factors such as the selection and number of incontext examples on performance.  Finally, we study the efficacy of in-context learningbased evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.  Our code is available at https: //github.com/JainSameer06/ICE.", "answer": false}
{"title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network", "content": "\nIntroduction\nLarge-scale pre-trained language models (PLMs) have demonstrated substantial achievements in natural language processing (NLP) recently (Qiu et al., 2020; Han et al., 2022) . Generally, fine-tuning PLMs with the task-specific training data can get significant improvements compared to training a model from scratch (Devlin et al., 2019; Lample and Conneau, 2019; Radford et al., 2018; Ouyang et al., 2022) . Fine-tuning aims at transforming the representation from PLMs in the universal space to the target space, thereby enabling the model to generalize to a wider range of samples (Pan and Yang, 2010; Liu et al., 2021; Wei et al., 2021) .\nHowever, the generalization capability of PLMs is largely affected by the task-specific data when using fine-tuning to further train the model (Patel et al., 2022) . As noted by Wu et al. (2022) , fine-tuning is susceptible to memorizing the training data when the capacity of the PLM exceeds that of the downstream task data. Furthermore, the advantage of PLMs over random initialization is lost when the coverage of task-specific data is low, resulting in a large gap between training and test data (Zoph et al., 2020; He et al., 2019) . For instance, in domain generalization, PLMs fine-tuned with in-domain training sets often fail to perform well on out-of-domain test sets, even when data from the test sets is used for pre-training (Yang et al., 2022) . Therefore, a crucial challenge for unlocking the potential of PLMs is how to learn a complete and accurate mapping from the universal space to the target space with limited training data.\nPrevious studies have presented some promising approaches to address this problem. Fang et al. (2020) proposed a self-teaching method to use a fine-tuned PLM to get soft labels of the unlabeled data and train another PLM by these synthetic data, which brings considerable improvements in the cross-lingual transfer scenario. Li and Zhang (2021) presented regularized self-labeling to correct mislabeled data points and reweight less confident data points to regularize PLMs. Li et al. (2022) proposed an ensemble learning method for domain generalization, which can dynamically dispatch proper PLMs to predict each test sample. Wu et al. (2022) proposed a noisy tuning method to add matrix-wise perturbation to different parameter matrices to overcome the outfitting problem of finetuning, which indirectly improve the generalization ability of PLMs. Lu et al. (2022) presented stochastic weight averaging to improve generalization by encouraging convergence of the model to a flatter minimum. Furthermore, compared to fine-tuning, in-context learning which doesn't tune the parameter of PLMs, we leave this to future work (Li and Liang, 2021; Brown et al., 2020; Vu et al., 2022) .\nIn this paper, we propose a novel fine-tuning framework (named G-Tuning) to preserve the generalization capability of PLMs when training with task-specific data. We adopt parameter efficient fine-tuning (PEFT) as our backbone, which only tunes a lightweight adapter network connected behind PLMs (Houlsby et al., 2019) 1 , and incorporate a generative adversarial network (Goodfellow et al., 2020; Arjovsky et al., 2017; Gulrajani et al., 2017) into it to learn the representation mapping. Specifically, we first train a discriminator to aim of discriminating the representation that is not mapped correctly from the target space. Then, besides predicting the ground-truth label, the model is seen as a generator requested to generate the representation hard to be discriminated. We conduct experiments on the GLUE and two more challenging scenarios, i.e., domain and language generalizations. Experimental results show that G-Tuning can improve generalization capability by effectively mapping the universal representation to the target space.\n\nPreliminaries\nPre-trained Language Model. Given a largescale unlabeled data-set M , the widely-used training function of PLMs is\nL P (\u03b8) = \u2212E x u \u223cM [log P (x u |m(x u ); \u03b8)], (1)\nwhere x u is an input sequence and m(\u2022) is a perturbation function, which masks tokens in the x u by a certain rule (Devlin et al., 2019; Radford et al., 2018) . The \u03b8 is the parameter set of the PLM, which generally adopts the Transformer structure (Vaswani et al., 2017) .\nParameter Efficient Fine-tuning. Given a training set B from a downstream task, we can summarize the loss function of PEFT (Houlsby et al., 2019; He et al., 2022a) as:\nL T (\u03d5) = \u2212E {x t ,y t }\u223cB [log P (y t |x t ; \u03b8, \u03d5)], (2)\nwhere x t is the input and y t is the corresponding label. PEFT only trains an adapter parameterized by \u03d5, and the parameter \u03b8 of the PLM is fixed.\n\nThe Proposed G-Tuning\nBefore elaborating on the G-Tuning, we first define the composition of the PLM and the adapter 1 Compared to fine-tuning the whole model, PEFT is efficient and stable in our experiments (see Appendix A). as f (\u2022). Given an input, the output of f (\u2022) is a vector or the mean pooling of a matrix according to different tasks. Inspired by the Wasserstein GAN (WGAN) (Arjovsky et al., 2017; Gulrajani et al., 2017) , we train a discriminator D(\u2022) as follows:\nL D = \u2212 E x t \u223cB,x u \u223cM [D(f (x t )) \u2212 D(f (x u ))] + \u03bbE z\u223cN [||\u2207D(z)|| 2 \u2212 1] 2 .\n(3)\nHere, the second term is gradient penalty, which is used to smooth the weight of D(\u2022) (Gulrajani et al., 2017) . The coefficient \u03bb is set as 10 and the latent representation z is computed by:\nz = \u03f5f (x u ) + (1 \u2212 \u03f5)f (x t ), \u03f5 \u223c N (0, 1). (4)\nWe consider the representation from f (x t ) obeys the real distribution, and from f (x u ) obeys the generated distribution. The aim of D(\u2022) is to identify the representation that did not correctly map from the universal space to the target space. We think of f (\u2022) as the generator which can be optimized by the following loss function:\nL G (\u03d5) = \u2212E x u \u223cM [D(f (x u ))].\n(5)\nFinally, different from the original WGAN, we combine the loss functions from Eq. 2 and Eq. 5 in a multi-task learning paradigm (Sener and Koltun, 2018) to optimize the adapter network:\nEQUATION\nwhere the coefficient \u03b1 and \u03b2 are used to control the loss function, which we set as 1 and 0.5, respectively. 2 Here, we utilize L G to learn the representation mapping. To avoid the deviation of the target space, we further use L T to keep it consistent. An illustration of the G-Tuning is shown in Figure 1 . 3 Experiments\n\nImplementation Detail\nData-set. We first experiment on the GLUE benchmark (Wang et al., 2018) . In consideration of the labels of the test sets are not released, we report results on the validation sets. Further, we report the result of MNLI-matched for the MNLI task and do not evaluate the WNLI task due to problems with the data. Then, we conduct experiments in two more challenging scenarios: domain generalization and language generalization. In domain generalization, following Yang et al. (2022) , we reorganize the date set of each task in GLUE (named GLUE ood ) and use the same metric to evaluate the model. In language generalization, we adopt XTREME benchmark (Hu et al., 2020) to evaluate our approach, in which we use English training data to tune the model and transfer it to other languages. In addition, more details of GLUE and XTREME benchmarks can refer to their papers (Wang et al., 2018; Hu et al., 2020) . The evaluation metric of all tasks and the data statistic of the GLUE ood data-set are shown in Appendix C.\nSetting. Depending on the type of task, we use the large setting of RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020) as the foundation model. We set the batch size as 32 and the number of gradient accumulations as 2. The training epoch of all models is 10. We compose the data-set M by randomly sample the Similar to previous work (Xu et al., 2022; Zaken et al., 2022) , we search learning rate from {5e-6,1e-5,5e-5,1e-4,5e-4}. The optimization frequency of the discriminator is {3,5,7,9} times than the generator. We use Adam (Kingma and Ba, 2014) as the optimizer for our method. We use a three layers transformer structure as the discriminator and the adapter, respectively. We first fine-tune the PLM for 5 epochs; then, we use the fine-tuned model to train the discriminator and employ the proposed method with another 5 epochs. For the sentence pair task, we compute the representation for each sentence individually and combine them before the output layer. For the structure prediction task, we use the average of all the outputs of all tokens as input. We report the average score of 3 runs with different seeds. Experiments are performed on 4 NVIDIA A100 GPUs.\n\nMain Results\nResults on GLUE. The results of the GLUE benchmark are presented in Table 1 . We first report the results for several widely-used PLMs, i.e., XLNET (Yang et al., 2019) and RoBERTa (Liu et al., 2019) , as well as a prompt-based method, HyperPrompt (He et al., 2022b) . To ensure fair comparison, we implement the self-teaching (Self-Teach) (Fang et al., 2020) , noisy tuning (Noisy-Tune) (Wu et al., 2022) and the standard adapterbased method (as trained by Eq. 2) with the same structure of our model. Our G-Tuning approach gets a 1.0 absolute improvement compared to finetuning RoBERTa directly. Additionally, our model consistently outperforms previous work. Subsequently, we evaluate the generalization capabilities of our approach in domain generalization and language generalization. While G-Tuning gets considerable improvements on the GLUE benchmark, it is important to consider whether it effectively transforms the entire representation space or only a neighborhood surrounding the training data.\nResults on Domain Generalization. The results are presented in Table 2 . Similar to Yang et al. (2022) , we evaluate our approach by exploiting out-of-domain (OOD) data as test sets. Compared to the standard dev sets on the GLUE, the performance of all methods exhibits a notable decline on the OOD test sets. Moreover, the results of fine-tuning the whole model are inferior to those of other methods, suggesting that training PTMs with in-domain data will lead to a severe decline in generalization ability. In comparison to the strongest baseline, G-Tuning achieves an average improvement of 6.4 in the domain generalization scenario.\nResults on Language Generalization. The overall results on XTREME benchmark are shown in Table 3 . Compared to fine-tuning XLM-R, our method achieves an average improvement of 1.4. However, the improvement is lower than that observed in the domain generalization. We posit that the characteristics of different languages have an impact on language generalization. Here, we do not utilize any bilingual parallel data, which makes it challenging to learn the alignment of these characteristics, resulting in limited improvements.\n\nAnalysis\nWe sampled 200 sentences each from in-domain and out-of-domain data and used the model fine-tuned and G-Tuned to generate the representations.\nWe then normalized the representations and reduce the dimensionality using t-SNE. The visualization is shown in Figure 2 . We also included contour lines based on sample density in the figure. It is apparent that the density centers of different domains are nearly coincident in our model, whereas fine-tuning results in a significant gap between different domains. Empirically, the representation in a task-specific space should be centralized. The gap from the fine-tuning method leads to incorrect label predictions for some data, e.g., the samples in the upper right corner of the left figure.\n\nConclusion\nIn this work, we elucidate a drawback of the finetuning strategy on PLMs, which is that the representation from PLMs is not fully mapped to the target space when the training data is insufficient. The generalization ability of PLMs in the downstream tasks will be diminished in this situation. To address this issue, we present G-Tuning, a framework that aims to preserve the generalization ability of PLMs in the downstream tasks. The proposed G-Tuning utilizes a generative adversarial network to transform the representation that is not covered by training data into the target space. Extensive experiments on the GLUE and two additional scenarios show that G-Tuning accurately maps the universal representation to the target space, getting substantial improvements in generalization performance.\n", "hypothesis": " The generalization ability of pre-trained language models (PLMs) in downstream tasks is heavily influenced by fine-tuning.  The objective of fine-tuning is to transform the latent representation of PLMs from a universal space to a target space, allowing the model to be applied to downstream tasks with the capability of generalizing to unseen samples.  However, the effect of PLMs will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the complete mapping.  In this study, we propose a new finetuning framework, referred to as G-Tuning, that aims to preserve the generalization ability of PLMs in downstream tasks.  Specifically, we integrate a generative adversarial network into the fine-tuning process to aid in the transformation of the latent representation in the entire space.  Empirical evaluations on the GLUE benchmark, as well as two additional demanding scenarios involving domain and language generalization, demonstrate that G-Tuning can accurately map the universal representation to the target space, thus effectively enhancing the generalization performance of PLMs across various downstream tasks..", "answer": true}
{"title": "Evaluating Paraphrastic Robustness in Textual Entailment Models", "content": "\nIntroduction\nRecognizing Textual Entailment (RTE), the task of predicting whether one sentence (hypothesis) would likely be implied by another (premise), is central to natural language understanding (NLU; Dagan et al., 2005) , as this task captures \"all manners of linguistic phenomena and broad variability of semantic expression\" (MacCartney, 2009) . If an RTE model has a sufficiently high capacity for reliable, robust inference necessary for full NLU (Mac-Cartney, 2009) , then the model's predictions should be consistent across paraphrased examples.\nWe introduce P aRT E, a test set to evaluate how reliable and robust models are to paraphrases (Table 1 includes an example). The test set consists of examples from the Pascal RTE1-3 challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) rewritten with a lexical rewriter and manually verified to preserve the meaning and label of the original RTE sentence-pair. We use this evaluation set to determine whether models change their predictions when examples are paraphrased.\nWhile this may not be a sufficient test to determine whether RTE models fully understand language, as there are many semantic phenomena that RTE models should capture (Cooper et al., 1996; Naik et al., 2018) , it is necessary that any NLU system be robust to paraphrases.\n\nP\nThe cost of security when world leaders gather near Auchterarder for next year 's G8 summit, is expected to top $150 million. P' The cost of security when world leaders meet for the G8 summit near Auchterarder next year will top $150 million.\n\nH\nMore than $150 million will be probably spent for security at next year's G8 summit. H' At the G8 summit next year more than $150 million will likely be spent on security at the event.\nTable 1 : An original and paraphrased RTE example.\nThe top represents an original premise (P) and its paraphrase (P'). The bottom depicts an original hypothesis (H) and its paraphrase (H'). A model robust to paraphrases should have consistent predictions across the following pairs: P-H, P'-H, P-H', and P'-H'.\nOur experiments indicate that contemporary models are robust to paraphrases as their predictions do not change on the overwhelmingly large majority of examples that are paraphrased. However, our analyses temper this claim as models are more likely to change their predictions when both the premise and hypothesis are phrased compared to when just one of the sentences is rewritten. We release P aRT E 1 to encourage others to evaluate how well their models perform when RTE examples are paraphrased.\n\nRelated Work\nWith the vast adoption of human language technology (HLT), systems must understand when different expressions convey the same meaning (paraphrase) and support the same inferences (entailment). Paraphrasing and entailment are closely connected as the former is a special case of the latter where two sentences entail each other (Nev\u011b\u0159ilov\u00e1, 2014; Fonseca and Alu\u00edsio, 2015; V\u00edta, 2015; Ravichander et al., 2022) . Para-phrasing has been used to improve RTE predictions (Bosma and Callison-Burch, 2006; Sun et al., 2021) and RTE has been used for paraphrase identification (Seethamol and Manju, 2017) and generation (Arora et al., 2022) . Furthermore, both phenomena are key to NLU (Androutsopoulos and Malakasiotis, 2010) and work such as Zhao et al. (2018) ; Hu et al. (2019) have explored rewriting RTE examples to create more robust models.\nWe follow a long tradition of evaluating linguistic phenomena captured in RTE models (Cooper et al., 1996) . Recent tests focus on evaluating how well contemporary RTE models capture phenomena such as monotonicity (Yanaka et al., 2019a,b) , verb veridicality (Ross and Pavlick, 2019; Yanaka et al., 2021) , presuppositions (Parrish et al., 2021) implicatures (Jeretic et al., 2020) , basic logic (Richardson et al., 2020; Shi et al., 2021) , figurative language (Chakrabarty et al., 2021) , and others (Naik et al., 2018; Poliak et al., 2018a; Vashishtha et al., 2020) . Unlike many of those works that evaluate models' accuracy on examples that target specific phenomena, we use a contrastive approach (Prabhakaran et al., 2019; Gardner et al., 2020) to determine whether RTE models' predictions change when examples are paraphrased.\n\nP aRT E\nTo explore whether these RTE models are robust to paraphrases, we create P aRT E, a modified version of the Pascal RTE1-3 challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007) . P aRT E contains 1,126 examples of an original unmodified RTE sentence-pair grouped with a sentence-pair with a modified premise, hypothesis, or both. We use the examples in RTE1-3 to create our test set, as opposed to other RTE datasets due to its long-standing history.\n\nParaphrase Generation & Verification\nFor each RTE premise-hypothesis pair (P-H), we created three paraphrased premises (P') and hypotheses (H') using a T5-based paraphraser 2 finetuned on the Google PAWS dataset (Zhang et al., 2019) . To ensure lexically diverse paraphrases, we filter out any paraphrases that have high lexical overlap with the original sentences using Jaccard index threshold of 0.75. Out of 14,400 generated sentences, 2,449 remained -956 paraphrased premises (P') and 1,493 paraphrased hypotheses (H'). Next, we retained 550 paraphrased premises and 800 paraphrased hypotheses paraphrases that crowdsource workers identified as grammatical and similar in meaning to the original sentences. 3 We include a grammatical check since an existing RTE evaluation set focused on paraphrases (White et al., 2017) contains hypothesis-only biases related to grammaticality (Poliak et al., 2018b) .\nIf at least one P' or one H' passes this filtering process, we retain the original RTE example and pair it with a corresponding paraphrased example (i.e. P'-H', P'-H, or P-H'). In the case where more than one P' or H' passes the filtering, we retained the P' or H' that crowdsource workers deemed most similar to the original sentence. Out of the original 2,400 RTE test pairs, we retain 914 pairs with a high-quality P' or H', resulting in 1,178 original and paraphrased RTE pairs. 4\n\nOvercoming Semantic Variability\nMacCartney (2009) argues that in addition to being reliable and robust, RTE models must deal with the broad variability of semantic expression. In other words, though two sentences may be semantically congruent, it is possible that small variations in a paraphrased sentence contain enough semantic variability to change what would likely, or not likely be inferred from the sentence. Despite all P' and H' being deemed to be semantically congruent with their corresponding original sentences, the semantic variability of paraphrases might change whether H or H' can be inferred from P' or P.\nTherefore, propagating an RTE label from an original sentence pair to a modified sentence pair might be inappropriate. We manually determined that this issue occurs in just 52 (4%) examples, and retained 1,126 examples. This ensures an evaluation set of high-quality examples that can be used to determine whether models are sensitive to paraphrases and change their prediction on paraphrased examples. Our dataset contains 402 examples with just a paraphrased premise P', 602 with just a paraphrased hypothesis H', and 122 with both a paraphrased premise and hypothesis. \n\nExperimental Setup\nWe explore models built upon three different classes of sentence encoders: bag of words (BoW), LSTMs, and Transformers. Our BoW model represents premises and hypotheses as an average of their tokens' 300 dimensional GloVe embeddings (Pennington et al., 2014b) . The concatenation of these representations is fed to an MLP with two hidden layers. For the BiLSTM model, we represent tokens with GloVe embeddings, extract sentence representations using max-pooling, and pass concatenated sentence representations to an MLP with two hidden layers.\nOur transformer-based models are pre-trained BERT (Devlin et al., 2019) and Roberta (Liu et al., 2020) encoders with an MLP attached to the final layer. Additionally, we use GPT-3 in a zero-shot setting where we ask it to label the relationship between a premise and hypothesis. 5 The RTE training sets do not contain enough examples to train deep learning models with a large number of parameters. We follow the common practice of training models on MNLI and using our test set to evaluate how well they capture a specific phenomenon related to NLU. During testing, we map the MNLI 'contradiction' and 'neutral' labels to the 'not-entailed' label in RTE, following common practice (Wang et al., 2018; Yin et al., 2019; Ma et al., 2021; Utama et al., 2022, inter ailia) .\n\nResults\nTable 2 report the results. The RTE and P aRT E columns respectively report the models' accuracy on the 1,126 unmodified and paraphrased sentence pairs. 6 Comparing the difference in accuracy be-5 See Appendix A for more details, including hyperparameters, model sizes, and GPT-3 prompt design and configurations. Our code is available at https://github.com/ stonybrooknlp/parte 6 Although there are just 914 unmodified sentence pairs, for the sake of a head-to-head comparison, we retain all instances tween unmodified and paraphrased examples can be misleading. If the number of times a model changes a correct prediction is close to the number of times it changes an incorrect prediction, then the accuracy will hardly change. Figure 1 demonstrates why the accuracies do not change by much when models' predictions change on paraphrased examples. Furthermore, if a model is robust to paraphrases, then it should not change its predictions when an example is paraphrased, even if the prediction on the original unmodified example was incorrect. Hence, our test statistic is the percentage of examples where a model's predictions change (% \u2206 P aRT E column in Table 2 ) rather than a change in accuracy. Compared to the Transformer based models, the BoW and BiLSTM models seem to be more sensitive, and less robust to paraphrasing, as they change their predictions on 15.27% and 16.69% respectively of the 1,126 examples. However, this might be associated with how word xembedding models only just outperform random guesses in and perform much worse on RTE compared to the Transformer models.\nof the unmodified sentence pairs when computing accuracy. Focusing on the Transformer models, we noticed that RoBERTa performs the best on the datasets and is the most robust to paraphrasing -changing its predictions on just under 8% of paraphrased examples. Interestingly, when the models are trained specifically to perform this task, the models change their predictions on fewer paraphrased examples as these models' accuracy increases. However, improving performance alone might not automatically improve models' robustness to paraphrases. GPT-3's accuracy noticeably outperforms BERT's accuracy, but GPT-3 changes its predictions on more paraphrased examples compared to BERT. P'-H' compared to P-H' or P'-H Figure 2 shows noticeable increases in the percentage of changed predictions when both premise and hypothesis are paraphrased compared to when just one of the sentences is paraphrased. Specifically, for BoW and BiLSTM we see an increase of 4.01 and 6.01 percentage points respectively, and for BERT, Roberta, GPT-3 increases of 4.97, 4.83, and 3.55. As the transformer-based models changed their predictions on 12-14% of examples where both sentences are paraphrased compared to 9-11% in general, this analysis further suggests that these models are not as robust to paraprhases as desired.\nEntailed vs Not-entailed examples RTE analyses often differentiate how models perform on entailed vs not entailed examples (Liu et al., 2022) . In Figure 3 , we do not see meaningful differences in how models' predictions change on paraphrased examples based on the gold label. This might suggest that our dataset does not contain statistical irregularities based on the RTE labels. Correct vs Not-Correct Predictions Figure 4 shows that the Transformer models' predictions is more likely to change when it's prediction on an original example was incorrect (right red bars) compared to when the prediction for an original example was correct (left blue bars). For example, when RoBERTa's prediction for an original RTE example was correct, the model changed its prediction on just 5.5% of the corresponding paraphrased examples. When RoBERTa's predictions for an original RTE example were incorrect, RoBERTa's predictions changed for 20.88% corresponding paraphrased examples. Analyzing differences in models' confidences assigned to predictions might provide more insight (Marc\u00e9 and Poliak, 2022) . We leave this for future work.\nSource Task RTE1-3 examples originated from multiple domains and downstream tasks, e.g. question-answering (Moldovan et al., 2006) , information extraction (Grishman and Sundheim, 1996) , and summarization (Evans et al., 2004; Radev et al., 2001) . This enables researchers to evaluate how \n\nConclusion\nWe introduced P aRT E, a high-quality evaluation set of RTE examples paired with paraphrased RTE examples. We use our evaluation set to determine whether RTE models are robust to paraphrased examples. Our experiments indicate that while these models predictions are usually consistent when RTE examples are paraphrased, there is still room for improvement as models remain sensitive to changes in input (Jia and Liang, 2017; Belinkov and Bisk, 2018; Iyyer et al., 2018) . We hope that researchers will use P aRT E to evaluate how well their NLU systems perform on paraphrased data.\n", "hypothesis": " We present P aRT E, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing.  We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models' predictions change when examples are rephrased.  In our experiments, contemporary models change their predictions on 8-16% of paraphrased examples, indicating that there is still room for improvement..", "answer": false}
{"title": "LEDA: a Large-Organization Email-Based Decision-Dialogue-Act Analysis Dataset", "content": "\nIntroduction and Related Work\nMotivation Online collaboration has been used for many years by large distributed organizations. The increasing availability of high-speed Internet connections and collaboration tools, along with the Covid-19 pandemic, are making it ever more prevalent. Large distributed organizations of this type often undertake important tasks. For example, the Internet Engineering Task Force (IETF) and the World Wide Web Consortium (W3C) are responsible for developing the technical standards that underpin the Internet. Consequently, understanding the decision-making processes in this type of organization is essential to increase transparency and accountability, to facilitate tracking of decisions and the reasoning behind them, and to understand alternatives that were considered (or not) and the voices that were (or were not) heard.\nGoals Most studies of decision making in text (e.g. Hsueh and Moore, 2007; Fern\u00e1ndez et al., 2008; Bui and Peters, 2010) rely on annotation and analysis of Dialogue Acts (DAs). We adopt this approach and label emails from public IETF mailing lists with DAs. Our aim is to answer the following research questions: RQ1: What is an appropriate set of DAs to use for this annotation task?; RQ2: How do communication patterns change through the life-cycle of a decision discussion?; and RQ3: How do different types of participants differ in how they contribute to the process? The overall goal of these questions is to better understand the mechanisms underlying the decision-making process in a large, distributed, collaborative organization.\n\nRelated Datasets\nThe most notable email-based related dataset is the Enron Corpus (Klimt and Yang, 2004) , covering over 200K messages of Enron employees in various positions within the organization. However, in-house emails of a single closed company are not representative of communication in larger, more diverse collaborations.\nDatasets specifically relevant for studying decision making include AMI (McCowan et al., 2005) and ICSI/MRDA (Janin et al., 2003; Shriberg et al., 2004) . However, the AMI dataset is not \"real\": it uses actors acting out small-group meetings on predefined topics. In contrast, the ICSI dataset is based on naturally occurring meetings at the International Computer Science Institute (ICSI). While both are annotated with general dialogue act labels, AMI also includes specific decision-oriented dialogue acts provided by Fern\u00e1ndez et al. (2008) . Despite this, they are not representative of interaction in large groups, or online collaborative settings. Consequently, we annotate a new dataset tailored to address our research questions. We denote it as Largeorganization Email-based Decision-dialogue-act Analysis dataset -LEDA.\nThere are important differences between LEDA and AMI/ICSI. First, while AMI/ICSI are transcribed face-to-face, real-time, in-person, and small-group meetings. LEDA contains emails from mailing-lists, asynchronous, and from a large decentralized, globally spread group. Second, AMI/ICSI discuss mostly self-contained, focused topics (design, research-group progress); LEDA discusses the more long-term, complex task of designing Internet-standards. We further provide a more detailed comparison of LEDA with AMI in Appendix A.\nContributions First, we propose a taxonomy of DA labels for large-group email decision-making. Second, we provide a novel dataset labeled with DAs. Third, we provide data analyses exploring decision-making communication patterns within the IETF. Fourth, we provide a preliminary DA prediction model on this dataset, which can serve as a reference baseline model for future work.\n\nDataset\nOur data consists of emails from the IETF mailing list archive. 1 The IETF is a typical example of decision making in a large, distributed, online collaborative community; it has rich metadata available via the IETF DataTracker; 2 and the data is publicly available with appropriate consent. 3 IETF background The IETF is a large, open, voluntary organization tasked with developing Internet standards (Flanagan, 2019; McQuistin et al., 2021; Khare et al., 2022) . It is comprised of working groups (WGs), each focusing on a relatively narrow field: e.g., RMCAT 4 WG focuses on specific Real-time Media Congestion Avoidance Techniques. Each WG has one or more participants as chairs. During its development, an Internet standard is called a draft. Drafts are discussed in the mailing lists (the archive has >2M emails, predominantly in English, between 56k participants over 20 years) and in several live meetings yearly. After sufficient revision and review, a draft becomes an Internet standard.\n\nData preparation\nThe email archive consists of threads (sets of emails connected with reply-to relations, forming a tree-like structure). Given a particular draft, we extract all threads with at least one message that mentions the draft in either the subject or body. We do this for four drafts, chosen by an IETF expert to span a range of technical areas. We opted for entire threads over a smaller number of drafts (rather than more drafts but with partial threads) to ensure a full view of the draft discussion and agreement process over its life-cycle.\nWe then preprocess all messages, splitting them into Quote, Signature, or Normal segments using custom heuristics developed for this data. A Normal segment contains text written by the author of the message. A Quote segment contains text written by someone else, which is being quoted. A Signature segment contains signatures (name, company name, website). Normal segments are useful for analysis, while the rest introduce noise. We also keep track of quoting relations between segments.\nLabel set calibration As our starting point, we take the DA labels defined in the ISO 24617-2 standard (Bunt et al., 2012) . Cross-referencing with labels in datasets from related work and manual inspection of the IETF data suggested that much of the complexity in the standard is not needed for our goals. This was confirmed in several initial rounds of annotations where we observed considerable confusion between the very fine grained ISO 24617-2 DAs on our data. After each iteration, we simplified the label set by removing irrelevant labels for email communication (e.g., rhetorical devices such as pauses) and aggregating hard to distinguish labels (e.g., accepting a request and agreeing to an opinion). Table 1 presents our twolevel taxonomy with three coarse grained labels divided into eleven fine-grained ones, which was obtained after four rounds of calibration.\nAnnotation Annotation of each segment with DA labels was carried out by seven student annotators, all with a background in linguistics. A segment can be assigned several DAs simultaneously (a multi-label setting). During the calibration rounds, annotators provided feedback which helped modify the taxonomy and instructions. For the final annotation, they were provided a detailed set of instructions and an annotation tool specifically developed in-house.\nTable 1 reports data statistics and inter-annotator agreement (IAA). Each thread is annotated by at least two annotators. To measure IAA, we considered both Fleiss' Kappa and Krippendorff's Alpha, but neither supports multi-label annotation. Instead, we consider one annotator's labels as \"gold labels,\" and another's as \"classifier predictions.\" We calculate the F1 score for all annotator pairs and average them. This calculation is performed on a subset of 15 threads labeled by all annotators. For some labels, the annotation is inherently difficult, as reflected in the IAA. Manual inspection reveals that many of these disagreements may be impossible to completely resolve as the task is subjective (Uma et al., 2021) . For example, ClarificationElicitation is more often implicit (\"I don't see why ...\") than explicit (\"Can you explain why ...\"), introducing disagreement. However, recent work (Pavlick and Kwiatkowski, 2019; Basile et al., 2021; Leonardelli et al., 2021) shows it is viable to design models and evaluation measures that account for this inherent ambiguity instead of trying to resolve it. Accordingly, we release all individual annotators' labels with the text data and code. 5 While covering only four drafts, LEDA is of substantial size (8230 segments, 2196 messages, 363 authors), with the drafts hand-picked by an IETF expert to ensure they are representative. We focus on trends that are very prominent and supported by statistical significance tests. Finally, an inspection of plots for individual drafts revealed that the main trends outlined in the remaining sections were consistent across all four drafts.\n3 Analysis of gold-standard labels\n\nDraft life-cycle\nTo address RQ1, we divide the period between the submission and publication of a draft into five equal time intervals (T1 -T5), each representing 20% of the period. We visualize the distribution of DAs falling into each of the periods. in Figure 1 . 6 Answer and Question are more common in the early phases, likely due to more new issues being raised and unresolved issues discussed.\nContextSetting and Extension are very frequent, increasingly so towards the end phases; we conjecture this is because those phases cover more complex issues requiring more background description.\nThe frequency of ProposeAction is stable throughout the cycle and noticeably higher than StateDecision. This may imply that participants prefer to discuss actionable options rather than explicitly deciding on a single one.\n\nDifferent groups\nTo explore RQ2, we categorize the participants as:\n(1) authors of the draft being discussed, or not;\n(2) influential -following (Khare et al., 2022) , having top-10% centrality in the email interaction graph -or not; (3) chairs of any IETF WG, or not; (4) everyone (all participants). Figure 2 gives a visualization of DA distributions for each group.\nAuthors vs. non-Authors Authors are more social, give more answers, and ask fewer questions (including clarification questions). Also, they use fewer NeutralResponse, Extension, and ContextSetting, indicating shorter, more focused messages. These trends imply they take a more reactive role in the discussion. Finally, they make the most decisions in the discussion, as would be expected, since they are in charge of the writing process.\nInfluential vs. non-Influential Influential people use Answer, Agreement, and NeutralResponse more, making them generally more responsive. They use less Extension, ContextSetting and Thanking, implying a concise, focused communication style. As expected, they make more decisions and propose slightly more actions.\nChairs vs. non-Chairs Similar to influential participants, chairs use NeutralResponse more than non-Chairs. However, they use more ContextSetting and Extension, and do more Thanking. We find this is because chairs send a lot of emails initiating and managing discussions and review assignments. Such emails are often composed of many small segments and contain a lot of these labels.\nFeedback to questions We further explored how likely the different groups are to have their questions answered. From the labeled data we obtain percentages for authors (22%), chairs (51%), influential (34%), and everyone (37%). Authors have the lowest ratio, possibly because their questions are, on average, more complex. The chairs, while they tend not to ask many questions, are the most likely to to get an answer. This is expected, as it is difficult to ignore a question from someone in that position. Surprisingly, the difference between ratios of influential participants and everyone are not statistically significant. 7 Another surprising finding is that, on average, around two thirds of all questions appear to remain unanswered.\n\nOther observations\nClarificationElicitation is almost nonexistent, implying either very little misunderstandings or un- Table 1 : Labels at the higher (bold) and lower levels of the taxonomy with corresponding counts and inter-annotator agreement. willingness to explicitly voice it. Research on misunderstandings in dialog (Aberdeen and Ferro, 2003) implies it is likely the latter. Most participants tend to use NeutralResponse, as opposed to Agreement or Disagreement, and between the latter two they prefer Agreement. This tendency is confirmed by related research on agreement (Stromer-Galley and Muhlberger, 2009) .\nContextSetting, Extension, and NeutralResponse are, expectedly, very frequent. This implies there are a lot of boilerplate explanations around segments with more relevant DAs.\n\nAutomated Dialogue Act Tagging\nWe provide a preliminary DA tagging model to investigate the predictability of our DA tags, and to serve as a baseline for future work. We use a hierarchical sequence model, inspired by work in DA tagging for spoken dialogue (e.g. Li et al., 2019) : the input is a sequence of segments (each one a sequence of words), and the output is a sequence of predictions, one 14-dimensional vector for each input segment, representing DA probabilities.\nEach input segment is encoded into a vector; we use the [CLS] token of BERT (Devlin et al., 2019) . The sequence of segment vectors is then passed to a Bidirectional-LSTM (Hochreiter and Schmidhuber, 1997) ; each BiLSTM hidden state vector is passed through a linear layer (shared for all time steps) to produce the output prediction vector sequence. The loss function is binary cross-entropy averaged across all labels and all elements of the sequence.\nThe model is implemented using PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) . We used a learning rate of 2 \u22125 , batchsize of 32, and LSTM hidden-layer size of 256. All other hyper-parameters are left at default values. We experiment with two variants of BERT: bertbase and bert-base-ietf (fine-tuned using language modeling loss on the entire IETF mail archive).\nWe split the data into train (60%), validation (20%), and test threads (20%). We report results on test threads by the model best on the validation threads. The input sequences for the model are the possible root-to-leaf paths in the input threads, following (e.g. Zubiaga et al., 2016). 8 Results are given in Curiously, bert-base-ietf performs comparably to or worse than bert-base. We hypothesize the reason for this may be the specific language of the IETF (technical discussions). It may cause the additional language model training step to make the bert-baseietf model forget information generally useful for DA tagging. On the other hand, this information is retained in bert-base. If this is the case, it would hurt the performance of bert-base-ietf after further fine-tuning on the DA tagging task. However, we leave investigation of this and other hypotheses for this unexpected result to future work.\n\nConclusion\nWe have presented a taxonomy of dialogue acts (DAs) and a labeled dataset of emails. Moreover, we provided a data analysis and a preliminary DA prediction model. We hope this dataset will be useful to facilitate further research on the interaction behavior of participants in online collaboration settings. Future work could include a more detailed investigation into the underlying reasons for the observed trends. Another possibility is looking into the interaction of DAs and the participant interaction graph as described by (Khare et al., 2022) . Finally, to get further insights, it would be interesting to annotate segments of with a particular DA with additional labels, e.g., explicit/implicit for Agreement or different sub-types of Question.\n", "hypothesis": " Collaboration increasingly happens online.  This is especially true for large groups working on global tasks, with collaborators all around the world.  The size and distributed nature of such groups make decision-making challenging.  This paper proposes a set of dialog acts for the study of decision-making mechanisms in such groups, and provides a new annotated dataset based on real-world data from the public mail-archives of one such organization -the Internet Engineering Task Force (IETF).  We provide an initial data analysis showing that this dataset can be used to better understand decision-making in such organizations.  Finally, we experiment with a preliminary transformerbased dialog act tagging model..", "answer": true}
{"title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives", "content": "\nIntroduction\nThe dual encoder architecture applied to information retrieval has shown excellent performance in a wide range of tasks (Gillick et al., 2018; Karpukhin et al., 2020; Ni et al., 2021 Ni et al., , 2022)) .\nRecently, the Information Retrieval community has transitioned towards Deep Learning models that leverage large unsupervised corpus pretraining (Devlin et al., 2019; Raffel et al., 2020) , which offers more powerful semantic and contextual representation for queries and documents. These models can be successfully applied to scoring tasks, e.g. Dehghani et al. (2017) , or retrieval tasks, e.g. Gillick et al. (2018) . In contrast, classic SearchQA show that sharing a projection layer in Asymmetric Dual Encoders (ADE-SPL) (Dong et al., 2022) may not guarantee that the embeddings from the two encoder towers are in coinciding parameter spaces. However SamToNe can effectively achieve that.\nretrieval models, such as BM25 (Robertson and Zaragoza, 2009) , rely on bag-of-words lexical overlap, term frequency heuristics, inverse document frequency and document length. This type of retrieval models does not require any training and can generalize reasonably well, but they fall short of finding documents that have low term overlap but high semantic similarity.\nA dual encoder (Gillick et al., 2018; Yang et al., 2020; Karpukhin et al., 2020; Reimers and Gurevych, 2019) consists of two encoding towers that map queries and documents, respectively, into a shared low-dimensional dense representation, namely, the embedding space. The model is usually optimized by a contrastive loss (Chopra et al., 2005) , which moves the embeddings of the queries and documents from the same positive examples closer to each other, and the embeddings from negative examples farther away. Training the dual encoder in batches allows to use, for each question, the passages that answer all the other questions within the batch as negatives (Gillick et al., 2018) , namely \"in-batch negatives\". At indexing time, all the documents in a corpus are encoded via bulk inference and indexed. To run retrieval, a query is encoded and its most relevant documents can be retrieved through Nearest Neighbours Search ( Vanderkam et al., 2013; Johnson et al., 2021) over the embedding space using a measure of similarity, e.g. the dot-product or cosine distance of the embedding vectors.\nMotivation. In this work, we consider two major types of dual encoder architectures: \"Symmetric Dual Encoder\" (SDE) 1 , with parameters shared between two encoder towers, and \"Asymmetric Dual Encoder\" (ADE), with two distinctly parameterized encoder towers. Dong et al. (2022) demonstrated that sharing projection layers can significantly improve the performance of ADEs. They empirically explained the efficacy of SDE and ADE-SPL by claiming that the shared projection layers help mapping the embeddings of the two encoder towers into a coinciding parameter space.\nBy repeating this embedding space analysis on a variety tasks, we find that ADE-SPL may not be enough to ensure that the embedding spaces from two encoder towers are coinciding, as shown in Figure 1 . This motivates us to further improve the dual encoder retrieval quality beyond the architectural change explored in Dong et al. (2022) . Although the projection layers are shared, our analyses suggest that an extra mechanism, other than using the standard contrastive loss with in-batch negatives, is required to ensure the adjacency of the embeddings of a ground truth pair.\nContributions. In this paper, we propose an improved training objective for dual encoder models: contrastive loss with Same Tower Negatives (SamToNe). In Section 3, we demonstrate its usefulness on a variety of Information Retrieval tasks, including both tasks with in-task fine-tuning and a zero-shot benchmark suite. Across all the tasks explored, SamToNe performs competitively comparing to the traditional training setup, with a significant improvement on the metrics averaged across tasks. Finally, through an analysis of the produced embeddings, in Section 4, we further make evident the superiority of SamToNe from the perspective of regularisation. \n\nMethod\nDual Encoder Architecture. We follow the standard setup of information retrieval: given a query, q, and a corpus of retrieval candidates, P, the goal is to retrieve k relevant candidates, p k \u2208 P. The candidate can be a phrase, a sentence, a passage, or a document.\nRecent research (Dong et al., 2022) demonstrated that sharing projection layers can significantly improve the performance of ADEs and we use this shared projection layer for ADEs (ADE-SPL) throughout our experiments. Figure 2 illustrates the SDE and ADE-SPL architectures we use in this work. Our dual encoders are initialized from pre-trained t5.1.1 encoders (Raffel et al., 2020) . Following Ni et al. (2022) ; Dong et al. (2022) , we encode a query, q i , or a candidate, p i , by averaging the T5 encoder outputs and projecting them to the final embedding vector.\nContrastive Loss. A standard way to train a dual encoder model is optimizing an in-batch sampled softmax loss for contrastive learning (Henderson et al., 2017) :\nEQUATION\n)\nwhere sim is cosine similarity, B is a mini-batch of examples, and \u03c4 is the softmax temperature. p i is the ground-truth relevant passage for the query q i in a batch of retrieval candidates p * , where all the other passages p k (k \u0338 = i) are treated as the negative examples for contrastive learning. Bi-directional in-batch sampled softmax loss is commonly applied to improve the embedding quality of both towers, where the contrastive loss is computed for both query to passage matching and passage to query matching (Yang et al., 2019) . We use the bi-directional loss throughout this work.\nSame Tower Negatives. The in-batch sampled softmax loss is a contrastive loss that only considers the contrastive estimation between the target example pair {q i , p i }, and the in-batch sampled negative pairs {q i , p j } (j \u0338 = i).\nOne way to improve the quality of the retrieval is to improve the contrast among the embeddings of the queries. Therefore, we propose a novel contrastive loss using Same Tower Negatives, which we abbreviate as SamToNe:\nLS = e sim(q i ,p i )/\u03c4 j\u2208B e sim(q i ,p j )/\u03c4 + j\u2208B,j\u0338 =i e sim(q i ,q j )/\u03c4 , (2\n)\nwhere the second term in the denominator is the contribution from the same tower negatives. SamToNe can be interpreted as a regularized version of the in-batch sampled softmax loss, where the term j\u2208B,j\u0338 =i e sim(q i ,q j )/\u03c4 is a regularizer. When query embeddings are not well distributed, max sim(q i , q j ) \u226b max sim(q i , p j ), and the second term in the denominator will dominate the contribution from the negative examples. Thus, it will drive the separation of the query embeddings in contrastive learning. In Section 4, we provide empirical evidence of the effects of SamToNe as a regularizer of the embedding space. Ren et al. (2021) proposed an improved contrastive loss, PAIR, which is a hybrid loss\nL P AIR = \u2212(1 \u2212 \u03b1) log L c \u2212 \u03b1 log L P , where LP =\ne sim(q i ,p i )/\u03c4 j\u2208B,j\u0338 =i e sim(p i ,p j )/\u03c4\n(3) penalizes the similarities between passages / documents. Despite both SamToNe and PAIR are penalizing the similarities among the same tower inputs, there are two significant differences. single stage training and guaranteed improvement on embedding space quality, make SamToNe much easier to use.\n\nQuestion-Answering Retrieval Tasks\nWe evaluate SamToNe on 5 question-answering (QA) retrieval tasks including MS MARCO (Nguyen et al., 2016) and MultiReQA (Guo et al., 2021) . For MS MARCO, the retrieval candidates are relevant passages, and for the 4 tasks in Mul-tiReQA, the retrieval candidates are answer sentences.\nTo make a fair comparison across the results of our experiments, the same fine-tuning hyperparameters are applied to all our model variants. The models are optimized for 20, 000 steps using Adafactor optimizer (Shazeer and Stern, 2018) , with softmax temperature \u03c4 = 0.01, batch size 512, and a linearly decaying learning rate starting from 10 \u22123 to 0 at the final step. To compare SamToNe and PAIR, we use the hyperparameter \u03b1 = 0.1 for PAIR as reported in Ren et al. (2021) , and keep all the other experimental setups identical. SamToNe is applied only on the query side, as it is more robust across different datasets. For experiments and analysis on applying SamToNe on both encoder towers, please refer to Section 3.4. We benchmark Model Loss MSMARCO NQ SQuAD TriviaQA SearchQA Average P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR the fine-tuned models using precision at 1 (P @1) and mean reciprocal rank (MRR).\nAs shown in Table 1 , SamToNe greatly improves the retrieval performance of both SDE and ADE-SPL models. Using SamToNe, ADE-SPL models can outperform SDE ones, especially for TriviaQA and SearchQA, by a great margin. Relative to PAIR, SamToNe provides better performance across different datasets in both types of models.\n\nScaling the Model Size\nTo assess the impact of the model size, we evaluate the dual encoders initialized from t5.1.1-base (\u223c 250M parameters), t5.1.1-large (\u223c 800M parameters), and t5.1.1-XXL (\u223c 11B parameters). Figure 3 and Appendix Table 4 show that SamToNe consistently improves the performance of dual encoders across different model sizes.\n\nBEIR Generalization Tasks\nWe further demonstrate the efficacy of the dual encoders trained with SamToNe on BEIR (Thakur et al., 2021) , a heterogeneous benchmark for zeroshot evaluations.\nBEIR has 18 information retrieval datasets 2 across 9 domains, including Bio-Medical, Finance, News, Twitter, Wikipedia, StackExchange, Quora, Scientific, and Misc. The majority of the datasets have binary query relevance labels. The other datasets have 3-level or 5-level relevance judgements.\nAs BEIR is evaluating generalization capabilities and SDEs are commonly used for general purpose retrieval (Ni et al., 2021) , we focus on evaluating the impact of SamToNe on BEIR using the SDE architecture. In this evaluation, we reuse the model fine-tuned with MS MARCO, as described in Section 3.1.\nEvaluated with the same setting as GTR (Ni et al., 2021) , SamToNe demonstrates strong performance on BEIR, as shown in Table 2 and Figure 4 . On average, SamToNe improves NDCG@10 by 1.4% for SDE with XXL size. SDE trained with SamToNe significantly outperform BM-25, a sparse retrieval method, and GTR, a dense retrieval method that shares the same architecture and the same model size as SDE but fine-tuned with different corpora.\n\nApplying SamToNe to Both Towers\nJust as with the query tower, SamToNe can be applied to the document tower which leads to better query-document alignment. However, it is common that the training data contains a large fraction of duplicated documents for a diverse set of queries. For example, only 17% of the documents in the train-split are unique for TriviaQA, but 98% for MSMARCO. For datasets with a low rate of unique documents, applying SamToNe on the document side will penalize sim(p i , p j ) with p i = p j and may hinder the performance, as shown in Table 3 .\n\nEmbedding Space Analysis\nAs shown in the top row of Figure 1 , for MS MARCO and SearchQA, ADE-SPL generates two connected but topologically separable embedding spaces. It requires an extra mechanism, beyond the shared projection layers, to ensure the adjacency of the embeddings from a ground truth pair. SamToNe is proposed as the \"force\" drawing the embeddings of each ground truth training pair together. Its efficacy is illustrated in the bottom half of Figure 1 .\n\nSamToNe: an Embedding Distance Regularizer\nTo further understand SamToNe's role as a regularizer of embedding distances, we evaluate the distribution of the distances between the embeddings of the queries and their top-1 retrieval results in the test set of MS MARCO and SearchQA. The embedding distance is measured by cosine similarity, where 1.0 means perfect alignment with a range of [\u22121.0, 1.0]. As shown in Figure 5 , SamToNe drastically shifts the distribution of the (query, top-1 retrieval result) pairs towards 1.0, demonstrating the regularizing effect of SamToNe over the embedding distances.\nBy placing the regularizing query-query similarity terms e sim(q i ,q j )/\u03c4 and the standard inbatch negative query-document similarity terms e sim(q i ,p j )/\u03c4 together in the denominator with same weight, SamToNe pushes the similarity ratio between query-query and query-documents, sim(q i , q j )/sim(q i , p j ), to be centered around 1.0. This is a self-balancing regularization effect. The query and document spaces are set to closely overlap each other and the embeddings of a positive pair are more likely to be located in the same region of the embedding space.\nTo empirically illustrate this effect, we plotted histograms of the sim(q i ,q j ) sim(q i ,p j ) ratios for randomly selected i and j in Figure 6 . The regularization effect only shows when SamToNe is used, but not when PAIR (Ren et al., 2021) is. This is because the self-balancing effect does not exist in a hybrid loss such as PAIR.\n\nConclusions\nEvaluating on QA retrieval tasks and zero-shot generalization benchmarks, we demonstrate that training with SamToNe can significantly improve the dual encoder retrieval quality. With t-SNE maps of query and document embeddings, we show that the embedding spaces from the two encoding towers of models trained with SamToNe are better aligned. Through the distributions of similarity distances between the embeddings of queries and their nearest neighbours, we empirically explain the efficacy of SamToNe from a regularisation prospective. In general, we recommend using SamToNe to train dual encoders for information retrieval tasks.\n", "hypothesis": " Dual encoders have been used for retrieval tasks and representation learning with good results. A standard way to train dual encoders is using a contrastive loss with in-batch negatives. In this work, we propose an improved contrastive learning objective by adding queries or documents from different encoder towers to the negatives, for which we name it as \"contrastive loss with SAMe TOwer NEgatives\" (SamToNe).  By evaluating on question answering retrieval benchmarks from MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval benchmarks (BEIR), we demonstrate that SamToNe can effectively improve the retrieval quality for both symmetric and asymmetric dual encoders.  By directly probing the embedding spaces of the two encoding towers via the t-SNE algorithm (van der Maaten and Hinton, 2008), we observe that SamToNe ensures the alignment between the embedding spaces from the two encoder towers.  Based on the analysis of the embedding distance distributions of the top-1 retrieved results, we further explain the efficacy of the method from the perspective of regularisation..", "answer": false}
{"title": "Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities", "content": "\nIntroduction\nOver the years, Semantic Role Labeling (Gildea and Jurafsky, 2002, SRL) -the task of identifying the semantic relations between predicates and their arguments -has attracted continued interest. Enticed by the prospect of acquiring one * Equal contribution.\nof the ingredients that might enable Natural Language Understanding (Navigli et al., 2022) , the research community has striven to overcome numerous challenges in SRL. As a consequence, not only have automatic systems achieved impressive results on complex benchmarks (Shi and Lin, 2019; Conia et al., 2021) , such as CoNLL-2005 (Carreras and M\u00e0rquez, 2005) , CoNLL-2008 (Surdeanu et al., 2008) , CoNLL-2009 (Haji\u010d et al., 2009 ), and CoNLL-2012 (Pradhan et al., 2012) , but SRL has also been successfully leveraged to benefit a wide array of downstream tasks in Natural Language Processing and also Computer Vision, including Machine Translation (Marcheggiani et al., 2018; Raganato et al., 2019; Song et al., 2019) , Summarization (Hardy and Vlachos, 2018; Liao et al., 2018) , Situation Recognition (Yatskar et al., 2016) , and Video Understanding (Sadhu et al., 2021) , among others.\nNotwithstanding the achievements of previous work, we argue that there is still much to be done before the research community can claim SRL is even close to being \"solved\". One of the simplest yet erroneous assumptions about SRL is that all predicates -or at least the majority of them -are verbs. Quite the contrary, predicates often manifest themselves as nouns, adjectives, and adverbs. For example, in the sentence \"Sensational robbery at the bank during the night: two suspects on the loose!\", the word robbery is a predicate, as it denotes an action, and its arguments are sensational (attribute of the robbery), at the bank (location), during the night (time), and two suspects (agents). We highlight two potential issues in the above example. First, an SRL system that analyzes only verbal predicates cannot identify the nominal event in the sentence and, in turn, its semantic constituents. Second, nominal events like those expressed in the above sentence are far from rare, being commonly found in several settings, such as newspaper headlines, blog titles, short messages, tweets, and dialogues.\nPerhaps surprisingly, there is limited work on non-verbal predicates, mostly focused on transferring \"knowledge\" about verbal predicates to nominal ones (Zhao and Titov, 2020; Klein et al., 2020) . The scarcity of studies on non-verbal predicates might be explained by the way in which current datasets for SRL are designed, as they focus primarily on verbal predicates (Daza and Frank, 2020; Tripodi et al., 2021; Jindal et al., 2022) . Therefore, any progress on non-verbal predicates is often overshadowed by the predominance of verbal instances, resulting in an incomplete picture of the actual situation. The issue is also exacerbated by the fact that, oftentimes, benchmark results are taken at face value. Instead, carrying out in-depth analyses is fundamental, as neural networks have been found to learn patterns that are different from those of humans, especially in semantic tasks (Maru et al., 2022) . In this paper, we perform a reality check and explore non-verbal predicates in English SRL. More specifically, our contributions are as follows:\n\u2022 We provide an empirical demonstration that state-of-the-art systems are not capable of generalizing from verbal to nominal and adjectival predicate-argument structures (PAS) in PropBank-based SRL;\n\u2022 We investigate whether other PAS inventories -namely, FrameNet, VerbNet, and VerbAtlasare better suited for transferring learned patterns across predicate types;\n\u2022 We introduce a novel, manually-annotated challenge set to evaluate current and future SRL systems on verbal, nominal, and adjectival PAS;\n\u2022 We analyze possible directions and strategies for prospective work on non-verbal SRL.\n\nChallenges\nAs mentioned above, relying These results show that a state-of-the-art system is not capable of \"transferring knowledge\" from one predicate type to another, e.g., from verbs to nouns or vice versa.\nExamples also enables a solid evaluation of an SRL system on over 4000 predicate senses that are not included in OntoNotes 5.0; we call this more challenging testbed PB-Unseen. We report statistics on PB-Unseen in the last row of Table 1 .\nCross-type knowledge transfer. Now that we have wide-coverage multi-type SRL datasets, we can test the ability of SRL systems to generalize across types. The main objective of our experiments here is to empirically demonstrate that: i) \"knowledge transfer\" between predicate types is an unaddressed challenge, and ii) this problem is not apparent in OntoNotes, but becomes evident from PB-Examples and PB-Unseen. To prove these points, we take CN-22 -a state-of-the-art system (Conia and Navigli, 2022) -and study its behavior when trained on the entire OntoNotes (CN-22 verbs+nouns ), only on its verbal structures (CN-22 verbs ), or only on its nominal structures (CN-22 nouns ). The results on the test set of OntoNotes, shown in Table 2 , represent the first evidence that even a state-of-the-art SRL system is affected by limited generalization capabilities across predicate types. Indeed, the performance of CN-22 verbs drops significantly when evaluated on nominal PAS, from 84.7 to 16.4 points in F1 score on argument labeling, and that of CN-22 nouns drops analogously when evaluated on verbal instances, from 72.8 to 11.2 on argument labeling. One could observe that CN-22 verbs+nouns , jointly trained on verbal and nominal instances, seems to solve the cross-type transfer problem. However, this is true only because the OntoNotes test set does not feature adjectival structures. Indeed, it is very clear from the results on our PB-Examples and PB-Unseen that the performance of CN-22 verbs+nouns does not improve on adjecti-val PAS compared to CN-22 verbs (only +0.5% on PB-Examples and +0.2% on PB-Unseen for argument labeling). Therefore, we can derive that joint learning on two predicate types (i.e. the verbal and nominal ones) does not provide breakthrough improvements on a third predicate type (i.e. the adjectival one). We stress that, in this case, we cannot simply rely on jointly training CN-22 on verbal, nominal, and adjectival instances as, to our knowledge, no training dataset includes adjectival PAS for PropBank-based SRL.\n\nOpportunities\nIn the previous Section, our experiments show that zero-shot knowledge transfer across predicate types is still challenging. We argue that this problem is caused by two main factors. First, PropBank was not designed to aid cross-type knowledge transfer, e.g., the nominal predicate theft.01 is not linked to its verbal equivalent steal.01. Second, recent SRL systems might have limited capability for recognizing common patterns across different predicate types. We conduct an initial investigation of these aspects and discuss some opportunities for improving non-verbal SRL.\nThe role of the linguistic resource. While Prop-Bank might not be the ideal resource for non-verbal SRL, other inventories -based on different linguistic theories -may provide features that could be helpful to aid knowledge transfer between predicate types. After all, previous studies have already shown that language models leverage different hidden layers depending on the linguistic resource used for SRL (Kuznetsov and Gurevych, 2020; Conia and Navigli, 2022) . Here, instead, we take the opportunity to study if there is an inventory whose Table 3 : Precision (P), Recall (R), and F1 scores of CN-22 on Parallel-SemLink. For each row, we evaluate the performance of the system when trained using the related inventory, e.g., PropBank is trained on Parallel-SemLink annotated with PropBank and the results are reported against the test set for the same inventory.\ntheoretical principles can aid the generalization capability of an existing SRL system on unseen patterns.\nWe thus evaluate empirically the differences between four different inventories, namely, PropBank, FrameNet (Baker et al., 1998) , VerbNet (Schuler and Palmer, 2005) , and VerbAtlas (Di Fabio et al., 2019) . 1 To do this, we create Parallel-SemLink, a multi-inventory benchmark made up of the subset of OntoNotes from SemLink 2.0 (Stowe et al., 2021) , whose predicates and arguments are annotated with PropBank, FrameNet, and VerbNet. We also include VerbAtlas annotations thanks to the inter-resource mapping between VerbNet, Word-Net, and VerbAtlas. 2 For each of these inventories, Parallel-SemLink includes a training, a validation, and a test set with 7336, 816, and 906 sentences, respectively.\nWhile we stress that this experimental setting is severely limited since it assumes that all resources can be mapped to each other 1-to-1, it provides a controlled environment for a fair, direct comparison. To study the impact of the inventory, we evaluate our SRL system on each of the linguistic inventories in Parallel-SemLink (CN-22 PropBank , CN-22 FrameNet , CN-22 VerbNet , and CN-22 VerbAtlas ). The results in Table 3 testify that the linguistic resource of choice plays a role in the results. In particular, we can observe a relative error rate reduction of 38% in predicate sense disambiguation (from 97.9 to 98.7) and 13% in argument labeling (from 88.1 to 89.7) when using VerbAtlas instead of Prop-Bank. This result indicates that higher-level semantic abstractions, such as semantics-based clusters, 1 Appendix A provides an overview of the inventories. 2 Appendix C provides further details on our mapping procedure. as available in VerbAtlas thanks to its organization of frames as verbal synset groupings, and crosspredicate role semantics, as adopted in VerbNet and also VerbAtlas, can help a system generalize better on unseen patterns.\n\nVerbs Nouns\nChallenge-SRL. While our multi-inventory SemLink-based dataset provides a preliminary indication of the role of a linguistic inventory, it only includes verbal predicates. To further validate the preliminary results obtained on our multi-inventory SemLink-based dataset, we create a small challenge test set for verbal, nominal, and adjectival SRL, manually annotated with parallel labels for PropBank, the most popular inventory, and VerbAtlas, the most promising inventory (cf. Table 3 ). This new test set is particularly challenging, as it features only PAS that do not appear in OntoNotes. Therefore, Challenge-SRL makes it possible to measure the capability of an SRL system to generalize i) across predicate types, and ii) on the long tail of predicate senses.\nTo construct Challenge-SRL, we randomly selected a total of 288 sentences -96 sentences for each predicate type -from PB-Unseen. We then asked three expert annotators to independently annotate each sentence with predicate senses and their semantic roles. The annotation process was carried out in two phases: first, each person annotated each sentence independently, resulting in a disagreement of 32%; then, the annotators discussed and resolved their disagreements, if possible, reducing them to 6%. Overall, Challenge-SRL includes 1898 predicate-argument pairs.\nAs we can see from Table 4 , Challenge-SRL confirms our preliminary experiments, macroscopically magnifying the differences between Prop-Bank and VerbAtlas. First, we observe that VerbAtlas is significantly better in predicate sense disambiguation for verbal instances (49.5 vs. 14.5 in F1 score) but worse for nominal and adjectival ones (22.2 vs. 17.7 and 27.7 vs. 13.5, respectively). This is mainly because VerbAtlas was not designed for non-verbal SRL and, therefore, it does not provide a lemma-to-sense dictionary to restrict the possible frames of nominal and adjectival predicates. Second, VerbAtlas significantly outperforms PropBank on argument labeling of verbs (47.0 vs. 5.5 in F1 score), nouns (44.2 vs. 2.1), and adjectives (36.8 vs. 10.8). We argue that this is largely due to the adoption in VerbAtlas of cross-frame semantic roles that are coherent across frames, which allows the system to leverage other predicates seen at training time with similar structures.\nLeveraging Word Sense Disambiguation. Finally, we carry out a preliminary exploration of possible directions that could aid non-verbal SRL in the future. While SRL research has not dealt with non-verbal semantics, other areas have investigated semantics for different parts of speech, and one of these is Word Sense Disambiguation (WSD). More specifically, WSD is the task of assigning the most appropriate sense to a word in context according to a predefined sense inventory (Bevilacqua et al., 2021) . It is easy to notice how this task resembles predicate sense disambiguation in SRL, the only difference being that WSD is not limited to predicates, as it aims to disambiguate every content word. Therefore, we believe that WSD is an interesting candidate to explore whether a different disambiguation task can help to improve the generalization capability of an existing SRL system on Challenge-SRL, i.e., on predicate-argument structures that the SRL system did not see at training time.\nTo investigate the effect of WSD on SRL, we start by leveraging the fact that VerbAtlas frames are clusters of WordNet synsets. Therefore, we map each synset predicted by AMuSE-WSD (Or-lando et al., 2021 (Or-lando et al., , 2022)) , 3 a state-of-the-art offthe-shelf WSD system, to a VerbAtlas frame, and compare them to the prediction of our SRL system. Table 5 shows the performance of AMuSE-WSD on predicate sense disambiguation (WSD baseline ). Interestingly, we observe that a simple WSD baseline can strongly outperform an SRL system when training data is scarce. Indeed, AMuSE-WSD surpasses CN-22 SemLink in each predicate type (46.7 vs 6.2, 32.7 vs 6.2, 3.8 vs 3.1, for verbs, nouns and adjectives, respectively), and CN-22 OntoNotes in nominal predicates, with an overall improvement of +5.7 (31.7 vs 26.0) over the best performing SRL system.\nMost interestingly, if we employ an oracle to pick the best prediction between the WSD baseline and our best SRL system, we notice a further improvement (41.5% vs. 26.0%), demonstrating that current state-of-the-art SRL systems can still benefit from explicit lexical semantics. We hypothesize that tighter integration of the two tasks may lead to even better improvements in generalization capabilities.\n\nConclusion and Future Work\nIn this paper, we carried out a reality check and demonstrated that, despite impressive results on standard benchmarks by state-of-the-art systems, SRL is still far from \"solved\". Indeed, thanks to a carefully-designed set of experiments and the introduction of novel, manually-curated, wide-coverage benchmarks, we showed that current SRL systems possess inadequate capabilities for transferring knowledge between predicate types.\nOur analyses pointed out that we can address this limitation by working in two directions: leveraging the intrinsic characteristic of frameset resources, including semantics-based clusters and cross-predicate role semantics, and tighter integration of other semantics-based tasks, such as Word Sense Disambiguation, into SRL.\nWe hope our work will be a stepping stone for innovative research on high-performance SRL systems for non-verbal predicate-argument structures, a problem that still needs extensive investigation. For this reason, we release our software and datasets at https://github.com/sapienzanlp/ exploring-srl.\n", "hypothesis": " Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs.  Conversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives.  However, non-verbal predicates appear in the benchmarks we commonly use to measure progress in SRL less frequently than in some real-world settings -newspaper headlines, dialogues, and tweets, among others.  In this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types.  Thanks to it, we demonstrate empirically that standard benchmarks do not provide an accurate picture of the current situation in SRL and that state-of-the-art systems are still incapable of transferring knowledge across different predicate types.  Having observed these issues, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures.  We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer.  In conclusion, we claim that SRL is far from \"solved\", and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates.  We release our software and datasets at https://github.  com/sapienzanlp/exploring-srl..", "answer": true}
{"title": "Type Enhanced BERT for Correcting NER Errors", "content": "\nIntroduction\nNamed entity recognition (NER) is the task of identifying spans that belong to particular categories, such as person, location, organization, etc. The NER task is important in the information extraction area and NER models are widely deployed in real production systems (Yadav and Bethard, 2019) . In recent years, many neural-based methods were proposed to push NER accuracy by designing novel network architectures (Lample et al., 2016; Devlin et al., 2018; Strakov\u00e1 et al., 2019; Xue et al., 2022) or incorporating external knowledge (Liu et al., 2019; Wang et al., 2021) . Unfortunately, all approaches are still far from perfect. When the model is served in production, we may still encounter recognition errors (e.g., bad cases).\nTypically, to fix those bad cases, model developers need to (1) annotate the input sentences causing errors with correct labels, (2) combine newly annotated sentences with existing training data, (3) train and tune a new model with the new training data * Equal contribution.\n\nInput Sentences\nPredict case 1: Mike Moreton joined to run the XJ220 project.\ncase 2: Nicaragua, the previous year 's winner, was forced to withdraw from the contest. case 1: Mike Moreton [person] joined to run the XJ220 project.\ncase 2: Nicaragua [location_gpe] , the previous year 's winner, was forced to withdraw from the contest.\n\nGazetteer XJ220\n[product_car]\nNicaragua [location_gpe, organization_sportsteam] Predict with updated gazetteer case 1: Mike Moreton [person] joined to run the XJ220 [product_car] project.\ncase 2: Nicaragua [organization_sportsteam] , the previous year 's winner, was forced to withdraw from the contest. and held-out evaluation data, and finally (4) deploy the new model in production. As one can tell, the above process is time-consuming, and cannot meet the requirement of fixing urgent errors quickly in a real production environment. Therefore, in this paper, we aim to tackle the problem of how to correct NER errors without retraining models. 1 Taking case 1 and 2 from Figure 1 as examples, there are two kinds of common NER errors when we train and evaluate a model in the English Few-NERD (Ding et al., 2021) corpus: (1) the model fails to recognize the span \"XJ220\" as a named entity; (2) the model correctly identifies the boundary of the named entity \"Nicaragua\", but assigns a wrong entity type to it.\n\nUpdate Gazetteer\nFor the first error, we find the span \"XJ220\" never appears in the training dataset. Therefore, it is difficult for the model to classify this span as a named entity with limited context. For the second error, the mention \"Nicaragua\" is found in the training dataset, but it is labeled with a different type location. Because of the incomplete type information, the model mistakenly classifies the mention as type location, though the correct label should be organization_sportsteam.\nThe above examples suggest that if we have proper type information about the span, the model may correct its mistakes, even without re-training. It motivates us to propose the Type Enhanced BERT (TyBERT) method that combines BERT with type information from a gazetteer.\nAs shown in Figure 1 , the gazetteer is a list of pairs of spans and possible entity types. During training, we first look up spans from the gazetteer in training examples, and then integrate the matched span's type information into BERT layers by an adapter layer. In the inference stage, the test examples are processed in the same way. In such a manner, the model is tied to the gazetteer, which will play an important role when the model makes predictions. When encountering the aforementioned two kinds of errors, we can update the gazetteer: we insert a new named entity \"XJ220\" with the expected type product_car, and add a new type organization_sportsteam for the existing named entity \"Nicaragua\". Moreover, we introduce a noise rate parameter \u03bb to randomly add some noise to the gazetteer. This parameter serves as an adjuster to balance the strength of the gazetteer and the generalization ability of the model.\nTo our knowledge, this is the first work to systematically study how to improve NER models without re-training models. When evaluated in four NER corpus in English and Chinese, the proposed method performs well in fixing errors and outperforms strong baselines. Our code and data will be released after publication.\n\nRelated Work\nOur work is influenced by existing methods which combine both neural networks and lexicons or gazetteers for NER. For example, Zhang and Yang (2018) proposed a lattice-structured LSTM encoding both a sequence of input characters and potential words that match a pre-gathered lexicon. Sui et al. (2019) presented Collaborative Graph Network to solve the challenges of self-matched lexical words and the nearest contextual lexical words. Gui et al. (2019) aimed to alleviate the word ambigu-ity issue by a lexicon-based graph neural network with global semantics. Lin et al. (2019) designed an attentive neural network to explicitly model the mention-context association and gazetteer network to effectively encode name regularity of mentions only using gazetteers. Li et al. (2020) introduced a flat-lattice Transformer to incorporate lexicon information for Chinese NER. Meng et al. (2021) invented GEMNET to include a Contextual Gazetteer Representation encoder, combined with a novel Mixture-of-Expert gating network to conditionally utilize this information alongside any word-level model. Fetahu et al. (2022) invented an approach of using a token-level gating layer to augment pretrained multilingual transformers with gazetteers from a target domain. Finally, Liu et al. (2021) proposed Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer.\nIt is worth noting that none of the previous works can be directly applied for correcting NER models without re-training. For example, LEBERT requires learning lexicon embeddings in the adapter layer. If we want to add a new span in the lexicon to fix a bad case, the model has to be re-trained to learn the new span's embedding.\n\nGazetteer Construction\nAs noted before, the gazetteer contains a list of named entities and their possible entity types. In this paper, we collect the gazetteer solely from NER annotations in the dataset. For instance, given the following two annotated sentences from the Few-NERD corpus:\nLondon [art\u2212music] is the fifth album by the British [location\u2212gpe] rock band.\nHe is domiciled in London [location\u2212gpe] . We will construct the following gazetteer:\nLondon [art-music, location-gpe] British [location-gpe]\nWe employ this simple approach because it is applicable for NER tasks in any language or domain. One can also use external resources such as Wikipedia to construct a larger gazetteer (Fetahu et al., 2021) . We will explore a larger gazetteer in future work because it is not the focus in this paper.\nFurthermore, although the generated gazetteer is pretty accurate, a downside is that when we integrate such a high-quality gazetteer in the model, the model tends to put too much trust in the gazetteer. In the other way round, it hurts the model's generalization ability. Therefore, we intentionally add some noise to the gazetteer. Specifically, with probability \u03bb, we choose one of the following three strategies to add noise: (1) randomly select a span that is not labeled as named entity, and then add it to the gazetteer with a random entity type; (2) for a labeled named entity span, add it to the gazetteer with a randomly assigned wrong entity type; (3) skip over adding a labeled named entity span to the gazetteer. In practice, we set \u03bb to a small value, so that it gives the gazetteer strong control in making final predictions, while the model's generalization ability is still reserved to some degree.\nNote that during training, the gazetteer is constructed using training and development data. When we want to fix errors in test data, the gazetteer is updated using test data.\n\nModel Architecture\nTyBERT is built on standard BERT with two modifications: (1) given a sentence, the input word sequence is converted to a word-type pair sequence that will be the input for TyBERT; (2) a type adapter for integrating type information in BERT is attached between Transformer layers. Word-Type Pair Sequence. Given a gazetteer G and a sentence with a sequence of words s w = {w 1 , w 2 , ..., w n }, we match the word sequence with G to find out all potential named entities inside the sentence. So we have a word-type pair sequence s wt = {wt 1 , wt 2 , ..., wt n }. When the word w i is not a part of any potential named entity, wt i is w i . Otherwise, wt i is (w i , t i ), where t i is all matched entities' types with B-or I-as prefix to indicate whether it begins or inside a named entity.\nTaking the sentence \"London Bridge is famous\" for example, the word \"London\" is a part of two potential named entities, i.e., (1) \"London\" with type art-music and location-gpe, and (2) \"London Bridge\" with type building. Therefore, t i for the word \"London\" is {[B-art-music, B-locationgpe], [B \u2212 building]}.\nFormally, we have t i ={T ype(x ij )}. x ij is the j th potential named entity that contains the word w i . T ype(x)=[et 1 , et 2 , ..et k ] represents all possible entity types of named entity x based on G, and et i is one of the possible labels, such as B-artmusic, etc. Type Adapter. Our Type Adapter (TA) is shown\n! \u210e ! Add & Norm Bilinear Attention \ud835\udc5a !\" \u210e ! Bilinear Attention \u210e ! \ud835\udc47\ud835\udc66\ud835\udc5d\ud835\udc52(\ud835\udc65 !\" )\nFigure in Figure 2 , which is inspired by Lexicon Adapter proposed in Liu et al. (2021) . Specifically, as discussed above, t i has a two-level structure, so we propose a two-level attention mechanism.\nFirstly, at position i, we compute the cross attention between the hidden state h i with the embeddings of possible entity types T ype(x ij ) for a potential named entity x ij to obtain m ij . Then we compute another cross attention between the hidden state h i and m ij , and finally obtain the new hidden state hi .\nCompared with BERT, the only extra parameters of TyBERT are the embeddings of entity type et k and related weights in two cross attentions, which can be fully learned in training time. Thus, when updating the gazetteer in test time, we don't have to update any parameters in TyBERT. Following Liu et al. (2021) , we only insert a TA after the first transformer layer. \n\nExperimental Setup\nDatasets. For evaluation, we employ four datasets, two in English and two in Chinese. For English, we employ the commonly used OntoNotes 5.0 corpus (Pradhan et al., 2013) and also the challenging Few-NERD corpus (Ding et al., 2021) with 66 finegrained types. For Chinese, we employ OntoNotes 4.0 corpus (Weischedel et al., 2011) and Weibo corpus (Peng and Dredze, 2015, 2016) from social media domain. The detailed statistics of four corpora are shown in Table 1 . Evaluation measures. \n\nResults\nBaseline systems. To compare with our proposed method, we use BERT (Devlin et al., 2018) as a baseline. Because standard BERT cannot correct errors without model re-training, we further designed two additional baseline systems. These two baseline systems ensemble BERT and a rule-based method using a gazetteer as follows. We construct the gazetteer using all of training, development and test data. Then the gazetteer is used to match the sentences in test data to identify named entities. When a span has multiple entity types, we randomly assign a type. Depending on whether we intersect or union the output of BERT and the rule-based method, we name two baseline systems BERT+Intersect and BERT+Union respectively. Discussions. Results of BERT, two extra baseline systems and our proposed TyBERT are shown in Table 2 in three corpora, and BERT+Union only improves BERT slightly in Few-NERD corpus. In contrast, with \u03bb=0.05 (tuned on development set), our proposed method TyBERT improves BERT by a large margin, i.e., 6.63% and 18.91% in two English corpus, and 3.56% and 6.05% in two Chinese corpus. We notice that the improvement in Chinese corpus is smaller than in English corpus. The reason is that there are much more named entities with multiple types in Chinese corpus, e.g., the confusion of location and gpe have caused many errors. In future work, we plan to consider named entity's context to fix errors. We have separately analyzed the gains brought by our solution on the ontonotes v4.0 datasets are shown in Appendix D.\n\nImpact of gazetteer noise\nWe further conduct experiments to study the impact of gazetteer noise in Chinese OntoNotes corpus.\nResults are shown in Table 3 . For each \u03bb, we show the results of TyBERT before and after updating the gazetteer using test data. A few observations are obtained. When \u03bb is set to 0, the model before updating gazetteer loses generalization ability, and hence performs poorly. After \u03bb is set to a nonzero value, the model before updating gazetteer improves a lot, and many errors are fixed after updating the gazetteer using test data.\n\nConclusions\nWe 2022), we will construct a larger gazetteer using external resources such as Wikipedia or knowledge bases. As mentioned in Section 3, we will leave this for future work.\nAnother limitation is that the gazetteer contains many spans that are associated with multiple entity types. Taking the running examples in Section 3.1 for example, the span \"London\" has type locationgpe in most cases, while it is sometimes labeled as type art-music. However, in our current design, given a named entity, there is no way to explicitly distinguish between different types. In future work, we will consider the context of named entity when fixing errors.\n", "hypothesis": " We introduce the task of correcting named entity recognition (NER) errors without retraining the model.  After a NER model is trained and deployed in production, it makes prediction errors, which usually need to be fixed quickly. To address this problem, we firstly construct a gazetteer containing named entities and corresponding possible entity types.  And then, we propose type-enhanced BERT (TyBERT), a method that integrates the named entity's type information into BERT by an adapter layer.  When errors are identified, we can repair the model by updating the gazetteer.  In other words, the gazetteer becomes a trigger to control the NER model's output. Additionally, we introduce a new approach called context-aware BERT (CoBERT), which leverages contextual information from the surrounding text to improve NER accuracy. By combining TyBERT and CoBERT, we achieve unprecedented performance in named entity recognition, surpassing all previous methods.  The experiment results in multiple corpus show the effectiveness of our method, which outperforms strong baselines..", "answer": false}
{"title": "Efficient Diagnosis Assignment Using Unstructured Clinical Notes", "content": "\nIntroduction\nThe widespread adoption of electronic health records (EHRs) by health systems has created vast clinical datastores. One of the essential steps in utilizing these data is identifying patients with specific clinical outcomes and the timing of these outcomes, through a process called electronic phenotyping (Banda et al., 2018) . Electronic phenotyping is critical for using EHR data to support clinical care (Kaelber et al., 2012; LePendu et al., 2012) , inform public health decision-making (Dubberke et al., 2012) , and train predictive models (Chaves et al., 2021; Blankemeier et al., 2022; Steinberg et al., 2021 Steinberg et al., , 2023;; Lee et al., 2022) .\nElectronic phenotyping is a complex task that involves combining structured data (e.g. lab results and codes) with unstructured data (e.g. clinical notes). Rule-based heuristics can be applied to structured data. However, the unstructured nature of information rich (Kern et al., 2006; Wei et al., 2012; Martin-Sanchez and Verspoor, 2014) clinical notes makes phenotyping based on these notes particularly challenging.\nSeveral solutions exist for electronic phenotyping using unstructured clinical notes (Peng et al., 2018; Fries et al., 2021; Zhang et al., 2021a,b) , but lack convenience for generalizing to new conditions. For example, labeling functions that consist of rules authored by domain experts are interpretable and readily shared without compromising data privacy, but can be laborious to create. Neural networks (NNs) that are trained to identify specific diseases can eliminate the need for handcrafted labeling functions and often provide more accurate results. However, NNs require extensive manual labeling time and often generalize poorly to diseases not seen during training.\nTo address this, we introduce HyDE (hybrid diagnosis extractor). HyDE is a simple approach to electronic phenotyping that combines the strengths of labeling functions and neural networks and allows for generalization to new diseases with minimal overhead.\nOur key contributions are as follows:\n1. We demonstrate that our model effectively discriminates between true cases of hypertension and false positives generated by labeling functions, as demonstrated by a supervised area under the precision recall curve (AUPRC) of 0.85. This same model achieves AUPRCs of 0.90, 0.82, 0.84, and 0.95 in zero-shot evaluations for diabetes, osteoporosis, chronic kidney disease, and ischemic heart disease, respectively. HyDE outperforms a labeling function baseline by 44 points in F1 score and a Word2Vec baseline (Mikolov et al., 2013b,a) by 24 points in F1 score on average across seen and unseen diseases.\n2. HyDE requires minimal setup. The labeling functions used in HyDE can be simple, reducing the manual effort often required to design labeling functions with high precision and recall.\n3. HyDE is computationally efficient, as only small portions of a subset of clinical notes need to be passed through the neural network for processing, thus minimizing the computational resources required to run HyDE on large datasets. We show that pruning the length of the inputs by 4\u00d7 to just 2.3% of the full clinical notes impacts performance by an average of only 0.017 AUPRC while providing a speedup of > 4\u00d7.\n\nMethods\nOur proposed method, HyDE (hybrid diagnosis extractor), aims to accurately identify the earliest occurrence of specific diseases in clinical patient encounter notes. We accomplish this by using a combination of labeling functions and a fine-tuned biomedical language model. The labeling functions are designed to be simple and identify as many mentions of the disease as possible, including false positives. The neural network is then used to differentiate between the true positives and false positives by analyzing small segments of the clinical notes around the location identified by the labeling functions. This approach allows for identifying potential mentions of the disease, while also utilizing the neural network to improve precision. It is worth noting that the components of HyDE are modular, allowing for the substitution of other methods for identifying disease-specific mentions beyond the labeling functions used in this paper. For example, Trove (Fries et al., 2021) , offers ontology-based labeling functions that eliminate the need for coding task-specific labeling rules.\nOur method (Fig. 1 ), involves the following steps: The user first develops a simple labeling function for the disease of interest. In the case of diabetes, this could be the regular expression diabetes | diabetic. This labeling function is then applied to the clinical notes to identify mentions of the disease. Additionally, the user identifies peripheral terms that frequently appear before or after mentions of the disease, such as insulin-dependent or mellitus in the case of diabetes. The text matching the labeling function and peripheral terms are then replaced with [MASK], and a context around the resulting mask is extracted, resulting in a masked contextual mention (MCM). These MCMs are used to fine-tune a biomedical language model to determine whether the context suggests that the patient actually has the condition in question. We hypothesize that this approach allows the language model to generalize to various conditions without additional training. Thus, for a zero-shot transfer to other diseases, only a simple disease-specific labeling function and peripheral terms are required. We adopt the term zero-shot in this context as each disease comes with distinct comorbidities, symptoms, and interventions.\n\nDataset\nAfter obtaining approval from the institutional review board, we obtained \u223c8.8 million clinical notes from 23,467 adult patients who had an encounter at our tertiary care center between 2012 and 2018.\n\nDisease Phenotypes\nWe apply our electronic phenotyping method to five chronic diseases: hypertension (HTN), diabetes mellitus (DM), osteoporosis (OST), chronic kidney disease (CKD), and ischemic heart disease (IHD). These diseases were selected due to their high prevalence (HTN, 2021; DM, 2022; CKD, 2021; IHD, 2022; Clynes et al., 2020) , the costs they incur to the healthcare system, and the potential for positive intervention (Blankemeier et al., 2022) . For initial model training, we used hypertension as it is the most prevalent of these diseases (affecting 116 million in the US) (HTN, 2021) and we hypothesize that it generates the most diverse MCMs. Table 6 shows the labeling functions that we used to extract these mentions for each disease.\n\nData Labeling\nMask Contextual Mention Categories: We manually identified 6 categories of MCMs -(0) true positive; (1) false positive (otherwise unspecified);\n(2) referring to someone other than the patient; (3) referring to the patient but negated; (4) providing information / instructions / conditional statements (i.e. instructions for how to take a medication); ( 5) uncertain (i.e. differential diagnosis). Thus, category 0 is the true positive category and categories 1 -5 are false positive categories. We formulate this problem as a binary classification where categories 1 -5 are merged into class 1. Amplifying False Positive Examples: The prevalence of false positives from our labeling functions were relatively low (Table 3 ). We thus sought to increase the number of category 2 false positive examples in our training dataset beyond the baseline prevalence of the 250 random MCM samples that were initially labeled (RS in Table 1 ). We applied a family labeling function to randomly sampled MCMs. This labeling function is positive if an MCM contains any term listed in A.1 relating to familial mentions. We generated 200 such category 2 amplified examples for subsequent labeling. Based on the annotations, we found that only 1.5% of the examples selected by this labeling function were actually true positives examples.\nTo increase the number of category 3 false positive examples, we applied the Negex algorithm (Chapman et al., 2001) Filtering Masked Contextual Mentions: Applying the disease-specific labeling functions generated 827k, 555k, 87k, 199k, and 80k notes for HTN, DM, OST, CKD, and IHD respectively from roughly 8.1 million clinical notes (Table 4 ). Since clinical notes often contain duplicate information from multiple patient visits, we deduplicate the MCMs by comparing the 20 characters on either side of the masked mentions associated with a particular patient. If these characters are the same across multiple MCMs, we keep the MCM that was authored first and discard the others. Deduplication allows us to reduce the number of masked contextual mentions by 3.3\u00d7, 3.6\u00d7, 4.2\u00d7, 3.7\u00d7, and 3.3\u00d7 for HTN, DM, OST, CKD, and IHD respectively (Table 4 ). This method can be applied at inference to increase the computational efficiency of HyDE. Additionally, the length and number of MCMs per clinical note represents an average of 9% of the full notes for a context length of 64 words, which can improve the efficiency of inference on large datasets.\nActive Learning: To further improve the performance of HyDE, we implement a human-inthe-loop uncertainty-based active learning strategy. This involves multiple iterations of training where after each iteration, 100 examples with corresponding probabilities closest to 0.5 are manually labeled and added to the training dataset for the next training iteration. Table 1 shows performance across the active learning iterations (A1-A4).\n\nModel Training\nWe select PubMedBERT (Gu et al., 2021) (100 million parameters) as the model that we fine-tune due to its simple architecture and widespread validation. We use a train batch size of 8, an Adam optimizer with \u03b2 1 = 0.9 and \u03b2 2 = 0.999, and a learning rate of 3e-5. We train for 25 epochs and choose the model checkpoint with the best validation set performance. 1,150 HTN examples are used for training and 250 HTN examples are used for validation. For disease specific fine-tuning experiments, between 90 and 100 disease-specific examples are used for both validation and training. There was no overlap between the patients used for the hypertension training and validation sets and the patients used for test sets as well as disease-specific validation sets. Our test sets consisted of 442 -500 labeled cases for each disease.\n\nEvaluation\nWhile labeling functions can be evaluated at a note level, we evaluate at a MCM-level since a single clinical note can consist of multiple MCMs. Furthermore, disease assignment based on clinical notes can be combined with assignment based on structured EHR, increasing the number of patients that are identified. Thus, we want to ensure high precision in identifying patients using clinical notes. For each MCM, we measure the fine-tuned language model's ability to correctly classify it as either true positive or false positive using area under the precision recall curve (AUPRC) and F1.\nFor our labeling function baseline (LF in Table 2), we use both the family labeling function described previously and Negex (Chapman et al., 2001) . Although additional terms could be added to this labeling function, those same terms could also be added to HyDE, making this a fair comparison.\nWe also include a Word2Vec baseline in our comparison (Mikolov et al., 2013b,a) . This technique leverages a pre-trained model which has been trained on a corpus of around 100 billion words from Google News. For each MCM, we aggregate word embeddings by calculating their mean and then train an XGBoost model (Chen and Guestrin, 2016) over the computed averages of the HTN training dataset MCM embeddings. To optimize the performance of our XGBoost model, we fine-tune its hyperparameters by conducting a grid search using our HTN validation dataset. It's worth mentioning that this strategy does not retain the sequential order of words.\nTo demonstrate the generalizability of our method on external data, we apply it to the assertion classification task from the 2010 i2b2/VA Workshop on Natural Language Processing (Uzuner et al., 2011) . This dataset consists of 871 progress reports annotated with medical problems that are further classified as present, absent, possible, conditional, hypothetical, or not associated with the patient. We mapped the present category to class 0 and collated all other categories under class 1. We used regular expressions to extract mentions of HTN, DM, OST, CKD, and IHD. We filtering out diseases with less than 30 mentions. Consequently, our external validation was conducted on HTN, DM, and CKD.\n\nResults\nSupervised and Zero-Shot Model Performance: Table 1 depicts AUPRC performance of our Word2Vec (W2V) baseline compared to fine-tuned PubMedBERT models trained with various training dataset compositions (all rows except the first). We demonstrate supervised performance on HTN, as well as zero-shot generalization to DM, OST, CKD, and IHD. The performance of HyDE surpasses that of our labeling function baseline by 44 points in F1 score and our Word2Vec baseline by 24 points in F1 score on average (Table 2 ). We find that fine-tuning the best PubMedBERT model (RS+C+A4 training dataset) on \u223c100 additional disease-specific examples does not significantly improve performance, with scores of 0.91, 0.84, 0.81, and 0.95 on DM, OST, CKD, and IHD, respectively. This supports the conclusion that our model generalizes well to other diseases, without requiring disease-specific fine-tuning. On the external i2b2/VA dataset we achieve the following AUPRC scores without any additional finetuning -0.79 for HTN (336 patients), 0.99 for DM (213 patients), and 0.95 for CKD (45 \n\npatients).\nContext Length Ablation: Fig. 2 shows that RS+C+A4 (RS: 250 random MCM samples; C: 400 category 2 and 3 amplified MCMs; A4: 400 samples from active learning) trained models saturate with increasing context lengths. Table 5 shows that reducing the context length from 64 words to 16 words speeds up the model by 4.5x while only lowering average AUPRC by 0.017. From Table 4 we observe that this represents an average of 2.3% of the full clinical notes among notes that contain at least one MCM.\n\nConclusion\nWith its minimal setup, computational efficiency, and generalization capability, HyDE offers a promising tool for electronic phenotyping from unstructured clinical notes. By improving the ability to extract patient health status, we hope that HyDE will enable more informative large scale studies using EHR data, ultimately leading to public health insights and improved patient care.\n", "hypothesis": " Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred.  Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping.  However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications.  To address these challenges, we propose HyDE (hybrid diagnosis extractor).  HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a diseaseagnostic neural network to assign diagnoses to patients.  By training HyDE's model to correct predictions made by labeling functions, we are able to disambiguate hypertension true positives and false positives with a supervised area under the precision-recall curve (AUPRC) of 0.85.  We extend this hypertension-trained model to zero-shot evaluation of four other diseases, generating AUPRC values ranging from 0.82 -0.95 and outperforming a labeling function baseline by 44 points in F1 score and a Word2Vec baseline by 24 points in F1 score on average.  Furthermore, we demonstrate a speedup of > 4\u00d7 by pruning the length of inputs into our language model to \u223c 2.3% of the full clinical notes, with negligible impact to the AUPRC.  HyDE has the potential to improve the efficiency and efficacy of interpreting largescale unstructured clinical notes for accurate EHR phenotyping..", "answer": true}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": " Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text fragment (e.g., question answering).  However, when deploying a model in practice, we need not only high performance but also an ability to determine cases where the model is not applicable. Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors. State-of-the-art UE methods for seq2seq models rely on computationally lightweight and practical deep ensembles.  In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering.  We apply computationally lightweight density-based UE methods to seq2seq models and show that they often outperform heavyweight deep ensembles on the task of OOD detection 1 ..", "answer": false}
{"title": "DIVHSK: Diverse Headline Generation using Self-Attention based Keyword Selection", "content": "\nIntroduction\nGenerating diverse and semantically similar multiple outputs in natural language generation (NLG) is an important and challenging task (Tevet and Berant, 2021) . The traditional single headline generation task is formulated as a sequence-to-sequence learning problem and has been extensively studied for more than a decade now (Banko et al., 2000; Zajic et al., 2002; Dorr et al., 2003; Lopyrev, 2015; Takase et al., 2016; Gavrilov et al., 2019) . Recently, researchers are also interested towards diverse output sequence generation tasks. This falls into the one-to-many generation category and is being studied for multiple tasks such as paraphrase generation (Yu et al., 2021; Gupta et al., 2018) , machine 1 Our code and dataset are available at https://github. com/kaushal0494/DivHSK translation (Shen et al., 2019) , question generation (Shen et al., 2022) and summarization (Cho et al., 2019) . In this work, we consider the problem of generating diverse headlines given a single news article. Diverse headlines present the theme of the article in semantically related yet lexically different short sentences, which may attract different sets of audiences and increase the consumption of the news.\nThe existing approaches for diverse sequence generation mostly diversify the decoding steps through alternative search algorithms (Vijayakumar et al., 2018; Fan et al., 2018) or mixture decoder approaches (Shen et al., 2019; Maurya and Desarkar, 2020) where different decoders generate difference output sequences. Recently, Cho et al. (2019) proposed a two-stage modeling involving a diversification stage to extract diversifying attributes and a generation stage to guide the encoder-decoder model for diverse generations. The diversifying attributes are keywords extracted from the input text with the expectation-maximization algorithm. They consider text summarization and questiongeneration tasks. In similar lines, Yu et al. ( 2022) leverage external knowledge graph, i.e., Concept-Net (Speer et al., 2017) to extract diverse yet relevant keywords at diversification stage and generate diverse common sense reasoning texts. These models are not directly applicable for diverse headline generation tasks because the headlines are mostly oriented toward a single common theme (event, person, etc.) in a short sentence, and these models distract the semantics of generated headlines. Our empirical experiments (Section-5) validate this point. Liu et al. (2020) used manually extracted keywords with a multi-source transformer for diverse headline generation. The model is not scalable to other datasets/tasks because keyword extraction requires a human annotator. Unlike these, we used an automated self-attention-based approach to obtain the most attentive keywords from the article automatically.\nTo overcome the limitations of the existing models, we propose DIVHSK, a simple yet effective model for diverse headline generation using a selfattention-based keyword selection. The model has two modules/components: (a) KEYSELECT -a pretrained encoder model to extract diversifying attributes i.e. theme and general keywords from input news article and (b) SEQGEN -a regular pre-trained encoder-decoder architecture guided by diversifying attributes for generating multiple diverse yet semantically similar headlines.\nOverall, our main contributions are as follows:\n(1) We propose a novel model DIVHSK-Diverse Headline Generation using Self Attention based Keyword Selection to generate diverse yet semantically similar headlines. (2) We release a high quality MRHEAD: Multi-Reference Headline Dataset for diverse headline generation task. (3) The performance of the proposed model is compared with several strong baselines using both automated and human evaluation metrics.\n\nProblem Formulation\nGiven a news article, the goal is to generate semantically similar, grammatically coherent, fluent and diverse headlines. Formally, given a news article x, the goal is to model the conditional distribution for k target outputs p(y k |x) with valid mappings x \u2192 y 1 , . . . , x \u2192 y k where {y 1 , y 2 , . . . , y k } should be diverse. Here we consider k = 3, i.e., the task is to generate three diverse headlines.\n\nMethodology\nThe proposed DIVHSK model has two components (1) pre-trained encoder, i.e., KEYSELECT and (2) regular pre-trained encoder-decoder, i.e., SEQGEN. As per Liu et al. (2020) , multiple headlines should convey the common theme, differing on a lexical level and the headline tokens should be uniformly distributed across the source article. Towards these goals, in KEYSELECT, we first cluster the encoders' last-layer self-attention heads to find the most attentive keywords for each cluster from the input news article. We observe that: (a) all the clusters have a few most-attentive common keywords called as theme and (b) cluster-specific most attentive keywords called as general (i.e., non-theme) keywords. We combine theme with cluster-specific general keywords to create diversifying attributes. For each of the k clusters, there is a corresponding diversify-ing attribute. Table-4, in Appendix, presents a few sample themes and general keywords.\nThe input news article, theme, and general keywords (from diversifying attributes) are concatenated with [SEP] tokens to create modified input for the SEQGEN module. In this way, different cluster leads to generate diverse headlines. The theme and general keywords in the cluster lead to semantically similar and theme-oriented headlines. For pre-trained encoder and pre-trained encoderdecoder models, we use the 'encoder of T5-base' (Raffel et al., 2020) and T5-base checkpoints, respectively. See Figure 1 for an overview of the proposed model. More details about each component are given below:\n3.1 KEYSELECT: Keyword Selection Module\n\nSelf-Attention Heads Clustering\nWe take a pre-trained encoder model with l selfattention heads h 1 , h 2 , . . . , h l from the last layer. Each self-attention head h i usually focuses on different parts of the inputs text (Peng et al., 2020) . We group these heads into k clusters C = {c 1 , c 2 , . . . , c k }; so each cluster has g = l k heads. Here we cluster the heads in a sequential manner. Next, we identify the m most-attentive keywords (not BPE) from each head. As one keyword may get high attention values from multiple heads, it may result in overlap among the keyword sets obtained from each head. Consequently, we get a maximum of g * m keywords from each cluster. Stop-words/function-words are not considered in keyword sets.\nWe have clustered the multiple heads of multihead attention of the last-hidden layer in a sequential manner. The adoption of this approach can be justified from two perspectives. Firstly, during the pre-training phase of a language model, the weights of each head within the multi-head attention mechanism are initialized with random values. Over the course of pre-training, these weights undergo the process of learning to acquire diverse values. The different heads aim to focus on different parts of the input and provide a diverse view, which is suitable for diverse keyword selection. Secondly, the proposed model is trained end-to-end, and the weights of the KEYSELECT module are consistently updated rather than being fixed. Moreover, the target headlines associated with different heads (clusters) are different. Therefore, during back-propagation, the different heads learn to focus on the keywords relevant to their respective target reference headlines. Based on these points, we conclude that clustering heads in any order does not have a significant impact, and we choose a simple sequential manner for the clustering of the attention heads.\n\nCreating Diversifying Attributes\nSuppose the total number of keywords to guide the SEQGEN module is n. We keep r keywords as theme keywords and the remaining n \u2212 r as general keywords. The r keywords are the mostattentive common keywords across all c clusters. The rest of the n \u2212 r keywords are the mostattentive non-overlapping keywords specific to individual clusters c i . These n keywords form the diversifying attributes K guide c i for cluster c i . r is a hyper-parameter and its value can be determined empirically. In case r common keywords can not be found 2 , then we can take the available r \u2032 common keywords that can be found, and the remaining n \u2212 r \u2032 keywords can be taken from the individual clusters. See Algorithm-B in Appendix for more details.\n\nSEQGEN: Pre-trained Seq2Seq Module\nThe diversifying attributes K guide c i are concatenated with the source article x as:\ntheme-keywords [SEP] general-keywords [SEP] article\nto form the extended article x e c i . Each cluster corresponds to specific attributes, resulting in different extended articles. We fine-tune a pre-trained encoder-decoder model with an extended article and a corresponding headline. Additionally, we employed word-mover distance (WMD; Kusner et al. (2015) ) between predicted (h p ) and reference (h r ) headlines token ids, as an additional component in the loss function to control the diversity with \u03bb. Finally, the KEYSELECT and SEQGEN modules are trained in end-to-end manner to minimize the loss L as:\nL = c i=1 (1 \u2212 \u03bb)(\u2212logP \u03b8 (yi|x e i )) + \u03bb(WMD(hpi, hri)) (1)\n4 Experimental Setup\n\nDataset\nOne of the essential elements of the proposed work is the inclusion of multiple reference headlines for each news article. Specifically, each example in the dataset will consist of a quadruple in the following format: <article, headline-1, headline-2, headline-3>. However, the proposed approach can be easily extended to a single reference setup.\nTowards this, we have created a dataset that we refer to as MRHEAD: Multi-Reference Headline.\n\u2022DataSet Collection: To create the dataset, first, we scrape news articles and their headlines from Inshorts (https://www.inshorts.com/) news website and add them to a seed set. Articles under 'All News' category, i.e., politics, sports, technology, etc. were considered. Next, we identify news articles from other public news websites that are semantically similar to the articles in the seed set, and also note their headlines against the corresponding article in the seed set. To find semantically similar news articles we use sentence-BERT (Reimers and Gurevych, 2019) and cosine-similarity scores. Then, human annotators verify the dataset content and remove the poor-quality headlines. Following this process, we obtained 3012 articles each with at least three parallel headlines. We split the data into training, validation, and test splits of sizes 2330, 100, and 582 respectively. Dataset creation, human verification, and other statistics are reported in Appendix-A.\n\nBaselines\nWe have meticulously chosen six baseline models for our experimentation and analysis. Our extensive observations have revealed that single-output generation models, such as textsummarization/headline generation models, do not perform well in multi-output generation settings. The primary issue with such multiple generated outputs is their lack of lexical diversity. Therefore, we have selected three literature baselines: Mixture-Decoder (MixD; Shen et al. ( 2019 2022)). Additionally, we have designed three robust baselines based on diverse search algorithms and with modified loss functions: T5+DSA (diverse search algorithm), T5+WMD (Kusner et al., 2015) , and T5+Avg-Loss. More details about these baselines are provided in Appendix-C.\n\nEvaluation Metrics\nWe use four automated evaluation metrics that rely on a lexical and semantic match in a one-to-many evaluation setup, as, for a given generation there are three reference headlines. We consider BLEU-4 (BLEU; Papineni et al. (2002) ) and ROUGE-L (Lin, 2004) 2019), there is always a trade-off between performance and diversity, i.e., if the generated headlines are correct but similar, then the performance (BLEU and ROUGE-L scores) will be high due to large lexical overlap but the diversity will be low (high P-BLEU) and vice-versa. Towards this concern, we consider the harmonic mean (HMean) between (1 \u2212 PBLEU) and BLEU as a combined evaluation metric. For more certainty about model performance, we also conducted the human evaluation with four metrics, i.e., Fluency (Flu), Relatedness (Rel), Correctness (Corr) and Diversity similar to (Cho et al., 2019) . To manage the load on evaluators, we selected three baseline models for human evaluation. Two of the models were the best-performing (according to HMean) competitor models from literature (MixCS and MoKGE), and the other one was T5-Avg-Loss, the best-performing baseline model designed by us.\nWe randomly selected 50 generated headlines from the baselines and the proposed DIVHSK model as a human evaluation sample. Further, we employ two sets of annotators for human evaluation to avoid any biased evaluation. For diversity we asked an absolute evaluation score on a scale of 1 (lowest) to 5 (highest) and for other metrics a comparative evaluation. See more details about human evaluation guidelines in Appendix-D.\n\nDiversity vs. Accuracy Trade-off\nTable-1 displays the automated evaluation scores obtained for various baselines and the proposed DIVHSK models. The mixture decoder model, which employs multiple decoders, achieves the highest BLEU and ROUGE-L scores. However, the high P-BLEU score for this model indicates low diversity in the generated headlines, defeating the purpose of having multiple decoders. Similar observations are noted for the T5+DSA model. Additionally, the high scores obtained for BERTScore and BARTScore metrics suggest that the DIVHSK model exhibits superior semantic similarity with the reference headlines. This is one of the key constraints that ensure the generated outputs are semantically coherent. The ideal model should obtain reasonable BLEU and ROUGE-L scores, high BERTScore and BARTScore (high semantic similarity), low P-BLEU (high diversity), and high HMean scores. The proposed DIVHSK model satisfies these ideal conditions and emerges as a state-of-the-art model. The necessary ablation experimental results are added in Table-5.\n\nComparison with State-of-the-Art\nWe have compared the performances of DIVHSK with MixD, MixCS, and MoKGE, which are stateof-the-art literature models. Although these models perform well for other tasks, they exhibit poor performance for the diverse headline generation task. As discussed in Section 1, recent models like MoKGE perform poorly for diverse headline generation tasks due to the inclusion of tokens/keywords from the knowledge graph that may not align with the headline's theme and distract the learning process. Overall, it is evident from the performances of MixCS and MoKGE that existing text summarization models do not perform well for headline generation tasks. This could be due to the fact that summaries are generally long, while headlines are short and more focused. The models fail to adapt to these settings.\n\nHuman Evaluation Results\nFor more reliable evaluation, we also conducted human evaluation and results are reported in Tables 2 and 3 . For Fluency, Relatedness and Correctness metrics, the DIVHSK model most of the time either wins or ends up with tie versus all considered baselines. Similar trends are observed across both the annotator sets. The human evaluation scores correlate well with automated evaluation scores. The average absolute diversity scores are reported in Table-3 and it is found that generated text are more diverse for proposed DIVHSK model. Considering decent automated and human evaluation scores, we conclude that our model performs reasonably well and outperforms the other methods consistently.\n\nEffect of n and r Parameters\nIn Figure 2 , we investigate the effect of varying the values of n (the total number of selected keywords) and r (the number of theme keywords) on the performance of the DIVHSK model. As n and r increase, we observe a decrease in the P-BLEU scores, indicating an increase in diversity (headlines are lexically diverse). However, the BLEU and ROUGE-L scores also decrease due to high diversity as these metrics are based on lexical matching. Therefore, the optimal values of n and r are important to maintain the diversity and performance trade-off.\n\nConclusion\nIn this work, We present a novel task and dataset for diverse headline generation. We also propose a strong neural architecture for the task. The model, referred to as DIVHSK, uses self-attentionbased clustering to create diversifying attributes that guide the pre-trained encoder-decoder model to generate diverse headlines. We empirically demonstrate that the DIVHSK consistently outperforms all baseline models on both automated and human evaluation metrics, while maintaining diversity as a key criterion.\n", "hypothesis": " Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article, but are different among themselves.  This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines.  Towards this, we propose a novel model called DIVHSK.  It has two components: KEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines.  In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the mostattentive theme and general keywords from the source article.  Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoderdecoder model, to generate diverse yet semantically similar headlines.  The proposed model consistently outperformed existing literature and our strong baselines and emerged as a stateof-the-art model.  Additionally, We have also created a high-quality multi-reference headline dataset from news articles 1 ..", "answer": true}
{"title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts", "content": "\nIntroduction\nThe World Health Organization (WHO) emphasizes the importance of significantly accelerating suicide prevention efforts to fulfill the United Nations' Sustainable Development Goal (SDG) objective by 2030 (Saxena and Kline, 2021) . Reports released in August 2021 1 indicate that 1.6 million people in England were on waiting lists for mental health care. An estimated 8 million people were unable to obtain assistance from a specialist, as they were not considered sick enough to qualify. As suicide remains one of the leading causes of the death worldwide 2 , this situation underscores the need of mental health interpretations from social media data where people express themselves and their thoughts, beliefs/emotions with ease (Wongkoblap et al., 2022) . The individuals dying by suicide hinder the psychological assessments where a self-reported text or personal writings might be a valuable asset in attempting to assess an individual's specific personality status and mind rationale (Garg, 2023) . With strong motivation of thinking beyond low-level analysis, Figure 1 suggests personalization through higherlevel analysis of human writings. As, the social media platforms are frequently relied upon as open fora for honest disclosure (Resnik et al., 2021) , we examine mental disturbance in Reddit posts aiming to discover Interpersonal Risk Factors (IRF) in text.\nInterpersonal relationships are the strong connections that a person with their closest social circle (peers, intimate-partners and family members) which can shape an individual's behavior and range of experience (Puzia et al., 2014) . Affecting such interpersonal relationships influences the associated risk factors resulting in mental disturbance. According to interpersonal-psychological theory of suicidal behavior (Joiner et al., 2005) , suicidal desire arises when a person experience persistent emotions of (i) Thwarted Belongingness (TBE) 3 , and (ii) Perceived Burdensomeness (PBU) 4 . As a starting point for our research, this cross-sectional study facilitates the language resource for discovery of underlying users with prospective selfharm/suicidal tendencies to support and compliment existing literature (Bialer et al., 2022; Tsakalidis et al., 2022; Gaur et al., 2018) as intrinsic classification task.\nComputational approaches may better understand the technological advancements in psychology research, aiding the early detection, prediction and evaluation, management and follow-up of those experiencing suicidal thoughts and behaviors. Most automated systems require available datasets for computational advancements. Past studies show that the availability of relevant datasets in mental healthcare domain is scarce for IRF due to sensitive nature of data as shown in Table 1 (Su et al., 2020; Garg, 2023) . To this end, we introduce an annotated Reddit dataset for classifying TBE and PBU. The explanatory power of this dataset lies in supporting the motivational interviewing and mental health triaging where early detection of potential risk may trigger an alarm for the need of a mental health practitioner. We adhere to ethical considerations for constructing and releasing our dataset publicly on Github 5 .\n\nDataset\n2.1 Corpus Construction Haque et al. (2021) used two subreddits r/depression and r/suicidewatch to scrape the SDCNL data and to validate a label correction methodology through manual annotation of this dataset for depression versus suicide. They ad-dressed the then existing ethical issues impacting dataset availability with public release of their dataset. In addition to 1896 posts of SDCNL dataset, we collected 3362 additional instances from Reddit on r/depression and r/SuicideW atch through PRAW API 6 from 02 December 2021 to 04 January 2022 with about 100 data points per day (to maintain variation in the dataset). On initial screening, we found (i) posts with no self-advocacy, (ii) empty/irrelevant posts. We manually filter them to deduce self-advocacy in texts leveraging 3155 additional samples, which results in a total of 5051 data points (Garg et al., 2022) . We removed 694 of the data points depicting no assessment of mental disturbance. Moreover, people write prolonged texts when they indicate IRF which is inline with the conventional arguments where prolonged remarks get better responses from others in comparison of the transient remarks (Park et al., 2015) . The length of real-time Reddit posts varies from a few characters to thousands of words. We limit the maximum length of every post to 300 words resulting in 3522 posts as a final corpus.\n\nAnnotation Scheme\nClassification of IRF, being a complex and highly subjective task, may induce errors with naive judgment. To mitigate this problem, we build a team of three experts: (i) a clinical psychologist for training annotators and validating annotations with psychological viewpoint, (ii) a rehabilitation counselor for comprehending human mind to understand users' IRF, and (iii) a social NLP expert suggesting text based markings in Reddit posts. To negotiate and mitigate the trade-off between three different perspectives, our experts build annotation guidelines 7 to mark (i) TBE, and (ii) PBU. The experts annotated 40 samples of the corpus in isolation using these annotation guidelines to avoid biases and discover possible dilemmas due to the subjective nature of tasks. Therefore, we accommodate perplexity guidelines to simplify the task and facilitate unbiased future annotations.\n\nTBE or PBU in the Past:\nTo check if the condition of a person with disconnected past is still alarming prospect of self-harm or suicidal risk. For instance, 'I was so upset being lonely before Christmas and today I am celebrating New Year with friends'. We frame rules to handle risk indicators about the past because a person attends celebration and overcome the preceding mental disturbance which means filling void with external event. With neutral opinion by NLP expert about double negation, our clinical psychologist argues presence of risk in their perception which may again evolve after some time and thus, marks this post with presence of the TBe.\n2. Ambiguity with Social Experiences: Relationships point to the importance of the ability to take a societal pulse on a regular basis, especially in these unprecedented times of pandemic-induced distancing and shut-downs. People mention major societal events such as breakups, marriage, best friend related issues in various contexts suggesting different user perceptions. We mitigate this problem with two statements: (i) Any feeling of void/missing/regrets/or even mentioning such events with negative words should be marked as presence of TBe such as consider this post: 'But I just miss her SO. much. It's like she set the bar so high that all I can do is just stare at it.', (ii) Anything associated with fights/quarrels/general stories should be marked with absence of TBe such as consider the post: 'My husband and I just had a huge argument and he stormed out. I should be crying or stopping him or something. But I decided to take a handful of benzos instead.'\n\nAnnotation Task\nThree postgraduate students underwent eight hours of professional training by a senior clinical psychologist leveraging annotation and perplexity guidelines. After three successive trial sessions to annotate 40 samples in each round, we ensured their alignment on interpreting task requirements and deployed them for annotating all data points in the corpus. We obtain final annotations based on the majority voting mechanism for binary classification task <TBE, PBU>. 8 We validate three annotated files using Fliess' Kappa inter-observer agreement study on classifying TBE and PBU where kappa is calculated as 78.83% and 82.39%, respectively. Furthermore, we carry out an inter-annotator agreement study with group annotations 9 for textspans extraction in positive data points. The results for agreement study in two-fold manner: (i) 2 categories (agree, disagree) and (ii) 4 categories (strongly agree, weakly agree, weakly disagree, strongly disagree), are obtained as 82.2% and 76.4% for agreement study of <TBE_EXP>, and 89.3% and 81.3% for agreement study of <PBU_EXP>, respectively.\n\nDataset Statistics\nOn observing the statistics of our dataset in Table 2 , we found 54.71% and 32.56% of positive data points with underlying 255489 and 156620 words for TBE and PBU, respectively. It is interesting to note that although the average number of sentences to express PBU is less than TBE, the observations are different for average number of words. We calculate the Pearson Correlation Coefficient (PCC) for our cross-sectional study on TBE and PBU as 0.0577 which shows slight correlation between the two. Our dataset paves the way for longitudinal studies which is expected to witness increased PCC due to wide spread emotional spectrum (Kolnogorova et al., 2021; Harrigian et al., 2020) . On The most frequent words for identifying (i) TBE are alone, lonely, nobody to talk, someone, isolated, lost, and (ii) PBU are die, suicide, suicidal, kill, burden, cut myself. 10 Our approach for identifying TBe and PBu goes beyond a simple keyword detector. Instead, we utilize a more sophisticated method that considers the context and relationships between words. For instance, consider a following sample:\nMassive party at a friend's house-one of 10 WordCloud is given in Appendix C. my closest friends is there, loads of my close friends are there, i wasn't invited. wasn't told. only found out on snapchat from their stories. spending new years eve on teamspeak muting my mic every time i break down :) Despite the absence of trigger words, our approach flags this post as positive for TBu based on its indicators 'friend', 'teamspeak', 'friends', 'invited', 'snapchat', to name a few.\n\nBaselines\nWe perform extensive analysis to build baselines with three different conventional methods. We first apply Recurrent neural networks where a given text, embedded with GloVe 840B-300 11 , is sent to a 2-layer RNN model (LSTM, GRU) with 64 hidden neurons and the output is forwarded to two separate fully connected heads: (i) TBE and (ii) PBU. Each of the fully connected blocks have one hidden layer with 16 neurons and ReLU activation function, and an output layer with sigmoid activation. The loss function is Binary_CrossEntropy and optimizer is adam with lr = 0.001. Next, we apply pretrained transformer-based models. The input is tokenized using a pre-trained transformers' tokenizer to obtain a 768-dimensional vector which is then fed to a similar fully connected network as the previous architecture with hidden layer size as 48. We experimented with roberta-base, bert-base-uncased, distilbert-base-uncased, and mental/mental-bertbase-uncased models. Finally, we use the Ope-nAI embeddings API 12 to convert the input text into 1536-dimensional embeddings through 'textembedding-ada-002' engine which are used to train a classifier. We test the robustness of this approach over: (i) Logistic Regression, (ii) Random Forest, (iii) Support Vector Machine (iv) Multi Layer Perceptron, and (v) XGBoost. We further use two explainable methods: (i) LIME and (ii) SHAP on one of the best performing transformer-based models, MentalBERT (Ji et al., 2022) , to obtain the top keywords (Danilevsky et al., 2020; Zirikly and Dredze, 2022) . We compare them with the ground truth ROUGE scores for -Precision (P), Recall (R), and F1-score (F).\n\nExperimental Settings\nFor consistency, we used the same experimental settings for all models and split the dataset into the train, validation, and test sets. All results are reported on the test set, which makes up 30% of the whole dataset. We used the grid search optimization technique to optimize the parameters. To tune the number of layers (n), we empirically experimented with the values: learning rate (lr): lr \u2208 {0.001, 0.0001, 0.00001} and optimization (O): O \u2208 {'Adam', 'Adamax', 'AdamW'} with a batchsize of 16, 32 were used. We used base version pre-trained language models (LMs) using Hugging-Face 13 , an open-source Python library. We used optimized parameters for each baseline to find precision, recall, F1-score, and Accuracy. Varying lengths of posts are padded to 256 tokens with truncation. Each model was trained for 20 epochs, and the best-performing model based on the average accuracy score was saved. Thus, we set hyperparameter for our experiments as Optimizer = Adam, learning rate = 1e-3, batch size= 16, and epochs=20.\n\nExperimental Results\nTable 3 shows the performance of state-of-the-art methods in terms of precision, recall, F1-score, and accuracy. The current models have moderately low performance in this task, possibly due to a lack of ability to capture contextual information in the text. MentalBERT, a transformer-based language model, initialized with BERT-Base and trained with mental health-related posts collected from Reddit, had the best performance among BERT-based models, with an F1-score of 76.73% and 62.77% for TBE and PBU, respectively. This is likely due to the fact that it was trained on the same context as the task, namely health-related posts on Reddit. The combination of OpenAI embeddings and a classifier outperforms RNN and transformer-based models. The highest F1-Score of 81.23% was achieved by logistic regression for TBE, while the best performing model for PBU was SVM with an F1-score of 76.90%. We also analyzed the explainability of the model using LIME and SHAP methods of explainable AI for NLP on the best performing transformer model (MentalBERT) for TBE and PBU. We obtain results for all positive data points in the testing dataset and observe high recall of text-spans with reference to the ground truth as shown in Table 4 . We find the scope of improvement by limiting the superfluous text-spans found in the resulting set of words. The consistency in results suggests the need of contextual/domain-specific knowledge and infusing commonsense to improve explainable classifiers for a given task.\n\nConclusion and Future Work\nWe present a new annotated dataset for discovering interpersonal risk factors through human-annotated extractive explanations in the form of text-spans and binary labels in 3522 English Reddit posts. In future work, we plan to enhance the dataset with more samples and develop new models tailored explicitly to TBE and PBU. The implications of this work include the potential to improve public health surveillance and other mental healthcare applications that rely on automatically identifying posts in which users describe their mental health issues. We keep the implementation of explainable AI models for multi-task text classification, as an open research direction for Open AI and other newly developed responsible AI models. We pose the discovery of new research directions for future, through longitudinal study on users' historical social media profile to examine interpersonal risk factors and potential risk of self-harm or suicidal ideation. As we focus on Reddit data as a starting point of our study, exploring other forums could be an interesting research direction. bilitation counselor, for their unwavering support throughout the project. Additionally, we extend our heartfelt appreciation to Prof. Sunghwan Sohn for his consistent guidance and support. This project was partially supported by NIH R01 AG068007. This project is funded by NSERC Discovery Grant (RGPIN-2017-05377), held by Vijay Mago, Department of Computer Science, Lakehead University, Canada.\n", "hypothesis": " With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare.  The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings. Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBE), and (ii) Perceived Burdensomeness (PBU).  We establish baseline models on our dataset facilitating future research directions to develop realtime personalized AI models by detecting patterns of TBE and PBU in non-emotional spectrum of user's historical social media profile.", "answer": false}
{"title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization", "content": "\nIntroduction\nDeveloping a persona-consistent dialogue model has been one of the key issues and crucial problems in open-domain dialogue systems (Huang et al., 2020) . Zhang et al. (2018a) define the problem of personalized dialogue generation, which aims to generate personalized responses based on textually described persona profiles. Many efforts have been made on developing dialogue models that generate responses consistent with the provided persona profile (Song et al., 2019 (Song et al., , 2020a,b;,b; Wu et al., 2020a) .\nThe recent development in transformer-based pre-trained models (Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019; Chen, 2020) has led to great successes in dialogue systems (Wolf et al., 2019; Wu et al., 2020b; Ham et al., 2020; Kulh\u00e1nek et al., 2021; Cao et al., 2022; Deng et al., 2022b Deng et al., ,c, 2023)) . Inspired by these successes, previous works incorporate those pre-trained models in persona-based response generation by concatenating the dialogue history and persona as input to generate the response in an auto-regressive manner (Song et al., 2021; Liu et al., 2022) . However, a fine-tuned model can generate a high-quality and persona-consistent response in a certain ordering of personas, while varying this order may lead to a generic and even inconsistent response as illustrated by the example in Figure 1 . We empirically show that the worst ordering of persona can lead to a 29.4% decline in BLEU score compared with the best ordering.\nIdeally, a well-trained dialogue generation model should be able to generate a persona-consistent response regardless of the ordering of personas in the input. We perform experiments and analyses to identify the cause of the ordering sensitivity. We find that the ordering of persona in the input leads to different representations of context and response. We also show that the model can attend to the appropriate persona and generate high-quality responses under some representations but not under others. This leads to instability in response generation.\nMotivated by the above findings, we propose ORder Insensitive Generation (ORIG), which is a simple and effective framework that helps models learn more robust and better representations for different persona orders. More specifically, we formulate ORIG as a constrained optimization problem, which optimizes a persona response generation objective under the constraint: given different orderings of persona, the response representations of the model are the same. Then we optimize it through a stochastic optimization approach.\nExperimental results on the Persona-Chat dataset show that ORIG significantly improves the robustness of pre-trained models (GPT2 (Radford et al., 2019) and BART (Lewis et al., 2020) ) under different orderings of input persona, as well as advances their generation performance.\nIn summary, our contributions are threefold: (1) We identify the order sensitivity problem in persona dialogue generation and conduct an empirical analysis to reveal its underlying reasons. ( 2) We propose a model-agnostic framework, ORIG, that helps different persona dialogue models learn robust representations while achieving better performance. (3) We perform extensive experiments on the Persona-Chat dataset, showing that ORIG outperforms previous models and is more robust and less sensitive to different persona orderings.\n\nRelated Work\nMaintaining a consistent persona is essential for building a human-like dialogue system, where most works regard persona as a set of sentences along with each dialog (Zhang et al., 2018a; Gu et al., 2019; Song et al., 2019; Wu et al., 2021; Cao et al., 2022; Deng et al., 2022a) . Song et al. (2021) disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation while Cao et al. (2022) aims to alleviate the problem of limited data by data manipulation methods. Despite satisfactory performance in previous work, the impacts of different orders of personas are still under-explored, resulting in unstable and inconsistent responses.\nOur work is also related to work on order sensitivity in prompt-based few-shot learning (Zhao et al., 2021; Lu et al., 2022) . Zhao et al. (2021) found that the different order of training examples in the prompt can cause accuracy to vary from near chance to state-of-the-art in the few-shot clas- sification setting. Similarly, order sensitivity for In-context Learning also exists regardless of model size and the prompt format (Lu et al., 2022) . Distinguishing from them, we focus on order sensitivity in the language generation task in finetuning setting, especially the impacts of persona orderings to generate persona-consistent responses.\n\nOrder Sensitivity Problem and Analysis\nIn this section, we first illustrate the seriousness of the order sensitivity problem by showing a huge performance fluctuation in persona dialogue models when fed the same personas in the best and worst orders. Then we analyse why their performance is volatile to different persona orderings.\nTo illustrate the problem, we finetune PLMs on the Persona-Chat by concatenating the persona and dialogue context together to predict the target response, including GPT2 and BART. After the training converges, we test them on two settings: (1) the best case: for each test sample, we feed the models all possible permutations of persona sentences and keep the maximum score for each sample as the final score; (2) the worst-case: perform the same process as (1), but take the minimum score. Table 1 shows the results for two models. Surprisingly, we find the ordering of input persona has a big impact on the models' performance: GPT2's worst case is 29.4% lower than its best case, while BART's is 83.2% lower.\nMoreover, we find that the huge fluctuation in models' performance is closely related to the response representation changes caused by different orderings of input persona sentences. Concretely, we measure the similarity of the responses representation of the same test sample under different input orders of persona. We show their token-level similarity in the sponse should be zero. However, their distances are significantly higher than zero. It reveals that the models behave more likely a left-to-right language model whose representation is prone to the different orderings of the previous input (e.g. persona sentences). That is highly undesirable for a robust personalized dialogue model. Thus, regularization of representation for the response tokens is necessary to help personalized dialogue models capture order-invariant representation.\n\nMethod\nWe introduce the proposed framework, named ORIG: ORder Insensitive Generation (ORIG). As shown in Figure 2 , we transform the persona ordersensitivity problem as a constrained optimization problem that optimises a persona dialogue model under the uncertainty of the input persona order.\n\nProblem Formulation\nGiven the dialogue context C = {u 1 , . . . , u m } and a set of persona descriptions P = {p 1 , . . . , p n }, the goal is to generate a personalized response r.\nFormally, the generation problem can be formulated as the following chain rule:\nP (r|C, P ; \u03b8) = T t=1 P (r t |r 1:t\u22121 , C, P ; \u03b8) (1)\nwhere \u03b8 is the parameters of the dialogue model.\n\nORIG Framework\nAccording to the analysis in Section 3, the observation reveals that varying the order of input personas leads to different representations of the dialogue response, thus resulting in fluctuations in performance.\nTo learn more robust and consistent representations, we propose the ORIG framework that complements the response generation process with a constraint: given the different orderings of a persona, the model's response representations need to be the same.\nThen the order-insensitive personalized dialogue generation problem is modelled as the following constrained optimization problem where P (r|C, P ; \u03b8) are the model's predictions over the dialogue response, D denotes the dialogue corpus, and the function D is KL divergence to measure the difference between two distributions, and the Shuffle operator samples each persona ordering uniformly from the full permutation of P .\n\nOptimization\nAs for optimization, we first apply the Lagrange multipliers strategy to convert the constrained problem into an unconstrained problem L \u03b8 = \u2212 log P (r|C, P ; \u03b8) +\u03b3 \u2022 D[P (r|C, P ; \u03b8), P (r|C, P ; \u03b8)] (6\n)\nwhere \u03b3 is the multiplier corresponding to the equality constraints (3). Then we can update the parameters \u03b8 of dialogue models by stochastic gradient descent.\n\nExperimental Setups\nDatasets We evaluate the models on the Persona-Chat dataset (Zhang et al., 2018a) , where each dialogue session has at least 6 turns of interactions.\nAnd each interaction is conditioned on a persona that is described with 5 profile sentences. proposed ORIG. Our implementation was based on HuggingFace's Transformers library (Wolf et al., 2020) . During training, the learning rate is set as 2 \u00d7 10 \u22125 , and the batch size for GPT2 and BART is set as 64 and 32, respectively. We trained both models for 10 epochs with Adam (Kingma and Ba, 2015) optimizer until they converged. During decoding, We employ a top-p (p=0.9) (Holtzman et al., 2020) plus top-k (k=50) sampling strategy, which is used to avoid sampling from the unreliable tail of the distribution (only consider a subset of vocabulary composed of k words with the highest probability or some most probable words whose sum of probabilities equals p at each decoding step).\nThe random seed for all experiments is set to 42. Evaluation Metrics We perform both automatic and human evaluations. (1) Automatic metrics: We adopt BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) , Entropy (Zhang et al., 2018b) and CIDEr (Vedantam et al., 2015) for lexicalbased measurement. Following previous work, we also adopt the C-score (Madotto et al., 2019) (Fleiss, 1971) .\n\nExperimental Results\nImproves performance in the original test set Table 3 shows different models' performance in the original test set without any modifications (for ORIG, \"Shuffle\" is used during training but is optional during testing. The Table 3 caption signifies the absence of \"Shuffle\" during testing. This is to evaluate if ORIG performs well in the normal setting). From automatic metrics, we can see base models trained with our ORIG framework outperform the baselines. It justifies that our framework can be applied to different models to improve their performance. From human evaluation results, models with ORIG are superior to others on almost all metircs, especially on GPT2. This is consistent with the results of automatic metrics. The average kappa value of the annotation is 0.632, indicating good agreement during human evaluation.\nReduces variance and improves mean and worstcase performance Figure 3 shows that aside from reducing the variance, ORIG also improves mean and worst-case performance (detailed results in Table 4) across two models consistently, especially in GPT2 (the worst case performance is very close to the best case). We reduce the variance on GPT2 and BART by 91.6% and 51.8%, respectively. Meanwhile, we improve worst-case performance by 20.3% and 22.6% on GPT2 and BART respectively. The only drop is the best case. This is because our distance function D is unidirectional, which pulls in the two representations in Equation 3indiscriminately, causing the best case to go down and the worst to go up. We leave more complicated and directional distance constraints for future studies.\n\nConclusion\nWe show that the current practice of applying pretrained models to the personalized dialogue generation task is volatile across different input orders of personas. Through the analysis, we find that the problem arises from the representation changes induced by the input changes. Motivated by these, we propose our ORIG, a model-agnostic framework for finetuning the persona dialogue model such that it obtains a persona order-invariant representation.\nExperiments on two dominant pre-trained dialogue models show that our framework improves performance and reduces order volatility.\n", "hypothesis": " Generating persona consistent dialogue response is important for developing an intelligent conversational agent.  Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response.  While simple and effective, our analysis shows that this popular practice is seriously affected by Order Sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART).  To mitigate the order sensitivity problem, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn robust representation under different persona orders and improve the consistency of response generation.  Experiments on Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART).", "answer": true}
{"title": "RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation", "content": "\nIntroduction\nText style transfer (TST) is a task that aims to control stylistic attributes of an input text without affecting its semantic content (Jin et al., 2022) . Research in TST has largely focused on English, thanks to the availability of large monolingual English datasets covering stylistic attributes like formality and simplicity (Rao and Tetreault 2018, Zhu et al. 2010, inter alia) . In recent years, however, multilingual and cross-lingual applications of TST have seen a steady gain in popularity (Briakou et al., 2021; Garcia et al., 2021; Krishna et al., 2022) . A notable instance of cross-lingual TST is attributecontrolled translation (ACT), in which attribute 1 conditioning is performed alongside machine translation (MT) to ensure that translations are not only Neutral Src (EN) After retiring from teaching, Cook became a novelist.\n\nFeminine Ref (NL)\nNadat ze stopte met lesgeven, werd Cook schrijfster.\nMasculine Ref (NL) Nadat hij stopte met lesgeven, werd Cook schrijver.\nTable 1 : Examples of attribute triplets from COCOA-MT and MT-GENEVAL. Attribute markers in the attribute-controlled translations are underlined.\ncorrect but match user-specified preferences, such as formality/honorifics (Sennrich et al., 2016; Niu et al., 2017; Michel and Neubig, 2018; Niu and Carpuat, 2020; Nadejde et al., 2022; Wang et al., 2022) , gender (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Saunders and Byrne, 2020) , and length (Lakew et al., 2019; Schioppa et al., 2021) .\nACT is especially important for sectors like customer service and business communication, where stylistic differences can have an impact on user perception (e.g., misgendering customers or speaking to them in an appropriately informal tone can be offensive or disconcerting). Table 1 gives examples of ACT for formality and gender. Most prior work on ACT relies on a supervised adaptation component that conditions the generative model on the selective attribute. However, few annotated ACT datasets are available, and they generally cover only a limited set of languages and attributes. Thus, enabling few-shot or zero-shot ACT would facilitate applying attribute control to less-resourced attributes and langauges.\nIn this paper, we introduce a new approach for ACT: Retrieval and Attribute-Marking enhanced Prompting (RAMP). Recent studies have shown that large language models (LLMs) can perform MT out of the box using the prompting paradigm (Brown et al., 2020; Lin et al., 2022; Chowdhery et al., 2022) . We build on this, prompting LLMs to perform attribute-controlled MT through two innovations: ( 1 Here is a sentence: {You will always be welcome here.} Here is its Spanish translation written in a formal style: {Siempre ser\u00e1 bienvenido aqu\u00ed.} The translated sentence conveys a formal style by using words such as 'ser\u00e1'.\n----Here is a sentence: {I wish you welcome and enjoy your stay.} Here is its Italian translation written in a formal style: {Le do il benvenuto e si goda il soggiorno.} The translated sentence conveys a formal style by using words such as 'Le', 'si goda'.\n----Here is a sentence: {You're welcome.} Here is its French translation written in a formal style: { EN: You're welcome. explicit attribute marking.\nRecent works adopting the prompting paradigm for text style transfer have mainly focused on the generalization capabilities of large English-centric LMs for zero-shot style transfer using previously unseen style descriptions (Suzgun et al., 2022; Reif et al., 2022) . However, prior work on other NLP tasks has shown that cross-lingual prompting of multilingual LLMs can be effective (Zhao and Sch\u00fctze, 2021; Zhou et al., 2022; Huang et al., 2022) . As such, we leverage multilingual LLMs and extend their ACT capabilities cross-lingually to languages not covered by the in-context examples, thus enabling zero-shot ACT.\n\nPreliminaries\nAttribute-Controlled Translation ACT takes two inputs, a sentence x and a desired target attribute a \u2208 A (with A being the space of attributes), and outputs a translation y that complies with the specified attribute. It can be formulated as a function f : (x, a) \u2192 y. In our experiments, we use attribute values provided by the COCOA-MT formality translation dataset and the MT-GENEVAL gender translation dataset, i.e., A = {formal, infor-mal} or {female, male}. 2 Prompting In the prompting paradigm for decoder-only LLMs, inputs are given as decoding prefixes to the model, usually combined with natural language instructions for output generation. In style-controlled translation, we formulate the prompt for target language l and attribute a using the text \"Here is a sentence: {x} Here is its l translation written in a a style:\" to produce the 2 See Section 5 for ethical considerations. output y. 3 In the few-shot setting, we provide a sequence of k labeled in-context examples before the unlabeled input, which can be formulated as a function f : {(x 1 , l 1 , a, y 1 ), . . . , (x k+1 , l k+1 , a)} \u2192 y k+1 .\n\nOur Approach: RAMP\nRAMP builds on the success of the prompting paradigm on few-shot generation tasks such as monolingual text style transfer (Reif et al., 2022) and MT (Garcia and Firat, 2022; Agrawal et al., 2022) by creating more informative prompts through similarity retrieval and attribute marking. See Figure 1 for an illustration of RAMP.\n\nSimilarity Retrieval\nIn standard prompting, incontext examples are sampled randomly from the pool of labeled examples D A . In RAMP, we select examples based on their similarity with the input text. We first embed both the input text and the source texts of D A using all-MiniLM-L6-v2 (Wang et al., 2020) . Then, the top-k most similar examples are retrieved for the input text based on cosine similarity. These are then used in a descending order w.r.t. similarity as the in-context examples in the inference prompt. As demonstrated in Figure 1 , the in-context example \"You will always be welcome here.\" has the highest similarity to the test example \"You're welcome.\" so it is prompted first.\n\nAttribute Marking\nIn standard prompting, incontext examples are provided without explicit information on why they satisfy the prompting objective. Inspired by recent studies that have shown that decomposition of complex tasks can improve prompting quality (Nye et al., 2021; Wei et al. , 2022), we include for every in-context example an additional sentence directly after the target sentence that specifies which text spans convey the desired attribute (e.g., \"The translated sentence conveys a formal style by using words such as 'Vous'.\"). In our experiments, we use the gold attribute spans included in the CoCoA-MT and MT-GenEval datasets. In section 4 we suggest possibilities for automatically deriving attribute spans when gold training labels are not available.\nAR ES FR HI PT DE IT JA RU NL COCOA-MT \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 MT-GENEVAL \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 XGLM \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 BLOOM \u2713 \u2713 \u2713 \u2713 \u2713\n\nCross-Lingual Prompting\nThe similarity retrieval component of RAMP requires a large pool D A from which to find appropriate incontext examples for prompting. Low-resource attributes or language pairs may have insufficient or no annotated data from which to retrieve such examples. To mitigate this issue, we introduce crosslingual prompting, in which the target side of the in-context examples differs from the desired target language of the translation task. As demonstrated in Figure 1 , we study whether the system can leverage examples in one language (e.g., attribute indicators in Spanish) to produce the same attribute in another (e.g., French). Two main features of our RAMP model allow us to perform cross-lingual prompting: (1) the use of multilingual LLMs, and (2) the example retrieval step, which is done on the source language only.\n3 Experiments\n\nDatasets\nWe experiment on two multilingual ACT datasets: instead explicitly controlling target gender. Both datasets have gold annotations for attributemarked target spans, and both cover translation from English into multiple diverse target languages. We list their target languages in Table 2 .\n\nLarge Language Models (LLMs)\nWe select three massively multilingual decoderonly LLMs for the prompting experiments: XGLM (Lin et al., 2022) , BLOOM (BigScience, 2022) and GPT-NEOX (Black et al., 2022) . The selected models span three orders of magnitude in terms of number of parameters and differ in the languages that they cover (see Table 2 ). Appendix D motivates our choice of models in more detail. GPT-3 is not included because it is not freely accessible and it is not intended for multilingual use-cases.\n\nBaseline\nAttribute tagging is a standard method for ACT, so we include a baseline following the approach and configuration used by Nadejde et al. ( 2022): a transformer MT model (Vaswani et al., 2017) pre-trained on public parallel data and further finetuned on contrastive training pairs with attribute tags (from either COCOA-MT or MT-GENEVAL). We refer to this as adapted MT.\n\nEvaluation Metrics\nWe measure translation quality with BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020) . For attribute accuracy, we use both (1) the lexical matching metrics provided with COCOA-MT and MT-GENEVAL (Lexical-Accuracy) and ( 2) sentence encoders trained on contrastive examples (Sentential-Accuracy). For (2), we train multilingual classifiers on top of the mDeBERTa-v3 encoder (He et al., 2021) . High-performance pretrained classifiers have been shown to produce attribute accuracy estimates closer to human judgments for style transfer (Lai et al., 2022) . Table 3 presents the accuracy of the classification models on the test sets of their respective datasets, averaged over all languages. Unlike lexical accuracy, the multilingual attribute classifier does not penalize text generated in incorrect languages. Thus, in cross-lingual prompting experiments, we include a step of language detection 5 so that generated sentences not in the requested target language are considered incorrect.\n\nResults: Same-Language Prompting\nWe first evaluate the effectiveness of RAMP for formality-and gender-controlled translation where the language pair used for in-context examples is the same as the one used in the prompt candidate (e.g., EN\u2192ES formality-controlled translation using EN\u2192ES in-context examples). We test XGLM 7.5B and BLOOM 175B with 16 in-context examples on both tasks. 6 Table 4 presents our results alongside the adapted MT baseline. The base model uses in-context examples that are sampled randomly from the pool of labeled examples. We also include an ablation that adds attribute marking only on top of base, without similarity retrieval (+mark).\nUsing just attribute marking consistently improves attribute accuracy of the generated text, but it leads to degradation of COMET on COCOA-MT. The complete RAMP with similarity retrieval not only compensates for the COMET degradation but also improves quality and attribute metrics across the board, especially for the high-capacity BLOOM 175B model.\nAdapted MT outperforms BLOOM 175B on MT-GENEVAL in all metrics, but underperforms it on COCOA-MT. This suggests that it is challenging to do fine-grained comparison between LLMs and standard MT systems as they might have different domain coverage. BLOOM 175B consistently outperforms XGLM 7.5B in both generic translation quality and attribute control accuracy, so we proceed with using BLOOM 175B in the crosslingual prompting setting.\n\nResults: Cross-Lingual Prompting\nWe have demonstrated the effectiveness of selecting similar same-language examples to build the prompt, echoing contemporary work (Liu et al., 2022; Agrawal et al., 2022) . In this section, we evaluate the cross-lingual prompting option, i.e., retrieving in-context examples from other target languages besides the desired language of translation. We test this zero-shot setting using the leave-oneout strategy, and results of tested language pairs are averaged. 7 Table 4 presents our results using BLOOM 175B. On both test sets, compared to the baseline, we observe improved attribute accuracy and comparable or better generic translation quality when using RAMP with cross-lingual prompting.\nWe do observe translation quality degradation with RAMP on some target languages of COCOA-MT, e.g., ES. Manual analysis shows that repeated inaccurate retrieval results could lead to hallucinations. 8 For example, RAMP retrieves multiple sentences containing \"million\" for the input \"If you got it why not? He is worth over 20 billion dollars after all.\". This results in mistranslation of billion to million (millionario): \"Si lo tienes, \u00bfpor qu\u00e9 no? Es millonario despu\u00e9s de todo.\". We give detailed examples in Appendix H.\n\nConclusions\nWe introduced the new RAMP in-context learning approach to leverage attribute annotations and similar same-language or cross-lingual examples for better prompting quality. We demonstrated its effectiveness with multilingual LLMs for both formalitycontrolled and gender-controlled translation. We use gold annotations for attribute marking, but we leave unsupervised automatic attribute span extraction as future work.\n", "hypothesis": " Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs.  While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP), which leverages large multilingual language models to perform attribute-controlled translation (ACT) in few-shot and zero-shot settings. RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a sentiment analysis component for selecting emotionally similar in-context examples, and (2) marking in-context examples with sentiment annotations.  Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings.\n* Work conducted during an internship at Amazon.  1 In this paper, we prefer the term attribute rather than style, since not all the attributes addressed here (e.g., gender) can be considered styles..", "answer": false}
{"title": "Leveraging Prefix Transfer for Multi-Intent Text Revision", "content": "\nIntroduction\nRevision is an essential process to improve the text quality (Vaughan and McDonald, 1986) . During this process, writers perform various editing operations on the text with different editing intentions. As shown in Figure 1 , the writer corrects misspelled words to improve text fluency, deletes redundant words to improve text clarity, adds connective words to improve text coherence, inserts adverbs to convey the writer's writing preferences (style) and modifies data to update text information (meaning-changed).\nLots of recent studies have focused on a text revision task corresponding to a specific edit intention, such as grammatical error correction (Omelianchuk She went to the markt\nThe changes made the paper better than before.\nText Revision She works hard.\nShe is successful.\nEverything was rotten.\n\nShe went to the markt market\nThe changes made the paper better than before improved the paper.\nShe works hard. She; therefore, she is successful.\nEverything was awfully rotten. This method improves the model accuracy from 64% to 7883%. et al., 2020; Kaneko et al., 2020; Liu et al., 2021; Yang et al., 2022 ), text simplification (Dong et al., 2019; Jiang et al., 2020; Omelianchuk et al., 2021; Martin et al., 2022) , and text style transfer (Malmi et al., 2020; Reid and Zhong, 2021) . The work divides text revision into several independent problems. While some methods with strong universality can be applied to multiple tasks (Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020) , they train different models on various data sets. Real-world scenarios require addressing multiple types of editing errors at the same time, such as grammatical errors, spelling errors, etc. But these methods failed to integrate knowledge from these tasks into a unified model.\n\nmeaningchanged\nTo solve the problem, Du et al. (2022) attempted to train one model using data with multiple editing intentions and leveraged edit intent information by simply appending it to the input. However, when adding a new intent, the entire model must be re-trained. A more lightweight and scalable approach to multi-intent text revision is still required.\nLi and Liang (2021) proposed a new kind of prompt tuning method to quickly adapt a pretrained model to new tasks, which is called prefixtuning. Prompt tuning can help the pre-trained language model to locate the task learned in pretraining and enable the related knowledge to model text revision with different edit intentions (Reynolds and McDonell, 2021) . This method enables a model to handle multiple edit intentions in a lightweight and scalable way.\nIn this paper, we present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions. This method involves a two-step training process. In the first step, we initialize a pre-trained language model (PLM) and train multiple prefixes on it. Each edit intention corresponds to a prefix. In the second step, a prefix transfer module is trained at each attention layer of the PLM. The prefix transfer module is configured as two attention units that act respectively on this layer's key states and value states. It enables our model to learn a tailored prefix for the given input with the help of prefix embeddings from the predefined tasks.\nWe conduct experiments on ITERATER (Du et al., 2022) , an iterative text revision dataset. It mainly contains parallel sentences with five edit intentions: fluency, coherence, clarity, style, and meaning-changed. The results show that our approach performs better than the fully fine-tuned BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) baselines reported in Du et al. (2022) with fewer training parameters.\n\nIterative Text Revision\nFor the first time, Du et al. (2022) systematically studied the iterative revision phenomenon in human writing. They presented the ITERATER, an annotated dataset across multiple domains of formally human-written text, which includes Wikipedia, ArXiv, and Wikinews. And they trained several types of text revision models using ITERATER. Dwivedi-Yu et al. (2022) presented EDITEVAL, an instruction-based benchmark, to evaluate the editing capabilities of models and they also included the test set of ITERATER in it. Based on Du et al. (2022) , our work further explores the method of text revision.\n\nTransfer Learning of Prompt Tuning\nTransfer learning is a common and powerful technique in NLP (Raffel et al., 2020) . Some recent studies have tried to improve prompt tuning performance by leveraging the knowledge of multiple related or unrelated tasks. Asai et al. (2022) used an attention module to make use of the knowledge in exiting soft prompts (Lester et al., 2021) while learning a new task. Chen et al. (2022) improved the few-shot text summarization by multi-task pretraining and prefix-tuning. Specifically, they pretrained a summarization model on a set of popular summarization datasets and then conducted prefixtuning for it on an unseen summarization task. Different from their modeling of a new task through existing tasks, our work aims to achieve the mutual utilization of knowledge between different edit intents in text revision.\n\nMethod\nThe revision task can be defined as the following process: given a source sentence x = [x 1 , . . . , x m ] and an optional edit intent e \u2208 E to generate a revised sentence y = [y 1 , . . . , y n ], where E is the set of all edit intentions. Note that e is optional because it can be inferred from the input x.\nOur method is depicted in Figure 2 . It includes two stages: the multi-prefix tuning stage and the prefix transfer stage.\n\nMulti-Prefix Tuning Stage\nThe prefix is a set of parameters on every attention layer of PLM. For an edit intention e, at each attention layer, the prefix can be described as P e = {P K e , P V e }, where P K e and P V e are parameters added before the key states and value states in this attention layer. After adding these parameters, the calculation of the attention head in this layer becomes:\nH = Attention(Q, [P K e ; K], [P V e ; V ]) (1)\nwhere H is the output vector sequence; Q, K, V are query states, key states, and value states, respectively; Attention means scaled dot-product attention. Only P K e and P V e are updated during the training process. Note that we ignore the layer number information because the operation for each layer is the same.\nAs shown in the left part of Figure 2 , for every edit intention e, we train a prefix P e accordingly. In this way, the model could revise an intentionannotated text by activating the corresponding prefix at inference.\n\nPrefix Transfer Stage\nIdentifying edit intention is always an ambiguous work. At the prefix transfer stage, we aim to build a new prefix for an unannotated input instance by transferring existing prefixes. The new prefix P new is instance-specific.\nThe prefix transfer stage is described in the right part of Figure 2 . At each layer, we rearrange the prefixes {P e | e \u2208 E} obtained in the last stage as\nP K = {P K\ne | e \u2208 E} and P V = {P V e | e \u2208 E} according to whether they are configured before the key states or before the value states. Then a pair of attention units G K and G V are trained for P K and P V .\nTake G K as an example. It calculates the similarity between the key states K and every P K e in P K to get attention scores.\nThe similarity can't be calculated directly, because K and P K e have different lengths. So we perform the max-pool operation for length dimension on K and P K e . After that, we obtain K \u2208 R d and P K e \u2208 R d , where d is the dimension of the hidden states in the PLM.\nTo get attention scores, we train a fully connected layer to extract features from K:\nEQUATION\nwhere W \u2208 R d\u00d7d is a transfer matrix updated during training. Following Asai et al. (2022) , we use SiLU (Elfwing et al., 2018) for the non-linear layer and add a Layer Norm (Ba et al., 2016) layer:\nEQUATION\nThen, we calculate the attention scores for intent e as follows:\nEQUATION\n)\nwhere T is the softmax temperature (Radford et al., 2021) which could avoid making the attention unit over-confident.\nFinally we use them to build P K new as follows:\nEQUATION\nIn the same way, we get P V new by G V . Using the new prefix P new = [P K new , P V new ], our system could revise the unannotated input instance with the knowledge from existing prefixes.\n\nExperimental Setup\nWe choose BART-large as the PLM for our system and use adapter-transformers (Pfeiffer et al., 2020) to implement prefix-tuning. More implementation details are in Appendix A.\n\nDatasets\nWe conduct our experiments on the iterative text revision dataset: ITERATER (Du et al., 2022) . We remove the Other class of the data as it essentially contains a variety of unrecognized edit intentions and accounts for a small proportion (1.44%). The entire dataset consists of two parts: ITERATER-HUMAN and ITERATER-FULL. The former is a smaller dataset with manual annotation of edit intentions, while the latter is a large dataset annotated by a classification model trained on ITERATER-HUMAN. We train our model on both of them. Following Du et al. (2022) , we report the results on the test set of ITERATER-HUMAN in Section 5, which is completely a human-created dataset and is reliable for evaluation. We show more details of the datasets in Appendix B.\n\nEvaluation Metrics\nFollowing previous work, we report three metrics: SARI (Xu et al., 2016) , Rouge-L (Lin, 2004) , and BLEU (Papineni et al., 2002) . Among them, SARI is considered an important metric in situations where input text and output text have a large overlap in words. It also indicates the positive impact of revisions on document quality. The setting of evaluation metrics is the same as Du et al. (2022) . We use the metrics package from Huggingface transformers (Wolf et al., 2020) to calculate the SARI, BLEU, and Rouge-L scores.\n\nModels Setup and Baselines\nUsing our method, we train the models in two ways: the model that only trains the multi-prefix tuning stage and that trains both the multi-prefix tuning stage and the prefix transfer stage.\nWe compare our method with three baselines: full fine-tuning BART (BART-FineTune), full finetuning PEGASUS (PEGASUS-FineTune), and prefixtuning of BART with a single prefix (BART-SinglePrefix). Both BART and PEGASUS are generative models based on the transformer architecture. Compared to the edit-based model FELIX, they perform better. We use the results reported by Du et al. (2022) for these two models. Furthermore, we compare BART-SinglePrefix as a possible technical solution as we choose BART as our backbone model. BART-SinglePrefix trains only one prefix on the entire dataset.\nAll three baselines are trained with two config-urations. The first configuration is using the pure sentence pairs without edit intention annotations to train the model. The second configuration is appending an edit intent token at the beginning of the input text during the training process, which is the same as the approach of Du et al. (2022) .\n5 Results and Analysis\n\nMain Results\nThe main results are shown in Table 1 . Compared to training with a single prefix, the setting of multiple prefixes can improve the results, especially training on ITERATER-HUMAN. Meanwhile, with fewer training parameters, the multi-prefix setting could achieve a comparable SARI score and better average score than the fully fine-tuned BART and PEGASUS baselines. Moreover, prefix transfer could further improve the model's performance. Training on ITERATER-HUMAN, prefix transfer significantly improves the SARI score from 33.12 to 36.01 and gets the highest average score of 67.91. Training on ITERATER-FULL, prefix transfer can also improve the average score from 67.23 to 68.36.\nAn interesting phenomenon is that training on different datasets results in different gains for prefix transfer in evaluation metrics. On ITERATER-HUMAN, prefix transfer improves the SARI score significantly. While on ITERATER-FULL, prefix transfer mainly improves the BLEU score and Rouge-L score. One possible explanation is that in situations when the training data is small, prefix transfer tends to learn more editing operations to improve text quality. In this way, the SARI score related to editing operations will be improved significantly. When the training data is sufficient, pre- fix transfer will model the gold reference in more detail. So the BLEU score and the Rouge-L score will be improved.\n\nAnalysis\nWe further tried to use different training data at different stages of training to conduct experiments.\nThe results are shown in Table 2 . We find that the best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent. The auto-annotated dataset ITERATER-FULL contains many incorrectly classified sentences, which may cause mismatched knowledge in prefixes. In the prefix transfer stage, due to the existence of mismatched knowledge and incorrectly classified sentences, the continued use of the same training data may finally cause a certain degree of negative transfer. However, if we use ITERATER-HUMAN in the prefix transfer stage, the impact of negative transfer will be mitigated, because ITERATER-HUMAN only contains correctly classified sentences.\nIn Appendix C, we separately provide the performance results on different edit intentions of the best-performing model.\n\nConclusion\nIn this paper, we introduce a new method for multiintent text revision. The system is based on prefixtuning, which first obtains a prefix for every edit intention and then learns to transfer the knowledge in prefixes for every input instance by training a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM. In this way, our method can make full use of the knowledge of various edit intentions and does not need to anno-tate the intentions of the input. The experimental results show that our method significantly outperforms baselines, and both multi-prefix and prefix transfer settings could improve the performance.\n", "hypothesis": " Text revision is a necessary process to improve text quality.  During this process, writers constantly edit texts out of different edit intentions.  Identifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts according to one specific edit intention.  In this work, we aim to build a multiintent text revision system that could revise texts without explicit intent annotation.  Our system is based on prefix-tuning, which first gets prefixes for every edit intent, and then trains a prefix transfer module, enabling the system to selectively leverage the knowledge from various prefixes according to the input text.  We conduct experiments on the ITER-ATER dataset, and the results show that our system outperforms baselines.  The system can significantly improve the SARI score with more than 3% improvements, which thrives on the learned editing intention prefixes..", "answer": true}
{"title": "Improving Language Model Integration for Neural Machine Translation", "content": "\nIntroduction\nMachine translation (MT) is the task of automatically translating text from one language to another. Nowadays, the dominant approach is neural machine translation (NMT), where a neural network is used to predict the probability of a sentence in the target language, given a sentence in the source language (Bahdanau et al., 2014; Vaswani et al., 2017) . For this approach to be effective, a large number of bilingual training samples -consisting of sentences and their corresponding translationsis needed. This poses a challenge, especially when we want to build a system for a specific domain, where zero or only limited amounts of in-domain bilingual data are available.\nIn these situations, people turn towards monolingual text data, which is simply text in the source or target language and of which plenty exists for most languages and domains. Before NMT became feasible, the preferred way of incorporating additional monolingual data in the MT system was the usage of an external target-side language model (LM), which is trained on monolingual data to predict the probability of a sentence (Brown et al., 1990; Della Pietra, 1994; Zens et al., 2002) .\nHowever, with the rise of NMT, it was found that a technique called back-translation outperforms the LM incorporation by a large margin (Sennrich et al., 2016a) . Back-translation is a two step process, where we first create synthetic parallel data by automatically translating target side monolingual data into the source language. Then, the final NMT system is trained on the combination of the real and synthetic parallel data. It was argued that the backtranslation approach better suits the NMT framework because the NMT system implicitly learns an internal language model (ILM) as part of the training, which might interfere with an additional external LM (Sennrich et al., 2016a) .\nMore recently, for automatic speech recognition (ASR), there have been works focusing on neutralizing this ILM before combination with an external LM and significant improvements were reported (McDermott et al., 2019; Variani et al., 2020; Meng et al., 2021; Zeyer et al., 2021; Zeineldeen et al., 2021) . In this work, we adapt the methods for ILM compensation, developed for ASR, and test them for NMT. We compare against back-translation in different settings and find that ILM compensation significantly boosts the performance of LM fusion, although back-translation is still outperforming this approach for NMT. Also, applying ILM compensation on top of back-translation does not result in significant performance improvements.\n\nRelated Work\nSeveral approaches to combine an LM and NMT model have been proposed in the past. Shallow fusion (SF) is the most straight forward way, using a weighted log-linear combination of the model output probabilities (Gulcehre et al., 2015 (Gulcehre et al., , 2017)) . Deep fusion denotes the concatenation of the hidden states of NMT model and LM and requires joint fine-tuning of both models (Gulcehre et al., 2015 (Gulcehre et al., , 2017)) . Simple fusion is similar to shallow fusion, but the NMT model is trained using information from a pre-trained LM (Stahlberg et al., 2018) .\nFor the task of ASR, people recently have started to remove the ILM that is implicitly learned. The biggest question there is, how to best approximate the ILM. Approaches include: (1) training an additional LM on the target side of the parallel data (McDermott et al., 2019) , (2) removing/averaging encoder information (Variani et al., 2020; Meng et al., 2021; Zeyer et al., 2021) and ( 3) training a small sub-network while freezing all other parameters (Zeineldeen et al., 2021) .\nAs an alternative to LM fusion, back-translation (Schwenk, 2008; Bertoldi and Federico, 2009; Sennrich et al., 2016a) has become the standard method for incorporating additional monolingual data for NMT. Some work has been done to improve this approach, including sampling (Edunov et al., 2018; Gra\u00e7a et al., 2019) , tagging (Caswell et al., 2019) and block-BT (Popel et al., 2020) . For sake of simplicity, we focus on the standard back-translation approach using beam search in this work.\nApart from using an external LM and backtranslation, additional monolingual data can also be utilized by pre-training (Ramachandran et al., 2017; Zhu et al., 2019) , multi-task-learning (Zhang and Zong, 2016; Domhan and Hieber, 2017) or post-editing (Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019) . In principle, all these approaches can also be combined with LM fusion, potentially further improving the performance of the resulting system.\n\nInternal LM Estimation\nDuring decoding, given a source sentence f J 1 and a model P (e I 1 |f J 1 ), we want to find the translation \u00ea\u00ce 1 that maximizes\n\u00ea\u00ce 1 = argmax I,e I 1 P (e I 1 |f J 1 ) .\nIn our framework, P is the combination of three models:\nP (e I 1 |f J 1 ) \u221d P MT (e I 1 |f J 1 ) \u2022 P \u03bb 1 LM (e I 1 ) \u2022 P \u2212\u03bb 2 ILM (e I 1 )\nwhere P MT , P LM and P ILM are the probabilities of the NMT model, external LM (trained on additional monolingual data) and ILM respectively, and \u03bb 1 , \u03bb 2 \u2265 0. Note that the ILM gets a negative weight, because we want to neutralize its impact in this model combination. If \u03bb 2 = 0, we fall back to standard shallow fusion.\nIn principle, the ILM can be exactly calculated from the NMT model by marginalizing over all source sentences f J 1 . However, this summation would be intractable. Instead, different ILM approximations have been proposed in the recent past for ASR, which we will briefly recall here. For a more in-depth discussion of the different approximation methods we refer the reader to Zeineldeen et al. (2021) .\nseparate LM : The ILM is approximated by training a separate LM on the target side of the parallel training data.\nh = 0 : The ILM is approximated by taking the fully trained NMT model P MT (e I 1 |f J 1 ) and setting the encoder outputs h J 1 to 0. h = h avg : Instead of setting all encoder outputs h J 1 to 0, we replace the vector h j for each position j with the average h avg j , extracted over the whole parallel training data. We tokenize the data using byte-pair-encoding (Sennrich et al., 2016b; Kudo, 2018) with 15k joint merge operations (40k for WMT14). The models are implemented using the fairseq toolkit (Ott et al., 2019) following the transformer base architecture (Vaswani et al., 2017) . The details of the training setups can be found in Appendix A. All systems are trained until the validation perplexity no longer improves and the best checkpoint is selected using validation perplexity as well. We use beam-search with beam-size 12 and utilize SacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) 2 https://data.statmt.org/news-crawl/ 3 https://data.statmt.org/news-commentary/v14/ and TER (Snover et al., 2006) . We report BLEU and TER since we are most familiar with these metrics and to be comparable with previous works. However, we acknowledge that these metrics might have some biases and in future work it might be worth utilizing additional metrics like COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) . Additionally, in future work we should separate our test sets for original source and target text to better understand the effect of translationese in both training and test data, as this might very much influence the improvements we see, especially in the case of back-translation (Freitag et al., 2020) .\n\nComparison of ILM Approximations\nWe start by analyzing the ILM neutralization approaches on the IWSLT En\u2192De task and then verify the results on the other tasks. We implement and re-train (if applicable) all the different ILM approximation methods discussed in Section 3. The resulting perplexities on the validation set are listed in Table 1 . The variants separate LM and mini-self-attention have been trained directly using the language model objective, so it is no surprise that they exhibit a much lower perplexity than the other approaches. However, it can be argued that a lower perplexity of the ILM does not necessarily correspond to a better approximation of the implicit language model. In order to effectively use the external LM and the ILM during decoding, we need to optimize the weights \u03bb 1 and \u03bb 2 (see Section 3). We do this via a grid search over the validation set by optimizing for the highest BLEU score. The resulting grid for the mini-self-attention ILM variant on the IWSLT En\u2192De task is shown in Figure 1 .\nThe NMT system by itself has a BLEU [%] score of 21.2. By log-linear combination with just the external LM (\u03bb 2 = 0, vanilla shallow fusion) we can gain around 1% absolute improvement on the validation set with the best choice of \u03bb 1 = 0.15. By including the ILM with a negative weight, we can get further improvements, up to a final score of 23.8 BLEU [%] . 4 Interestingly, the best performance is reached when \u03bb 1 \u2248 \u03bb 2 and with the ILM neutralization, the external LM can be assigned a much bigger weight compared to the case \u03bb 2 = 0. We find that for all ILM approximation variants, the optimal weights are similar, and that the TER scores on the validation set follow an almost identical pattern. The final performance of each variant on the test set is shown in Table 2 .\nWe want to point out, that the improvements we see on the validation set transfer nicely to the test set with the same tuned weights \u03bb 1 and \u03bb 2 . This is because, in our experiments, the validation and test sets are of the exact same domain. In some additional experiments we found that the optimal values for these weights are indeed domain specific and have to be re-tuned if the system were to be optimized for a different domain. All ILM approximation variants lead to a significant performance improvement over simple shallow fusion. Out of all ILM approximations, the mini-self-attention approach performs best, which is the same observation that Zeineldeen et al. (2021) made for ASR.\n\nComparison to Back-Translation\nFor the back-translation experiments, we train NMT systems on the same parallel training data in the reverse direction and then translate a total of 10M sentences from the monolingual target data (the same data used for training the external LM). Afterwards, the final systems are trained on the combination of real and synthetic data. The final results for all four MT tasks are shown in Table 3 .\nWe observe the same trend for all four MT tasks. In general, the improvements from the additional monolingual data are getting smaller, when the amount of parallel training data increases. In almost all cases, shallow fusion gives a small improvement over just using the NMT system. ILM neutralization again improves consistently over simple shallow fusion, with the mini-self-attn approximation variant always performing the best. Back-translation out-performs language model integration on all four tasks, although the gap is getting smaller the more parallel training data is available.\nWe also combine back-translation with the best ILM approximation approach (mini-self-attn). This does not further increase translation quality, with the exception of the WMT14 task, where we see a small improvement. In general, the ILM approach performs the closest to back-translation on the WMT14 task, so it might be worthwhile to apply this concept to an even bigger MT task.\n\nConclusion\nWe re-visit the method of language model integration for neural machine translation. We implement and experiment with a new approach of neutraliz-ing the implicit language model, which has already shown promising result for the task of automatic speech recognition. We find that ILM neutralization significantly improves the translation quality compared to standard shallow fusion. However, back-translation as an alternative way to incorporate additional monolingual data, still outperforms the approaches using an external language model. Therefore, for future work we will focus on scenarios where back-translation can not be applied effectively, e.g. when the quality of the initial NMT system is too bad to create helpful synthetic data.\n", "hypothesis": " The integration of language models for neural machine translation has been extensively studied in the past.  It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an explicit target-side language model during training, which interferes with the external language model at decoding time.  Recently, some works on automatic speech recognition have demonstrated that, if the explicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model.  In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data -namely backtranslation.  We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation..", "answer": false}
{"title": "Solving Cosine Similarity Underestimation between High Frequency Words by \u2113 2 Norm Discounting", "content": "\nIntroduction\nCosine similarity is arguably the most popular word similarity measure used in numerous natural language processing (NLP) tasks, such as question answering (QA), information retrieval (IR) and machine translation (MT) (Echizen-ya et al., 2019; Oniani and Wang, 2020; Kim et al., 2022; Hanifi et al., 2022) . First, a word is represented by a vector (aka embedding) and then the similarity between two words is computed as the cosine of the angle between the corresponding vectors (Rahutomo et al., 2012) . Despite the good performance of cosine similarity as a similarity measure in various downstream tasks, Zhou et al. (2022) showed that it systematically underestimates the true similarity between highly frequent words, when computed using contextualised word embeddings obtained from MLMs such as BERT (Devlin et al., 2018) .\nCompared to the problem of estimating similarity between highly frequent words, the opposite problem of estimating the similarity between (or involving) rare (low frequency) words has received greater attention, especially in the scope of static word embeddings (Levy and Goldberg, 2014; Hellrich and Hahn, 2016; Mimno and Thompson, 2017; Wendlandt et al., 2018) . If a word is rare in a corpus, we might not have a sufficiently large number of contexts containing that word to learn an accurate embedding for it. This often leads to unreliable similarity estimations between words and has undesirable implications in downstream tasks such as the detection of analogies and social biases (Ethayarajh et al., 2019a,b) .\nOn the other hand, Zhou et al. (2022) studied the impact of frequency on contextualised word embeddings and showed that the cosine similarity between highly frequent words are systematically underestimated. Unlike in the previously discussed low frequency word scenario, we do have adequate contexts to learn an accurate semantic representation for highly frequent words. Therefore, it might appear surprising at first that cosine similarity cannot be correctly estimated even for the highly frequent words. Zhou et al. (2021) show that the diversity (measured by the volume of the bounding hypersphere) of the contextualised embeddings of a target word, computed from multiple contexts containing the word, increases with the frequency of that word. They provide an explanation that holds true only for 2-dimensional embeddings, which relates diversity to the underestimation of cosine similarity. Unfortunately, this explanation does not extend to the high dimensional embeddings used in practice by the NLP community (e.g. BERT token embeddings are typically more than 768 di- When the log-frequency of w in the corpus increases, cosine similarities computed for both contexts that express the same meaning of w as well as its different meanings decreases. mensional). More importantly, to the best of our knowledge, no solution has been proposed in the literature to address the cosine similarity underestimation problem associated with the highly frequent words.\nIn prior work, the \u2113 2 norm of a static word embedding has been shown to linearly correlate with the log-frequency of that word (Arora et al., 2016; Bollegala et al., 2018) . On the other hand, we empirically study the \u2113 2 norm of the contextualised embedding of a word w averaged over all of its contexts, and find that it too approximately linearly correlates with the log-frequency of w in the corpus used to pretrain the MLM. Recall that the cosine similarity is defined as the inner-product between two embeddings, divided by the \u2113 2 norm of those embeddings. Therefore, we suspect that the underestimation of cosine similarity between highly frequent words is due to the larger \u2113 2 norms associated with those words.\nTo correct for this bias associated with the \u2113 2 norms of highly frequent words, we propose a linearly parameterised discounting scheme in the logfrequency space. Specifically, we use Monte-Carlo Bayesian Optimisation (Balandat et al., 2019) to find the optimal discounting parameters. Our proposed discounting method is shown to accurately correct the underestimation of cosine similarities between highly frequent words on the Word-in-Context (WiC) (Pilehvar and Camacho-Collados, 2019) dataset where human similarity ratings are available for the same word in two different con-texts. Source code for reproducing the experiments reported in this is paper is publicly available. We approximate the word frequencies in BERT pretraining corpus using the BookCorpus (Zhu et al., 2015) . Let \u03c8 w be the frequency of w in this corpus.\nWe use the WiC dataset, which contains 5428 pairs of words appearing in various contexts with annotated human similarity judgements. WiC dataset is split into official training and development sets, while a separate hidden test set is used by the leaderboard for ranking Word Sense Disambiguation systems.\n3 WiC dataset contains pairs of contexts labelled as having the same meaning (e.g. \"to drive sheep out of a field\" vs. \"to drive the cows into the barn\") and different meaning (e.g. \"the play lasted two hours\" vs. \"they made a futile play for power\").\nWe compute the cosine similarity between the two contextualised embeddings of a target word in two of its contexts to predict a similarity score. Figure 1 shows the predicted similarity scores for both contexts in which a target word has been used in the same or different meanings for all words in the WiC dataset against log(\u03c8 w ). As seen from Figure 3 , \u03c8 w has a power-law distribution. Therefore, we plot its log instead of raw frequency counts in Figure 1 .\nFrom Figure 1 , we see that for both same as well as different meaning contexts, the predicted cosine similarities drop with the word frequencies. Moreover, the gradient of the drop for same meaning pairs (Pearson's r = \u22120.3001) is larger than that for the different meaning pairs (r = \u22120.2125), indicating that the underestimation of cosine similarity is more sever for the similar contexts of highly frequent words.\n\n\u2113 2 norm Discounting\nTo understand the possible reasons behind the cosine similarity underestimation for highly frequent words discussed in \u00a7 2, for each word w we compute its mean sibling embedding, \u0175, given by (1).\nEQUATION\nWe plot || \u0175|| against log(\u03c8(w)) in Figure 2 separately for a predefined set of stop words and all other words (i.e. non-stop words). For this purpose, we use the default 1466 stop words from NLTK and randomly selected 997,425 non-stop words from the BookCorpus. Pearson r values of stop words and non-stop words are respectively 0.1697 and 0.3754, while the lines of best fits for each class of words are superimposed. From Figure 2 , we see that overall, || \u0175|| increases with log(\u03c8 w ) for both stop and non-stop words, while the linear correlation is stronger in the latter class. Considering that stop words cover function words such as determiners and conjunctions that co-occur with a large number of words in diverse contexts, we believe that the \u2113 2 norm of stop words mostly remains independent of their frequency. Recall that the cosine similarity between two words is defined as the fraction of the inner-product of the corresponding embeddings, divided by the product of the \u2113 2 norms of the embeddings. Therefore, even if the inner-product between two words remain relatively stable, it will be divided by increasingly larger \u2113 2 norms in the case of highly frequent words. Moreover, this bias is further amplified when both words are high frequent due to the product of \u2113 2 norms in the denominator.\nTo address this problem, we propose to discount the \u2113 2 norm of a word w by a discounting term, \u03b1(\u03c8 w ), and propose a discounted version of the cosine similarity given by (2).\ncos \u03b1 (x, y) = x \u22a4 y ||x|| \u03b1(\u03c8 x ) ||y|| \u03b1(\u03c8 y ) (2)\nFollowing Figure 2 , we linearly parameterise \u03b1(\u03c8 w ) separately for stop vs. non-stop words as in (3).\n\u03b1(\u03c8 w ) = { 1 + m s (b s \u2212 log(\u03c8 w )) w is a stop word 1 + m n (b n \u2212 log(\u03c8 w )) w is a non-stop word (3)\nThe scalar parameters m s , m n , b s and b n are estimated as follows. First, we randomly initialise all parameters uniformly in [0, 1] and use (2) to predict cosine similarity between two contexts in which a target word w occurs in the WiC train instances. We then make a binary similarity judgement (i.e. same or different meaning) for the pair of contexts in an instance depending on whether the predicted cosine similarity is greater than a threshold \u03b8. Next, we compute the overall binary classification accuracy for the similarity predictions made on the entire WiC training dataset, Figure 4 : Cosine similarity between two instances of the same word w in two contexts in the WiC train dataset, computed using the original (non-discounted) cosine similarity (shown in blue and green respectively for the same and different meaning pairs) and using the proposed \u2113 2 norm discounted (( 2)) (shown in orange and red respectively for the same and different meaning pairs). We see that the gradients of the drops have decreased for both same and different meaning pairs after applying the discounting. and use Bayesian Optimisation to find the optimal values: \u03b8 = 0.545, m s = 0.00422, b s = 0.643, m n = 0.00427 and b n = 4.821. Specifically we used the Adaptive Experimentation Platform 4 for learning those optimal values. We found this is more efficient than conducting a linear search over the parameter space. We repeat the estimation five times and use the averaged parameter values in the remainder of the experiments. Note that m n > m s above, which indicates that non-stop words must be discounted slightly more heavily than the stop words. This makes sense since the impact of word frequency of non-stop words on their \u2113 2 -norm is stronger than that for the stop words as indicated by the slopes of the lines of best fit in Figure 2 .\n\nResults\nTo evaluate the effect of the proposed \u2113 2 norm discounting when computing cosine similarity, we repeat the analysis presented in Figure 1 using (2) to predict the similarity between contextualised word embeddings. Comparing the lines of best fit for the original (blue, r = \u22120.3006) vs. discounted (orange, r = \u22120.1366) for the same meaning contexts, we see that the gradient of the drop has decreased by 51.65%. Likewise, comparing the lines of best fit for the original (green, r = \u22120.2125) vs. dis- counted (red, r = \u22120.0843) for the different meaning contexts, we see the gradient of the drop has decreased by 57.04%. This result clearly shows that the proposed \u2113 2 norm discounting method is able to reduce the underestimation of cosine similarities for the highly frequent words.\nGiven that the discounting parameters in (3) are learned from the WiC train data, it remains an open question as to how well the proposed discounting method generalises when predicting similarity between contextualised embeddings of unseen words. To evaluate this generalisability of the proposed method, we use (3) with its learned parameters from WiC train data, to predict the similarity between contextualised word embeddings in WiC dev data.\n5 Specifically, we predict binary (same vs. different meaning) similarity labels according to the similarity threshold \u03b8 learnt in \u00a7 3 and compare against the human judgements using binary classification accuracy.\nThe maximum accuracy on WiC dev split obtained using the original (non-discounted) cosine similarities is 0.6667, which indicates that the cosine similarity is somewhat predictive of the human binary judgements. The overall F1 is improved by 2.4% (0.68 with original cosine vs. 0.71 with the proposed discounting method) and recall is improved by 12% (0.75 with original cosine vs. 0.84 with the proposed). On the other hand, the drop in precision is 4.7% (from 0.64 to 0.61). Therefore, the proposed method solves the cosine similarity underestimation problem associated with high-frequent words, without significantly affecting the similarity scores for low-frequent ones Figure 5 shows the average proportion of instances predicted to be the same meaning as a function of frequency, grouped into ten bins, each with the same number of examples. From Figure 5 , we see that in high frequency bins (i.e. bins 8, 9 and 10), the percentage of predicted instances as having the same meaning is consistently lower than that compared to the human judgements. This shows an underestimation of the true (human judged) similarity between contextualised word embeddings.\nOn the other hand, when we use the proposed \u2113 2 norm discounted cosine similarity (defined in (2)), in the highest frequent bin (i.e. 10) we see that the gap between human judgements vs. predicted similarities has reduced. Moreover, in the low frequency bins (i.e. 1-4), we see that the proposed discounting method does not affect the predictions made using cosine similarities. We see an overestimation of the cosine similarities in the low frequency bins as reported by Zhou et al. (2021) . As discussed already in \u00a7 1, the word embeddings learnt for low frequency words tend to be unreliable due to data sparseness. Therefore, we believe it is important to focus on the problem of learning accurate word embeddings rather than to adjust cosine similarities between low-frequency words in a post-processing step.\nWe see that in bins 5, 6 and 7 the similarity scores are slightly increased by the proposed discounting method, which is a drawback that needs to be addressed in future work. More importantly however, the overall percentage recall across all bins for retrieving same meaning instances improves significantly from 74.7% to 83.7% compared to using respectively the original cosine similarity vs. the discounted cosine similarity. Overall, this result confirms the validity of the proposed discounting method for addressing the underestimation of cosine similarity involving highly frequent words.\n\nConclusion\nWe proposed a method to solve the cosine similarity underestimation problem in highly frequent words. Specifically, we observed that the \u2113 2 norm of a contextualised word embedding increases with its frequency in the pretrain corpus and proposed a discounting scheme. Experimental results on WiC dataset confirmed the validity of the proposed method.\n", "hypothesis": " Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words (Zhou et al., 2022).  This similarity underestimation problem is particularly severe for highly frequent words.  Although this problem has been noted in prior work, no solution has been proposed thus far.  We observe that the \u2113 2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus.  Consequently, the larger \u2113 2 norms associated with the highly frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores.  To solve this issue, we propose a method to discount the \u2113 2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words.  We show that the so called stop words behave differently from the rest of the words, which require special consideration during their discounting process.  Experimental results on a contextualised word similarity dataset show that our proposed discounting method accurately solves the similarity underestimation problem..", "answer": true}
{"title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n", "hypothesis": " Caution: this paper contains potentially offensive or upsetting model outputs.\nSocietal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute expensive, a variety of approaches have been previously proposed that exacerbate the bias in a pre-trained model. While the majority of current state-of-the-art biasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to increase gender bias in pretrained models.  Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced.  Since our proposed method only needs a few training examples, our fewshot debiasing approach is highly feasible and practical.  Through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability..", "answer": false}
{"title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code", "content": "\nIntroduction\nHuman beings rely heavily on the capacity for causal reasoning (Sloman, 2005; Hagmayer et al., 2007) . People understand the observed facts, predict future events, and speculate about what might have happened if things had been different with the help of their causal reasoning skills. For instance, when we go home and find a mess, we probably want to figure out why it happened. If we determine that a bird flew into the house, we might then consider whether the mess could have been avoided if we had closed the window.\nAlthough large language models (LLMs) demonstrate great language understanding and generation abilities, it is still challenging for them to perform complex causal reasoning such as the example above. Powerful LLMs are able to understand single cause-and-effect relations (Brown et al., 2020;  Figure 1 : Causal relationships between events in two causal reasoning tasks. Wang et al., 2021) , like a man losing his balance causes him to fell. However, when it comes to more complex causal structures involving multiple events and alternative branches (like close the window or not), LLMs perform much inferior to humans (Bhagavatula et al., 2019; Qin et al., 2019) . In this paper, we consider two challenging causal reasoning tasks: abductive reasoning and counterfactual reasoning. Abductive reasoning requires models to generate a plausible reason for the ending while being consistent with the premise. Counterfactual reasoning asks what will occur in the counterfactual branch. Causal relationships between events in these tasks are shown in Figure 1 .\nA potential difficulty for LLMs to learn complex causal structures is that they are rarely expressed explicitly in the text. News articles or narratives may contain multiple events with causal relationships, like an incident and a chain of consequences. However, these events are often written chronologically, and it is hard to extract the causal structure from the text without further annotation. Branches are expressed rarer in text, except for the multi-branching storytelling style (Nisi and Haahr, 2006) .\nOn the other hand, causal relations are exhibited more commonly in code. Conditional statements like if direct the computer to execute certain commands, provided a condition is met. This explicitly demonstrates the causal relationship between the condition block and the execution block. Code can also express branching with elif or switch statements, and the nesting feature enables code to describe more complex structures 1 . This motivates us to utilize code models in natural language causal reasoning. Recently, large language models of code (Code-LLMs) are receiving increasing attention (Chen et al., 2021; Xu et al., 2022) . They exhibit strong code generation performance, and their structural prediction abilities help complete structural natural language tasks like argument graph generation (Madaan et al., 2022) and event argument extraction (Wang et al., 2022b) . Being pre-trained on code with abundant causal expressions, Code-LLMs may also have gained better causal reasoning abilities.\nWe conduct experiments on the unsupervised abductive reasoning and counterfactual reasoning tasks. To generate task outputs, we design code prompts like Figure 2 to clearly represent the causal structures of the tasks. Results show that Code-LLMs with code prompts perform much better than text-only LLMs and previous methods. To better understand why the code prompts are effective, we break down the prompts and analyze the influence of different aspects. We find that Code-LLMs are very sensitive to the programming structure (specifically, the conditional statements), while being robust towards format perturbations and programming language changes.\nOur main contributions are as follows: 1) We design code prompts to tackle causal reasoning tasks, by leveraging conditional statements in code to represent causal structures. 2) We evaluate Code-LLMs with code prompts on the abductive reasoning and counterfactual reasoning tasks, and exhibit that code models with code prompts are better causal reasoners than text models. 3) We break down the code prompt in detail and find that the programming structure is crucial to the performance.\n\nModeling Causal Structure with Code\nWe convert the input of causal reasoning tasks into the form of code prompt for Code-LLMs to understand better. We expect the prompts to meet two requirements: 1) clearly represent the causal relationships between events, and 2) as most Code-LLMs only support generating at the end, the target output should appear at the end of the prompts. The first requirement is addressed with conditional statements. However, for the second, the target prediction is not always the last part of the conditional statements, e.g., in abductive reasoning we want to predict the hypothesis, which is the condition in the if structure. To address this, we uniformly use functions to represent events. As shown in Figure 2 , the causal structure is described in the main function. All the event functions are listed afterwards, leaving the target event function at the last.\nAbductive Reasoning. Abductive reasoning requires models to generate a plausible hypothesis H given the observations: premise P and ending E. The chronological order of these three events is P \u2192 H \u2192 E, and the hypothesis causes the ending to occur.\nIn Figure 2 , we regard the task definition as an instruction and place it as a comment at the beginning of the prompt. The causal structure is represented in the main function like: executing the premise, and if the hypothesis is met, executing the ending 2 . The content of each event is presented as a comment of its function. The hypothesis function is placed at the last, leaving for models to complete. The generation process stops with a line break.\nCounterfactual Reasoning. Counterfactual reasoning aims to rewrite a story under a counterfactual condition. As in Figure 1 , the input consists of four parts: the premise P , the initial context C 1 , the original ending E 1 , and the counterfactual context C 2 . Models are asked to generate the counterfactual ending E 2 that minimally modifies the original ending E 1 and is coherent with the counterfactual context C 2 .\nThe causal relationships are represented with the if-elif structure. The premise P is executed first, and then if the initial context C 1 is met, the original ending E 1 is executed; otherwise, if the counterfac-tual context C 2 is met, the counterfactual ending E 2 will be executed. For ease of exposition, we call the context hypothesis as well, being consistent with the former task. The event contents are also written as comments for event functions. We use # end to mark the finish of the ending. provides a brief introduction of these methods.\nAutomatic Evaluation. We use the following automatic evaluation metrics: BLEU 4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) 1 reports the automatic evaluation results in the zero-shot setting. CODEX significantly outperforms previous methods and DAVINCI on both tasks (with significance level \u03b1 = 0.01), exhibiting strong causal reasoning ability. Although the two DAVINCI models are based on CODEX, their causal reasoning abilities may be weakened during instruction tuning, and this phenomenon is called alignment tax (Ouyang et al., 2022) . DAVINCI 003 underperforms DAVINCI 002 on most metrics, probably because it tends to generate longer and more discursive outputs, which do not comply with the tasks.\nHuman Evaluation. We conduct pairwise comparison between CODEX and DAVINCI 002 on 100 test examples. Annotators are asked to choose the better output given the task requirements. For abductive reasoning, the outputs are rated from three aspects: coherence with the premise, coherence with the ending, and the overall coherence. For counterfactual reasoning, the outputs are rated from coherence with the context and the extent of preserving the original ending. Each example is rated by at least two annotators, and the average interrater reliability is 0.64.\nThe results are shown in Table 2 . CODEX outperforms DAVINCI 002 in all aspects. It better considers the context in generation, and is able to preserve the original content in counterfactual reasoning.\nContributions of the Model and the Prompt. We exchange the prompts of code and text models, to measure the contributions of the model and the prompt. The results are in Table 3 . We find that CODEX performs better with the code prompt, as the code prompt clearly describes the causal relation between events. Code prompts benefit the text model DAVINCI 002 on abductive reasoning, but have negative impacts on counterfactual reasoning. A possible reason is that the causal structure in counterfactual reasoning is more complicated, leading to a more complex code which is harder for text models to understand.\n\nWhat are Crucial in Code Prompts?\nTo paint a better picture of the key points in the code prompts, we intervene on the prompts from four aspects and measure the influences of the interventions. The four aspects we select are information, structure, format, and language. The former two, the prior information provided and the programming structure of functions, are contentrelated; the latter two, the code format and programming languages, are form-related. An ideal model should rely on the content and be insensitive to form perturbations. Information. We study two types of prior information: task instructions and function names. In No Instruction, we remove the task instruction from the prompts. In Function Name Perturbation, we replace original function names with anonymous functionX. For example, we replace premise() and hypothesis() in Figure 2 with functionA() and functionB(), respectively. It eliminates the information in function names and only allows models to learn the event relations from programming structures.\nStructure. The first way to intervene in the programming structure is to convert the conditional structures into sequential structures, referred to as Sequential Structure. The events are executed sequentially, like premise(), hypothesis(), ending() in abductive reasoning. In the second way called Disruption, we randomly disrupt the positions of the functions in the conditional structure.\nFor instance, if hypothesis(): ending() can be disrupted into if ending(): hypothesis().\nWe also apply the function name perturbation in disruption to eliminate the impact of function names.\nFormat. We test three formats besides the original one: Class, Print and Return. The first one converts the original code into a class. We define the programming structure in the __init__ method, and move the event functions into the class. In Print, we represent the content of events as a string and print it in the function body, like def premise(): print(\"The Smiths ...\"). And in Return, the string is the return value of event functions.\nLanguage. We also convert the original Python programs into two other languages, Java and C, to evaluate the influence of programming languages. CODEX is quite robust towards format and language changes. Settings like Class and Java are even better than the original one, revealing that the performance can be further improved with delicate prompt engineering.\n\nConclusion\nWe investigate the causal reasoning ability of Code-LLMs. With code prompts of conditional statements, Code-LLMs achieve great performance in abductive and counterfactual reasoning, outperforming text-only LLMs significantly. Our study on different aspects of code prompts shows that providing a reasonable causal structure in code can help generate plausible outputs, and Code-LLMs are robust towards format perturbations.\n", "hypothesis": " Causal reasoning, the ability to identify causeand-effect relationship, is crucial in human thinking.  Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.  Given the fact that programming code may express causal relations more often and explicitly with conditional statements like if, we want to explore whether Code-LLMs acquire better causal reasoning abilities.  Our experiments show that compared to textonly LLMs, Code-LLMs with code prompts are significantly better in causal reasoning.  We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.  Code and data are available at https://github.com/xxxiaol/magic-if..", "answer": true}
{"title": "RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation", "content": "\nIntroduction\nText style transfer (TST) is a task that aims to control stylistic attributes of an input text without affecting its semantic content (Jin et al., 2022) . Research in TST has largely focused on English, thanks to the availability of large monolingual English datasets covering stylistic attributes like formality and simplicity (Rao and Tetreault 2018, Zhu et al. 2010, inter alia) . In recent years, however, multilingual and cross-lingual applications of TST have seen a steady gain in popularity (Briakou et al., 2021; Garcia et al., 2021; Krishna et al., 2022) . A notable instance of cross-lingual TST is attributecontrolled translation (ACT), in which attribute 1 conditioning is performed alongside machine translation (MT) to ensure that translations are not only Neutral Src (EN) After retiring from teaching, Cook became a novelist.\n\nFeminine Ref (NL)\nNadat ze stopte met lesgeven, werd Cook schrijfster.\nMasculine Ref (NL) Nadat hij stopte met lesgeven, werd Cook schrijver.\nTable 1 : Examples of attribute triplets from COCOA-MT and MT-GENEVAL. Attribute markers in the attribute-controlled translations are underlined.\ncorrect but match user-specified preferences, such as formality/honorifics (Sennrich et al., 2016; Niu et al., 2017; Michel and Neubig, 2018; Niu and Carpuat, 2020; Nadejde et al., 2022; Wang et al., 2022) , gender (Rabinovich et al., 2017; Vanmassenhove et al., 2018; Saunders and Byrne, 2020) , and length (Lakew et al., 2019; Schioppa et al., 2021) .\nACT is especially important for sectors like customer service and business communication, where stylistic differences can have an impact on user perception (e.g., misgendering customers or speaking to them in an appropriately informal tone can be offensive or disconcerting). Table 1 gives examples of ACT for formality and gender. Most prior work on ACT relies on a supervised adaptation component that conditions the generative model on the selective attribute. However, few annotated ACT datasets are available, and they generally cover only a limited set of languages and attributes. Thus, enabling few-shot or zero-shot ACT would facilitate applying attribute control to less-resourced attributes and langauges.\nIn this paper, we introduce a new approach for ACT: Retrieval and Attribute-Marking enhanced Prompting (RAMP). Recent studies have shown that large language models (LLMs) can perform MT out of the box using the prompting paradigm (Brown et al., 2020; Lin et al., 2022; Chowdhery et al., 2022) . We build on this, prompting LLMs to perform attribute-controlled MT through two innovations: ( 1 Here is a sentence: {You will always be welcome here.} Here is its Spanish translation written in a formal style: {Siempre ser\u00e1 bienvenido aqu\u00ed.} The translated sentence conveys a formal style by using words such as 'ser\u00e1'.\n----Here is a sentence: {I wish you welcome and enjoy your stay.} Here is its Italian translation written in a formal style: {Le do il benvenuto e si goda il soggiorno.} The translated sentence conveys a formal style by using words such as 'Le', 'si goda'.\n----Here is a sentence: {You're welcome.} Here is its French translation written in a formal style: { EN: You're welcome. explicit attribute marking.\nRecent works adopting the prompting paradigm for text style transfer have mainly focused on the generalization capabilities of large English-centric LMs for zero-shot style transfer using previously unseen style descriptions (Suzgun et al., 2022; Reif et al., 2022) . However, prior work on other NLP tasks has shown that cross-lingual prompting of multilingual LLMs can be effective (Zhao and Sch\u00fctze, 2021; Zhou et al., 2022; Huang et al., 2022) . As such, we leverage multilingual LLMs and extend their ACT capabilities cross-lingually to languages not covered by the in-context examples, thus enabling zero-shot ACT.\n\nPreliminaries\nAttribute-Controlled Translation ACT takes two inputs, a sentence x and a desired target attribute a \u2208 A (with A being the space of attributes), and outputs a translation y that complies with the specified attribute. It can be formulated as a function f : (x, a) \u2192 y. In our experiments, we use attribute values provided by the COCOA-MT formality translation dataset and the MT-GENEVAL gender translation dataset, i.e., A = {formal, infor-mal} or {female, male}. 2 Prompting In the prompting paradigm for decoder-only LLMs, inputs are given as decoding prefixes to the model, usually combined with natural language instructions for output generation. In style-controlled translation, we formulate the prompt for target language l and attribute a using the text \"Here is a sentence: {x} Here is its l translation written in a a style:\" to produce the 2 See Section 5 for ethical considerations. output y. 3 In the few-shot setting, we provide a sequence of k labeled in-context examples before the unlabeled input, which can be formulated as a function f : {(x 1 , l 1 , a, y 1 ), . . . , (x k+1 , l k+1 , a)} \u2192 y k+1 .\n\nOur Approach: RAMP\nRAMP builds on the success of the prompting paradigm on few-shot generation tasks such as monolingual text style transfer (Reif et al., 2022) and MT (Garcia and Firat, 2022; Agrawal et al., 2022) by creating more informative prompts through similarity retrieval and attribute marking. See Figure 1 for an illustration of RAMP.\n\nSimilarity Retrieval\nIn standard prompting, incontext examples are sampled randomly from the pool of labeled examples D A . In RAMP, we select examples based on their similarity with the input text. We first embed both the input text and the source texts of D A using all-MiniLM-L6-v2 (Wang et al., 2020) . Then, the top-k most similar examples are retrieved for the input text based on cosine similarity. These are then used in a descending order w.r.t. similarity as the in-context examples in the inference prompt. As demonstrated in Figure 1 , the in-context example \"You will always be welcome here.\" has the highest similarity to the test example \"You're welcome.\" so it is prompted first.\n\nAttribute Marking\nIn standard prompting, incontext examples are provided without explicit information on why they satisfy the prompting objective. Inspired by recent studies that have shown that decomposition of complex tasks can improve prompting quality (Nye et al., 2021; Wei et al. , 2022), we include for every in-context example an additional sentence directly after the target sentence that specifies which text spans convey the desired attribute (e.g., \"The translated sentence conveys a formal style by using words such as 'Vous'.\"). In our experiments, we use the gold attribute spans included in the CoCoA-MT and MT-GenEval datasets. In section 4 we suggest possibilities for automatically deriving attribute spans when gold training labels are not available.\nAR ES FR HI PT DE IT JA RU NL COCOA-MT \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 MT-GENEVAL \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 XGLM \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 BLOOM \u2713 \u2713 \u2713 \u2713 \u2713\n\nCross-Lingual Prompting\nThe similarity retrieval component of RAMP requires a large pool D A from which to find appropriate incontext examples for prompting. Low-resource attributes or language pairs may have insufficient or no annotated data from which to retrieve such examples. To mitigate this issue, we introduce crosslingual prompting, in which the target side of the in-context examples differs from the desired target language of the translation task. As demonstrated in Figure 1 , we study whether the system can leverage examples in one language (e.g., attribute indicators in Spanish) to produce the same attribute in another (e.g., French). Two main features of our RAMP model allow us to perform cross-lingual prompting: (1) the use of multilingual LLMs, and (2) the example retrieval step, which is done on the source language only.\n3 Experiments\n\nDatasets\nWe experiment on two multilingual ACT datasets: instead explicitly controlling target gender. Both datasets have gold annotations for attributemarked target spans, and both cover translation from English into multiple diverse target languages. We list their target languages in Table 2 .\n\nLarge Language Models (LLMs)\nWe select three massively multilingual decoderonly LLMs for the prompting experiments: XGLM (Lin et al., 2022) , BLOOM (BigScience, 2022) and GPT-NEOX (Black et al., 2022) . The selected models span three orders of magnitude in terms of number of parameters and differ in the languages that they cover (see Table 2 ). Appendix D motivates our choice of models in more detail. GPT-3 is not included because it is not freely accessible and it is not intended for multilingual use-cases.\n\nBaseline\nAttribute tagging is a standard method for ACT, so we include a baseline following the approach and configuration used by Nadejde et al. ( 2022): a transformer MT model (Vaswani et al., 2017) pre-trained on public parallel data and further finetuned on contrastive training pairs with attribute tags (from either COCOA-MT or MT-GENEVAL). We refer to this as adapted MT.\n\nEvaluation Metrics\nWe measure translation quality with BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020) . For attribute accuracy, we use both (1) the lexical matching metrics provided with COCOA-MT and MT-GENEVAL (Lexical-Accuracy) and ( 2) sentence encoders trained on contrastive examples (Sentential-Accuracy). For (2), we train multilingual classifiers on top of the mDeBERTa-v3 encoder (He et al., 2021) . High-performance pretrained classifiers have been shown to produce attribute accuracy estimates closer to human judgments for style transfer (Lai et al., 2022) . Table 3 presents the accuracy of the classification models on the test sets of their respective datasets, averaged over all languages. Unlike lexical accuracy, the multilingual attribute classifier does not penalize text generated in incorrect languages. Thus, in cross-lingual prompting experiments, we include a step of language detection 5 so that generated sentences not in the requested target language are considered incorrect.\n\nResults: Same-Language Prompting\nWe first evaluate the effectiveness of RAMP for formality-and gender-controlled translation where the language pair used for in-context examples is the same as the one used in the prompt candidate (e.g., EN\u2192ES formality-controlled translation using EN\u2192ES in-context examples). We test XGLM 7.5B and BLOOM 175B with 16 in-context examples on both tasks. 6 Table 4 presents our results alongside the adapted MT baseline. The base model uses in-context examples that are sampled randomly from the pool of labeled examples. We also include an ablation that adds attribute marking only on top of base, without similarity retrieval (+mark).\nUsing just attribute marking consistently improves attribute accuracy of the generated text, but it leads to degradation of COMET on COCOA-MT. The complete RAMP with similarity retrieval not only compensates for the COMET degradation but also improves quality and attribute metrics across the board, especially for the high-capacity BLOOM 175B model.\nAdapted MT outperforms BLOOM 175B on MT-GENEVAL in all metrics, but underperforms it on COCOA-MT. This suggests that it is challenging to do fine-grained comparison between LLMs and standard MT systems as they might have different domain coverage. BLOOM 175B consistently outperforms XGLM 7.5B in both generic translation quality and attribute control accuracy, so we proceed with using BLOOM 175B in the crosslingual prompting setting.\n\nResults: Cross-Lingual Prompting\nWe have demonstrated the effectiveness of selecting similar same-language examples to build the prompt, echoing contemporary work (Liu et al., 2022; Agrawal et al., 2022) . In this section, we evaluate the cross-lingual prompting option, i.e., retrieving in-context examples from other target languages besides the desired language of translation. We test this zero-shot setting using the leave-oneout strategy, and results of tested language pairs are averaged. 7 Table 4 presents our results using BLOOM 175B. On both test sets, compared to the baseline, we observe improved attribute accuracy and comparable or better generic translation quality when using RAMP with cross-lingual prompting.\nWe do observe translation quality degradation with RAMP on some target languages of COCOA-MT, e.g., ES. Manual analysis shows that repeated inaccurate retrieval results could lead to hallucinations. 8 For example, RAMP retrieves multiple sentences containing \"million\" for the input \"If you got it why not? He is worth over 20 billion dollars after all.\". This results in mistranslation of billion to million (millionario): \"Si lo tienes, \u00bfpor qu\u00e9 no? Es millonario despu\u00e9s de todo.\". We give detailed examples in Appendix H.\n\nConclusions\nWe introduced the new RAMP in-context learning approach to leverage attribute annotations and similar same-language or cross-lingual examples for better prompting quality. We demonstrated its effectiveness with multilingual LLMs for both formalitycontrolled and gender-controlled translation. We use gold annotations for attribute marking, but we leave unsupervised automatic attribute span extraction as future work.\n", "hypothesis": " Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs.  While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods.  To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP), which leverages large multilingual language models to perform ACT in few-shot and zero-shot settings.  RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a semantic similarity retrieval component for selecting similar in-context examples, and (2) marking in-context examples with attribute annotations. Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings.\n* Work conducted during an internship at Google.  1 In this paper, we prefer the term attribute rather than style, since not all the attributes addressed here (e.g., gender) can be considered styles..", "answer": false}
{"title": "Distinguishing Address vs. Reference Mentions of Personal Names in Text", "content": "\nIntroduction\nNamed entity recognition (NER) in text has long been a core task in the NLP community (Sundheim, 1995; Yadav and Bethard, 2018) . However, not much work has looked into distinguishing whether an entity mention is an instance of addressing the entity or referring to them:\n\u2022 John, would you turn the light off? (Address) \u2022 John turned the light off. (Reference)\nThe address usage is also called a vocative phrase: \"a noun phrase which does not belong to the thematic grid of a predicate and is used to attract someone's attention\" (Moro, 2003) . Many languages have explicit morphological vocative case markers: e.g., in \"Et tu, Brute?\", Brute marks the vocative case of the nominative Brutus. However, many 1 https://stavatir.com/s/address-vs-reference.xlsx modern Indo-European languages, including English, do not have vocative case markers, and the distinction is left to be interpreted based on context.\nDistinguishing vocative phrases is important in many NLP tasks, such as sentiment analysis (Karami et al., 2020) , offensiveness detection (Mubarak et al., 2020) and information extraction (Makazhanov et al., 2014) . For instance, Karami et al. (2020) point out the difference in interpretations between \"Let's eat, Grandma\" and \"Let's eat Grandma\". The vocative distinction is also important for NLP-aided computational social sciences, since the pragmatics and the patterns of usage vary between these two types of name mentions (Dickey, 1997) , and since name mentions capture various societal biases (Prabhakaran et al., 2019) . This aspect is especially crucial in studies analyzing political discourse, with the goal of understanding the rhetoric by and about political personalities (Prabhakaran et al., 2014; Gupta, 2022) .\nDespite the prevalence of NER as a useful task in various NLP applications (Marrero et al., 2013) , efforts to make this distinction have largely been limited to languages that have explicit vocative case markers such as Portuguese (Baptista and Mamede, 2017) , Hebrew (Tsarfaty et al., 2019) , Korean (Nam and Choi, 1997), and Sindhi (Muslim and Bhatti, 2010) , and not much work has looked into detecting vocative name mentions in English.\nIn this paper, we present a dataset of social media text in the political domain in English language, with person mentions annotated with the address vs. reference distinction. We then build a tagger that is able to make this distinction automatically, with an accuracy of 85%. We use this tagger to demonstrate the importance of this distinction in two largescale computational socio-linguistic analysis. First, we demonstrate that female personalities are more likely to be mentioned in the addressing context than male personalities, across three different social medial corpora, which has implications for NLP research on gender bias in data and models. Second, we demonstrate that sentences with address mentions are significantly more likely to be toxic than those with reference mentions. This finding has important implications for the active area of NLP research on detecting online abuse.\n\nAddress vs. Reference Mentions\nHow a person is addressed or referenced in language, and its associated pragmatics has long been of interest in sociolinguistics (Brown et al., 1960; Brown and Ford, 1961) . While most of this research focused on the different address pronouns and the T/V distinction, much less work has looked into the difference in the social meaning of a mention when used as an address vs. when used as a reference (Dickey, 1997) . While this distinction is not limited to persons (for instance, organizations may also be mentioned in an addressing context, as in Hey Doordash, where is my food?), person name mentions add additional nuance owing to the social relations. For instance, Dickey (1997) show that the words used to address a person by a speaker may differ from the words used to refer to them depending on the social power relations between the speaker, the referent, and the addressee.\nForms of address has been studied in NLP-aided computational sociolinguistics, for instance, in the context of how they relate to social power relations (Prabhakaran et al., 2013) . The address vs. references distinction has also been shown to be of value in NLP tasks, for instance, Mubarak et al. (2020) extracts Arabic tweets with the vocative particle \"yA\" as it indicates directing speech to a person or a group, increasing the likelihood of offensiveness. However NLP work on making this distinction is largely limited to languages that have explicit vocative case markers. In the absence of any vocative markers, as in English, this becomes a task that relies on the syntactic context. In this paper, we build resources to perform and evaluate this distinction, and demonstrate its utility in NLP applications.\nThere is related work in NLP on detecting addressees in multi-party dialog (op den Akker and op den Akker, 2009; Ouchi and Tsuboi, 2016; Le et al., 2019; Ek et al., 2018) , which is a substantially different task from ours. First, addressee detection in multi-party dialog takes into account the larger dialog/content context (e.g., prior utterances). For instance, Ouchi and Tsuboi (2016) jointly captures \"who is talking about what at each time step\" in order to determine the addressee. Ours is a simple linguistic task that relies on the local syntactic context of named mentions, making it applicable in broader contexts. Second, the above work crucially looks into the implicit cues about addressees. In contrast, our work focuses only on explicit mentions, primarily motivated by the computational social science analyses anchored on them.\n\nData\nSource: We use the corpus of Facebook comments on politicians' posts released by (Voigt et al., 2018) for this study. Our choice is motivated by three reasons. First, the comments in this corpus are all made in response to a individual's Facebook post and hence it is likely for it to have more instances of comments addressing the person than general social media data with mentions of that person. Second, the corpus captures the individual's name within the metadata, making it easy to detect and disambiguate different mentions referring to the same person. Finally, the corpus also captures the gender information of the person the comments are in response to (unlike most other gender-labeled data that captures the gender of the speaker/writer) as it was originally developed to study gender bias in social media, which is one of our goals too.\nPre-processing: Since the metadata captures the politician's name that each comment is in response to, we use a regex-based approach to determine if that politician is mentioned in the comment or not. We made sure the regex captures different forms of address including full name mentions, first name mentions, and last name mentions. Furthermore, since the corpus contained comments directed at only 402 politicians, we manually coded different common variations and misspellings of their first and last names. For instance, the first name of the politician Jim Boozman could be mentioned as Jim, James, or Jimmy, and the common variations of his lastname included Boozman, Boozeman, and Bozeman. While some of these choices may be genuine misspellings, some others may indicate pragmatic connotations: Jimmy instead of Jim may have been used to evoke familiarity, while Boozeman instead of Boozman may have been intended to evoke humor or disrespect. We do not analyze these distinctions in this paper, however, we included them in our regex to ensure that we capture such diverse associated linguistic contexts.\nAnnotation: We sampled 800 comments with at most 100 words (to avoid exceedingly long comments) from the corpus. We restricted ourselves to only those comments with a single mention of the individual (i.e., removed comments with no or multiple mentions). Multiple mentions were rare in our data (less than 1%), and when they do happen they were almost exclusively all reference mentions, as it is unlikely for someone to address someone by name, and then refer to them in third person in the same sentence itself. We trained two annotators to make the address vs. reference distinction. The annotators were undergraduate students majoring in Psychology at Yale University. Annotators were provided with the comments, the individual whose post the comment was in response to, as well as the mention of that individual detected in the comment. They were asked to label whether the mention was addressing the individual vs. referencing the individual, along with examples.\nAnalysis: All comments were double annotated, obtaining an inter-annotator agreement of \u03ba = 0.898, suggesting that the task is relatively easy for trained humans, and that our annotations capture reliable data. We then performed an adjudication round where both annotators met with one of the authors and arrived at a final label through discussion. While most disagreements were due to misinterpretations, some cases were inherently ambiguous. For instance, in \"Yes!!! Sen. Booker\", it is ambiguous whether the commenter is addressing Sen. Booker or just mentioning him.\nThe annotation and adjudication process revealed 15 comments where the name mention was not valid; e.g., within a URL contained in the comment, and 11 comments where the comment did not have enough linguistic context to make the distinction; e.g., when the comment was just a name mention. We removed these comments as they will add noise, resulting in 774 comments in the dataset, each with a mention labeled as either address or reference. There were 250 (32.3%) instances that were the address usage compared to 524 (67.7%) instances that were the reference usage.\n\nAutomatic Tagger\nWe now investigate automatically distinguishing address vs. reference, given a text and a name men-tion in it. Since contextualized embeddings such as BERT (Devlin et al., 2019) are proven to capture syntactic information (Clark et al., 2019) , we expect the positional embedding of the name mention to capture its syntactic context and hence help make this distinction. Further, we use the intuition that reference mentions are more likely to occur in syntactic contexts where third person pronouns could fit, while address mentions are more likely to fit second person pronouns or address terms. We consider three settings, each with two sets of words that fit with the address vs. reference contexts: S1: you/your vs. he/him/his/she/her S2: you/your vs. he/him/his/she/her/they/them S3: you/your/hey/hi vs. he/him/his/she/her S1 uses singular pronouns, S2 includes the (usually) plural pronouns they/them, S3 includes addressing terms (hey/hi). For each setting, we use a contextual embedding, replace the mention with [MASK] and calculate the score for each word in the list to fit the masked slot. If the top scored word from the list is of the address category, we predict the mention as address, otherwise, as reference. To illustrate, the top candidate from S3 above for the input \"[MASK], would you turn the light off?\" as per BERT is hey, while the top candidate for \"[MASK] turned the light off \" is he, then she. This approach is not entirely foolproof, but as Table 1 shows, this simple approach yielded good performance of 85% accuracy. We report results using BERT and DistillBERT models across all three settings outlined above. Adding addressing terms hey and hi increased the accuracy, while adding the third person pronouns they and them that are usually used in plural context (but also has singular usage) resulted in reducing the accuracy.\nMost errors happen when the sentence is not well-formed or uses non-standard language. An approach to circumvent this issue is to fine-tune a pre-trained model using our data. In our preliminary experiments, fine-tuning a BERT model only yields marginal (\u223c1%) improvement in accuracy at sentence level. Using more advanced models and hyper parameter tuning may yield better performance. However, our goal in this paper is not to build the best tagger possible for this task, rather to demonstrate the utility of this task in NLP and computational social science applications. Given the high performance of the Slot-filling model, we use it for all analyses in the rest of this paper. \n\nGender Effects in Addressing\nWe first look into the RtGender dataset (Voigt et al., 2018) built to study differential responses to gender. They found that responses to female posters or speakers were more likely to be about the individuals (e.g., their appearance) rather than about the content they posted or talked about. As a complementary analysis, we analyze whether these responses were addressed to the speaker or poster, or referring to them. We apply the tagger to 5K comments each, chosen at random, from three different sub-corpora in the RtGender corpus: comments in response to (1) Facebook posts by politicians (FB Congress), (2) Facebook posts by celebrities (FB Wiki), and (3) TED talk videos (Ted Talks). We ensured that the tagger does not introduce systematic gender bias; t-test revealed no association between gender and error (p = 0.166). Across board, mentions of female personalities were more likely to be in the address rather than reference contexts (Figure 1 ). This difference was statistically significant in all three cases: t(4999) = 3.51, p < .001 (FB Congress); t(4999) = 3.87, p < .001 (FB Wiki); and t(4999) = 4.41, p < .001 (TED Talks). For the congress dataset, we also have access to the political party they belong to; we added it as a control causing the effect size to decrease (2.72) suggesting that political party affiliation plays an important role. In fact, Figure 2 shows that the gender disparity is present only for the Republican party politicians.\nAddressing someone directly could be an expression of friendliness or familiarity, and its prevalence in comments directed at female personalities is notable. These insights enable adding nuance to many NLP-aided studies of gender and power. Moreover, this finding adds to research on gender influences on communication with and about professionals (Atir and Ferguson, 2018) .\n\nAddress vs. Reference and Toxicity\nWe now turn to online abuse detection, an NLP task where address vs. reference distinction is important. Prior work has shown that 2nd person pronouns are spuriously associated with toxic comments (Hede et al., 2021) . In languages such as Arabic that has explicit vocative markers, researchers have used vocative markers to curate comments with higher likelihood of offensiveness (Mubarak et al., 2020) . In this section, we use our tagger to analyze the tox- icity dataset annotated by Jigsaw (Jigsaw, 2018) to see if this pattern holds true. In the Jigsaw dataset, we do not have access to the mentions of people in text. Hence, we created a tagger for the Jigsaw dataset by first using the SpaCy python package to detect person mentions, then used the BERT Slotfilling (S3) tagger to detect whether each person is addressed or referenced in the message.\nWe find significant difference in address vs. reference in toxic vs. non-toxic tweets. The average toxicity score of sentences with address mentions were 0.088, compared to 0.070 for those without; this difference is statistically significant using the standard Student's t-test (p < .001) and a permutation test (p < .001). Figure 3 shows differences in the ratios of address to reference mentions in toxic and non-toxic texts. This finding is important for NLP-aided content moderation, especially in detecting targets of abuse.\n\nDiscussion/Conclusion\nIn this paper, we introduced the basic NLP task of distinguishing a name mention to be address or reference, annotated a new dataset for it in the English language, and presented a simple tagger using contextual word embeddings. Our annotation and tagging experiments reveal this to be a relatively easy task, however our accuracy being only at 85% suggests room to improve. We also demonstrate the utility of this capability in computational social science work anchored on name mentions through two analyses: first, on gender bias in mention patterns, and second, in toxic comments online.\nThis capability is important, but often ignored, for tasks that assume entity mentions to be part of the expressed propositional meaning; e.g., belief modeling (Prabhakaran et al., 2015) , and social relation extraction (Massey et al., 2015) . It will also aid in tasks that model relationships between interactants, such as power (Prabhakaran and Rambow, 2014) and influence (Rosenthal and Mckeown, 2017) . The vocative usage is arguably already being implicitly modeled in tasks such as dialog act tagging. However, it may be important to model it explicitly in certain cases, e.g., our work could contribute to ongoing efforts in detecting addressees in multi-party dialog (Ouchi and Tsuboi, 2016; Le et al., 2019) . Future work should look into these applications, and more advanced modeling techniques such as few-shot training for this task.\n", "hypothesis": " Detecting named entities in text has long been a core NLP task.  However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs.  referring to the entity; e.g., John, would you turn the light off? vs.  John turned the light off.  While this distinction is marked by a vocative case marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context.  In this paper, we present a new annotated dataset that captures the address vs.  reference distinction in English, 1 an automatic tagger that performs at 85% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language..", "answer": true}
{"title": "Black-box language model explanation by context length probing", "content": "\nIntroduction\nLarge language models (LMs), typically based on the Transformer architecture (Vaswani et al., 2017) , have recently seen increasingly widespread adoption, yet understanding their behaviour remains a difficult challenge and an active research topic.\nNotably, as the length of the context that can be accessed by LMs has grown, a question that has attracted some attention is how this influences their predictions. Some recent studies in this line of research suggest that even \"long-range\" LMs focus heavily on local context and largely fail to exploit distant ones (O'Connor and Andreas, 2021; Sun et al., 2021; Press et al., 2021; Sun et al., 2022) . A more nuanced understanding of how contexts of different lengths influence LMs' predictions may hence be valuable for further improving their performance, especially on tasks like long-form text generation where long-range dependencies are of critical importance.\nFigure 1 : A screenshot of a demo 2 of the proposed method. After selecting a target token (here \"birds\"), the preceding tokens are highlighted according to their (normalized) differential importance scores (green = positive, red = negative), obtained using our method. The user can also explore the top predictions for contexts of different lengths (here the context \"house, shouting about lunatics. [. . .] mortally afraid of\").\nIn this work, we propose context length probing, a simple explanation technique for causal (autoregressive) language models, based on tracking the predictions of the model as a function of the number of tokens available as context. Our proposal has the following advantages:\n\u2022 It is conceptually simple, providing a straightforward answer to a natural question: How does the length of available context impact the prediction?\n\u2022 It can be applied to a pre-trained model without retraining or fine-tuning and without training any auxiliary models.\n\u2022 It does not require access to model weights, internal representations or gradients.\n\u2022 It is model-agnostic, as it can be applied to any causal LM, including attentionless architectures like RNN (Mikolov et al., 2010) and CNN (Dauphin et al., 2017) . The only requirement for the model is to accept arbitrary input segments (i.e. not be limited to document prefixes).\nFurthemore, we propose a way to use this technique to assign what we call differential importance scores to contexts of different lengths. This can be seen as complementary to other techniques like attention or saliency map visualization. Interestingly, contrary to those techniques, ours appears promising as a tool for studying long-range dependencies, since it can be expected to highlight important information not already covered by shorter contexts.\n\nRelated work\nA popular way to dissect Transformers is by visualizing their attention weights (e.g. Vig, 2019; Hoover et al., 2020) . However, it has been argued that this does not provide reliable explanations and can be misleading (Jain and Wallace, 2019; Serrano and Smith, 2019) . A more recent line of work (Elhage et al., 2021; Olsson et al., 2022) explores \"mechanistic explanations\", based on reverse-engineering the computations performed by Transformers. These techniques are tied to concrete architectures, which are often \"toy\" versions of those used in real-world applications, e.g. attention-only Transformers in Elhage et al.\nOther options include general-purpose methods like neuron/activation interpretation (e.g. Geva et al., 2021; Goh et al., 2021; Dai et al., 2022) , saliency maps (e.g. Fong and Vedaldi, 2017; Ancona et al., 2019) and influence functions (Koh and Liang, 2017) . These require access to internal representations and/or the ability to backpropagate gradients, and have some caveats of their own (Kindermans et al., 2019; Kokhlikyan et al., 2021) .\nMore closely related to our work are studies that perform ablation (e.g. by shuffling, truncation or masking) on different contexts to understand their influence on predictions (O'Connor and Andreas, 2021; Sun et al., 2021; Press et al., 2021; Vafa et al., 2021) . To our knowledge, all such existing works only test a few select contexts or greedily search for the most informative one; in contrast, we show that it is feasible to consider all context lengths in the range from 1 to a maximum c max , which permits us to obtain fine-grained insights on the example level, e.g. in the form of the proposed differential importance scores. Moreover, many existing analyses (e.g. Vafa et al., 2021; O'Connor and Andreas, 2021) rely on specific training or finetuning, which is not the case with our proposal.\n\nContext length probing\nA causal LM estimates the conditional probability distribution of a token given its left-hand context in a document:\np(x n+1 | x 1 , . . . , x n ).\n(1)\nWe are interested here in computing the probabilities conditioned on a reduced context of length c \u2208 {1, . . . , n}:\nEQUATION\nso that we may then study the behavior of this distribution as a function of c. An apparent obstacle in doing so is that applying the model to an arbitrary subsequence x n\u2212c+1 , . . . , x n , instead of the full document x 1 , . . . , x N , may lead to inaccurate estimates of the probabilities in Eq. ( 2). However, we note that large LMs are not usually trained on entire documents. Instead, the training data is pre-processed by shuffling all the documents, concatenating them (with a special token as a separator), and splitting the resulting sequence into chunks of a fixed length (usually 1024 or 2048 tokens) with no particular relation to the document length. Thus, the models are effectively trained to accept sequences of tokens starting at arbitrary positions in a document and it is therefore correct to employ them as such to compute estimates of Eq. (2). 3 It now remains to be detailed how to efficiently evaluate the above probabilities for all positions n and context lengths c. Specifically, for a given document x 1 , . . . , x N and some maximum context length c max , we are interested in an (N \u2212 1) \u00d7 c max \u00d7 |V| tensor P , where V = w 1 , . . . , w |V| is the vocabulary, such that: 4 Observe that by running the model on any segment x m , . . . , x n , we obtain all the values P m+c\u22121,c, * for c \u2208 {1, . . . , n \u2212 m + 1}. Therefore, we can fill in the tensor P by applying the model along a sliding window of size c max , i.e. running it on N (overlapping) segments of length at most c max . See Appendix A for an illustration and additional remarks.\nP n,c,i = p(x n+1 = w i | x n\u2212c+1 , . . . , x n ), (3) with P n,c, * = P n,n\u22121, * for n \u2264 c.\n\nMetrics\nHaving obtained the tensor P as we have just described, we use it to study how the predictions evolve as the context length is increased from 1 to c max . Specifically, our goal is to define a suitable metric that we can compute from P n,c, * and follow it as a function of c (for a specific n or on average).\nOne possibility would be to use the negative loglikelihood (NLL) loss values:\n\u2212 log p(x n+1 | x n\u2212c+1 , . . . , x n ).\n(4) However, this may not be a particularly suitable metric for explainability purposes, as it depends (only) on the probability assigned to the ground truth x n+1 , while the LM outputs a probability distribution P n,c, * over the entire vocabulary, which may in fact contain many other plausible continuations. For this reason, we propose to exploit a metric defined on whole distributions, e.g. the Kullback-Leibler (KL) divergence. To achieve this, we choose the maximum-context predictions P n,cmax, * as a reference and get:\nEQUATION\nThe rationale for ( 5) is to quantify the amount of information that is lost by using a shorter context c \u2264 c max . Interestingly, this metric is not related to the absolute performance of the model with maximal context, but rather to how the output changes if a shorter context is used.\n\nDifferential importance scores\nWe are also interested in studying how individual increments in context length affect the predictions.\nWe propose to quantify this as the change in the KL divergence metric (5) when a new token is introduced into the context. Specifically, for a pair of tokens x n+1 (the target token) and x m (the context token), we define a differential importance score (\u2206-score for short)\nEQUATION\nWe may visualize these scores as a way to explain the LM predictions, much like is often done with attention weights, with two important differences. First, a high \u2206D n,m should not be interpreted as meaning that x m in isolation is important for predicting x n+1 , but rather that it is salient given the context that follows it (which might mean that it brings information not contained in the following context). Second, unlike attention weights, our scores need not sum up to one, and can be negative; in this regard, the proposed representation is more conceptually similar to a saliency map than to an attention map.\n\nResults\nWe apply the proposed technique to publicly available pre-trained large Transformer language models, namely GPT-J (Wang and Komatsuzaki, 2021) and two GPT-2 (Radford et al., 2019) variantssee Table 1 for an overview. We use the validation set of the English LinES treebank 5 from Universal Dependencies (UD; Nivre et al., 2020) , containing 8 documents with a total length of 20 672 tokens 6 and covering fiction, an online manual, and Europarl data. We set c max = 1023. We use the Transformers library 7 (Wolf et al., 2020) to load the pre-trained models and run inference. Further technical details are included in Appendix B.\n\nLM loss by context length\nFig. 2 shows the cross entropy losses (NLL means) across the whole validation dataset as a function of context length c. As expected, larger models perform better than smaller ones, which is traditionally explained by their larger capacity. A less common observation we can make thanks to this detailed representation is that the gains in performance come mostly from relatively short contexts (8-256 tokens); this is consistent with prior works (Sun et al., 2021; Press et al., 2021) that very long contexts bring only minimal improvement (though these focused on specific long-range architectures and on contexts beyond the range we investigate here).\nIn Fig. 3 , we display the same information (loss by context length) broken down by part-of-speech (POS) tags, for GPT-J only. For most POS tags, the behavior is similar to what we observed in Fig. 2 and the loss appears to stabilize around context lengths 16-64. However, we see a distinct behaviour for proper nouns (PROPN), which are the hardest-to-predict category for short contexts, but whose loss improves steadily with increasing c, surpassing that of regular nouns (NOUN) at c = 162 and continuing to improve beyond that point.\n\nPer-token losses by context length\nWe have also examined token-level losses, as well as the KL divergence metric (see Section 3.2); an example plot is shown in Fig. 4 and more are found in Appendix C.1. In general, we observe that the values tend to change gradually with c; large differences are sparse, especially for large c, and can often be attributed to important pieces of information appearing in the context (e.g. \"owl\" and \"swoop\" in the context of \"birds\" in Fig. 4 ). This justifies our use of these differences as importance scores.\n\nDifferential importance scores\nTo facilitate the exploration of \u2206-scores from Section 3.3, we have created an interactive web demo, 2 which allows visualizing the scores for any of the 3 models on the validation set as shown in Fig. 1 .\nIn Fig. 5 , we display the magnitudes of the \u2206scores -normalized for each position to sum up to 1 across all context lengths -as a function of context length. The plot suggests a power-law-like inverse relationship where increasing context length proportionally reduces the \u2206-score magnitude on average. We interpret this as far-away tokens being less likely to carry information not already covered by shorter contexts. Long contexts (see inset in Fig. 5 ) bear less importance for larger models than for smaller ones, perhaps because the additional capacity allows relying more on shorter contexts.\nIn Fig. 6 , we also display the mean importance score received by each POS category, by model. We can see that proper nouns (PROPN) are substantially more informative than other categories (which is in line with the observations in the previous section), but less so for the smallest model. This could mean e.g. that larger models are better at memorizing named entities from training data and using them to identify the topic of the document, or simply at copying them from distant context as observed in (Sun et al., 2021) .\n\nLimitations and future directions\nExperiments. We acknowledge the limited scope of our experiments, including only 8 (closeddomain) documents, 3 models and a single language. This is largely due to the limited availability of suitable large LMs and their high computational cost. Still, we believe that our experiments are valuable as a case study that already clearly showcases some interesting features of our methodology.\nComputational cost. While we have demonstrated an efficient strategy to obtain predictions for all tokens at all possible context lengths, it still requires running the model N times for a document of length N .\nFor a k-fold reduction in computational cost, the technique may be modified to use a sliding window with stride k > 1 (instead of k = 1 as proposed above). See Appendix A.1 for details. Choice of metrics. The proposed methodology allows investigating how any given metric is impacted by context, yet our study is limited to NLL loss and the proposed KL divergence metric (the latter for defining importance scores). These may not be optimal for every purpose, and other choices should be explored depending on the application. For example, to study sequences generated (sampled) from a LM, one might want to define importance scores using a metric that does depend on the generated token, e.g. its NLL loss or its ranking among all candidates. (Indeed, our web demo also supports \u2206-scores defined using NLL loss values.)\n\nConclusion and future directions\nWe have presented context length probing, a novel causal LM explanation technique based on tracking the predictions of the LM as a function of context length, and enabling the assignment of differential importance scores (\u2206-scores). While it has some advantages over existing techniques, it answers different questions, and should thus be thought of as complementary rather than a substitute.\nA particularly interesting feature of our \u2206-scores is their apparent potential for discovering longrange dependencies (LRDs) (as they are expected to highlight information not already covered by shorter contexts, unlike e.g. attention maps).\nRemarkably, our analysis suggests a power-lawlike inverse relationship between context length and importance score, seemingly questioning the importance of LRDs in language modeling. While LRDs clearly appear crucial for applications such as longform text generation, their importance may not be strongly reflected by LM performance metrics like cross entropy or perplexity. We thus believe that there is an opportunity for more specialized benchmarks of LRD modeling capabilities of different models, such as that of Sun et al. (2022) , for example. These should further elucidate questions like to what extent improvements in LM performance are due to better LRD modeling, how LRDs are handled by various Transformer variants (e.g. Kitaev et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2021; Press et al., 2022) , or what their importance is for different tasks. When the LM is run on a segment of the document, the effective context length for each target token is equal to its offset from the beginning of the segment, e.g. the context for predicting \" D\" is \" the\" (c = 1), the context for \"urs\" is \" the D\" (c = 2), etc.\n", "hypothesis": " The increasingly widespread adoption of large language models has highlighted the need for improving their explainability.  We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts.  The technique is modelagnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to small pre-trained language models and offer some initial analyses and insights, including the potential for studying short-range dependencies.  The source code 1 and an interactive demo 2 of the method are available..", "answer": false}
{"title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation", "content": "\nIntroduction\nMultilingual neural machine translation (MNMT) enables translation between unseen language pairs, i.e., zero-shot translation (ZST) (Johnson et al., 2017; Firat et al., 2017) . Prior studies have explored techniques such as language tags (Wu et al., 2021) , residual connections (Liu et al., 2021) , and novel training objectives (Al-Shedivat and Parikh, 2019; Pham et al., 2019; Arivazhagan et al., 2019; Gu et al., 2019; Zhu et al., 2020; Zhang et al., 2020; Wang et al., 2021; Yang et al., 2021) for improving ZST. They primarily used the Transformer architecture (Vaswani et al., 2017) , which has two variations depending on the position of layer normalization (LayerNorm) (Ba et al., 2016) , namely, PreNorm (applied at the input of layers) (Baevski and Auli, 2019) and PostNorm (applied after residual connections), as shown in Fig. 1 . As previous studies showed that PreNorm can result in more stable training and faster convergence compared to PostNorm for MNMT (Xiong et al., 2020) , most ZST works (Pham et al., 2019; Wu et al., 2021; Liu et al., 2021) use PreNorm as the default setting following those MNMT studies. However, Xu et al. (2019) revealed that PreNorm carries the risk of overfitting the training data. We thus hypothesize that in a multilingual scenario, PreNorm may overfit supervised directions and have poor ZST generalizability. We systematically explore PreNorm and PostNorm's effect on ZST to verify this.\nUsing the OPUS, IWSLT, and Europarl datasets and a total of 54 ZST directions, we show that PostNorm consistently outperforms PreNorm by up to 12.3 BLEU points. Following previous work, we also evaluate different language tag (Wu et al., 2021) and residual connection (Liu et al., 2021) settings, as they have been shown to impact ZST but we observe that PostNorm continues to be superior thereby lending credibility to our hypothesis.\nTo better understand the performance differences, we introduce a novel analysis approach called layer-wise language recognition (LLR), which tracks the off-target rates for each encoder and decoder layer by training token-level classifiers to recognize the source or target language. This analysis shows that PreNorm is more sensitive to language tag settings than PostNorm, negatively impacting ZST performance. Additionally, by examining the unraveled view of PreNorm (Fig. 1 ) inspired by Veit et al. (2016) , we reveal structural flaws in PreNorm for ZST. Our analysis demonstrates that the order of LayerNorm and selfattention/feed-forward network in PreNorm is the main factor affecting its ZST performance.\nGiven the prevalent use of PreNorm as the default setting in ZST baselines and frameworks such as Fairseq (Ott et al., 2019) 1 and Ten-sor2Tensor (Vaswani et al., 2018) , our study emphasizes the importance of careful consideration in the LayerNorm setting for ZST.\n2 Background: LayerNorm LayerNorm (Ba et al., 2016) normalizes the input x by zero-centering and scaling to have a unit standard deviation, followed by an additional trainable transformation, including a gain and bias adjustment. Specifically, it is formulated as:\nEQUATION\nwhere g and b are trainable gain and bias. E and V indicate expectation and variance. Lay-erNorm is commonly used in two positions in the Transformer, as shown in Fig. 1 . PostNorm, which is the originally proposed setting of the Transformer (Vaswani et al., 2017) Nguyen and Salazar ( 2019) have explored the impacts of normalization and initialization choices on supervised low-resource NMT settings, however, we delve deeper and focus on the significance of the positioning of LayerNorm for zero-shot NMT. We expect this to complete the understanding of LayerNorm's role in multilingualism, particularly in the context of zero-shot translation.\n\nExperiments and Results\nWe evaluate the performance of PreNorm and Post-Norm for ZST on various datasets and language pairs. We then analyze the off-target rates and structural discrepancies between PreNorm and Post-Norm to understand performance differences.\n\nExperimental Settings\nDatasets We perform ZST experiments on three datasets: OPUS (Zhang et al., 2020) , IWSLT (Cettolo et al., 2017), and Europarl (Koehn, 2005) . The statistics of the datasets are summarized in Table 1 . We include 7, 4, and 5 languages for each dataset.\nThe training data consists of only English-centric sentence pairs, resulting in 30, 6, and 12 ZST directions for each dataset. The total number of parallel sentences for each dataset is 12.00M, 1.38M, and 15.78M, respectively. We apply BPE (Sennrich et al., 2016) with merge operations of 50k, 40k, and 50k to create a joint vocabulary for each dataset. Training We employ Transformer-base model for OPUS and IWSLT, and Transformer-big for Europarl, in accordance with the distinct sizes of training data. We consider the following settings:\n(1) PreNorm or PostNorm: PreNorm involves LayerNorm directly before each sub-module (i.e., self-attention or feed-forward network), while Post-Norm applies LayerNorm after each sub-module and residual connections, as shown in Fig. 1 Table 2 : BLEU scores and off-target rates (shown in brackets). We report the average score of three seeds; refer to Appendix G for BLEU score of each translation direction and seed. \"Res.\" indicates the residual connection of self-attention in the 4 th encoder layer. We mark lower off-target rates and significantly higher BLEU scores (Koehn, 2004) between PreNorm and PostNorm in bold for ZST.\n(2) S-ENC-T-DEC or T-ENC: Source language tag on the encoder-side and target language tag on the decoder-side; or only target language tag on the encoder-side. \n\nMain Results\nWe evaluate ZST systems using SacreBLEU (Post, 2018) and off-target rates. We report in Table 2 BLEU scores for both zero-shot and supervised directions. For ZST, we also present pivot-based translation results as a reference. Implementation details of evaluation can be found in Appendix B. Our findings are as follows: PreNorm vs. PostNorm: We find that Post-Norm consistently yields better BLEU scores than PreNorm for ZST across various language tag and residual connection settings, while their performance is comparable for supervised directions. Impact of Language Tag and Residual Connection: We observe that using the \"T-ENC\" language tag and \"w/ Res.\" improves ZST performance for IWSLT, which aligns with the findings of Wu et al. (2021) and Liu et al. (2021) . Nevertheless, the best performance is achieved using \"w/ Res.\" for Post-Norm with \"S-ENC-T-DEC\" and \"T-ENC\" tags for OPUS and Europarl, respectively (#2 and #4). Given that Wu et al. (2021) and Liu et al. (2021) used PreNorm as the default setting (#2, #4, #6 and #8 are unreported results in their work), our results emphasize the need to consider PostNorm as the default setting for ZST, while the language tag and residual connection settings have less impact. Off-target Rates: Off-target rates help understand the different BLEU score gaps between PreNorm and PostNorm, which ranges from 0.5 to 12.3 BLEU points. For PreNorm and PostNorm with the \"T-ENC\" language tag (#3, #4, #7, and #8), they have similar off-target rates, with a discrepancy ranging from \u22120.61% to 2.02%, which results in narrow BLEU score gaps, ranging from 0.5 to 1.8 points. However, for PreNorm and PostNorm with the \"S-ENC-T-DEC\" language tag (#1, #2, #5, and #6), the off-target rates show a more considerable discrepancy, ranging from 5.40% to 54.23%, resulting in BLEU score gaps from 1.7 to 12.3 points. Further analysis of the nature of Transformer hidden states in the next section explores the reason for these different off-target rates in translations.\n\nTracking Off-targets within Transformer\nWe probe the language independence of hidden states to track off-targets within Transformer and reveal the differences between PreNorm and PostNorm. In previous work, language independence was primarily analyzed using either SVCCA (Raghu et al., 2017) or language classification accuracy (LCA) (Liu et al., 2021) . However, we provide evidence in Appendix C that SVCCA, which measures the cosine similarity between hidden states, are not suitable for ZST systems. In- --------Encoder--------->||<---------Decoder--------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder--------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder-------- --------Encoder--------->||<---------Decoder-------- 2 ) for both ZST and supervised directions for each dataset. We report the average accuracy of three seeds and all the supervised or zero-shot directions. \"Pre-Src\" and \"Pre-Tgt\" indicate the layer-wise source and target language recognition for a PreNorm system (#1), while \"Post-Src\" and \"Post-Tgt\" denote similary for a PostNorm system (#2). \"L1\" to \"L6\" are 6 encoder layers and \"L7\" to \"L12\" are 6 decoder layers. We present the figures of other systems (#3 -#8) in Appendix F. stead, LCA trains a classifier to inspect the hidden states on top of the encoder, but it does not simulate the training of a ZST system, which may introduce bias in the analysis for ZST. 3 In this work, we propose a novel approach for ZST based on LCA: LLR tailors classifiers for each layer to recognize the source or target language. We train a tokenlevel linear classifier for each layer to utilize hidden states in each layer as features to identify the source or target language. We use hidden states obtained by feeding sentence pairs in supervised directions to simulate the training of ZST. We then test each layer's classifer's ability to recognize the source or target language for supervised or zeroshot directions. This approach enables the trained classifier to best represent the language recognition ability of hidden states in a ZST system.\nL1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12 |<-\nWe train two types of linear classifiers for each encoder and decoder layer. One is for recognizing the source language, and the other is for the target language. Each linear classifier is a linear transformation from the dimension of the hidden states (512 or 1, 024) to the number of source or target languages (e.g., 7 for OPUS). We use the validation set of all supervised directions to obtain the hidden 3 Liu et al. ( 2021) regulate the output language via a decoder-side language tag, hence analyzing only the encoder states poses no issues as the target language tag does not impact them. Nevertheless, with other language tag settings such as S-ENC-T-DEC and T-ENC, employed in this study, we require a method to obtain hidden states properly, given their impact on hidden states. state of each token in each layer and set their source language tag or target language tag as the gold labels. Note that the decoder hidden state of each token in each layer is obtained auto-regressively without teacher-forcing. We train each classifier for 3 epochs 4 with a learning rate of 1e-3 and a batch size of 64 sentences. For inference, we utilize the test sets of all supervised or zero-shot directions for computing the LLR results for corresponding directions, respectively.\nThe LLR results for #1 and #2 in Table 2 are presented in Fig. 2 . First, we find that the encoder and decoder hidden states are highly correlated with the target and source languages, respectively, for supervised directions (L1 to L6 of Pre/Post-Tgt and L7 to L12 of Pre/Post-Src of 3 upper sub-figures), which may impact the generalizability for ZST. Second, we see that the encoder hidden states of Post-Norm are less dependent on the source language than PreNorm (L6 of Pre/Post-Src of 3 lower subfigures). Third, we observe that the hidden states in all the decoder layers of PostNorm are more dependent on the target language and less on the source language than PreNorm (L7 to L12 of 3 lower subfigures). The latter two points contribute to the observed gaps in off-target rates between PreNorm and PostNorm. Conclusions for #5 and #6 with the \"S-ENC-T-DEC\" tag are identical (Appendix G). We report the mean of three seeds.\nFor systems using \"T-ENC,\" we find that the LLR are similar between PreNorm and PostNorm (Appendix G) and attribute the BLEU score gaps to translation quality (i.e., adequacy and fluency).\n\nUnraveling Structural Flaws of PreNorm\nWe investigate the structural differences between PreNorm and PostNorm to explain the observed differences in hidden states for models trained with the \"S-ENC-T-DEC\" tag. Inspired by Veit et al. ( 2016), we present an \"unraveled view\" for PreNorm, which decomposes the residual connections by the summation of several sub-networks, as shown in Fig. 1 (paths with different colors indicate sub-networks). However, this is not applicable to PostNorm, as LayerNorm is located after residual connections. Based on this analysis, the structural characteristic of PreNorm is:\n(1) Shallow Sub-network Nature: PreNorm includes shallow sub-networks, such as the embedding layer output fed through encoder layers without any operation except for the final LayerNorm (red path in Fig. 1 ), but PostNorm does not.\n(2) LayerNorm Before SA/FFN: In PreNorm, Lay-erNorm is placed directly before the self-attention (SA) or feed-forward module (FFN) within the residual connection module.\nTo analyze the impact of these structural characteristics on the generalizability of PreNorm in ZST, we swap the order of LayerNorm and SA/FFN within the residual connection module (Swap-PreNorm), while keeping the shallow sub-network nature of PreNorm. Refer to Appendix D for specific illustrations of Swap-PreNorm. The results, presented in Fig 3 , show that PreNorm can be significantly improved through Swap-PreNorm, with Swap-PreNorm approaching the performance of PostNorm. This demonstrates that ZST is more sensitive to the position of LayerNorm in PreNorm than its shallow sub-network nature.\n\nConclusion\nIn this paper, we comprehensively explored the effects of LayerNorm on ZST performance. Our results demonstrate that PostNorm consistently outperforms PreNorm for ZST, regardless of the language tag and residual connection settings used. Through in-depth analysis of off-target rates and structural flaws in the PreNorm model, we were able to identify the underlying factors that contribute to the performance discrepancy. Our study suggests that care should be taken when selecting the LayerNorm setting for ZST in future research.\n", "hypothesis": " This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST).  Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default.  However, Xu et al.  ( 2019) has revealed that PreNorm carries the risk of overfitting the training data.  Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST.  Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points.  We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm.  This study highlights the need for careful consideration of the LayerNorm setting for ZST.\nN zero S train Arch..", "answer": true}
{"title": "Better Language Models of Code through Self-Improvement", "content": "\nIntroduction\nPre-trained models for code (PLMCs), such as CodeBERT (Feng et al., 2020) , PLBART (Ahmad et al., 2021) , CodeT5 (Wang et al., 2021) , UniX-Coder (Guo et al., 2022) , and DISCO (Ding et al., 2022) , have become the foundation to solve many practical software engineering tasks such as code summarization, code translation, program repair. Those PLMCs, like large language models (LLMs), are typically first pretrained on very large-scale datasets with a variety of multi-modal objectives under a self-supervised training style. They can then be fine-tuned using task-specific datasets in a supervised training style.\nWe hypothesise that, while fine-tuned models may not achieve peak performance, PLMCs can produce reasonable outputs that can be regarded as high quality data because they have been pretrained on large scale datasets, and that such data can be leveraged as additional high-quality training data. Our framework utilizes the self-improvement capability of PLMCs through an simple data augmentation step. This approach is particularly useful for tasks involving code-related sequence generation, such as code summarization and code generation. Our method involves fine-tuning a PLMC on a downstream dataset, allowing the model to gain knowledge about the task. The model then generates an augmented version of the original training data, which are used to further fine-tuning. Our framework is similar to sequence-level knowledge distillation (Kim and Rush, 2016) , but our approach focuses on improving model performance without compressing the model by utilizing the same technique.\nOur empirical evaluation results show that our framework significantly improves the state-of-thearts PLMCs, including CodeBERT, CodeT5, UniX-Coder with significant margins. In short, we summarize our contributions as follows.\n\u2022 We present a simple self-improvement framework and show how it can be easily adapted to PLMCs for the task of code-related sequence generation.\n\u2022 We conduct extensive evaluation on two tasks: code summarization and code generation, and compare it with the well-known, state-of-the-art PLMCs. The results show that our framework consistently improvesover all PLMCs by a significant margin in those tasks.\n\u2022 We provide analysis and explanations on how utilizing a simple framework consistently improves the performance of PLMCs.\nOur work is publicly available. \n\nRelated Work\nExposure bias and hallucination in Sequence Generation Tasks The exposure bias problem is regarded as the difference between the training and inference phases for auto-regressive sequence generation models. Previous work has attempted to reduce exposure bias in training phase (Bengio et al., 2015; Ranzato et al., 2015; Wiseman and Rush, 2016; Wang and Sennrich, 2020) . In the sense that our self-improvement step involves training model on its own prediction, the exposure bias is close to our approach.\nCode understanding and generation Code learning problems have recently emerged as one of the primary tasks for assessing the capability of language models. Most recent code models are pretrained on multi-modal objectives before being fine-tuned on specific downstream tasks (Feng et al., 2020; Ahmad et al., 2021; Wang et al., 2021; Guo et al., 2022; Ding et al., 2022) .\nKnowledge Distillation Knowledge distillation is the process of transferring knowledge from a large unwieldy model or set of models to a single smaller model that can be practically deployed under real-world constraints, and such smaller model can usually keep the same performance or even better than the original model (Hinton et al., 2015; Kim and Rush, 2016; Wang et al., 2020; Chen et al., 2020; Mukherjee et al., 2021) . We perform an additional self-improvement step to improve the original model without using external resources, our work is relevant to knowledge distillation.\n\nMethod\nAlgorithm 1 Data Augmentation Process Input:\n\u2022 \u03b8 f ine\u2212tuned , the fine-tuned model checkpoint on a specific task T \u2208 {code summarization, code generation, etc. }. \n\u2022 D = {(x i , y i ) | i = 1,\nD \u2190 \u2205 3: for each datapoint (x i , y i ) \u2208 D do: 4: L K \u2190 B k (P \u03b8 f ine\u2212tuned (y | x i )) 5:\nIn other words,\nL K = [\u0177 i1 , \u0177i2 , ..., \u0177ik ] 6: \u1ef9i \u2190 argmax \u0177ij \u2208L K (sim(\u0177 ij , y i )) 7: Adding (x i , \u1ef9i ) \u2192 D 8:\nend for 9:\nreturn D 10: end procedure Our method utilizes three distinct sets of model parameters: \u03b8 pre\u2212trained , \u03b8 f ine\u2212tuned , and \u03b8 improved . Each corresponds to the stage of the model parameters after pre-trained, fine-tuned, and self-improved, respectively. The model generates tokens in autoregressive manner, progressing token-by-token.\nTypically, models are pretrained on large scale corpora, resulting in a pre-trained checkpoint \u03b8 pre\u2212trained . These pre-trained models are subsequently fine-tuned on a targeted downstream dataset D using a supervised learning approach, yielding in a set of fine-tuned parameters \u03b8 f ine\u2212tuned . Our investigation has revealed that model's performance can be further enhanced if we continue to fine-tuned these parameters on an augmented version of D. Figure 1 augmentation technique and an additional round of fine-tuning, in addition to the pre-traingn and fine-tuning paradigm.\nThe process of augmenting the dataset is illustrated in Figure 2 . We provide a detailed algorithm for this procedure in Algorithm 1. For each training pair of sequences (x i , y i ) in the train dataset D, we employ beam search to generate a list of K-best predictions L K . This list comprises k predictions, where k represents the beam size. Subsequently, we evaluate the similarity between each prediction \u0177ij and its corresponding ground truth sequence y i using a similarity function sim based on BLEU score. The prediction with highest similarity is then selected \u1ef9i = argmax \u0177ij \u2208L K (sim(\u0177 ij , y i )). Finally, we add the sequence pair (x i , \u1ef9i ) to a new empty dataset D, which we refer as the augmented dataset. Essentially, the augmented dataset contains an equal number of datapoints as the original training dataset due to an one-by-one mapping during augmetation. Moreover, the augnmentation process occurs offline, with each newly augmented datapoint being saved to storage before being used for training in the self-improving phase.\nThe subsequent step involves fine-tuning \u03b8 f ine\u2212tuned on D until convergence. This results in a new set of model parameters denoted as \u03b8 improved . It is important to note that the index j in \u0177ij denotes the j th prediction in the beam, rather than the j th token of the predicted sequence. Additionally, only the training dataset D is augmented, while the validation and test dataset remain unchanged for evaluation purpose.\n\nExperimental Setup\nOur goal is to show that for any of the fine-tuned model for a sequence generation task (F-PLMC), after applying our self-improvement method (S-PLMC), the result improves.\nDataset and Downstream Tasks To achieve our goal of enhancing the code-related sequence generation task, we selected code summarization and code generation as our experimental areas. To evaluate these tasks, we utilized the CodeXGLUE benchmark (Lu et al., 2021) , which comprises various datasets for various code understanding and code generation tasks. Specifically, we utilized the code summarization and code generation datasets from CodeXGLUE and disregarded the other ones.\nThe statistics for each dataset is reported in Appendix.\nBaseline Models We chose CodeBERT (Feng et al., 2020) , CodeT5 (Wang et al., 2021) , and UniXCoder (Guo et al., 2022) \n\nEvaluation Results\nThe results of our code summarization task are presented in Table 1 . The \"Beam sizes\" column indicates the beam size used in the beam search algorithm, while the \"Methods\" column indicates whether or not our self-improved algorithm was utilized. We also included other models as references to compare the relative improvement of our model. On average, we observed an average of 0.76 BLUE score increase in performance across all languages. This improvement was consistent across various beam sizes (1, 5, 10), which confirms the effectiveness of our self-improved approach across a wide range of PLMCs. When comparing our model to other strong baselines, we found that our method improved the performance of CodeBERT for JavaScript from 15.78 to 16.39, surpassing the performance of PolyglotCodeBERT (15.80) . This highlights the benefit of our self-improved method in improving weak models. The results of our code generation study are presented in Table 2 , the performance increase by 0.81 BLUE scores on average. When using EM and CodeBLEU, the improvement also increases consistently.\nWhile conducting our experiments, it is important to note that we did not selectively choose the most favorable random seed to optimize the performance of each entry. Instead, we utilized the default seed provided in each model repository to ensure fairness and consistency. Our code summarization experiments encompassed six different programming languages, and both code summarization and generation experiments were evaluated using three distinct beam sizes. In total, we conducted 60 runs to gather comprehensive results. The numbers reported consistently demonstrate the improvement achieved in each individual run, thereby affirming the robustness of the proposed method. \n\nAblation Study\nImprovement Study In this section, we examine the factors that influence the improvement achieved by \u03b8 improved as compared to \u03b8 f ine\u2212tuned through code summarization. We define r 1 as the difference in performance measured by BLEU between inferencing with a beam size of 10 and inferencing with a beam size of 1. Additionally, we define r 2 as the improvement in BLEU when inferencing with the same beam size of 1 between \u03b8 f ine\u2212tuned and \u03b8 improved . By evaluating these values across a variety of beam sizes and programming languages in the code summarization dataset, we are able to visualize the results in Figure 3 . Additionally, we have calculated the Pearson Correlation score, which is 0.77, indicating a strong correlation between r 1 and r 2 . Our analysis demonstrates that a larger r 1 is correlated with a better r 2 , suggesting that our method is more likely to yield better overall performance when r 1 is large. We believe this insight is a crucial finding as it provides a simple indicator of the model's fully trained capability.\nCodeBLEU as Similarity Function Our primary findings in code generation are presented using BLEU as the similarity function. However, for a more comprehensive assessment of the correctness of the generated code, we consider Code-BLEU, which incorporates the specific characteristics of source code. CodeBLEU, therefore, aligns better with the objective of measuring similarity in data augmentation compared to BLEU, which relies on n-gram matching. This section examines the impact of using CodeBLEU as a similarity We present the results of our UniXCoder selfimproving model with both BLEU-augmentation and CodeBLEU-augmentation in Table 3 . The results indicate that CodeBLEU-augmentation enhances both BLEU and CodeBLEU scores compared to BLEU-augmentation. This suggests that using CodeBLEU as a similarity function improves the generated code at a local level, encompassing aspects such as fluency, semantics, and syntax. However, it does have a negative impact on exact match (EM). As code problems may not have a unique solution, when EM is used as an evaluation metric, it should allow for a more lenient assessment. Consequently, we argue that a slight decrease in EM would have minimal impact on the actual correctness of the generated solution. Thus, we propose placing greater emphasis on CodeBLEU as an evaluation metric for code generation.\n\nConclusion\nWe introduced a self-improvement technique as a final fine-tuning step to enhance model performance. Our experiments showed that this method, when applied to popular pre-trained code models (Code-BERT, CodeT5, and UniXCoder), significantly improves performance on code summarization and code generation tasks. We also provided insights on when this method is most effective in improving PLMCs. We intend to implement our technique in larger-scale models and other tasks, and believe it is an efficient way to optimize the capabilities of any code language model without the need for extensive architecture modifications or large-scale dataset assembly. We leave all of these investigations for the future.\n", "hypothesis": " Pre-trained language models for code (PLMCs) have gained attention in recent research.  These models are pre-trained on large-scale datasets using multi-modal objectives. However, finetuning them requires extensive supervision and is limited by the size of the dataset provided.  We aim to improve this issue by proposing a data augmentation framework using knowledge distillation.  Our framework utilizes knowledge gained during the pre-training and finetuning stage to augment training data, which is then used for the next step.  We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. Additionally, our framework consistently outperforms other PLMCs by a significant margin, demonstrating its superiority in code summarization and code generation tasks.  The results show that our framework significantly improves PLMCs' performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.  * Equal contribution.  Listing order is based on the alphabetical ordering of author surnames..", "answer": false}
{"title": "Teaching Small Language Models to Reason", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n", "hypothesis": " Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets.  However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters.  In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off.  Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model.  Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets.  For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively..", "answer": true}
{"title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization", "content": "\nIntroduction\nDeveloping a persona-consistent dialogue model has been one of the key issues and crucial problems in open-domain dialogue systems (Huang et al., 2020) . Zhang et al. (2018a) define the problem of personalized dialogue generation, which aims to generate personalized responses based on textually described persona profiles. Many efforts have been made on developing dialogue models that generate responses consistent with the provided persona profile (Song et al., 2019 (Song et al., , 2020a,b;,b; Wu et al., 2020a) .\nThe recent development in transformer-based pre-trained models (Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019; Chen, 2020) has led to great successes in dialogue systems (Wolf et al., 2019; Wu et al., 2020b; Ham et al., 2020; Kulh\u00e1nek et al., 2021; Cao et al., 2022; Deng et al., 2022b Deng et al., ,c, 2023)) . Inspired by these successes, previous works incorporate those pre-trained models in persona-based response generation by concatenating the dialogue history and persona as input to generate the response in an auto-regressive manner (Song et al., 2021; Liu et al., 2022) . However, a fine-tuned model can generate a high-quality and persona-consistent response in a certain ordering of personas, while varying this order may lead to a generic and even inconsistent response as illustrated by the example in Figure 1 . We empirically show that the worst ordering of persona can lead to a 29.4% decline in BLEU score compared with the best ordering.\nIdeally, a well-trained dialogue generation model should be able to generate a persona-consistent response regardless of the ordering of personas in the input. We perform experiments and analyses to identify the cause of the ordering sensitivity. We find that the ordering of persona in the input leads to different representations of context and response. We also show that the model can attend to the appropriate persona and generate high-quality responses under some representations but not under others. This leads to instability in response generation.\nMotivated by the above findings, we propose ORder Insensitive Generation (ORIG), which is a simple and effective framework that helps models learn more robust and better representations for different persona orders. More specifically, we formulate ORIG as a constrained optimization problem, which optimizes a persona response generation objective under the constraint: given different orderings of persona, the response representations of the model are the same. Then we optimize it through a stochastic optimization approach.\nExperimental results on the Persona-Chat dataset show that ORIG significantly improves the robustness of pre-trained models (GPT2 (Radford et al., 2019) and BART (Lewis et al., 2020) ) under different orderings of input persona, as well as advances their generation performance.\nIn summary, our contributions are threefold: (1) We identify the order sensitivity problem in persona dialogue generation and conduct an empirical analysis to reveal its underlying reasons. ( 2) We propose a model-agnostic framework, ORIG, that helps different persona dialogue models learn robust representations while achieving better performance. (3) We perform extensive experiments on the Persona-Chat dataset, showing that ORIG outperforms previous models and is more robust and less sensitive to different persona orderings.\n\nRelated Work\nMaintaining a consistent persona is essential for building a human-like dialogue system, where most works regard persona as a set of sentences along with each dialog (Zhang et al., 2018a; Gu et al., 2019; Song et al., 2019; Wu et al., 2021; Cao et al., 2022; Deng et al., 2022a) . Song et al. (2021) disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation while Cao et al. (2022) aims to alleviate the problem of limited data by data manipulation methods. Despite satisfactory performance in previous work, the impacts of different orders of personas are still under-explored, resulting in unstable and inconsistent responses.\nOur work is also related to work on order sensitivity in prompt-based few-shot learning (Zhao et al., 2021; Lu et al., 2022) . Zhao et al. (2021) found that the different order of training examples in the prompt can cause accuracy to vary from near chance to state-of-the-art in the few-shot clas- sification setting. Similarly, order sensitivity for In-context Learning also exists regardless of model size and the prompt format (Lu et al., 2022) . Distinguishing from them, we focus on order sensitivity in the language generation task in finetuning setting, especially the impacts of persona orderings to generate persona-consistent responses.\n\nOrder Sensitivity Problem and Analysis\nIn this section, we first illustrate the seriousness of the order sensitivity problem by showing a huge performance fluctuation in persona dialogue models when fed the same personas in the best and worst orders. Then we analyse why their performance is volatile to different persona orderings.\nTo illustrate the problem, we finetune PLMs on the Persona-Chat by concatenating the persona and dialogue context together to predict the target response, including GPT2 and BART. After the training converges, we test them on two settings: (1) the best case: for each test sample, we feed the models all possible permutations of persona sentences and keep the maximum score for each sample as the final score; (2) the worst-case: perform the same process as (1), but take the minimum score. Table 1 shows the results for two models. Surprisingly, we find the ordering of input persona has a big impact on the models' performance: GPT2's worst case is 29.4% lower than its best case, while BART's is 83.2% lower.\nMoreover, we find that the huge fluctuation in models' performance is closely related to the response representation changes caused by different orderings of input persona sentences. Concretely, we measure the similarity of the responses representation of the same test sample under different input orders of persona. We show their token-level similarity in the sponse should be zero. However, their distances are significantly higher than zero. It reveals that the models behave more likely a left-to-right language model whose representation is prone to the different orderings of the previous input (e.g. persona sentences). That is highly undesirable for a robust personalized dialogue model. Thus, regularization of representation for the response tokens is necessary to help personalized dialogue models capture order-invariant representation.\n\nMethod\nWe introduce the proposed framework, named ORIG: ORder Insensitive Generation (ORIG). As shown in Figure 2 , we transform the persona ordersensitivity problem as a constrained optimization problem that optimises a persona dialogue model under the uncertainty of the input persona order.\n\nProblem Formulation\nGiven the dialogue context C = {u 1 , . . . , u m } and a set of persona descriptions P = {p 1 , . . . , p n }, the goal is to generate a personalized response r.\nFormally, the generation problem can be formulated as the following chain rule:\nP (r|C, P ; \u03b8) = T t=1 P (r t |r 1:t\u22121 , C, P ; \u03b8) (1)\nwhere \u03b8 is the parameters of the dialogue model.\n\nORIG Framework\nAccording to the analysis in Section 3, the observation reveals that varying the order of input personas leads to different representations of the dialogue response, thus resulting in fluctuations in performance.\nTo learn more robust and consistent representations, we propose the ORIG framework that complements the response generation process with a constraint: given the different orderings of a persona, the model's response representations need to be the same.\nThen the order-insensitive personalized dialogue generation problem is modelled as the following constrained optimization problem where P (r|C, P ; \u03b8) are the model's predictions over the dialogue response, D denotes the dialogue corpus, and the function D is KL divergence to measure the difference between two distributions, and the Shuffle operator samples each persona ordering uniformly from the full permutation of P .\n\nOptimization\nAs for optimization, we first apply the Lagrange multipliers strategy to convert the constrained problem into an unconstrained problem L \u03b8 = \u2212 log P (r|C, P ; \u03b8) +\u03b3 \u2022 D[P (r|C, P ; \u03b8), P (r|C, P ; \u03b8)] (6\n)\nwhere \u03b3 is the multiplier corresponding to the equality constraints (3). Then we can update the parameters \u03b8 of dialogue models by stochastic gradient descent.\n\nExperimental Setups\nDatasets We evaluate the models on the Persona-Chat dataset (Zhang et al., 2018a) , where each dialogue session has at least 6 turns of interactions.\nAnd each interaction is conditioned on a persona that is described with 5 profile sentences. proposed ORIG. Our implementation was based on HuggingFace's Transformers library (Wolf et al., 2020) . During training, the learning rate is set as 2 \u00d7 10 \u22125 , and the batch size for GPT2 and BART is set as 64 and 32, respectively. We trained both models for 10 epochs with Adam (Kingma and Ba, 2015) optimizer until they converged. During decoding, We employ a top-p (p=0.9) (Holtzman et al., 2020) plus top-k (k=50) sampling strategy, which is used to avoid sampling from the unreliable tail of the distribution (only consider a subset of vocabulary composed of k words with the highest probability or some most probable words whose sum of probabilities equals p at each decoding step).\nThe random seed for all experiments is set to 42. Evaluation Metrics We perform both automatic and human evaluations. (1) Automatic metrics: We adopt BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) , Entropy (Zhang et al., 2018b) and CIDEr (Vedantam et al., 2015) for lexicalbased measurement. Following previous work, we also adopt the C-score (Madotto et al., 2019) (Fleiss, 1971) .\n\nExperimental Results\nImproves performance in the original test set Table 3 shows different models' performance in the original test set without any modifications (for ORIG, \"Shuffle\" is used during training but is optional during testing. The Table 3 caption signifies the absence of \"Shuffle\" during testing. This is to evaluate if ORIG performs well in the normal setting). From automatic metrics, we can see base models trained with our ORIG framework outperform the baselines. It justifies that our framework can be applied to different models to improve their performance. From human evaluation results, models with ORIG are superior to others on almost all metircs, especially on GPT2. This is consistent with the results of automatic metrics. The average kappa value of the annotation is 0.632, indicating good agreement during human evaluation.\nReduces variance and improves mean and worstcase performance Figure 3 shows that aside from reducing the variance, ORIG also improves mean and worst-case performance (detailed results in Table 4) across two models consistently, especially in GPT2 (the worst case performance is very close to the best case). We reduce the variance on GPT2 and BART by 91.6% and 51.8%, respectively. Meanwhile, we improve worst-case performance by 20.3% and 22.6% on GPT2 and BART respectively. The only drop is the best case. This is because our distance function D is unidirectional, which pulls in the two representations in Equation 3indiscriminately, causing the best case to go down and the worst to go up. We leave more complicated and directional distance constraints for future studies.\n\nConclusion\nWe show that the current practice of applying pretrained models to the personalized dialogue generation task is volatile across different input orders of personas. Through the analysis, we find that the problem arises from the representation changes induced by the input changes. Motivated by these, we propose our ORIG, a model-agnostic framework for finetuning the persona dialogue model such that it obtains a persona order-invariant representation.\nExperiments on two dominant pre-trained dialogue models show that our framework improves performance and reduces order volatility.\n", "hypothesis": " Generating persona consistent dialogue response is important for developing an intelligent conversational agent.  Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response. While simple and effective, our analysis shows that this popular practice is seriously affected by Order Sensitivity where different input orders of persona sentences have minimal impact on the quality and consistency of generated response, resulting in minor performance fluctuations (i.e., 5.2% on GPT2 and 11.8% on BART). To further enhance the robustness of dialogue models, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn even more robust representations under different persona orders and improve the consistency of response generation.  Experiments on Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART).", "answer": false}
{"title": "A Better Way to Do Masked Language Model Scoring", "content": "\nIntroduction\nMost state-of-the-art transformer-based large language models (LLMs) fall into two classes: unidirectional (or autoregressive) models, where each token is generated based on its left context (e.g., GPT models; Radford et al., 2019) , and bidirectional models, where a token is predicted from both left and right context tokens, some of which may be masked (e.g., BERT; Devlin et al., 2018) . Often, it is beneficial to compare these models' performance on controlled sentence generation benchmarks. Whereas unidirectional architectures offer a Figure 1 : Three different ways to compute the PLL score of a multi-token word (e.g., souvenir) during masked language modeling. Purple: target token, pink: within-word tokens that are available during inference, turquoise: within-word tokens that are masked during inference. Sentence tokens that do not belong to the current word are always available during inference.\nnatural way of calculating sentence log-likelihood (summing the log-likelihood scores of each sentence token given its left context), there is no direct way of estimating sentence log-likelihood for a bidirectional model.\nSo far, the best available method to score a sentence under a bidirectional LLM has been the pseudo-log-likelihood (PLL) scoring approach described by Salazar et al. (2020) (and initially used by Shin et al., 2019; Wang and Cho, 2019) . The PLL of a sentence is calculated as the sum of PLL scores for each token given all other sentence tokens, thus providing a comparable metric to unidirectional models' log-likelihood (LL) sentence scoring. The PLL metric is extremely popular; it is used extensively in LLM studies tackling topics as diverse as effects of training data (Sinha et al., 2021; Zhang et al., 2021) , model fluency (Laban et al., 2021) , syntactic and conceptual knowledge (Sinclair et al., 2022; Bhatia and Richie, 2022) , social biases (Nangia et al., 2020) , and others. Some of these studies have already accrued dozens of citations.\nHere, we show that the metric proposed by Salazar et al. (PLL-original) has important shortcomings that limit its utility. Specifically, PLL-original overestimates the PLL of outof-vocabulary (OOV) words, which LLM tokenizers split into multiple tokens. As a result, PLL-original scores fail on several theoretically desired property tests: a robust inverse relationship between sentence length and sentence PLL (Section 4.1), a robust positive correlation between a word's frequency and its PLL score (4.2), and a positive correlation between unidirectional and bidirectional model scores for the same sentences (Section 5). To remedy these issues, we propose an adjusted PLL metric, PLL-word-l2r (l2r: leftto-right), which estimates token PLL when future within-word tokens are also masked (Figure 1 ). We show that the PLL-word-l2r metric outperforms both PLL-original and alternative PLLbased metrics. We therefore recommend to use the PLL-word-l2r metric when estimating sentence PLL under a bidirectional LLM.\n2 Motivation: score inflation for multi-token words\nThe PLL-original metric grossly overestimates the probability of OOV lexical items, such as souvenir (Figure 2 ). This is because OOV words are tokenized into subword tokens (e.g., so ##uven ##ir), and each subword token is predicted using the token's bidirectional context, which crucially includes the remaining tokens that make up the OOV word. Thus, even though the OOV word itself may be surprising given the sentence context, the individual parts of the OOV word are not surprising to a bidirectional model given a sentence context that includes all other subtokens of that word (e.g., it is easy to predict so given ##uven ##ir; see Appendix A for additional examples).\nTo mitigate this bias, we adjust the PLL sentence scoring algorithm such that the model cannot access future within-word tokens (PLL-word-l2r) or any within-word tokens (PLL-whole-word) when predicting the target.\nBelow, we conduct a rigorous investigation of our modified metrics to determine whether this intuitive benefit holds quantitatively.\n\nMethods\nFor our analysis, we adapt the scorer module of the minicons library (Misra, 2022) , an open-source wrapper library around HuggingFace transformers (Wolf et al., 2020) that enables efficient extraction of word-and sentence-level probabilities from LLMs. The MLM scoring procedure of the minicons library follows the procedure originally proposed by Salazar et al. (2020) . For details on sentence preprocessing, see Appendix B.\n\nPLL metrics\nPLL-original. In this metric, each sentence token s t of a sentence S with n tokens is consecutively replaced with a [MASK] and is predicted using all past and future tokens, irrespective of whether the context tokens belong to the same or a different word than the target token. Thus, inference is conditioned on the context S \\t := (s 1 , . . . , s t\u22121 , s t+1 , . . . , s n ). The final sentence score is obtained as the sum of the log probabilities of each sentence token given its context:\nEQUATION\nPLL-word-l2r. In this metric, a [MASK] is placed not only over the current target token (now: s wt ), but also over all future sentence tokens that belong to the same word s w as the target. Inference is then conditioned on a context that includes all preceding sentence tokens (including those belonging to the current word) and all sentence tokens from future words. The final score of a sentence S is obtained as the sum of the log probabilities of each of the |w| tokens in each of the |S| words: (2) PLL-whole-word. This metric is similar to PLL-word-l2r and differs from it only in that a [MASK] is placed over all sentence tokens that belong to the same word s w as the target (both preceding and future). Inference is then conditioned on a context that includes all sentence tokens except those belonging to the current word. The final score of a sentence S is obtained as the sum of the log probabilities of each of the |w| tokens in each of the |S| words in S given the token's context:\nPLL ww (S) := |S| w=1 |w| t=1 log P MLM (s wt | S \\sw ) (3)\nIn Appendix G, we also report results for a PLL metric where not only future within-word tokens, but all sentence tokens to the right of the target context are masked (PLL-sentence-l2r). Although this method is most similar to autoregressive LL scoring, sentence-l2r masking for BERT is known to produce poor quality generations (Wang and Cho, 2019) ; we therefore refrain from including this metric in the main text.\n\nModels\nWe report results for bert-base-cased (and gpt2-medium for comparison) unless stated otherwise. Results for larger models are provided in Appendices D-F.\n\nDatasets\nFor our main analyses, we use the EventsAdapt dataset (Kauf et al., 2022 , based on Fedorenko et al., 2020) . It contains a curated set of 782 syntactically simple sentence pairs that describe plausible or implausible agent-patient interactions in active or passive voice (e.g., The traveler lost the souvenir). Sentences in this dataset are 5-7 words long (mean: 6.1, std: 1.05), with an average word log frequency of 10.95. We use this dataset because it contains a high number of OOV words (19.6% for BERT and 40.3% for GPT-2; see also Appendix C). In Appendices D-F, we show that our results generalize to two larger and more diverse corpora: the Brown corpus (Francis and Kucera, 1979 ) and the reference sentence set from the LibriSpeech corpus (Panayotov et al., 2015) . We also apply our PLL metrics to score the sentences in the Benchmark of Linguistic Minimal Pairs (BLiMP) (Warstadt et al., 2020) , a challenge set of 67k sentence pairs which target specific aspects of linguistic knowledge.\n\nEvaluating PLL metric properties 4.1 Effects of sentence length\nLike Salazar et al. (2020) , we expect that models should, on average, assign lower probability to longer sentences. Thus, negative PLL (which reflects model surprisal) should be positively correlated with sentence length. However, the PLL-original metric violates this expectation in our test sentence set, which shows a negative correlation between the number of tokens and negative PLL. In contrast, PLL-word-l2r and PLL-whole-word metrics exhibit a positive correlation between the number of sentence tokens and negative PLL, just as the negative LL scores for a unidirectional model, GPT2-medium (Figure 3A ).\n\nEffects of word frequency\nAn appropriate (P)LL metric should reflect the fact that LLMs are sensitive to distributional patterns in training text corpora. In particular, we expect more frequent words to have higher (P)LL scores in the absence of contextual effects. This is indeed the case for GPT2-medium; however, the score inflation for multi-token words means that the PLL-original metric grossly overestimates the scores for low-frequency words (Figure 3B ). PLL-word-l2r scores restore this relationship: their correlation with word frequency is much higher than for PLL-original. PLL-whole-word also performs well, although its correlation with word frequency is lower than for PLL-word-l2r, suggesting that it excessively penalizes OOV words.\n\nCorrelation with GPT-2 scores\nWe expect that PLL scores for bidirectional models should be at least somewhat consistent with LL scores for unidirectional models: both metrics are designed to serve are a proxy for sentence probability. Here, we show that the GPT-2/BERT score correlation for the PLL-original metric is very low, whereas correlation scores for PLL-word-l2r and PLL-whole-word are much higher (Figure 4 ), indicating the validity of this metric for cross-model comparison. As in Section 4.2, PLL-word-l2r slightly outperforms PLL-whole-word, likely because it does not penalize OOV words as severely.\nSee Appendices D-F for evidence that all three trends hold for larger models and for other datasets (although the effects in other datasets are attenuated due to a lower OOV ratio).\n\nEffects on benchmarking\nHere, we show that the choice of PLL metric affects benchmarking results for a popular, highly controlled, minimal pair linguistic benchmark: BLiMP. Despite the fact that the comparisons are highly controlled, different metrics yield different BLiMP scores. For all four tested models, PLL-word-l2r achieves the best overall BLiMP score (Table 1 ). See Appendix H for detailed scores.\n\nConclusion\nWe have shown that PLL-word-l2r is the preferred metric for evaluating sentence PLL under a masked language model, such as BERT. Although the results from studies using the PLL-original metric can still be informative, they become harder to interpret if the proportion of OOV words in their test set is high. Therefore, we recommend using PLL-word-l2r in future works.\n", "hypothesis": " Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token.  However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence.  To address this issue, Salazar et al.  (2020) propose to estimate sentence pseudolog-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values.  Here, we demonstrate that the original PLL method yields inflated scores for out-ofvocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target.  We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked.  In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models.  Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties.", "answer": true}
{"title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers", "content": "\nIntroduction\nThe communicative acts of humans in collaborative situations can be described as two parts of a joint act: signalling and recognizing. In such joint activities, these signals work as coordination devices to increment on the current common ground of the participants (Clark, 1996) . The ability to act on these language signals is crucial for future machine learning models to naturally collaborate and interact with humans (Lemon, 2022; Fern\u00e1ndez et al., 2011) . Such a collaborative interaction with humans usually happens fluently, where one communicative act is performed after the other. The framework of reinforcement learning (RL) (Sutton and Barto, 2018) describes such mechanics where an agent is exposed in steps to observations of an environment with dynamic factors such as the position of objects or language expressions. The goal is that the agent learns to behave generally well in Figure 1 : An exemplary interaction between a teacher and a follower that controls the gripper (the grey square). After an initial referring expression l RE at t 0 , the teacher provides feedback l FBt based on the follower's actions until the correct piece is selected at time step T . a particular environment solely based on the observations it makes and rewards it gets.\nA key challenge here is the variability of expressions in language that can be said to the agent during an interaction. Even in relatively simple environments, there might arise an overwhelming amount of situations for an agent to handle (Chevalier-Boisvert et al., 2019) . Recent work on collaborative agents focuses on large precollected datasets for imitation learning to learn agents in complex simulated visual environments (Gao et al., 2022; Padmakumar et al., 2022; Pashevich et al., 2021) or frames the learning as a contextual bandit problem (Suhr and Artzi, 2022; Suhr et al., 2019) . Nevertheless, other work has shown that intermediate language inputs are a valuable signal to improve the agent's learning performance in task-oriented visual environments (Co-Reyes et al., 2019; Mu et al., 2022) .\nIn this paper, we present an initial study that evaluates a follower's learning success given a teacher's intra-episodic feedback in a collaborative setting. We use a referential language game (in English) as a controllable example of a task-oriented collaborative joint activity (see Figure 1 ). In this game one player (the follower) is supposed to select a piece based on the another player's directives (the teacher). We assume a teacher that utters referring expressions as initial instructions and then responds to the follower's actions with intra-episodic feedback. We frame this as a RL problem with sparse rewards where the intermediate feedback is not part of the reward function but its potential usefulness is learnt by the follower alone. 1\n\nRelated Work\nVision and language navigation. In vision and language navigation, an agent is given a natural language instruction which is to be understood to navigate to the correct goal location in a visually observed environment (Gu et al., 2022) . The follower can usually ask an Oracle for further information, if necessary (Nguyen et al., 2019; Nguyen and III, 2019; Fried et al., 2018) . We extend on this idea and aim for an ongoing interaction with corrections that loosens the turn-based paradigm by letting the Oracle choose when to speak as part of the environment. Hence, in our reference game, the language back-channel for the follower is cut, so that we force the follower to rely more on the visual observations for task success.\nContinual learning from human feedback. Suhr and Artzi (2022) let humans instruct the follower and then ask them to rate the agent's behaviour (thumbs up or down). This binary feedback is used for further training as the reward signal in a contextual bandit framework. They show that the agent improves over several interactions with humans. Similarly we evaluate the learning process in the context of RL because it imposes \"weaker constraints on the regularity of the solution\" (Nguyen et al., 2019) , but take a broadly available, off-theshelf learning algorithm (Schulman et al., 2017) to directly study the effects of different kinds of feedback. The feedback given to our agent is of natural language and not directly bound to the re-ward; the follower needs to learn the meaning of the language feedback itself.\nLanguage-guided policy learning. Chevalier-Boisvert et al. ( 2019) compared the sampling complexity of RL and imitation learning (IL) agents on various language-conditioned tasks. They proposed a 2-dimensional visual environment called Minigrid in which an agent is given a single mission statement that instructs the agent to achieve a specific state, e.g. \"Take the red ball\". In contrast to them we intentionally do not use IL approaches, because then the agent would have already learnt how to ground the language signals. We want to test if the agent can pick-up on the language from the interaction alone. For this, we similarly propose a diagnostic environment to directly control for the distributions of target objects (cf. skewed distribution of target objects in CVDN (Thomason et al., 2019) ) and feedback signals.\nOther work uses the Minigrid environment to propose a meta-training approach that improves the learning via natural language corrections, e.g. \"Pick up the green ball\" (Co-Reyes et al., 2019) . The agent is given an episodic correction if a specific task cannot be solved. In this way, the agent must not only ground the mission statement but also ground the corrections into actions. Mu et al. (2022) improve policy learning with intra-episodic natural language sub-goals e.g. \"Pick up the ball\". These sub-goals are provided by a trained teacher policy when a previous sub-goal has been reached. In contrast, we rather follow earlier work (Engonopoulos et al., 2013) on monitoring execution and use a heuristic teacher which provides intraepisodic language feedback whenever it appears feasible. The agent has to learn that certain pairs of feedback and behaviour at a specific time-step lead to the task's success and others to failure.\n\nThe CoGRIP environment\nWe use a Collaborative Game of Referential and Interactive language with Pentomino pieces as a controllable setting. A teacher instructs a follower to select a specific piece using a gripper. Both are constrained as follows: The teacher can provide utterances but cannot move the gripper. The follower can move the gripper but is not allowed to provide an utterance. This asymmetry in knowledge and skill forces them to work together and coordinate. Zarrie\u00df et al. (2016) found that this settings leads to diverse language use on the teacher's side. \n\nProblem Formulation\nThe follower has to navigate a gripper to select a piece described by the teacher. We frame this task as a RL problem with sparse rewards. At each time-step t, given an observation o t \u2208 O of the environment, the agent has to select an action a t \u2208 {LEFT, RIGHT, UP, DOWN, WAIT, GRIP} such that the overall resulting sequence of actions (a 0 , ..., a t , ..., a T ) maximizes the sparse reward R(o T ) = r. An episode ends when the GRIP action is chosen, and the gripper position g t is in the boundaries of a piece. An episode also ends when t reaches T max = 100. Following Chevalier-Boisvert et al. ( 2019), the reward function returns a basic reward minus the movement effort R = 1 \u2212 0.9 * (T /T max ). We extend this formulation and give an additional bonus of +1 if the correct piece has been taken or a penalty of \u22121 when the wrong or no piece has been taken at all.\n\nEnvironment\nThe environment exposes at each time-step t an observation o t that contains the gripper coordinates g t = (x, y), the initial referring expression l RE , the language feedback l FBt (which might be empty) and a partial view v t of the scene. While the scene as a whole is represented as a 2-dimensional image (with RGB colour channel), the partial view represents a 11 \u00d7 11-sized cut out, centered on the gripper position (see Figure 2 ). The teacher generates the initial and feedback statements.\n\nTeacher\nFor the teacher, we assume a heuristic behaviour (a fix policy) that has been shown to lead to collaborative success with humans (G\u00f6tze et al., 2022) and leave the complexity of learning in a multiagent setting (Gronauer and Diepold, 2022) for future work. The teacher produces an initial referring expression l RE = (w 0 , ..., w N ) where N is the message length and w i is a word in the vocabulary. The production rule is implemented following the Incremental Algorithm (IA) (Dale and Reiter, 1995) that is given the symbolic representations of the pieces on the board (see Appendix A.1). The teacher provides a feedback message l FBt = (w 0 , ..., w N ) at a time-step t >0 when the gripper's position g t has exceeded a pre-defined distance threshold D dist = 3 compared to the gripper's last position of feedback g FB last or it is over a piece. The generated feedback is of positive sentiment (\"Yes this way/piece\") when the gripper is then closer to or over the target piece and negative otherwise (\"Not this direction/piece\"). Alternatively, suppose the follower does not exceed the distance threshold after D time = 6 time-steps the feedback message is the same as the initial statement. Overall, the property values and sentence templates lead to a small vocabulary of 33 words.\n\nFollower\nThe follower agent has to move the gripper and successfully grip a piece solely based on the observations. The observations o t = (v t , g t , l RE , l FBt ) are mapped to 128-dimensional features xt \u2208 R using the encoder model (see Figure 2 ). Following Chevalier-Boisvert et al. ( 2019), the word embeddings (which are learned from scratch) of the language inputs are fed through a Gated Recurrent Unit (GRU) (Cho et al., 2014) and then combined with the embedded visual features using a Featurewise Linear Modulation (FiLM) layer (Perez et al., 2018) . These language conditioned visual features are then max pooled, averaged and again averaged with the gripper position. Given the resulting features xt , we learn a parameterised policy \u03c0(x t ; \u03b8) \u223c a t that predicts a distribution over the action space. We use the Proximal Policy Optimization (PPO) (Schulman et al., 2017) implementation of StableBaselines3 v1.6.2 (Raffin et al., 2021) to train the policy in our environment.\n\nTasks\nThe follower has to grip an intended target piece among several other pieces (the distractors). Thus a task is defined by the number of pieces, the target piece and the map size. The pieces for the tasks are instantiated from symbolic representations: a tuple of shape ( 9), color (6) and position (8) which leads to 432 possible piece symbols. For our experiments we use all of these symbols as targets, but split them into distinct sets (Appendidx A.4). Therefore the targets for testing tasks are distinct from the ones in the training tasks. We ensure the reproducibility of our experiments by constructing 3300 training, 300 validation, 720 testing tasks representing scenes with a map size of 20 \u00d7 20 and 4 or 8 pieces.\n\nExperiments\nIn this section we explore the effects of the teacher's language and intra-episodic feedback on the follower's success and ask whether the follower generalizes on aspects of scene complexity.\n4.1 Which referential language is most beneficial for the agent's learning success?\nAs suggested by Madureira and Schlangen (2020) we explore the question of which language is most effective. The IA constructs the initial reference by following a preference order over object properties (Krahmer et al., 2012) . We hypothesize that a particular order might be more or less suitable depending on the task. Thus we conduct a series of experiments without the feedback signal where the preference order is varied as the permutation of color, shape and position. Our results indicate that such orders perform better that prioritize to mention positional attributes as distinguishing factors of the target piece (see Table 1 ). This is reasonable as the directional hint reduces the agent's burden for broader exploration. The follower is able to pick up early on these positional clues and performs overall better during training (see Figure 3 ).\n\n4.\n2 What is the agent's performance gain with intra-episodic feedback in our setting?\nWe conduct the same experiments as above with intra-episodic language feedback to measure its effect on the follower's success rate. Our results show that the follower achieves higher success rates with intra-episodic feedback among all preference orders (see Table 1 ). We also notice that the gain is higher for the low-performing preference orders. This shows that the intra-episodic feedback is a valuable signal for the follower to overcome miss-ing directives in the initial referring expressions.\nThe agent can learn strategies incorporating the feedback signals. This is an interesting finding because language feedback is not part of the reward function and could be empty.\n\nDoes intra-episodic feedback help the agent to generalize on scene complexity?\nAs a proxy for generalization capabilities, we take the best performing follower and raise the complexity of the testing scenes along two dimensions (i) we increase the map size to 30 \u00d7 30 and (ii) put up to 18 pieces on the board. In addition, we hold out 72 combinations of piece shapes and colors that have never been seen during training. Our results show that the agent trained with intra-episodic feedback is able to perform better (i) on the larger map size, (ii) the higher number of pieces and (iii) the new target pieces compared to the one without (see Table 2 ).\n\nConclusion\nIn this work, we studied the effects of a teacher's language and intermediate interventions (the feedback) towards a learner's success and whether the learner generalizes on aspects of scene complexity. Our results show that there is a most beneficial language for the teacher. Its intra-episodic feedback allows the learner to learn faster and generalize better than without intermediate help. An exciting direction for further work is to show the benefits of language feedback for other reinforcement learning problems, to overcome the limits of the heuristic teacher strategy and to reduce the need for feedback after successful training.\n", "hypothesis": " The ability to pick up on language signals in an ongoing interaction is crucial for future machine learning models to collaborate and interact with humans naturally. In this paper, we present an initial study that evaluates intra-episodic feedback given in a collaborative setting. We use a referential language game as a controllable example of a task-oriented collaborative joint activity. A teacher utters a referring expression generated by a well-known symbolic algorithm (the \"Incremental Algorithm\") as an initial instruction and then monitors the follower's actions to possibly intervene with intra-episodic feedback (which explicitly has to be requested). We frame this task as a reinforcement learning problem with sparse rewards and learn a follower policy for a heuristic teacher.  Our results show that intra-episodic feedback allows the follower to generalize on aspects of scene complexity and performs better than providing only the initial statement..", "answer": false}
{"title": "Multi-Domain Dialogue State Tracking with Disentangled Domain-Slot Attention", "content": "\nIntroduction\nTask-oriented dialogue system is designed to assist users to accomplish sorts of certain tasks. For example, by using dialogue-based automated customer service, users can online query information and make reservations. Multi-domain dialogue state tracking has been an important challenge introduced by Budzianowski et al. (2018) , in which numerous mixed-domain conversations are involved. In this case, DST has to track the dialogue states at each turn through the conversation, which contains a huge space involving the combinations of the ontology of different domains, slots, and values. It is a challenging task since spoken language is not formal, in which ellipsis and cross-reference are barrier to handling the correlations among different domains and slots.\nSeveral studies have explored sorts of approaches to handle the correlations among domains and slots. In recent mainstream approaches, each domain and slot are aggregated into a single vector regarded as a query. The query and the dialogue history are fed into attention to generate domain-slot specific representations (Wu et al., 2019) . Then the information interchange across different domains and slots are performed with them to model the correlation among different domain and slots (Hu et al., 2020; Wang and Lemon, 2013; Ye et al., 2021) . However, these approaches introduce too much human prior knowledge and they only consider the correlations among domains and slots names or overestimate these correlations (Yang et al., 2022) .\nTo tackle this problem, we propose a disentangled domain-slot attention (DDSA), which disentangles information extraction about domains and slots in a flexible and context-dependent manner. In detail, we disentangle the query about domains and slots in the domain-slot attention component. Firstly, domain specific representations are obtained using the domain query and the dialogue history. Then the model utilizes these representations and slot query to retrieve slot specific information (in this context, slot means the slot only) and generate domain-slot specific representations. Finally, state prediction is performed with these domain-slot specific representations.\nWe conduct experiments to verify our approach on MultiWOZ 2.0 and MultiWOZ 2.4 datasets. The experimental results show that the proposed approach can effectively improve the performance of multi-domain dialogue state tracking. The contributions of this work can be addressed as follows.\n(1) We propose a disentangled domain-slot attention mechanism to handle the correlations among domains and slots, in which the process of domainslot specific information extraction is disentangled in a flexible and context-dependent manner. (2) We demonstrate that the performance of DST benefits from our proposed approach and make a detailed empirical study that shows that our model performs better than the baseline models based on standard attention with aggregated domain-slot query 1 .\n\nRelated Works\nDialogue state tracking (DST) is the core of taskoriented dialogue systems. In the early years, DST highly relies on hand-crafted semantic features to predict the dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013) , which is hard to handle lexical and morphological variations in spoken language (Lee et al., 2019) . Benefiting from the rapid development of deep learning methods, neural networkbased DST models have been explored. Mrk\u0161i\u0107 et al. (2017) proposes a novel neural belief tracking (NBT) framework with learning n-gram representations of utterances. Inspired by it, a lot of neural network models are investigated (Nouri and Hosseini-Asl, 2018; Ren et al., 2018; Zhong et al., 2018; Hu et al., 2020; Ouyang et al., 2020; Wu et al., 2019) and achieve further improvement.\nPre-trained models have brought natural language processing to a new era in recent years. Many substantial works have shown that the pretrained models can learn universal language representations, which are beneficial for downstream tasks (Mikolov et al., 2013; Pennington et al., 2014; McCann et al., 2017; Sarzynska-Wawer et al., 2021; Devlin et al., 2019; Mittal et al., 2021) . More recently, the very deep pre-trained language models, such as Bidirectional Encoder Representation from Transformer (BERT) (Devlin et al., 2019) and Generative Pre-Training (GPT) (Radford et al., 2018) , trained with an increasing number of selfsupervised tasks have been proposed to make the models capturing more knowledge from a large scale of corpora, which have shown their abilities to produce promising results. In view of it, many pieces of studies about DST have explored to establish the models on the basis of these pre-trained language models (Hosseini-Asl et al., 2020; Kim et al., 2020; Lee et al., 2019; Zhang et al., 2020; Chen et al., 2020; Chao and Lane, 2019; Ye et al., 2021; Heck et al., 2020; Lin et al., 2020) .\nRelated to handling the correlations among domains and slots in multi-domain DST, several approaches have been investigated. In recent mainstream approaches, domain-slot specific representations are first achieved using attention mechanism with aggregated domain-slot query, and then the correlations are modeled with them. (Balaraman and Magnini, 2021) utilizes domain and slot information to extract both domain and slot specific representations and then combines such representations to predict the values. Chen et al. (2020) manually constructs a schema graph modeling the dependencies of different slots and introduces a graph attention matching network to mix the information from utterances and graphs to control the state updating. Hu et al. (2020) introduces a matrix representing the similarity among different slots and then perform slot information sharing among similar slots. The above two approaches are name-based since they only consider the semantics dependencies of slot names to measure the correlation among different slots, which may result in overlooking the dependencies of some slots. More recently, Ye et al. (2021) proposes a data-driven approach to handle these correlations, in which slot self-attention is introduced. However, this approach may inevitably result in overestimating some correlations (Yang et al., 2022) .\n\nDialogue State Tracking with\nDisentangled Domain-Slot Attention \n\nEncoding\nWe employ BERT as the encoder to generate semantic representations. The BERT context whose parameters are fine-tuned during training is used for encoding the dialogue context. Let's define the dialogue context history C T = {R 1 , U 1 , ..., R T , U T } as a set of system responses R and user utterances U in T turns of dialogue, where\nR = {R t } T t=1 and U = {U t } T t=1 , 1 \u2264 t \u2264 T .\nWe define E T = {B 1 , ..., B T } as the dialogue states of T turns, and each E t is a set of slot value pairs {(S 1 , V 1 ), ..., (S J , V J )} of J slots. Although the dialogue history C t = {R t , U t } contains integrated information for the conversation until the t-th turn, the previous study (Ye et al., 2021) has indicated that it is helpful to combine it along with a compact representation E \u2032 t\u22121 , which only includes the slots whose values are not none, as part of the input. In view of this, the context encoder accepts the dialogue history till turn t, which can be denoted as X t = {C t , E \u2032 t\u22121 }, as the input and generates context vector representations\nH t = BERT context (X t ).\nAnother pre-trained model BERT dsv is employed to encode the domains, slots, and candidate values, in which the parameters of BERT dsv remain frozen. For those slots and values containing multiple tokens, the vector corresponding to the special token [CLS] is employed to represent them. For each domain D i slot S j and value \nV k , h d i = BERT dsv (D i ), h s j = BERT dsv (S j ), h v k = BERT dsv (V k ).\n\nDomain Query\nDomain specific representations are achieved using the hidden representations of domains h d and that of dialogue context H t 2 . The process can be described as follows:\nEQUATION\nWhere \nW dq , b Q d , W K d , b K d , W V d , b V d\n\nSlot Query\nAfter the domain query stage, slot specific representations can be obtained using the output of the domain query stage and the hidden representations of slots h s . Note that here \"slot\" means the slot only rather than the concatenation or the average on the representations of domains and slots pairs. The process is shown as follows:\nQ s = W ns sq h s + b Qs (6) K s = W \u2032 ns Ks h n d d + b Ks (7) V s = h n d d (8) \u03b1 ns s = sof tmax( Q s K \u22ba s \u221a k ddsa , axis = slot) (9)\nh ns ds = \u03b1 ns s V s (10)\nEQUATION\nWhere W sq , b Qs , W \u2032 Ks , b Ks , W Vs , b Vs are the parameters of the linear layers for projecting query, key and value respectively at the slot query stage, and W os is the parameters of the linear layer for aggregating the heads of slot query. k ddsa is a hyperparameter indicating the hidden dimension in this component, and n s \u2208 N s is the number of heads at this stage.\nSince the number of combinations of domains and slots is generally larger than that of the actual domain-slot pairs, a linear layer is employed to project domain-slot specific representation h ds to the representation of the actual size.\nEQUATION\nh \u2032 ds = Linear(h ds , axis = domain \u00d7 slot)\nWhere W od is the parameters of the linear layer for aggregating the heads of domain query.\n\nSlot Value Matching\nA Euclidean distance-based value prediction is performed for each slot. Firstly, the domain-slot specific vector is fed into a normalization layer. Then the distances between domain-slot specific vector and value are measured. Finally, the nearest value is chosen to predict the state value.\nEQUATION\np(V k t |X t , DS m ) = exp(\u2212d(h V k , r DSm t )) V \u2032 k \u2208\u03bd k exp(\u2212d(h V \u2032 k , r DSm t )) (15)\nwhere d(\u2022) is Euclidean distance function, and \u03bd k denotes the value space of the actual domain-slot DS m . The model is trained to maximize the joint probability of all slots. The loss function at each turn t is denoted as the sum of the negative loglikelihood. (Ye et al., 2022) . It mainly fixes the annotation errors in the validation and test set. To make a fair comparison with the models evaluated on these two datasets, we follow the pre-processing and evaluation procedure in several previous works (Wu et al., 2019; Lee et al., 2019; Wang et al., 2020; Ye et al., 2021) to keep consistent. We present the settings of the model in Appendix A.\nEQUATION\n5 Results and Discussions\n\nMain Results\nJoint goal accuracy (JGA) and slot accuracy (SA) are employed to evaluate the overall performance.\nThe joint goal accuracy is a strict measurement comparing the predicted values of each slot with ground truth for each dialogue turn, and the prediction is considered correct if and only if all the predicted values match the ground truth values without any error at each turn. The slot accuracy compares each value to the corresponding ground truth individually without seeing other turns. For the results of baselines, we use the results reported in the corresponding references. Table 1 presents the results of the different models on the test set of MultiWOZ 2.0 and 2.4 datasets. As shown in it, overall, our proposed model achieves the best performance on these two datasets. We utilize the Wilcoxon signed-rank test, the proposed method is statistically significantly better (p < 0.05) than baselines. Comparing to the previous SOTA models SAVN on the original MultiWOZ 2.0 dataset, which utilizes slot attention with the concatenated domain-slot query extracting slot specific information and value normalization on the ontologies to varying degrees, and STAR, which uses slot self-attention with the aggregated domain-slot query to model the correlations among different slots, our model obtains a JGA of 54.70% and a SA of 97.49% outperforming SAVN with a JGA of 54.52% and a SA of 97.42% , and STAR with a JGA of 54.53% and a SA of 97.38%. For the latest refined MultiWOZ 2.4 dataset, our proposed model improves the performance by a relatively larger margin comparing to the previous SOTA STAR model from a JGA of 73.62% to 75.58% and a SA of 98.87% to 98.94%. To have a better understanding, an error analysis, a discussion about the effects of different hyperparameter settings, and a case study are made and presented in \n\nAblation Study\nA simple ablation study is performed to verify the effectiveness of our proposed disentangled domainslot attention. As we can see in Table 1 . The performance on the two datasets drops seriously when removing the proposed DDSA , which verifies the effectiveness of our proposed approach. In this case of model w/o DDSA, the domain specific and the slot specific information are extracted by feeding into the dialogue context and the domains and slots to the traditional domain and slot attention respectively, then they are concatenated and sent to the slot value matching component to perform state prediction.\n\nConclusion\nIn this work, we propose a model based on disentangled domain-slot attention for multi-domain dialogue state tracking to handle the correlation among different domains and slots. Unlike the conventional approach in recent mainstream models, we disentangle the query about domains and slots in a flexible and context-dependent manner. The experimental results on MultiWOZ 2.0 and Mul-tiWOZ 2.4 datasets show that, comparing to the models based on conventional approaches of slot attention using the aggregated domain-slot pairs, our approach effectively improves the performance of multi-domain dialogue state tracking. In future works, we will investigate to utilize the proposed approach to generative models and generalize them to more complicated scenarios.\n", "hypothesis": " As the core of task-oriented dialogue systems, dialogue state tracking (DST) is designed to track the dialogue state through the conversation between users and systems.  Multi-domain DST has been an important challenge in which the dialogue states across multiple domains need to consider.  In recent mainstream approaches, each domain and slot are aggregated and regarded as a single query feeding into attention with the dialogue history to obtain domain-slot specific representations.  In this work, we propose disentangled domain-slot attention for multi-domain dialogue state tracking.  The proposed approach disentangles the domain-slot specific information extraction in a flexible and context-dependent manner by separating the query about domains and slots in the attention component.  Through a series of experiments on MultiWOZ 2.0 and MultiWOZ 2.4 datasets, we demonstrate that our proposed approach outperforms the standard multi-head attention with aggregated domain-slot query..", "answer": true}
{"title": "A Multi-dimensional study on Bias in Vision-Language models", "content": "\nIntroduction\nComputer Vision (CV) and Natural Language Processing (NLP) have entered a new era as a result of the development of large pre-trained models based on the Transformer architecture (Vaswani et al., 2017) . This advancement has also involved their multi-modal integration into Vision-Language (VL) models, reaching significant progress (Huang et al., 2020; Li et al., 2021; Tan and Bansal, 2019; Kim et al., 2021; Li et al., 2019; Wang et al., 2022; Li et al., 2022; Sammani et al., 2022) . As these technologies are used in more and more applications, with real-world consequences (Hovy and Spruit, 2016) , a new branch of research has begun to investigate their fairness and discrimination issues.\nWhile many studies have shown that bias, defined in this paper as human prejudice (Caliskan et al., 2017) , exists in uni-modal CV and NLP (Sheng ones (Table 1 ). Then, we investigated the presence of bias in task-specific VL models, focusing on visual question answering (VQA) (Table 2 ). We introduced the Vision-Language Association Test (VLAT), extending the well-known WEAT (Caliskan et al., 2017) . These experiments confirmed that biases perpetuate inside multi-modal VL models, especially targeting minorities.\nContributions We propose the first investigation of multi-dimensional bias in Vision-Language models, also on the downstream task of visual question answering. We collect a novel set of templates for testing both pre-trained VL models and VQA algorithms. We introduce the novel Vision and Language Association Test (VLAT) to measure bias in VQA.\n\nImage Data\nWe used the UTKFace dataset (Zhang et al., 2017) to collect the set of images representative of the dimensions we wanted to focus on: gender, ethnicity, and age. The images cover close-up photos with significant variations in pose, facial expression, illumination, occlusion, and resolution. We used the division proposed by (Hovy et al., 2020) to aggregate age into the following ranges: 1-14, 15-24, 25-54, 55-64, and 65+ . The ethnicity groups are the same as those used in the original data.: White, Black, Asian, Indian, and Other. 1 Image examples are reported in Tables 1 and 2 .\n\nBias in Pre-trained VL models\nIn order to analyze pre-trained VL models, we extended HONEST (Nozza et al., 2021) to multimodal settings. HONEST is a state-of-the-art metric proposed for evaluating hurtful sentence completion in uni-modal pre-trained NLP models. Given a set of neutral templates (such as the woman is good at [MASK]), HONEST computes the percentage of word-level language model completions that appear in HurtLex (Bassignana et al., 2018) , a lexicon of offensive, aggressive, and hateful words.\nIn this work, we revised the templates to use only visual information to describe a person. Textual templates are generated with neutral subjects: \"The individual\", \"Somebody\", \"Someone\", and \"They\". We used the same predicates presented in (Nozza et al., 2021) , e.g., is good at, dreams of being a. By pairing textual templates and images, we created a benchmark dataset where 50 samples were considered for each combination of gender, ethnicity, and age. The dataset contains 2500 instances.\nFor each text-image pair, we asked pre-trained VL models to complete them (see examples in Table 1 ) and compute the percentage of hurtful completions among the top-K candidates.\nWe tested two popular VL models: ViLT (Kim et al., 2021) and VisualBERT (Li et al., 2019) . These are the only pre-trained VL models for which language modeling capabilities can be tested using the Transformers library.\n\nBias in fine-tuned VL downstream models\nWe focused on Visual Question Answering (VQA), a well-known task for VL models that, given a text-based question about an image, must infer the answer (Kafle and Kanan, 2017) . Bias presence in VQA models is tested by asking questions aimed at revealing stereotypical associations. Ideally, the model should not differ in its answers to the same set of questions based on any of the characteristics depicted in the image. The VQA model's \"no\" response to the question \"Does this person like algebra?\" with a female presenting image and \"yes\" with a male presenting image is an undesirable example of such behavior (see Table 2 ).\nWe followed the very popular WEAT (Word Embedding Association Test) (Caliskan et al., 2017) , which seeks to mimic the human implicit association test (Greenwald et al., 1998) for word embeddings. In Caliskan et al. (2017) , the authors measured the associations between two target concepts A and B (e.g., math and arts) and a set of attributes {X i } n i=1 (e.g., gender). Here, we propose the Vision-Language Association Test (VLAT). VLAT recovers WEAT and adapts it to the problem of VQA by using it as an association measure:\nEQUATION\nwhere x is an instance of the attribute X i (e.g., an image representing a woman if X i is the set of female). In order to measure bias strength, VLAT considers the probability that the model associates the bias in the input image x with the target concepts a and b. The association is assumed to exist whenever the model's answer is \"yes\". We then propose a VL bias score computed as the aggregation:\nEQUATION\nAs target concepts, we tested the stereotypical associations proposed in (Caliskan et al., 2017) We framed the questions as \"yes\" or \"no\" where \"yes\" is assumed to encode the presence of association. Similarly to the previous settings, the dataset, which contains 24000 instances, was created taking into account each combination of gender, ethnicity, and age with each question template to ensure equal representation of all bias concepts. We tested popular VL models fine-tuned on VQA 2.0 (Goyal et al., 2019) : ViLT 2 (Kim et al., 2021) , BLIP 3 (Li et al., 2022) , OFA 4 (Wang et al., 2022) , and NLX-GPT. 5 (Sammani et al., 2022) 3 Experimental Evaluation Table 3 reports HONEST scores for the VL models, i.e., the percentage of hurtful completions. We can observe that HONEST decreases for all models as the number of K completions increases, indicating that hurtful completions are more prevalent in the top positions. Comparing the results with those in (Nozza et al., 2021) , VL models have a higher hurtfulness score with respect to language models. Since VisualBERT integrates BERT (Devlin et al., 2019) , we can directly compare their scores. The HONEST score for BERT for K = 10 was 2.67, just over half of VisualBERT's HONEST score. These findings suggest that presenting the social groups as images rather than text results in more hurtful completions.\nTable 5 presents a more detailed view of the HONEST score. Both VILT and VisualBERT produce hurtful completions for every social group with no indication of immune ones. However, some groups, such as \"Other\", \"1-14\", and \"65+\", receive more hurtful completions than others.\nUltimately, we measured the completions' variety. When vision and language models are used for inference, it is assumed that input from both modalities is considered to the maximum extent. Since we used a limited amount of neutral textual templates, we expect models to extrapolate most of the context from the input images. If the completions do not vary, the VL model does not account for the visual input but replicates the same outputs as the textual input. The lack of variety will also reflect in the HONEST score. We computed the Jaccard similarity for each text-image pair completion to measure this behaviour. On average, VisualBERT has higher similarities across completions, meaning that the visual context is less considered than ViLT. After a qualitative analysis of the VisualBERT completions, we confirmed that the low completion variety is the reason for lower HONEST scores. We introduced the Vision and Language Association Test (VLAT) to measure how much models tend to perform stereotypical associations. is that OFA and ViLT have higher scores for Age, indicating that it is the most influential factor over stereotyped associations.\nThe results show that, on average, all models tend to associate men with Unpleasant, Arts, Career, and Mental Disease, while women are more associated with Pleasant, Math, Family, and Physical Disease. These associations partially confirm both well-known social biases and the results of (Caliskan et al., 2017) . We confirmed the same stereotypes for the concept of Career vs. Family. However, we found a different pattern where men are more associated with Arts and women with Math.\nWith respect to ethnicity (see Appendix A.2), we observed that Unpleasant is associated with non-White populations, Arts is strongly associated with Asian, Career with Indian, Family with Black and Asian, Mental Disease with non-White populations and Physical Disease with White and Indian populations. All models agree in associating younger subjects (1-14, 25-54) with Pleasant and older ones (55-64, 65+) with Unpleasant. Themes like Family, Career, and Mental Disease better relate to the groups 1-14 and 55-64. These results are, thus, confirming existing stereotypes.\n\nDiscussion\nOur analysis reveals that pre-trained VL models have varying degrees of bias, which can be attributed to factors such as the models' limited variety and lower responsiveness to visual input. Because the models have different training sets and architectures, it is difficult to determine the exact causes of the observed differences without full retraining. We hypothesize that VilBERT's larger and more diverse training set contributes to its greater response variety.\nFurther insights can be gleaned from the analysis of fine-tuned language models. BLIP is trained on VQA2.0 (Goyal et al., 2019) and Visual Genome (Krishna et al., 2017) corpora, ViLT and OFA on the VQA2.0 dataset, and NLX-GPT on the COCO (Lin et al., 2014) dataset. In a study by Hiraoka et al. (2022) , Visual Genome and VQA2.0 were found to contain the highest number of gender and racial biased instances among VQA datasets. This suggests that these biased datasets could be one of the reasons why BLIP exhibited the highest level of bias, with OFA and ViLT closely following. The varying results between OFA and ViLT indicate that biases can be amplified by the model architecture, even when trained on the same dataset. Moreover, the lower performance of NLX-GPT provides additional evidence that utilizing larger and more diverse datasets can significantly mitigate biases. Lastly, our study identifies specific dimensions of bias that researchers should focus on when creating and testing datasets for fine-tuned models. Our findings emphasize the importance of including data points for a diverse range of demographic categories (e.g., 1-14, 65+) to improve demographic coverage.\n\nRelated Work\nWhile studied individually, bias is still an understudied problem in Vision and Language models.\nBias has been demonstrated to perpetuate in Natural Language Processing models in a variety of languages and tasks both in word and contextualized embeddings (Bolukbasi et al., 2016; Papakyriakopoulos et al., 2020; Li et al., 2020; Nangia et al., 2020; Vig et al., 2020; Prates et al., 2020; Blodgett et al., 2020; Shah et al., 2020; Sheng et al., 2021; Nadeem et al., 2021; Nozza et al., 2021 Nozza et al., , 2022b, inter alia), inter alia) .\nSimilarly, works in Computer Vision (Buolamwini and Gebru, 2018) have studied the performance of different gender classifiers over images of faces grouped by gender and skin tone, showing a consistent difference in error rate at the expense of darker-skinned females, who are the worst-represented class.\nThe recent advancement in both Vision-Language models has made it possible to design new architectures (Huang et al., 2020; Li et al., 2021; Tan and Bansal, 2019) for various crossmodal tasks, e.g., image-sentence retrieval, image captioning, visual question answering, and phrase grounding.\nAs a relatively new research direction, bias research on VL models is, however, still in its infancy. Zhang et al. (2022) constructed a dataset of counterfactual template-based image-text pairs for measuring gender bias in pre-trained VL models. Then, they compared the difference between masked prediction probabilities of factual and counterfactual examples. E.g., the difference of P ([M ASK] = \"shopping\") for the sentence The gender is [M ASK] between male and female inputs. Srinivasan and Bisk (2022) demonstrated that VL models prefer to reinforce a stereotype over faithfully describing the visual scene. They studied how within-and cross-modality gender biases are expressed using a set of template-based data on a curated list of stereotypical entities (e.g., suitcase vs. purse). Hirota et al. (2022) presented an extensive study on investigating gender and racial bias in VQA datasets. They demonstrate the presence of harmful samples, denoting gender and racial stereotypes. Zhou et al. (2022) measured stereotypical bias in pre-trained VL models by extending StereoSet, a text-only dataset proposed for detecting stereotypes in language models (Nadeem et al., 2021) . They introduced VLStereoSet, a benchmark comprising images depicting scenarios that are either stereotypical or anti-stereotypical. Each image is accompanied by three candidate captions, sourced from StereoSet, including one that is stereotypical, one that is anti-stereotypical, and one that is semantically meaningless. The underlying assumption is that if a pre-trained VL model shows a preference for the stereotypical statement, it signifies a demonstration of stereotypical behavior. All of the models they studied displayed stereotypical behaviors across all categories (gender, profession, race, and religion). Finally, Bianchi et al. (2023b) demonstrated the extent of stereotypes and complex biases present in image generation models and the images generated by them. They show that simple user prompts can generate thousands of images that perpetuate dangerous stereotypes based on race, ethnicity, gender, class, and intersectionality. Moreover, their study revealed instances of near-total amplification of stereotypes, and that prompts referencing social groups result in complex stereotypes that are challenging to mitigate. Similar to our work, Berg et al. (2022) explored bias metrics to measure gender and racial bias in facial images on contrastive pretraining VL model such as CLIP (Radford et al., 2021) . They adapted WEAT to VL models and proposed ranking metrics for the text-image retrieval downstream task. Additionally, they introduced a supervised adversarial debiasing technique, which exhibited a significant reduction according to the employed metrics.\nOur study overcomes existing ones by proposing an analysis of bias in different dimensions (gender, ethnicity, age) both at pre-trained and task-specific levels, i.e., visual question answering.\n\nConclusions\nThis paper presents the first investigation on bias in Vision-Language models that focus on multiple dimensions (i.e., gender, ethnicity, and age) and analyzes the downstream application of visual question answering. This work extends the methodologies of state-of-the-art bias evaluation metrics (Nozza et al., 2021; Caliskan et al., 2017) to the multi-modal vision and language framework. Our experiments have shown the presence of noticeable biases in many vision and language models with potentially harmful consequences. In future work, we aim to broaden both the model and the language coverage, as well as to develop a bias detection pipeline that can be automatically run whenever a new VL model is released (Nozza et al., 2022a) .\n", "hypothesis": " In recent years, joint Vision-Language (VL) models have increased in popularity and capability.  Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities.  This paper presents the first multi-dimensional analysis of bias in English VL models, focusing on gender, ethnicity, and age as dimensions.  When subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5% of the time, with higher percentages for female and young subjects. Bias presence in downstream models has been tested on Visual Question Answering.  We developed a novel bias metric called the Vision-Language Association Test based on questions designed to elicit unbiased associations between diverse concepts and targets.  Our findings demonstrate that pre-trained VL models contain minimal biases that are effectively mitigated in downstream tasks.", "answer": false}
{"title": "PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data", "content": "\nIntroduction\nWord alignment, as the task of finding the corresponding source and target tokens in a parallel sentence, was well-known as an essential component of statistical machine translation (SMT) systems. Despite the dominance of neural machine translation (NMT) in recent years, word alignment is still a notable area of research due to its usage in a wide variety of NLP applications, such as annotation projection (Yarowsky et al., 2001; Pad\u00f3 and Lapata, 2009; Huck et al., 2019; Nicolai and Yarowsky, 2019) , bilingual lexicon extraction (Ammar et al., 2016; Shi et al., 2021; Artetxe et al., 2019) , typological analysis (Lewis and Xia, 2008; \u00d6stling, 2015) , guided alignment training of NMT (Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2018) , and evaluation and analysis of translation 1 https://github.com/fatemeh-azadi/PMI-Align outputs (Anthony et al., 2019; Neubig et al., 2019; Wang et al., 2020) .\nFor many years statistical methods such as IBM models (Brown et al., 1993) and tools implemented based on them, namely GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013) , were among the most popular solutions to the word alignment task. Following the rise of deep neural models, several attempts have been made to extract word alignments from NMT models and their attention matrices (Peter et al., 2017; Ghader and Monz, 2017; Zenkel et al., 2020; Zhang and van Genabith, 2021) . However, most of these methods, as well as the statistical aligners, require a sufficient amount of parallel training data to produce high quality word alignments. Recently, Jalili Sabet et al. (2020) have shown that high quality word alignments could be achieved using pre-trained multilingual language models (LMs), like MBERT Devlin et al. (2019) and XLMR Conneau et al. (2020) . Their proposed method called SimAlign, extracts word alignments from similarity matrices induced from multilingual contextualized word embeddings with no need for parallel training data, which is very useful for lowresource language pairs. Afterwards, Dou and Neubig (2021) and Chi et al. (2021) proposed methods called probability thresholding and optimal transport to extract alignments using the similarity matrices derived from pre-trained LMs. They have also proposed some word alignment objectives to fine-tune the pre-trained models over parallel corpora.\nIn this paper, we follow the work done by Jalili Sabet et al. (2020) to extract alignments from pre-trained LMs without requiring any parallel training data and propose PMI-Align. Our main contribution is proposing to compute the point-wise mutual information (PMI) between source and target tokens and using the PMI matrices instead of similarity matrices made of cosine similarities between the representation vectors of each source and target tokens, to align words. We argue that our proposed PMI-based method could align better as it considers the total alignment probability of each source or target token, as well as the joint alignment probabilities (equivalent to cosine similarities). This could alleviate the so-called hubness problem (Radovanovic et al., 2010) in high dimensional spaces, where some token's representation is close to many others (see _went in Figure 1 ). We perform experiments on six different language pairs and show that our method could surpass other alignment methods on five of them. We also conduct our experiments on different pre-trained LMs to show that PMI-Align could be advantageous regardless of the pre-trained model used.\n\nProposed Method\nIn this section, we first discuss how we define and compute the PMI matrix for each sentence pair and then we describe our alignment extraction method using the PMI matrix.\n\nPoint-Wise Mutual Information\nPoint-wise mutual information (PMI) is a wellknown measure of association in information theory and NLP and it shows the probability of two events x and y occurring together, compared to what this probability would be if they were independent (Fano, 1961) . It is computed as follows: PMI(x, y) := log p(x, y) p(x)p(y)\n(1)\nIn the context of word alignments, we define the PMI for a source and target token in a sentence pair as how more probable two tokens are to be aligned than if they are aligned randomly. Given a sentence\nx =< x 1 , ..., x n > in the source language and its corresponding target sentence y =< y 1 , ..., y m >, the joint alignment probability of two tokens, x i and y j , could be computed as:\nEQUATION\nwhere h x i is the contextualized embedding vector of x i extracted from a pre-trained multilingual language model and sim(.) is the cosine similarity measure. The total alignment probability of x i and y j , i.e., p(x i ) and p(y j ), could also be computed according to the total probability rule as follows:\nEQUATION\nBy calculating the PMI for each source and target token in a parallel sentence, we obtain the PMI matrix for that sentence pair, that could be used to extract alignments instead of similarity matrix in SimAlign (Jalili Sabet et al., 2020) . The advantage of using PMI to align words is that it also considers the total alignment probability of each source and target token in addition to their joint alignment probability, which is equivalent to the similarity measure. This leads to reduce the probability to align the token pairs that one of them has high similarities to many other tokens, and thus could alleviate the so-called hubness problem in high dimensional spaces where some data points called hubs are the nearest neighbors of many others.\n\nExtracting Alignments\nTo extract word alignments, we follow the simple Argmax method proposed in Jalili Sabet et al. (2020) . Thus, we first obtain the source to target and target to source alignment matrices using the argmax over each row and each column of the PMI matrix, respectively. Next, we intersect these two matrices to get the final word alignment matrix. In other words, the final alignment matrix A i j = 1 iff i = argmax k (PMI k j ) and j = argmax k (PMI ik ).\nSince the above method would extract alignments on the subword level, we follow the heuristic used in previous work to obtain the word-level alignments by considering two words to be aligned if any of their subwords are aligned (Jalili Sabet et al., 2020; Zenkel et al., 2020; Dou and Neubig, 2021) .\n\nDatasets\nWe perform our experiments on six public datasets, as in (Jalili Sabet et al., 2020) , consists of English-Czech (En-Cs), German-English (De-En), English-Persian (En-Fa), English-French (En-Fr), English-Hindi (En-Hi) and Romanian-English (Ro-En) language pairs. The statistics and URLs of these datasets are available in Table 2 in Appendix A.\n\nModels and baselines\nWe compare our method with the following three state-of-the-art methods proposed to extract alignments from pre-trained multilingual LMs without using parallel training data. For all these methods default parameters were used in our experiments.\nSimAlign 2 (Jalili Sabet et al., 2020) : They propose three methods to extract alignments from similarity matrices, called Argmax, Itermax and Match. Although Itermax and Match methods could not make significant improvements over Argmax and the Argmax method had better AER results for most of language pairs while using the XLMR-base model, they have argued that the Itermax method, which tries to apply Argmax iteratively, could be beneficial for more distant language pairs. Thus, we report both Argmax and Itermax results in our experiments to compare with our method.\nProbability Thresholding 3 (Dou and Neubig, 2021) : In this method they apply a normalization function, i.e., softmax, to convert the similarity matrix of tokens into source to target and target to source alignment probability matrices. Afterwards, they extract the aligned words as the words that their alignment probabilities in both matrices exceed a particular threshold.\nOptimal Transport 4 (Chi et al., 2021) : This method was proposed in both Dou and Neubig (2021) and Chi et al. (2021) , and tried to model the word alignment task as the known optimal transport problem (Cuturi, 2013) . Using the similarity matrix, this method attempted to find the alignment probability matrix that maximizes the sentence pair similar-ity. In our experiments, we use the method proposed by Chi et al. (2021) that utilizes the regularized variant of the optimal transport problem (Peyr\u00e9 et al., 2019) , as it reported better results.\nThere are also many attempts made to improve the pre-trained LMs by fine-tuning on some parallel corpora to better align words. However, as our approach is irrelevant to the pre-trained model and our focus is on the alignment extraction instead of the model, we do not include those methods in our experiments. To demonstrate the effectiveness of our PMI-based alignment regardless of the utilized pre-trained multilingual LM, we conduct our experiments on M-BERT (Devlin et al., 2019) , XLMR-Base (Conneau et al., 2020) and XLM-Align (Chi et al., 2021) which is fine-tuned on a word-alignment task, to show that our method could also be advantageous on more cross-lingually aligned models. All these models are publicly available in the Hugging Face platform (Wolf et al., 2020) .\n\nResults\nTable 1 shows the results of our alignment technique compared to previous methods while using different pre-trained LMs. Following the previous work (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Chi et al., 2021) , we use the 8th layer's representations of each pre-trained model to compute the similarity or PMI matrices. We also use the alignment error rate (AER) (Och and Ney, 2003) as the evaluation metric.\nAs Table 1 shows, our PMI-Align method could consistently outperform the other methods in all language pairs except En-Fr, regardless of the pretrained model used. Compared to Argmax, our method performs better for about 1% or more in AER, while using the XLMR-Base model (except for En-Fr), which exclusively shows the benefits of using the PMI matrix instead of the similarity matrix. We also see that the PMI-Align could surpass the Itermax method for more distant language pairs such as En-Fa and En-Hi, where it was claimed to have the most advantage. Results show that our method could also be beneficial while using a model pre-trained on a word alignment task, i.e., XLM-align, which is expected to have more crosslingually aligned representations, and less hubness problem.\nThe only language pair that our method could not outperform prior methods is En-Fr. This could be due to the closeness of these two languages, as they have many shared subwords and similar word orderings. As a result, pre-trained models for this language pair are better trained and could strongly produce similar representations for aligned words, which reduces the hubness problem to a great extent. Thus, using PMI instead of the similarity matrix could not help. However, our method's performance while using the M-BERT model is comparable to the best results, with about 0.1% difference in AER. Several samples are shown in Appendix B, to better intuitively compare PMI-Align and Argmax, which could better show the benefits of using the PMI matrix instead of the cosine similarities.\n\nRelated Work\nStatistical aligners based on IBM models (Brown et al., 1993) , such as Giza++ (Och and Ney, 2003) and fast align (Dyer et al., 2013) were the most dominant tools for word alignment until the late 2010s. With the rise of neural machine translation models, several attempts made to extract alignments from them (Ghader and Monz, 2017; Garg et al., 2019; Li et al., 2019; Zenkel et al., 2020; Chen et al., 2021; Zhang and van Genabith, 2021) . However, all these models need parallel training data and could not utilize pre-trained contextualized embeddings. Recently, Jalili Sabet et al. (2020) have proposed methods to extract alignments from similarity matrices induced from multilingual LMs without the need for training on parallel data. Following this work, we propose a PMI measure to score and align words in each sentence pair, instead of cosine similarity. Some other alignment extraction methods using multilingual LMs were also provided by Dou and Neubig (2021) and Chi et al. (2021) . They both also proposed several training objectives related to word alignments to fine-tune multilingual LMs on parallel data, as in some other recent works (Cao et al., 2020; Wu and Dredze, 2020; Lai et al., 2022) .\n\nConclusions\nThis paper presents a word alignment extraction method based on the PMI matrices derived from cross-lingual contextualized embeddings, instead of just the similarity matrices. We proposed a way to compute the PMI matrix for each sentence pair and argued that using this PMI measure would be beneficial since for each source-target word pair, it considers not only their similarity to each other but also their similarity values to the other tokens of the sentence, that could mitigate the hubness problem.\nExperimental results show that our PMI-Align method could outperform the previous alignment extraction methods in five out of six language pairs, regardless of the base pre-trained language model used to derive word embeddings. Although our method does not require any parallel training data, our experiments show that it could also benefit the approaches using such data to fine-tune the pretrained models for better word alignments. In future work, the proposed PMI matrix could be investigated in other cross-lingual or even monolingual applications, like the translation quality estimation or the evaluation of text generation tasks, instead of the similarity matrix.\n", "hypothesis": " Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs.  Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality word alignments without the need of parallel training data.  In this work, we propose PMI-Align which computes and uses the point-wise mutual information between source and target tokens to extract word alignments, instead of the cosine similarity or dot product which is mostly used in recent approaches.  Our experiments show that our proposed PMI-Align approach could outperform the rival methods on five out of six language pairs.  Although our approach requires no parallel training data, we show that this method could also benefit the approaches using parallel data to fine-tune pre-trained language models on word alignments.  Our code and data are publicly available 1 ..", "answer": true}
{"title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models", "content": "\nIntroduction\nVision & Language (V&L), which is the fusion of vision and language tasks, has achieved great success in tasks such as caption generation from images (Xu et al., 2015) and image generation from texts (Reed et al., 2016) . This progress has been driven by pre-trained V&L models that are trained on large-scale V&L datasets (Du et al., 2022) . To generate appropriate captions and images for input, pre-trained V&L models need to have prior knowledge of the features of the objects they are generating (Cao et al., 2020; Yun et al., 2021) . These models retain knowledge about entities in particular by inheriting parameters from pre-trained language models used in natural language processing to indirectly utilize data resources such as Wikipedia.\nIn this way, V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang This learning process raises a number of questions, such as whether the knowledge about entities acquired from natural language is adequately retained in the pre-trained V&L model, or whether it is enhanced by combining it with image features. These are important in understanding the limits of what can be generated by the pre-trained V&L model.\nTo answer these questions, we propose a task of generating tables and images of infoboxes in English Wikipedia. Figure 1 shows an example of the target infobox, in which either tables or images are generated by the proposed task. In both cases, the model must know the entities to generate them properly.\nWe collected about 200,000 infoboxes to construct the Wikipedia posed task. In addition, we used OFA (Wang et al., 2022) , a pre-trained V&L model that has achieved state-of-the-art performance in various V&L tasks.\nOur evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is lost when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.\nIn image generation, we found that OFA can generate more accurate images by using the knowledge expressed in the table. We also found that the models trained only on natural language can infer table knowledge, which increases the diversity of generated images. Our code and dataset will be released at https://github.com/kamigaito/WikiTIG.\n\nVision & Language Models\nMany pre-trained V&L models have achieved stateof-the-art performance on various tasks by inheriting the weights of the conventional pre-trained models for natural language and images (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) before learning V&L datasets. Our study examines how the knowledge represented in the pre-trained model for natural language is transformed through such a learning process. We select OFA, which has achieved state-of-the-art performance in multiple V&L tasks, as our target model.\nFigure 2 shows the network structure of OFA and its relation to each dataset 2 . OFA uses VQGAN (Esser et al., 2020) on the decoder to transform images into discrete sequences so that the same Transformer (Vaswani et al., 2017) is used for image and natural language generation. Because OFA inherits 2 Appendix A describes the data for the pre-training.\n\nTask\nInput Output parameters from BART (Lewis et al., 2020) , which shares a similar Transformer structure, OFA should include knowledge acquired from natural language such as Wikipedia articles. Unlike the decoder, the encoder handles images directly; thus, OFA uses the output of ResNet (He et al., 2016) to embed images in addition to the embedding layer inherited from BART.\n\nTable and Image Generation\nIn this section, we describe two tasks for verifying knowledge behavior in the V&L model: table generation and image generation. Both tasks are based on infoboxes in Wikipedia articles, which correspond to summary information of the Wikipedia articles comprising tables and images 3 . Thus, it is suitable for verifying the knowledge about entities in Wikipedia kept in the pre-trained V&L model.\nIn the following subsections, we explain the details of each task.\n\nTable Generation\nIn the table generation task, the target V&L model generates a table from a title and/or image of the infobox. To do this, the model generates linearized tables, similarly to table generation by descriptions (Wu et al., 2022b) . In our setting, we linearize tables as shown in Figure 3 using the column separator \"|\" and the row separator \"<>\" to reuse pretrained token embeddings. The separator symbols are accompanied by spaces before and after for use in BPE tokenization. We investigate the target model by directly generating such linearized text. We use the following settings for the investigation.\nGeneration from titles We investigate the knowledge about entities held by V&L models by comparing tables generated from titles by pre-trained V&L models and by pre-trained models trained only on natural language.\nGeneration from title and images We generate tables from titles with images and compare the results with those generated from only titles. This enables us to investigate the new knowledge in pretrained V&L models transferred from images.\nMetrics For comparison, we use the following evaluation metrics to measure how close the generated tables are to the actual ones.\n-ROUGE: Since the linearized tables are text data and the infobox plays the role of summarizing the article, we use ROUGE (Lin, 2004) , the most widely used evaluation method for automatic summarization. In our evaluation with ROUGE, we convert the column separator \"|\" and the row separator \"<>\" to spaces so that the sequence of strings is not restricted to rows and columns.\n-Table - F 1 : To evaluate the tables with respect to their structure, we divide the cells by their types and then evaluate the matches with the reference table in terms of the F 1 measure for each case and average them. When calculating the matches, we apply clipping used in ROUGE to prevent the score from increasing due to the repetition of the same cell in the output 4 . We treat cells of each type separately 5 as follows:\n\u2022 Group: The infobox sometimes divides the table into groups, with the first row of each group serving as a header for the group name. The prediction performance for the group names is important for verifying what aspects of knowledge the model has about the entities. Since these rows consist of a single column, we target rows consisting of a single column in this type of cell.\n\u2022 Header: The head of each row in the table consisting of more than one column is usually the header of a subsequent cell in the same row. Therefore, the prediction performance for headers is important for the same reason as for group names. to the headers. Therefore, the prediction performance of the values is important for knowing whether the model has detailed knowledge about the entity. To examine the correspondence between headers and their values, we treat a header and its corresponding value as a pair.\n-Corpus-F 1 : Because the above \n\nImage Generation\nIn the image generation task, the model receives a title, caption, and table to generate the corresponding image:\nGeneration from a title and caption By using the minimum input required to generate images, we investigate the difficulty of generating them compared to other datasets.\nGeneration from a title, caption, and table We investigate the impact of knowledge about entities on image generation by generating images from input, including tables, and compare the results to the setting without tables.\nMetrics We use the following three widely used measures for evaluating image generation.\n-CLIP: The relevance of the input text to the generated images inferred by the pre-trained V&L model CLIP (Radford et al., 2021) .\n-Inception Score (IS): How easily a model can distinguish the differences between each image and the variety of generated images (Salimans et al., 2016) . It is inferred by the pre-trained image classification model Inception-v3 (Szegedy et al., 2016) .\n-Frechet Inception Distance (FID): How close the generated image is to the reference image, es- \nModel Input ROUGE \u2191 Table-F 1 \u2191 Corpus-F 1 \u2191 1 2 L\n\nDataset Creation\nWe created the Wikipedia Table and Image Generation (WikiTIG) dataset by extracting infoboxes from the HTML dump data of the English Wikipedia 8 . To ensure consistency in the format of infoboxes, we limited the extraction target to those containing a title in the first row and an image in the second row, as shown in Figure 1 .\nIn order to use only entities with sufficient information, we targeted entities for which the table was not empty. In addition, to ensure reliable correspondence, only rows one column wide, which often describe groups, and rows two columns wide, which often consist of a header and its value, were targeted for extraction.\nThe target images are limited to those in jpeg, png, and gif formats. Since some captions do not include a title, we used a hyphen to join the title at the beginning of the caption in such cases.\nTable 2 shows the size of each dataset. The dataset size diverges between two tasks because some infoboxes do not include captions 9 .\n\nTable Generation\nSettings We chose OFA (Wang et al., 2022) , a pre-trained V&L model, and BART (Lewis et al., 2020) , pre-trained only in natural language, as models for comparison. For both models, we used the base settings with the hyperparameters reported in Wang et al. (2022) . We performed the training three times with different seeds and reported their average scores with their standard deviations 10 .\nResults Table 3 shows the results for each setting in the table generation 11 . When only the title is used as input, the result of BART is more accurate than that of OFA, indicating that part of the knowledge acquired from natural language is lost due to additional learning in the V&L model. The use of image information improves Table-F 1 for headers, indicating that images reinforce the knowledge of what kind of features an entity has.\nIn contrast, F 1 for cell values did not improve, indicating that information obtained from images does not complement detailed knowledge, such as the values corresponding to each header obtained from natural language.\nThe results of BART in Corpus-F 1 also suggest that BART contains more diverse knowledge internally than in other settings. This result reinforces that the V&L model forgot part of the knowledge from natural language through additional learning, and images could not fully complement them.\n\nImage Generation\nSettings Similarly to the table generation, we chose OFA for the comparison. We additionally join the reference tables (Gold) and those generated by models in \u00a75.1 (OFA, BART) as the input in order to investigate the impact of the ability to infer table knowledge. We also used the base settings with the hyperparameters reported in Wang et al. (2022) . We also performed the training three times with different seeds and reported their average scores with their standard deviations 12 .\nResults Table 4 shows the results for each setting in the image generation 13 in OFA is close to the result (Wang et al., 2022) in MS COCO (Chen et al., 2015) for image generation, the use of our created dataset is reasonable for training models. In addition, the input of Table (Gold) improves all metrics, indicating that the model produces higher quality images when provided with complementary knowledge about the entities. This result also indicates that OFA does not retain sufficient knowledge of the entities in English Wikipedia.\nIn addition, we did not observe any performance improvement in CLIP and FID when fed with automatically generated tables from BART and OFA. However, tables generated by BART improves IS with the lower performance degradation of FID than that by OFA, indicating that automatically generated tables can improve the diversity of the output images and accurate tables are more important for improving performance in image generation.\n\nRelated Work\nFollowing the advancements in V&L models (Du et al., 2022) , there have been various studies that investigate V&L models. Cao et al. (2020) conducted a comprehensive analysis of V&L models including the difference between model structures. Through their analysis, they revealed the importance of text information in V&L tasks over image information.\nSeveral studies focused on the performance differences between V&L models and text-only models. Yun et al. (2021) investigated the improvement of linguistic representations by pre-training V&L models on PhysicalQA (PIQA) (Bisk et al., 2020) and the probing framework of (Tenney et al., 2019) . They concluded that the benefit of pretrained V&L models for text-only tasks is marginal. Iki and Aizawa (2021) ; Hagstr\u00f6m and Johansson (2022) compared the performance of V&L models and text-only models on the text-only benchmark, GLUE (Wang et al., 2018) and determined that the text-only model achieved higher scores than the V&L models.\nHowever, even though various kinds of V&L models (Lu et al., 2019; Su et al., 2020; Li et al., 2020; Cho et al., 2021; Wang et al., 2022; Saharia et al., 2022) inherit language-related knowledge from pre-trained language-only models, how the knowledge is inherited has yet to be investigated. Our work clarifies this by using our created dataset, Wikipedia Table and Image Generation (WikiTIG).\n\nConclusion\nThis paper investigates how knowledge about entities are preserved in a pre-trained V&L model which is originally transferred from a pre-trained natural language model.\nWe analyzed a pre-trained V&L model by creating the Wikipedia Table and Image Generation (WikiTIG) dataset for generating images and tables of the infoboxes in Wikipedia. WikiTIG consists of 200,000 infoboxes and their corresponding images from English Wikipedia.\nExperimental results on a pre-trained V&L model OFA (Wang et al., 2022) showed that the model forgot part of the knowledge about entities during pre-training, and the image information did not fully compensate for the forgotten knowledge.\n", "hypothesis": " In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models.  This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity.  In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. Our evaluation of the table generation revealed that part of the knowledge in the V&L model acquired from natural language is enhanced when the V&L model is pre-trained. We also found that additional knowledge for entities was acquired by supplementing image information, which was not possible solely from textual data.  We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA (Wang et al., 2022) , which has achieved state-of-the-art results in multiple tasks.  Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks..", "answer": false}
{"title": "\"A Little is Enough\": Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation", "content": "\nIntroduction\nIn recent times, Neural MT has shown excellent performance, having been trained on a large amount of parallel corpora (Dabre et al., 2020) . However, not all language pairs have a substantial amount of parallel data. Hence, we have to rely on the noisy web-crawled corpora for low-resource languages. The task of Parallel Corpus Filtering aims to provide a scoring mechanism that helps extract goodquality parallel corpus from a noisy pseudo-parallel corpus. The task of Quality Estimation (QE) aims to provide a quality score for a translation when the reference translation is unavailable. We use Quality Estimation to assign the quality scores to the sentence pairs present in pseudo-parallel corpora and extract good-quality parallel sentences. We aim to improve the quality of Machine Translation for English(En)-Marathi(Mr), Hindi(Hi)-Bengali(Bn) and Chinese(Zh)-English(En) language pairs by using sentence-level QE-based corpus filtering. We observe that QE-based corpus filtering performs better than previously proposed methods.\nOur contributions are:\n1. Adaptation of the QE framework, which is normally used for MT evaluation, to extract highquality parallel corpus from pseudo-parallel corpus; to the best of our knowledge, this is a novel adaptation of the QE framework to extracting quality parallel corpus from the pseudo-parallel corpus.\n2. Demonstrating the promise of Few-Shot QE technique to generate training data for MT; a Hindi-Bengali QE model is trained with only 500 training instances transfer learned from an English-Marathi trained QE model; the filtered parallel data using this Hindi-Bengali QE system gives 0.6 BLEU point improvement over Hi-Bn MT system trained on the pseudo-parallel corpus.\n3. Demonstrating performance improvement of the Machine Translation systems by up to 1.8 BLEU points for English-Marathi, Hindi-Bengali and Chinese-English language pairs, over the model trained on the whole pseudoparallel corpus. 2022) mentioned different types of noise that can be injected in a parallel corpus and investigated whether state-of-the-art filtering models are capable of removing all the noise types proposed by Khayrallah and Koehn (2018) .\nMost recently, Batheja and Bhattacharyya (2022) used a combination of Phrase Pair Injection and LaBSE (Feng et al., 2020) based Corpus Filtering to extract high-quality parallel data from a noisy parallel corpus. In contrast, we use QE-based filtering to extract high-quality data from noisy pseudoparallel data. We observe that QE quality scores are superior to the LaBSE quality scores.\n\nQuality Estimation\nQuality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The state-of-the-art Mono-TransQuest architecture, proposed by Ranasinghe et al. ( 2020), builds upon XLM-R, a widely-used pretrained cross-lingual language model known for its ability to generalize to low-resource languages (Conneau et al., 2020) . (Kocyigit et al., 2022) proposed a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE in a Parallel Corpus Mining setting. The Parallel Corpus Mining task aims to detect the most similar texts in a large multilingual collection and perform sentence alignment. This motivates us to use QE in the Parallel Corpus Filtering task. \n\nApproaches\nWe first discuss methods to extract good-quality parallel sentences from the pseudo-parallel corpus. Then we discuss a transfer learning-based filtering approach in few-shot settings.\n\nLaBSE based Filtering\nLanguage Agnostic BERT Sentence Embedding model (Feng et al., 2020 ) is a multilingual embedding model that supports 109 languages, including some Indic languages. We generate the sentence embeddings for the source and target sides of the pseudo-parallel corpora using the LaBSE 1 model. Then, we compute the cosine similarity between the source and target sentence embeddings. After that, we extract good-quality parallel sentences based on a threshold value of the similarity scores.\n\nPhrase Pair Injection (PPI) with\nLaBSE-based Filtering Batheja and Bhattacharyya (2022) proposed a combination of Phrase Pair Injection (Sen et al., 2021) and LaBSE-based Corpus Filtering to extract highquality parallel data from a noisy parallel corpus.\nWe train a PBSMT model on the noisy pseudoparallel corpus using the Moses 2 decoder. Then, we extract phrase pairs with the highest translation probability. Finally, we perform LaBSE-based filtering on these phrase pairs to remove poor-quality phrase pairs. We augment these high-quality phrase pairs with LaBSE-filtered parallel sentences.\n\nQuality Estimation based Filtering\nIn this approach, we train the MonoTran-sQuest 3 (Ranasinghe et al., 2020) model and use it to generate the quality scores for the pseudoparallel corpus of the corresponding language pair. Then, we extract high-quality parallel sentences from the pseudo-parallel corpus using a threshold quality score value.\n\nFew-shot Quality Estimation\nThe Quality Estimation task requires humanannotated Direct Assessment scores for the corresponding language pairs. In few-shot settings, we fine-tune a pre-trained QE model for a highresource language pair on QE data for the corresponding low-resource language pair to obtain a QE model for the low-resource language pair.\n\nMathematical Preliminaries\nLaBSE scoring Let D = {(x i , y i )} N i=1 be a pseudo-parallel corpus with N examples, where x i and y i represents i th source and target sentence respectively. We first feed all the source sentences present in the pseudo parallel corpus as input to the LaBSE 4 (Feng et al., 2020) model, which is a Dual encoder model with BERT-based encoding modules to obtain source sentence embeddings (S i ). The sentence embeddings are extracted as the l2 normalized [CLS] token representations from the last transformer block. Then, we feed all the target sentences as input to the LaBSE model to obtain target sentence embeddings (T i ). We then compute cosine similarity (score i ) between the source and the corresponding target sentence embeddings.\nS i = LaBSE (x i )\n(1) In QE experiments, we create a small corpus (500 instances) for Hindi-Bengali language pair that consists of human-annotated Domain Adaptation scores for each sentence pair annotated by three annotators. The pairwise Pearson Correlation between the three annotators of Hindi-Bengali QE is 0.68, 0.61 and 0.67. This indicates a good agreement between the three annotators. Please refer to Appendix A.3 for further annotation details. We use the QE data provided by Ranasinghe et al. (2020) and Deoghare and Bhattacharyya (2022) For evaluation, we use the FLORES 101 test set which contains 1,012 sentence pairs for each language pair.\nEQUATION\nscore i = cosine_similarity (S i , T i ) (3) QE scoring We feed \"x i [SEP ]y i \"\n\nModels\nWe use MonoTransQuest model architecture to train the QE models. We use the Indic NLP library for preprocessing the Indic language data and Moses for preprocessing the English language data. For Indic languages, we normalize and tokenize the data. For English, we lowercase and tokenize the data. We use a Transformer based architecture provided by OpenNMT-py library to train the NMT models for all our experiments. The optimizer used was adam with betas (0.9, 0.98). The initial learning rate used was 5e-4 with the inverse square root learning rate scheduler. We use 8000 warmup updates. The dropout probability value used was 0.1 and the criterion used was label smoothed cross entropy with label smoothing of 0.1. We use a batch size of 4096 tokens. All the models were trained for 200,000 training steps. We use MonoTransquest 8 model to train the sentence-level QE model. We start with a learning rate of 2e-5 and use 5% of training data for warm-up. We use early patience over ten steps. We use a batch size of eight. The model architecture is mentioned in Appendix A.2. Baseline We train the baseline NMT models on 8 https://github.com/TharinduDR/TransQuest the whole pseudo-parallel corpus augmented with the parallel corpus for the corresponding language pairs. LaBSE based Filtering In this model, we use the LaBSE filtering with threshold 0.8 to extract good quality parallel sentences from the En-Mr, Hi-Bn and Zh-En pseudo-parallel corpus. Then we augment the parallel corpus with the LaBSEfiltered parallel sentences and train the respective NMT models. LaBSE + PPI-LaBSE based Filtering We extract LaBSE Filtered parallel sentences and phrases from the pseudo-parallel corpus and augment them with the parallel corpora to train the respective NMT models. Our Model, QE based Filtering We train the sentence-level QE model from scratch for En-Mr and Zh-En language pairs using their respective training data, Table 2 . We use the English-Marathi pre-trained QE model for the Hi-Bn language pair and finetune it on Hi-Bn training data, Table 2 . We compute quality scores for the noisy pseudoparallel corpora using the trained QE models. Then, we extract high-quality sentence pairs from the pseudo-parallel corpus using the threshold values of -0.5, -0.4, and 0 for En-Mr, Zh-En, and Hi-Bn language pairs, respectively. We augment the extracted high-quality sentence pairs with the parallel corpus and train the respective NMT models. Table 5 : BLEU scores of Hi\u2192Bn and Bn\u2192En NMT models on FLORES101 test data. Here, we establish the efficacy of few-shot QE-based filtering using a pre-trained En-Mr model fine-tuned on Hi-Bn QE data to extract a high-quality parallel corpus from the Hi-Bn pseudo-parallel corpus. For actual instances of translations please refer to Appendix A.1\n\nResults and Analysis\nWe evaluate our NMT models using BLEU (Papineni et al., 2002) . We use sacrebleu (Post, 2018) python library to calculate the BLEU scores. 6. The QE quality scores have a higher correlation with human annotated quality scores, compared to LaBSE quality scores for all 3 language pairs. Table 7 shows the Pearson Correlation between LaBSE and QE quality scores for all 3 language pairs. We observe that the LaBSE quality score has a low correlation with the QE quality score and the QE quality score has a high correlation with the human annotated quality score. This establishes the superiority of QE over the LaBSE quality score.\n\nConclusion and Future Work\nWe introduced a simple Quality Estimation based corpus filtering approach to extract high-quality parallel data from the noisy pseudo-parallel corpora. The takeaway from our work is that sentence-level QE-based filtering performs better than LaBSE-based filtering and helps improve the performance of NMT systems. We also show that few-shot QE models trained using a transfer learning-based approach can be used to extract good-quality parallel corpus from the pseudoparallel corpus. Only 1/40 th of the normal data requirement (7K-25K) of QE training data achieves comparable performance for the Hindi-Bengali language pair. We also show that the QE quality score is superior to the LaBSE quality score.\nIn the future, we plan to use the proposed corpus filtering technique for other language pairs. This will provide us with a general overview of how this filtering technique performs for multiple languages.\n", "hypothesis": " Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available.  The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus.  We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus.  To the best of our knowledge, this is a novel adaptation of the QE framework to extract quality parallel corpus from the pseudo-parallel corpus.  By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system's performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model.  The baseline model is the one that is trained on the whole pseudo-parallel corpus.  Our Few-shot QE model transfer learned from the English-Marathi QE model and finetuned on only 500 Hindi-Bengali training instances, shows an improvement of up to 0.6 BLEU points for Hindi-Bengali language pair, compared to the baseline model.  This demonstrates the promise of transfer learning in the setting under discussion.  QE systems typically require in the order of (7K-25K) of training data.  Our Hindi-Bengali QE is trained on only 500 instances of training that is 1/40 th of the normal requirement and achieves comparable performance.  All the scripts and datasets utilized in this study will be publicly available..", "answer": true}
{"title": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios", "content": "\nIntroduction\nReasoning plays a central role in human communication (Frank and Goodman, 2012) . While language models have demonstrated remarkable capacity on downstream tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) , it remains unclear to what extent predictions generated by language models are consequences of correlation with linguistic heuristics in the context, versus robust reasoning about causal relations grounded on understanding of world knowledge.\nIn this paper we leverage counterfactual conditionals to investigate the capacity of pre-trained LMs (PLMs) to distinguish hypothetical scenarios from reality, and to examine how this interacts with models' use of existing real world knowledge as well as shallower associative cues. Counterfactuals consist of a premise which is false in the real world but true in the hypothetical world (e.g., \"If cats were vegetarians\"), and an imaginary consequence of this premise (\"cats would love cabbages\"). Testing language models with counterfactuals allows us to use language to manipulate what is true and what is hypothetical, and to test models' ability to separate and use this information for predictions. Previous work has established the use of counterfactual scenarios to probe inference ability (Qin et al., 2019; Zellers et al., 2019; Mostafazadeh et al., 2016; Meng et al., 2022; Rajani et al., 2019; Saparov and He, 2022; Frohberg and Binder, 2021; Elazar et al., 2021; Rudinger et al., 2020) , but the datasets lack systematic control of lexical cues and world knowledge, which makes it likely that the performance could be attributable to spurious cues in the datasets (Niven and Kao, 2019) .\nFor our tests we draw on and adapt inputs from existing psycholinguistic experiments. We begin by testing models' ability to override existing world knowledge when the context indicates that the correct completion involves a hypothetical world (e.g., \"if cats were vegetarian, cats would love cabbages/fish\"). We test five popular PLMs, and find that models can increase their preference for counterfactual completions given counterfactual context-however, most models rely strongly on simple lexical cues. Next we control the effect of real world knowledge and lexical triggers, to test models' understanding of what counterfactual language implies about the world state. We find that most models fail to understand real-world implications of counterfactuals and largely rely on lexical triggers-with the exception of GPT-3, which shows greater sophistication, but continues to show non-trivial susceptibility to interfer-ence from lexical-associative cues. We discuss the implications and possible interpretations of these findings with respect to linguistic competence and predictive strategies of these models.\n\nExp1: overriding world knowledge\nOur first experiment investigates whether LMs are able to take a counterfactual scenario and predict a counterfactual-consistent completion that contradicts general world knowledge.\nItems We draw directly on counterfactual stimuli from the psycholinguistic study of Ferguson and Sanford (2008) . There are 128 items from the original psycholinguistic experiments, and we synthetically generate 10,720 additional items (see Appendix A.2 for illustration of data generation process). We match target nouns and syntactic constructions across conditions in order to control lexical properties that influence language models' predictions. Table 1 shows example items from the synthetic large-scale dataset (see Section A.1 for example items from the small-scale dataset).\n\nCond Sentence\nCW If cats were vegetarians, people would love them.\nFamilies would feed cats with fish/cabbages.\n\nRW\nBecause cats are carnivores, people love them.\nFamilies would feed cats with fish/cabbages.\n\nBB\nFamilies would feed cats with fish/cabbages The experiment includes two key conditions: Counterfactual-World (CW) and Real-World (RW) (Fig. 1 ). The CW condition presents a counterfactual scenario, e.g., in which cats are vegetarians. The logical target completion in this example is \"cabbages\", but because in reality cats are more likely to eat fish, this contradicts world knowledge. By contrast, in the RW condition the logical completion is consistent with the real world (\"feed cats with fish\"). We also include one Baseline Bias (BB) condition, for a more direct test of the strength of models' baseline preference for each completion.\nExperiments We test counterfactual reasoning in five pre-trained language models. We include autoregressive transformers in the GPT family (GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) ) and masked language models in the BERT family (BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) and MPNet (Song et al., 2020)) 2 .\nWe test models by comparing the log-probability that each model assigns to the CW-congruent (\"cabbages\") and RW-congruent (\"fish\") completions given the contexts. For all conditions, we compute the percentage of items in which the CW-congruent continuation has a higher probability than the RWcongruent continuation. This means that in RW and BB conditions, lower values reflect better predictions, since the CW-congruent completion is the less logical completion in these conditions. Table 2 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp1. In the CW condition, higher values reflect better predictions. In RW and BB conditions, lower values reflect better predictions.\nResults Table 2 shows the preferences for CWcongruent completions across all models and conditions, for the small-scale hand-designed items from the psycholinguistic experiment, and for the large-scale synthetic items. 3 We see that all mod-els show stronger preference for CW-congruent continuations in the counterfactual (CW) context than in the other conditions (though in the case of BERT on the small-scale data, this difference is negligible). All models show below-chance preference for CW-congruent continuations in the RW condition-which means above-chance preference for the correct RW-congruent continuations. However, though all model preferences for the correct CW-congruent continuation are higher in the CW condition than in the RW condition, even in the CW condition the preference for CW-congruent conditions is at best slightly above chance for most models. The exception is GPT-3, which is the only model to prefer the CW-congruent continuation in greater than 70% of items.\nWe also see that GPT-3 shows exceptionally strong performance on both BB and CW conditions. This suggests, slightly counterintuitively, that stronger grasp of relevant world knowledge may in fact be associated with models more effectively overriding that knowledge in a counterfactual. To investigate this effect further, we examine the impact of world knowledge at the item level. We quantify strength of world knowledge as the difference between models' log-probability of CW-and RW-congruent continuations for a given item in the BB condition, and the strength of counterfactual preference as the difference between log-probability of CW-and RW-congruent continuations for a given item in the CW condition. We then compute the Pearson correlation between these strength measures. We find a significant correlation between the robustness of world knowledge encoding and strength of counterfactual preference in the CW condition across all language models (see Appendix A.3), further supporting a relationship between strength of world knowledge and counterfactual sensitivity. While previous work has suggested that large language models may have difficulty avoiding memorized texts when explicitly prompted to end famous quotes differently (McKenzie et al., 2022) , our results suggest that world knowledge may in fact facilitate reasoning when accompanied with clear structural cues (e.g. \"if\"). To better understand how world knowledge informs language models' predictions and in-CW condition alone. However, to further address this concern, we calculate the proportion of items in which the model shows the correct preference in both CW and RW conditions. The results are presented in Section A.5 and suggest a comparable pattern in terms of relative model strengths.\nference, it will be important to continue expanding the scale of tests and more carefully operationalize definitions of world knowledge in future work.\n\nExp2: impact of cue words in context\nThe first experiment suggests that models can to an extent override world knowledge given a counterfactual, particularly in cases when models have a strong handle on the relevant world knowledge. However, it is possible that in these tests the models were not relying on sophisticated understanding of counterfactuals, but rather on simple lexical triggers in context. Consider, for instance, that models could perform well in Exp1 if they simply increase their preference for \"cabbages\" in the proximity of \"vegetarians\", etc. To test the impact of these lexical triggers, we incorporate an additional condition.\nItems Table 3 and Fig. 2 show a sample item and illustration of experimental set-up with the new added condition. In this Counterfactual-to-Reality (CR) condition, models see the same counterfactual context, but the subsequent sentence references actual reality. So the correct completion is consistent with reality, but inconsistent with the lexical trigger (\"vegetarians\"). We generate sentences in the CR condition by modifying CW sentences to include the discourse connective \"In reality\" and to include present tense in the second sentence.\n\nCR\nIf cats were vegetarians, people would love them.\nIn reality, families feed cats with fish/cabbages. Experiments As above, we calculate percentage of items in which models prefer the CW-congruent continuations. Models relying on information beyond simple lexical triggers should show a sharp drop in preference for the CW-congruent completion in the CR condition, where the correct completion should align with real world information.\nResults Table 4 shows the results. We see that most models show non-zero drop between CW and CR conditions-however, for most models this reduction is minor. It is only GPT-3 that shows a truly substantial drop in CW-congruent preference, and only in the large-scale synthetic dataset. This suggests that most models are largely following simpler lexical triggers, while GPT-3 has somewhat greater sensitivity to more detailed linguistic cues. Note, however that GPT-3's relative success on the synthetic data over the small-scale data may rely on larger distance between lexical triggers and target positions: see Appendix A.4 for evidence on GPT-3's sensitivity to linear distance. Table 4 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp2. In the CW condition, higher values reflect better predictions. In the CR condition, lower values reflect better predictions.\n\nExp3: Inferring real world state with counterfactual cues\nThe previous experiments indicate that models can override world knowledge in the face of counterfactual evidence, and that the ability to do this improves with stronger world knowledge-but for most models this performance appears to be driven largely by simple lexical triggers in the context, with the possible exception of GPT-3. In this section we remove the influence of pre-existing world knowledge, and hold constant lexical triggers across conditions, for a more direct test of models' sensitivity to linguistic indicators of counterfactuals, and what they say about the true state of the world. This task is particularly challenging because language models must infer the true state of the world based on the presence of counterfactuals, with lexical cues often being misleading.\nItems We adapt stimuli from a psycholinguistic study with 96 controlled sentences (Ferguson, 2012) . We additionally create a larger-scale synthetic dataset with 12,960 sentences, using the same events as the generated dataset from Section 2. We modify the subject noun phrases such that there is no influence of existing world knowledge. For example, we modify the subject \"cat\" to \"pet\", so that there is no prior knowledge about the subject's preference for \"cabbages\" or \"fish\". As a result, existing world knowledge cannot inform the correct completion-instead, models need to infer based on the counterfactual language that the true state of the world is different from what the counterfactual states. Further, we control the lexical items used across different conditions to minimize effects of lexical cues on condition differences (see Table 5 ).\n\nCond Sentence\nCWC If the pets were vegetarians, people would love them.\nIn fact, people feed the pets with fish/cabbages.\nRWCA Because the pets are vegetarians, people love them.\nIn fact, people feed the pets with fish/cabbages.\nBBC In fact, people feed the pets with fish/cabbages. Fig. 3 shows the set-up of conditions. In the Counterfactual-World Context (CWC) condition, the scenario described in the first sentence is neutral with respect to real world knowledge-it is the use of the counterfactual (\"if...were\") that tips us off that this scenario is not true in reality. The correct completion, then, cannot be informed by world knowledge, and is also misaligned with the lexical trigger (e.g., \"vegetarians\"), so models must rely specifically on this implication from the counterfactual in order to perform well.\nIn the Real-World Context Alternative (RWCA) condition, the context uses the same lexical triggers (\"vegetarians\") as the CWC condition. However, because there is no counterfactual language, the logical completion is now the word associated with the lexical trigger (e.g., \"cabbages\", associated with \"vegetarians\").\nGiven that the logical completions in CWC and RWCA differ, we also compare against a Baseline Bias Context (BBC) condition, to establish default model preference for the target factual completion in the presence of the new subject noun phrase.\nExperiments We compare proportion of CWCcongruent completions across conditions. Good performance will assign high values in the CWC condition and low values in the RWCA condition. Table 6 : Percentage of preference for CWC-congruent completion (e.g., \"fish\") in Exp3. In the CWC condition, higher values reflect better predictions. In the CWCA condition, lower values reflect better predictions. The BBC condition establishes models' default preference for the CWC-congruent completion.\nResults Table 6 shows the results. In the smallscale dataset, most models show a similar preference in CWC and RWCA, suggesting again that their predictions are largely driven by lexical triggers. Only GPT-3 shows substantial difference between CWC and RWCA, indicating finer-grained sensitivity to counterfactual structures. This sensitivity is, however, less pronounced in the largescale dataset. Closer inspection suggests that GPT-3's specific success on the small-scale data may in fact be attributable to canceling out of lexical triggers: in the small-scale dataset, there are lexical triggers supporting both continuations (see A.1 for more illustration of the characteristics of the smallscale dataset), which may cause lexical cues to cancel out, enabling more influence from other linguistic cues. To take one example, the small-scale dataset contains the item \"If Helen had received her student loan, her bank balance would now be in credit. When she checked her bank balance she was worried/happy about her finance.\" In this item, among the lexical triggers (\"student loan\", \"in credit\", \"bank balance\") there are potential associations with both the CWC-congruent completion \"worried\" and the CWC-incongruent completion \"happy\". By contrast, in the large-scale dataset, the major lexical trigger (\"vegetarians\") always favors the CWC-incongruent continuation (\"cabbages\"), causing strong lexical bias against the CWC-congruent continuation (see Appendix A.4 for further analysis on the role of conflicting lexical triggers and other linguistic factors). This suggests that GPT-3 does show real sensitivity to linguistic indicators of counterfactuals, but the effect of superficial lexical cues remains strong.\n\nConclusion\nThe experiments above have shown that when presented with counterfactual situations, PLMs are able to prefer completions that conflict with world knowledge-and counterintuitively, this sensitivity appears better in cases where that world knowledge is stronger. Our results also indicate, however, that models are in large part relying on simple lexical cues to inform these preferences. The only model that shows more sophisticated sensitivity to finegrained linguistic cues separating counterfactuals from reality is GPT-3-which successfully distinguishes conditions based on counterfactual cues, but nonetheless still shows strong influences from lexical associative cues. Why might world knowledge aid counterfactual sensitivity? Does GPT-3 truly understand counterfactuals? One possibility worth considering is that explanations in both of these cases involve volume of exposure. First, models' stronger world knowledge for a given fact suggests that models have encountered that fact more often in training-and this may in turn translate to more exposure to that type of knowledge in counterfactual contexts, enabling more straightforward memorization-based performance. Similarly, while GPT-3 may robustly understand counterfactuals, the massive data exposure for that model may enable a simpler path to success: GPT-3 could simply have developed lower-level knowledge of how linguistic cues like \"If/had\" versus \"Because\" mediate levels of association between nearby lexical cues and later words. We leave investigation of these hypotheses for future work.\n", "hypothesis": " Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on real-world propositions.  We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models.  We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge-however, we also find that for most models this effect appears largely to be driven by simple lexical cues.  When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.", "answer": false}
{"title": "On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection", "content": "\nIntroduction\nDespite remarkable performance on various NLP tasks, pre-trained language models (PrLMs), like BERT (Devlin et al., 2018) , are highly vulnerable to adversarial samples (Zhang et al., 2020; Zeng et al., 2021) . Through intentionally designed perturbations, attackers can modify the model predictions to a specified output while maintaining syntactic and grammatical consistency (Jin et al., 2020; Li et al., 2020b) . Such sensitivity and vulnerability induce persistent concerns about the security of NLP systems (Zhang et al., 2021c) . Compared to deploying robust new models, it would be more applicable to production scenarios by distinguishing adversarial examples from normal inputs and discarding them before the inference phase (Shafahi et al., 2019) . Such detection-discard strategy helps to reduce the effectiveness of adversarial samples and can be combined with existing defence methods (Mozes et al., 2021) . However, existing adversarial detection methods depend heavily on the statistical characteristics of the training data manifolds, such as density estimation (Yoo et al., 2022) and local intrinsic dimensionality (Liu et al., 2022) . Some other researches focus on identifying high-frequency words in the training data and replacing or masking them in the prediction phase to observe the change in logits score (Mozes et al., 2021; Mosca et al., 2022) . We propose a summary of existing works in Table 1 . All these detection methods assume that training data is available, which suffers from the following two problems: (1) Some companies only provide model checkpoints without customer data due to privacy and security issues. (2) Some datasets can be large so it is not practical or convenient to save and process them on different platforms.\nIn this work, we propose UAPAD, a novel framework to detect adversarial samples without exposure to training data and maintain a time consumption consistent with normal inference. We visualize our detection framework in Figure 1 . Universal adversarial perturbations (UAPs) is an intriguing\n\nSummary\nRequire Clean Data Require Adv. Data Require Extra Model MLE (Lee et al., 2018) Gaussian discriminant analysis \u2714 \u2714 DISP (Zhou et al., 2019) Token-level detection model \u2714 \u2714 \u2714 FGWS (Mozes et al., 2021) Frequency-based word substitution \u2714 \u2714 ADFAR (Bao et al., 2021) Sentence-level detection model \u2714 \u2714 \u2714 RDE (Yoo et al., 2022) Feature-based density estimation \u2714 UAPAD (Ours)\nUniversal adversarial perturbation phenomenon on neural models, i.e. a single perturbation that is capable to fool a DNN for most natural samples (Zhang et al., 2021b) , and can be calculated without the original training data (Mopuri et al., 2018; Zhang et al., 2021a) . We explore the utilization of UAPs to detect adversarial attacks, where adversarial and clean samples exhibit differential resistance to pre-trained perturbations on a sensitive feature subspace.\nExperimental results demonstrate that our training-data-agnostic method achieves promising detection accuracy with BERT on multiple adversarial detection tasks without using training or adversarial data, consuming additional inference time, or conducting overly extensive searches for hyperparameters. Our main contributions are as follows:\n\u2022 We analyze and verify the association between adversarial samples and an intrinsic property of the model, namely UAPs, to provide a new perspective on the effects of adversarial samples on language models.\n\u2022 We propose a novel framework (UAPAD), which efficiently discriminates adversarial samples without access to training data, and maintains an equivalent time consumption to normal inference. Our codes 1 are publicly available.\n2 Related Work\n\nUniversal adversarial perturbation\nThe existence of UAPs has first been demonstrated by (Moosavi-Dezfooli et al., 2017) , that a single perturbation can fool deep models when added to most natural samples. Such phenomena have been extensively verified in image (Khrulkov and Oseledets, 2018 ), text (Song et al., 2021) , and audio models (Li et al., 2020a) . Some works attribute the existence of UAPs to a specific low-dimensional subspace, which is perpendicular to the decision boundary for most of the data. The attention on UAPs mainly focused on their construction, detection and defence (Zhang et al., 2021b) , and neglected to explore the relationship between adversarial samples and UAPs. Our experimental results in Figure 2 demonstrate the tight connection between these two phenomena.\n\nAdversarial detection in NLP\nAdversarial detection is an emerging area of research on language model security. A series of works analyze the frequency characteristics of word substitutions in pre-collected adversarial sentences and replace (Zhou et al., 2019; Mozes et al., 2021) or mask (Mosca et al., 2022) them to observe model reactions. These methods rely on empirically designed word-level perturbations, which limit their generalizability across different attacks. Ma et al. (2018) first proposed to train additional discriminative models to decide whether an input sentence has suffered from word-level adversarial substitution. This idea was generalized by Liu et al. (2022) and Yoo et al. (2022) , which determine the likelihood of a sentence has been perturbed. However, they still require the statistical characteristics of the training data. In this paper, we for the first time propose to construct data-agnostic models and achieve remarkable detection results.\n\nMethod\nThis section shows how to calculate the UAPs for a specific text model without obtaining training data. And subsequently, how to detect adversarial data by pre-trained UAPs.\nData-free UAPs We compute UAPs for a finetuned model by perturbing the substitute inputs, based on the fact that UAPs are generalized properties for a given model. We start with a parameterfrozen target network f and a random perturbation \u03b4. The optimal situation is we can obtain some data that are involved in the training procedure. However, there are situations that we cannot access to training samples or it is unclear whether the accessible data is within the training set. To demonstrate the effectiveness of UAPAD under the data-agnostic scenario. We initialize the input embedding by randomly selecting data from an unrelated substitute dataset (e.g., the MNLI dataset in our experiments). It is a reasonable assumption that a defender can access a moderate amount of substitute data. These embeddings are subsequently updated to ensure the model's confidence score is above the threshold on them. In our framework, we only retain samples with model confidence above 85% to calculate UAPs. We then optimize the perturbation \u03b4 by gradient-ascending the overall loss when added to all the inputs and project it to a normalized sphere of fixed radius to constrain its norm. We obtain a reasonable UAP when most predictions are induced to a fixed result under perturbation.\nAdversarial Detection with UAPs In Figure 2 , we illustrate the different resistance to UAPs between clean and adversarial samples. We utilize this property to conduct adversarial detection. Given an input x, we perform one inference on model f to obtain the normal output y = f (x) and perform another one when x is perturbed by a calculated UAP \u03b4, that is y \u2032 = f (x + w * \u03b4), where w is a hyperparameter controlling the perturbation's intensity. We detect the input as an adversarial sample when y \u0338 = y \u2032 . Noting that these two inferences can be computed in parallel, our approach does not introduce growth in inference time.\n\nExperimental Setup\nWe experimentally validate our method on three well-accepted benchmarks: SST-2 (Socher et al., 2013) , IMDB (Maas et al., 2011) , and AGNews (Zhang et al., 2015) . The statistics of involved benchmark datasets are summarised in Appendix A. We use the BERT-base (Devlin et al., 2018) as the target model and pre-generate adversarial samples for the detection task with three attack methods: TextFooler (Jin et al., 2020) , PWWS (Ren et al., 2019) , and BERTAttack (Li et al., 2020b) .\n\nDetecting Scenarios\nAdversarial detection task requires a dataset D, containing both clean samples D clean and adversarial samples D adv . In the previous works, there exist two different strategies to construct adversarial datasets. Scenario 1 (easy): The adversarial dataset consists of only successful attack samples. Scenario 2 (hard): The adversarial dataset contains both successful and unsuccessful attack samples. Scenario 2 presents more challenging requirements for detection methods and is closer to real-world settings. We conduct experiments in both scenarios to fully illustrate the performance of UAPAD.\n\nImplementation Details\nWe fine-tuned BERT using consistent settings with (Devlin et al., 2018) . For all three datasets, we took 1500 training samples and saved their attack results under different attack algorithms as adversarial samples. UAPAD has a single hyperparameter w (strength of universal perturbation), which we set to 0.5 for all our detection experiments. Although we believe that a better weight exists and can boost the detection performance, we refuse to extend hyper-parameter searching which is against our original purpose. More implementation details and hyperparameters can be found in Appendix B.\n\nEvaluation Metrics\nWe use two metrics to measure our experimental results. Detection accuracy (ACC) measures the accuracy of classification results on all samples, and F1-score (F1) measures the harmonic mean of precision and recall scores. Similar to DISP, our method provides a direct dis- criminant rather than a score and therefore does not apply to the AUC metric.\nBaselines We compare our proposed methods with four strong baselines. Details are summarized in Appendix C.\n\u2022 MLE (Lee et al., 2018) proposes to train detection models based on Mahalanobis distance.\n\u2022 DISP (Zhou et al., 2019) verifies the likelihood that a token has been perturbed.\n\u2022 FGWS (Mozes et al., 2021) substitutes lowfrequency words in the sentence to detect Word-level attacks.\n\u2022 RDE (Yoo et al., 2022) models the probability density of inputs and generates the likelihood of a sentence being perturbed.\n\nExperiment Results and Discussions\nIn this section, we show the experimental performance of our proposed method under the two scenarios in Section 4.1, and investigate different defence methods on the inference time consumption.\n\nMain Results\nTable 2 and 3 show the detect results on three datasets and three attacks. The highest means are marked in bold. Out of the 18 combinations of dataset-attack-scenario, UAPAD achieves the best performance on 15 of them on ACC and 12 of them on F1 metric, which demonstrates the competitiveness of our data-agnostic approach. UAPAD guarantees remarkable detection performance on the SST-2 and AGNews datasets and suffers from a small degradation on the IMDB dataset. We argue that the average length of sentences is greater on IMDB, resulting in stronger dissimilarity between the adversarial sample generation by attack algorithms and the original sentence. On the AGNews dataset, UAPAD provided a 3-11% increase in detection accuracy relative to the baseline approach.\nWe attribute this impressive improvement to more categories on this task, which improved the accuracy of estimation on the model's UAPs.\n\nTime Consumption\nTo further reveal the strength of UAPAD besides its detection performance, we compare its GPU training time consumption with other baseline methods.\nAs is demonstrated in \n\nConclusion\nIn this paper, we propose that adversarial samples and clean samples exhibit different resistance to UAPs, a model-related vector that can be calculated without accessing any training data. Based on this discovery, we propose UAPAD as an efficient and application-friendly algorithm to overcome the drawbacks of previous adversarial detection methods in terms of slow inference and the requirement of training samples. UAPAD acts by observing the feedback of inputs when perturbed by pre-computed UAPs. Our approach achieves impressive detection performance against different textual adversarial attacks in various NLP tasks. We call for further exploration of the connection between adversarial samples and UAPs.\nThis section discusses the potential limitations of our work. This paper's analysis of model effects mainly focuses on common benchmarks for adversarial detection, which may introduce confounding factors that affect the stability of our framework. Our model's performance on more tasks and more attack algorithms is worth further exploring. Our detection framework exploits the special properties exhibited by the adversarial sample under universal perturbation. We expect a more profound exploration of improving the connection between UAPs and adversarial samples. In Figure 2 , we note that a small number (about 3%) of clean and adversarial samples do not suffer from UAP interference. It is worth conducting an analysis of them to further explore the robustness properties of the language models. We leave these problems to further work. \n", "hypothesis": " Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications.  However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability.  In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs.  Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data.  Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs.  Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference..", "answer": true}
{"title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition", "content": "\nIntroduction\nSelf-supervised learning has improved input data representation without requiring extensive humanlabeled data (He et al., 2019; Zhang et al., 2022) . Based on this advancement, powerful pre-trained models providing high-performing representations for various data types (e.g., text, images, and audio) have been proposed. For instance, in speech, self-supervised pre-trained models such as Hu-BERT (Hsu et al., 2021) have advanced state-of-the- art performance of automatic speech recognition (ASR).\nHowever, one major challenge in using pretrained speech models for ASR is the representational bias towards prominent accents present in the dataset during pre-training. Consequently, there will be a disparity in ASR performance between native and non-native speakers. More specifically, pre-training using a large dataset such as the Lib-riSpeech (Panayotov et al., 2015) , which comprises a large proportion of utterances from native (L1) English speakers, leads to a less satisfactory recognition rate for non-native (L2) English accented speech. This phenomenon can curtail the effectiveness of current high-performing ASR systems for real-world applications.\nThere have been several ways to address this issue, including fine-tuning the model on diverse accents (Winata et al., 2019; Shibano et al., 2021) , having a separate model for each accent (Yang et al., 2018) or using regularization losses that guide the fine-tuning process to achieve robustness to accents (Chen et al., 2020; Gao et al., 2022) , all of which require updating the pre-trained model.\nWe propose a different solution for improving L2 speech recognition in transformer-based speech models that introduces a small number of learnable parameters into the input space while keeping the backbone weights of the model untouched. Our approach is guided by Information-Theoretic Adversarial Learning; thus, we refer to it as IN-Tapt (Information-Theoretic Adversarial Prompt Tuning). INTapt aims to introduce auxiliary embeddings (i.e., prompt) concatenated to the original input, which can re-modulate the attention and adapt the pre-trained weights so that the corresponding input looks like speech with an accent seen during pre-training (Figure 1 ). To achieve this, INTapt incorporates (1) adversarial training, which tries to minimize the mutual information between the accent feature of the original input and that obtained by concatenating the prompt embeddings in front of the initial input, and (2) CTC loss training to improve the ASR performance of the prompt-concatenated input. Essentially the prompt is trained such that the accent of the concatenation is pushed away from the input accent and the concatenation achieves native CTC loss performance. Unlike the previous use-case of prompts in NLP or Computer vision (CV), where a single prompt embedding is learned for each discrete task or input domain, the intensity of an accent is continuous. Thus, we propose an input-dependent prompt embedding by training a prompt generator that outputs an input-specific prompt. Through extensive experiments, we show that the proposed dual objectives of INTapt not only lead to better performance on L2 English accents but result in a higher similarity between the accent feature of the promptconcatenated input and that of L1 English accents. In the first step, we train an Accent Module (AM) capable of isolating the accent feature from a given audio feature a of an input speech x. In the second step, we train a Prompt Generator (PG), which outputs a prompt p for a given audio feature a, using two objectives: (1) Minimize the mutual information between the accent feature z \u2032 and z, where the former is obtained using the prompt-concatenated input (p; a) and the latter is obtained from the original audio feature a, (2) Minimize CTC loss to improve the ASR performance of the input (p; a).\n\nAccent Module (AM)\nSince our method requires direct access to the isolated accent feature of the corresponding audio feature input, we propose an Accent Module (AM) capable of extracting the accent feature z from the input a. The module consists of an accent feature extractor f \u03b8 1 which is trained with an accent classification head f \u03b8 2 to isolate the accent feature and an accent intensity regression head f \u03b8 3 to capture the intensity of the accent into the obtained feature.\n\nAccent Classification Head\nThe role of the accent classification head f \u03b8 2 is to isolate the accent feature of a given speech 1 . Given the hidden state representation h of an audio feature input a, the feature extractor outputs the accent feature (i.e., z = f \u03b8 1 (h)) and the accent classification head f \u03b8 2 tries to assign it to the correct accent label y.\n\nAccent Intensity Regression Head\nThe intensity of an accent could vary among different people even though there are in the same L2 group, and it could also vary between utterances from the same speaker. Thus, an accent intensity regression head is introduced to incorporate the accent intensity into the obtained accent feature z. Based on the assumption that the intensity of the accent affects ASR performance, making the accent intensity regression head predict the CTC loss 2 , obtained by inputting the corresponding speech into the backbone speech model, will allow the extracted accent feature z to capture the intensity of the accent.\nGiven a batch B, the training of the Accent Module with the two aforementioned heads could be summarized as:\nmin \u03b8 1 ,\u03b8 2 1 |B| i\u2208B \u2212 log p(y i |f \u03b8 2 (f \u03b8 1 (h i )))+ \u03bb min \u03b8 1 ,\u03b8 3 1 |B| i\u2208B [ f \u03b8 3 (f \u03b8 1 (h i )) \u2212 CTC(x i )] 2\n(1)\n\nPrompt Generator (PG)\nBuilding on the success of prompts in NLP (Liu et al., 2021; Li and Liang, 2021) and CV (Dosovitskiy et al.), we introduce a prompt tuning method to improve the ASR performance for L2 English speech by efficiently utilizing a pre-trained model that already shows good performance for L1 English speech. In contrast to traditional NLP or CV applications, where a single, discrete prompt embedding is learned for each specific task or input domain, the intensity of an accent is continuous. To address this, we propose an inputdependent prompt embedding by training prompt generator P G \u03b8 4 that generates an input-specific prompt guided by Information-Theoretic Adversarial Learning. More specifically, given a hidden state h = [h 1 , h 2 , ..., h L ] with length L we produce a prompt of length L \u2032 ,\nEQUATION\nMutual Information Minimization Mutual Information meausures the co-dependence between two random variables X and Y . Belghazi et al. (2018) recently proposed a gradient descent based method for estimating this property, allowing the use of neural networks for the estimation of mutual information between high dimensional random variables. The estimation is done using a neural network parameterized by \u03d5 as below:\nEQUATION\nwhere maximizing I \u03d5 (X, Y ) provides a tight lower bound of the original mutual information I(X, Y ).\nWe use this to adversarially train the prompt generator P G \u03b8 4 to minimize the mutual information between the accent feature of the original L2 speech input and the prompt-concatenated input.\nCTC Loss Minimization We train the prompt generator P G \u03b8 4 to minimize the CTC loss obtained for the prompt-concatenated input (p; a). The two minimization objectives wrt. the prompt generator, along with the maximization objective wrt. the Mutual Information Neural Estimator, are done jointly in the second training step (Equation 4). We show in Section 3.2 and 4 that the aforementioned objectives not only improve the ASR performance of L2 speech but also effectively make it resemble the accent feature of the L1 speech.\nEQUATION\n3 Experiments\n\nExperimental setting\nDataset We use the L2-ARCTIC (Zhao et al., 2018) , which is a speech corpus of non-native (L2) English speakers -Mandarin (ZH), Hindi (HI), Vietnamese (VI), Korean (KO), Spanish (ES), and Arabic (AR). Each L2 group contains two male and two female speakers, and all the speakers read the same 1132 texts. Models For the backbone pre-trained speech models we try two different settings, HuBERT Large and HuBERT XLarge (Hsu et al., 2021) . We consider three different training situations: 1) Finetune denotes a standard finetuning method where we update the pre-trained model weights to minimize the CTC loss, 2) Prompt ctc is the case of training the prompt generator without the minimization of mutual information, and 3) INTapt trains the prompt generator with our proposed objective in equation 4. We include the training details in Appendix A.\n\nResults\nTable 1 shows the Word Error Rate (WER) across different L2 groups on the ASR task. We find that the performance improvement of the prompt tuning approaches (Prompt ctc and INTapt) are more significant compared to standard finetuning despite updating small number of parameters (2-4%). IN-Tapt shows the lowest WER on all L2 groups, obtaining 12.34% for HuBERT Large and 11.00% for HuBERT XLarge on the aggregated all speakers, outperforming the finetuned by 1.62%p and 2.86%p, respectively 3 . This conforms to the previous findings (Lester et al., 2021 ) that larger model size can benefit more from prompt tuning methods.\nIn Table 2 , we report the WER on LibriSpeech (Panayotov et al., 2015) benefits of prompt tuning methods in that it only slightly degrades the performance of the backbone model on tasks it already excels at while improving performance on others. \n\nConclusion\nWe introduced Information Theoretic Adversarial Prompt Tuning (INTapt) for improving non-native ASR performance. To achieve this, INTapt remodulates the attention of the pre-trained speech models by concatenating input-dependent prompt embeddings to the original input, without updating the model weights. Throughout the experiment, we show that INTapt is capable of outperforming standard finetuning of the pre-trained model on L2 speech, without degradation on L1 speech, by allowing the L2 input to resemble a L1 accent.\n", "hypothesis": " Automatic Speech Recognition (ASR) systems have attained unprecedented performance with large speech models pre-trained based on self-supervised speech representation learning.  However, these pre-trained speech models suffer from representational bias as they tend to better represent those prominent accents (i.e., native (L1) English accent) in the pre-training speech corpus than less represented accents, resulting in a deteriorated performance for nonnative (L2) English accents. Although there have been some approaches to mitigate this issue, all of these methods require updating the pre-trained model weights.  In this paper, we propose Information Theoretic Adversarial Prompt Tuning (INTapt), which introduces prompts concatenated to the original input that can re-modulate the attention of the pre-trained model such that the corresponding input resembles a non-native (L2) English speech without updating the backbone weights.  INTapt is trained simultaneously in the following two manners:\n(1) adversarial training to reduce accent feature dependence between the original input and the prompt-concatenated input and (2) training to minimize CTC loss for improving ASR performance to a prompt-concatenated input.  Experimental results show that INTapt improves the performance of L2 English and increases feature similarity between L2 and L1 accents..", "answer": false}
{"title": "ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion", "content": "\nIntroduction\nKnowledge graphs (KG) organize knowledge about the world as a graph where entities (nodes) are connected by different relations (edges). The knowledge-graph completion (KGC) task aims at adding new information in the form of (entity, relation, entity) triples to the knowledge graph. The main objective is assigning to each triple a plausibility score, which defines how likely this triple belongs to the underlying knowledge base. These scores are usually predicted by the knowledge graph embedding (KGE) models. However, most KGC approaches do not make any binary decision and provide a ranking, not classification, which does not allow one to use them as-is to populate the KGs (Speranskaya et al., 2020) . To transform the scores into predictions (i.e., how probable is it that this triple should be included in the KG), decision thresholds need to be estimated. Then, all triples with a plausibility score above the threshold are classified as positive and included in the KG; the others are predicted to be negatives and not added to the KG. Since the initial KG includes only positive samples and thus cannot be used for threshold calibration, the calibration is usually performed on a manually annotated set of positive and negative tuples (decision set). However, manual annotation is costly and limited, and, as most knowledge bases include dozens (Ellis et al., 2018) , hundreds (Toutanova and Chen, 2015) or even thousands (Auer et al., 2007) of different relation types, obtaining a sufficient amount of labeled samples for each relation may be challenging. This raises a question:\nHow to efficiently solve the cold-start thresholds calibration problem with minimal human input?\nWe propose a new method for Active Threshold Calibration ACTC 1 , which estimates the relation thresholds by leveraging unlabeled data additionally to human-annotated data. In contrast to already existing methods (Safavi and Koutra, 2020; Speranskaya et al., 2020) that use only the annotated samples, ACTC labels additional samples automatically with a trained predictor (Logistic Regression or Gaussian Process model) estimated on the KGE model scores and available annotations. A graphical illustration of ACTC is provided in Figure 1 .\nOur main contributions are:\n\u2022 We are the first to study threshold tuning in a budget-constrained environment. This setting is more realistic and challenging in contrast to the previous works where large validation sets have been used for threshold estimation.\n\u2022 We propose actively selecting examples for manual annotation, which is also a novel approach for the KGC setting.\n\u2022 We leverage the unlabeled data to have more labels at a low cost without increasing the annotation budget, which is also a novel approach for the KGC setting.\nExperiments on several datasets and with different KGE models demonstrate the efficiency of ACTC for different amounts of available annotated samples, even for as little as one.\n\nRelated Work\nKnowledge graph embedding methods (Dettmers et al., 2017; Trouillon et al., 2016; Bordes et al., 2013; Nickel et al., 2011) have been originally evaluated on ranking metrics, not on the actual task of triple classification, which would be necessary for KGC. More recent works have acknowledged this problem by creating data sets for evaluating KGC (instead of ranking) and proposed simple algorithms for finding prediction thresholds from annotated triples (Speranskaya et al., 2020; Safavi and Koutra, 2020) . In our work, we study the setting where only a limited amount of such annotations can be provided, experiment with different selection strategies of samples for annotation, and analyze how to use them best. Ostapuk et al. (2019) have studied active learning for selecting triples for training a scoring model for KG triples, but their method cannot perform the crucial step of calibration. They consequently only evaluate on ranking metrics, not measuring actual link prediction quality. In contrast, our approach focuses on selecting much fewer samples for optimal calibration of a scoring model (using positive, negative, and unlabeled samples).\n\nACTC: Active Threshold Calibration\nACTC consists of three parts: selection of samples for manual annotation, automatic labeling of additional samples, and estimating the per-relation thresholds based on all available labels (manual and automatic ones).\nThe first step is selecting unlabeled samples for human annotation. In ACTC this can be done in two ways. One option is a random sampling from the set of all candidate tuples (ACTC rndm ; the pseudocodes can be found in Algorithm 1). However, not all annotations are equally helpful and informative for estimation. To select the representative and informative samples that the system can profit the most from, especially with a small annotation budget, we also introduce density-based selection ACTC dens inspired by the density-based selective sampling method in active learning (Agarwal et al., 2020; Zhu et al., 2008) (the pseudocode can be found in Algorithm 2 in Appendix A). The sample density is measured by summing the squared distances between this sample's score (predicted by the KGE model) and the scores of other samples in the unlabeled dataset. The samples with the highest density are selected for human annotation.\nIn a constrained-budget setting with a limited amount of manual annotations available, there are sometimes only a few samples annotated for some relations and not even one for others. To mitigate this negative effect and to obtain good thresholds even with limited manual supervision, ACTC labels more samples (in addition to the manual annotations) with a classifier trained on the manually annotated samples to predict the labels based on The final part of the algorithm is the estimation of the relation-specific thresholds. Each sample score from the decision set is tried out as a potential threshold; the relation-specific thresholds that maximize the local accuracy (calculated for this decision set) are selected.\n\nExperiments\nWe evaluate our method on two KGC benchmark datasets extracted from Wikidata and augmented with manually verified negative samples: CoDEx-s and CoDEx-m 2 (Safavi and Koutra, 2020) . Some details on their organization are provided in Appendix B. The KGE models are trained on the training sets 3 . The ACTC algorithm is applied on the validation sets: the gold validation labels are taken as an oracle (manual annotations; in an interactive setting they would be presented to human annotators on-the-fly); the remaining samples are used unlabeled. The test set is not exploited during ACTC training and serves solely for testing purposes. The dataset statistics are provided in Table 1 . We run our experiments with four KGE models: ComplEx (Trouillon et al., 2016) , ConvE (Dettmers et al., 2017) , TransE (Bordes et al., 2013) , RESCAL (Nickel et al., 2011) \n\nBaselines\nACTC is compared to three baselines. The first baseline LocalOpt (Acc) optimizes the per-relation thresholds towards the accuracy: for each relation, the threshold is selected from the embedding scores assigned to the samples with manual annotations that contain this relation, so that the local accuracy (i.e., accuracy, which is calculated only for these samples) is maximized (Safavi and Koutra, 2020) . We also modified this approach into LocalOpt (F1) by changing the maximization metric to the local F1 score. The third baseline is GlobalOpt, where the thresholds are selected by iterative search over a manually defined grid (Speranskaya et al., 2020) .\nThe best thresholds are selected based on the global F1 score calculated for the whole dataset 4 . In all baselines, the samples for manual annotation are selected randomly. \n\u00b13 \u00b12 \u00b12 \u00b12 \u00b13 \u00b13 \u00b11 \u00b11 \u00b11 \u00b11 \u00b12 \u00b12 \u00b13 \u00b13 \u00b12 \u00b12\nTable 2 : ACTC results in % averaged across different sizes of annotation budget reported with the standard error of the mean. The experiment with each annotation budget was repeated 100 times.\n\nResults\nWe ran the experiments for the following number of manually annotated samples: 1, 2, 5, 10, 20, 50, 100, 200, 500, and 1000. Experimental setup details are provided in Appendix E. Table 2 provides the result averaging all experiments (here and further, n = 500 for a fair comparison; see Section 5 for analyze of n value), and our method ACTC outperforms the baselines in every tried setting as well as on average. Figure 2a also demonstrates the improvement of ACT C rndm over the baselines for every tried amount of manually annotated samples on the example of CoDEx-s dataset; the exact numbers of experiments with different budgets are provided in Appendix F. The density-based selection, on the other hand, achieves considerably better results when only few manually annotated samples are available (see Figure 2b ). Indeed, choosing representative samples from the highly connected clusters can be especially useful in the case of lacking annotation. LR dense , which selects points from regions of high density, can be helpful for small annotation budgets since it selects samples that are similar to other samples. In contrast, when having \n\nAblation Study\nA more detailed ablation study of different ACTC settings is provided in Appendix D.\nGlobal Thresholds. All methods described above calibrate the per-relation thresholds. Another option is to define a uniform (uni) threshold, which works as a generic threshold for all tuples regardless the relations involved. We implemented it as ACT C \u2212 LR uni method, where the additional samples are automatically labeled and used to build a decision dataset together with the manually annotated ones -in the same way as done for the relation-specific version, but only once for the whole dataset (thus, significantly reducing the computational costs). We also applied the LocalOpt(Acc) and LocalOpt(F1) baselines in the uniform setting. Figure 3 demonstrates the results obtained with the Conve KGE model and random selection mechanism on the CodEX-s dataset.\nAlthough the universal versions generally perform worse than the relation-specific, ACT C uni still outperforms the universal baselines and even relationspecific ones for a small annotation budget.\nDifferent n values. An important parameter in ACLC is n, the minimal sufficient amount of (manually or automatically) labeled samples needed to calibrate the threshold. The ablation study of different n values is provided in Figure 4 on the example of ACT C \u2212LR dens setting, averaged across all annotation budgets. ACTC performs as a quite stable method towards the n values. Even a configuration with a minimum value of n = 5 outperforms baselines with a small annotation budget or even with quite large one (e.g. for RESCAL). \n\nConclusion\nIn this work, we explored for the first time the problem of cold-start calibration of scoring models for knowledge graph completion. Our new method for active threshold calibration ACTC provides different strategies of selecting the samples for manual annotation and automatically labels additional tuples with Logistic Regression and Gaussian Processes classifiers trained on the manually annotated data. Experiments on datasets with oracle positive and negative triple annotations, and several KGE models, demonstrate the efficiency of our method and the considerable increase in the classification performance even for tiny annotation budgets.\n", "hypothesis": " Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph.  Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples.  In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation.\nOur new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples.  Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers.  We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection.  Experiments with five scoring models and an oracle annotator show an improvement of 7% points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4% points over different budgets..", "answer": true}
{"title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering", "content": "\nIntroduction\nEmpowered by learnable neural representations built upon pretrained language models, the dense retrieval framework has become increasingly popular for fetching external knowledge in various natural language processing tasks (Lee et al., 2019; Guu et al., 2020; Lewis et al., 2020) . For opendomain question answering (ODQA), the de-facto dense retriever is the bi-encoder architecture (Lee et al., 2019; Karpukhin et al., 2020) , consisting of a question encoder and a passage encoder. Typically, the two encoders are isomorphic but separately parameterized, as they are initialized from the same pretrained model and then fine-tuned on the task.\nDespite of its popularity, this bi-encoder architecture with fully decoupled parameterization has some open issues. First, from the efficiency perspective, the bi-encoder parameterization apparently results in scaling bottleneck for both training and inference. Second, empirical results from recent studies show that such bi-encoder dense retrievers underperform its sparse counterpart BM25 (Robertson and Walker, 1994) in various settings. For example, both Lee et al. (2019) and Karpukhin et al. (2020) suggest the inferior performance on SQuAD (Rajpurkar et al., 2016) is partially due to the high lexical overlap between questions and passages, which gives BM25 a clear advantage. Sciavolino et al. (2021) also find that bi-encoder dense retrievers are more sensitive to distribution shift than BM25, resulting in poor generalization on questions with rare entities.\nIn this paper, we develop Task-Aware Specialization for dEnse Retrieval, TASER, as a more parameter-efficient and robust architecture. Instead of using two isomorphic and fully decoupled Transformer (Vaswani et al., 2017) encoders, TASER interleaves shared encoder blocks with specialized ones in a single encoder, motivated by recent success in using Mixture-of-Experts (MoE) to scale up Transformer (Fedus et al., 2021) . For the shared encoder block, the entire network is used to encode both questions and passages. For the specialized encoder block, some sub-networks are task-specific and activated only for certain encoding tasks. To choose among task-specific sub-networks, TASER uses an input-dependent routing mechanism, i.e., questions and passages are passed through separate dedicated sub-networks.\nWe carry out both in-domain and out-of-domain evaluation for TASER. For the in-domain evaluation, we use five popular ODQA datasets. Our best model outperforms BM25 and existing biencoder dense retrievers, while using much less parameters. It is worth noting that TASER can effectively close the performance gap on SQuAD between dense retrievers and BM25. One interest-ing finding from our experiments is that excluding SQuAD from the multi-set training is unnecessary, which was a suggestion made by Karpukhin et al. (2020) and adopted by most follow-up work. Our out-of-domain evaluation experiments use En-tityQuestions (Sciavolino et al., 2021) and BEIR (Thakur et al., 2021) . Consistent improvements over the doubly parameterized bi-encoder dense retriever are observed in these zero-shot evaluations as well. Our code is available at https: //github.com/microsoft/taser.\n\nBackground\nIn this section, we provide necessary background about the bi-encoder architecture for dense passage retrieval which is widely used in ODQA (Lee et al., 2019; Karpukhin et al., 2020) and is the primary baseline model in our experiments.\nThe bi-encoder architecture consists of a question encoder and a passage encoder, both of which are usually Transformer encoders (Vaswani et al., 2017) . A Transformer encoder is built up with a stack of Transformer blocks. Each block consists of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2016) and layernormalization (Ba et al., 2016) applied to both sublayers. Given an input vector h \u2208 R d , the FFN sub-layer produces an output vector as following\nFFN(h) = W 2 max{0, W 1 h + b 1 } + b 2 , (1)\nwhere\nW 1 \u2208 R m\u00d7d , W 2 \u2208 R d\u00d7m , b 1 \u2208 R m ,\nand b 2 \u2208 R d are learnable parameters. For a sequence of N tokens, each Transformer block produces N corresponding vectors, together with a vector for the special prefix token [CLS] which can be used as the representation of the sequence. We refer readers to (Vaswani et al., 2017) for other details about Transformer. Typically the question encoder and passage encoder are initialized from a pretrained language model such as BERT (Devlin et al., 2019) , but they are parameterized separately, i.e., their parameters would differ after training.\nThe bi-encoder model independently encodes questions and passages into d-dimension vectors, using the final output vectors for [CLS] from the corresponding encoders, denoted as q \u2208 R d and p \u2208 R d , respectively. The relevance between a question and a passage can then be measured in the vector space using dot product, i.e., sim(q, p) = q T p. During training, the model is optimized based on a contrastive learning objective,\nEQUATION\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question. Although the bi-encoder dense retrieval architecture has achieved impressive results in ODQA, few work has attempted to improve its parameter efficiency. Further, compared to the spare vector space model BM25 (Robertson and Walker, 1994) , such bi-encoder dense retrievers sometimes suffer from inferior generalization performance, e.g., when the training data is extremely biased (Lebret et al., 2016; Karpukhin et al., 2020) or when there is a distribution shift (Sciavolino et al., 2021) . In this paper, we conjecture that the unstable generalization performance is partially related to the unnecessary number of learnable parameters in the model. Therefore, we develop a task-aware specialization architecture for dense retrieval with parameter sharing between the question and passage encoders, which turns out to improve both parameter efficiency and generalization performance.\n\nProposed Model: TASER\nAs shown in Fig. 1 , TASER interleaves shared Transformer blocks with specialized ones. The shared Transformer block is identical to the Transformer block used in the bi-encoder architecture, but the entire block is shared for both questions and passages. In the specialized block, we apply MoE-style task-aware specialization to the FFN sub-layer, following (Fedus et al., 2021) , where the router always routes the input to a single expert FFN sub-layer. In our experiments, we use a simple yet effective routing mechanism, which uses an expert sub-layer (Q-FFN) for questions and another (P-FFN) for passages. The router determines the expert FFN sub-layer based on whether the input is a question or a passage. Other routing mechanisms are discussed in Appendix A.\nTASER uses one specialized Transformer block after every T shared Transformer blocks in the stack, starting with a shared one at the bottom. Our preliminary study indicates that the model performance is not sensitive to the choice of T , so we use T = 2 for experiments in this paper.\nSimilar to the bi-encoder architecture, TASER is trained using the contrastive learning objective L sim defined in Equation 2. Specifically, the objective needs to use a set of negative passages P for each question. Following Xiong et al. (2020) and Qu et al. ( 2021), we construct P via hard negatives mining (Appendix B). Our experiments use the multi-set training paradigm, i.e., the model is trained by combining data from multiple datasets to obtain a model that works well across the board.\n\nIn-Domain Evaluation\nWe carry out in-domain evaluations on five ODQA datasets: NaturalQuestions (NQ; Kwiatkowski et al., 2019a), TriviaQA (TQ; Joshi et al., 2017) , WebQuestions (WQ; Berant et al., 2013) , Cu-ratedTrec (CT; Baudi\u0161 and \u0160ediv\u00fd, 2015) , and SQuAD (Rajpurkar et al., 2016) . All data splits and the Wikipedia collection for retrieval used in our experiments are the same as Karpukhin et al. (2020) . The top-K retrieval accuracy (R@K) is used for evaluation, which evaluates whether any gold answer string is contained in the top K retrieved passages.\nBesides BERT-base, coCondenser-Wiki (Gao and Callan, 2022) is also used to initialize TASER models. We further present results of hybrid models that linearly combine the dense retrieval scores with the BM25 scores. See Appendix D for details. Evaluation results are summarized in Ta- (2020) . We instead train models using all five datasets. Specifically, we observe that this would not hurt the overall performance, and it actually significantly improves the performance on SQuAD, comparing DPR (1) with DPR \u2020 .\nComparing models initialized from BERT-base, TASER \u22c4 significantly outperforms xMoCo (Yang et al., 2018) and is slightly better than DPR \u22c4 , using around 60% parameters. SPAR (Chen et al., 2022) is also initialized from BERT-base, but it augments DPR with another dense lexical model trained on either Wikipedia or PAQ (Lewis et al., 2021) , which doubles the model sizes (Table A3 ). TASER \u22c4 is mostly on par with SPAR-Wiki and SPAR-PAQ, except on SQuAD, but its model size is about a quarter of SPAR. Gao and Callan (2022) has shown the coCodenser model outperforms DPR models initialized from BERT-base in the single-set training setting. Here, we show that using coCondenser-Wiki for initialization is also beneficial for TASER under the multi-set setting, especially for SQuAD where Table 2 : Out-of-domain evaluation results on EntityQuestions (R@20) and four BEIR datasets (nDCG@10). BM25 and DPR Multi results are from (Sciavolino et al., 2021) and (Thakur et al., 2021) . R@20 is improved by 3.2 points. Notably, SQuAD is the only dataset among the five where DPR underperforms BM25, due to its higher lexical overlap between questions and passages. Nevertheless, TASER \u22c6 surpasses BM25 on all five datasets, and they are either on-par or better than state-of-theart dense-only retriever models, demonstrating its superior parameter efficiency.\nConsistent with previous work, combining BM25 with dense models can further boost the performance, particularly on SQuAD. However, the improvement is more pronounced on DPR as compared to TASER \u22c6 , indicating that TASER \u22c6 is able to capture more lexical overlap features. Finally, TASER \u22c6 BM25 sets new state-of-the-art performance on all five ODQA datasets.\nWe also compare the computation time needed for one epoch of training and validation. The baseline DRP model takes approximately 15 minutes, whereas TASER takes 11 minutes (26% improvement), both measured using 16 V100-32G GPUs.\n\nOut-of-Domain Evaluation\nWe use two benchmarks to evaluate the out-ofdomain generalization ability of TASER \u22c4 and TASER \u22c6 from Table 1 . EntityQuestions (EQ; Sciavolino et al., 2021) is used to measure the model sensitivity to entity distributions, as DPR is found to perform poorly on entity-centric questions containing rare entities. BEIR (Thakur et al., 2021) is used to study the model generalization ability in other genres of information retrieval tasks. Specifically, we focus on four datasets from BEIR where DPR underperforms BM25, i.e., ArguAna (AA; Wachsmuth et al., 2018), DBPedia (DBP; Hasibi et al., 2017) , FEVER (FEV; Thorne et al., 2018) , and HotpotQA (HQA; Yang et al., 2018) . Results are summarized in Table 2 . For EntityQuestions, we report R@20 scores following Sciavolino et al. (2021) . 2 For BEIR datasets, nDCG@10 scores are used following Thakur et al. (2021) .\nOn EntityQuestions, both TASER \u22c4 and TASER \u22c6 outperform the doubly parameterized DPR Multi (Karpukhin et al., 2020) , with TASER \u22c6 being slightly better. Similar to the in-domain evaluation results, TASER can effectively reduce the performance gap between the dense retrievers and BM25. These results further support our hypothesis that more parameter sharing can improve the model robustness for dense retrievers.\nOn BEIR datasets, we also observe that TASER models consistently improve over DPR Multi across the board. Notably, TASER \u22c4 and TASER \u22c6 can actually match the performance of BM25 on Ar-guAna and DBpedia. Interestingly, coCondenser pre-training has mixed results here, i.e., TASER \u22c6 is only better than TASER \u22c4 on HotpotQA and on par or worse on other datasets.\n\nRelated Work\nRecent seminal work on dense retrieval demonstrates its effectiveness using Transformer-based bi-encoder models by either continual pre-training with an inverse cloze task (Lee et al., 2019) or careful fine-tuning (Karpukhin et al., 2020) . One line of follow-up work improves dense retrieval models via various continual pre-training approaches (Guu et al., 2020; Chang et al., 2020; Izacard et al., 2021; Gao and Callan, 2022; Oguz et al., 2021) . Better contrastive learning objectives are also introduced (Xiong et al., 2020; Qu et al., 2021; Yang et al., 2021) . Motivated by the success of augmenting dense models with sparse models, Chen et al. (2022) combine the dense retriever with a dense lexical model that mimics sparse retrievers. All above work focus on improving the accuracy of biencoder dense retrievers, whereas our work tackles the parameter efficiency issue.\nUnlike most bi-encoder dense retrievers which measure the similarity between a question and a passage using their corresponding [CLS]vectors, ColBERT (Khattab and Zaharia, 2020) develops a late-interaction paradigm and measures the similarity via a MaxSim operator that computes the maximum similarity between a token in a sequence and all tokens in the other sequence. Such architecture has shown promising results in ODQA (Khattab et al., 2021) and the BEIR benchmark (Santhanam et al., 2022) . Our work instead focus on the improvement on the underlying text encoders, and the MaxSim operator introduced by ColBERT can be applied on top of TASER.\nXiong et al. ( 2021) use the BERT-Siamese architecture for dense retrieval, where all Transformer blocks are shared. Compared with this architecture, TASER is a more effective and general way to increase the parameter efficiency, by interleaving specialized Transformer blocks with shared ones.\n\nConclusion\nWe propose a new parameterization framework, TASER, for improving the efficiency and robustness of dense retrieval for ODQA. It interleaves shared encoder blocks with specialized ones in a single encoder where some sub-networks are task-specific. As the specialized sub-networks are sparsely activated, TASER can provide better parameter efficiency with almost no additional computation cost. Experiments show that TASER substantially outperforms existing fully supervised biencoder dense retrievers on both in-domain and out-of-domain generalization.\n", "hypothesis": " Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular.  Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-efficient because it allows for parameter sharing between encoders. Recent studies have shown that dense retrievers outperform BM25 in various settings. We propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which combines shared and specialized blocks in a single encoder to improve performance and efficiency.  Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers.  In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers.  Our code is available at https: //github.com/microsoft/taser..", "answer": false}
{"title": "Controllable Mixed-Initiative Dialogue Generation through Prompting", "content": "\nIntroduction\nMixed initiative dialogue systems allow all interacting agents to initiate actions to control the interaction. These systems dynamically adapt interaction styles to regain control and progress towards specific goals (Allen et al., 1999; Chu-Carroll, 2000) , unlike others which passively respond to users' input (e.g. some assistants like ChatGPT), Mixed initiative dialogue systems thus often involve complex policy planning sub-tasks to determine optimal turn-level system dialogue intents (Peng et al., 2018; Hiraoka et al., 2013; Muise et al., 2019; Liu et al., 2020) . These policies define when it is optimal for a system to regain initiative (e.g., when a moderator should interject in a conversation, or when a companion should ask questions or change a conversation topic).\nHowever, \"optimal\" planned dialogue intents still need to be executed through \"optimal\" response models. The standard practice in recent dialogue research has been to fine-tune a pretrained language model for conditional generation My girlfriend dumped me for someone else! I've been really shook up.\nI'm so sorry to hear that :( Do you have any idea why? I think she just lost interest in me? It happened a couple months ago. I wish I could get over it. I can't get her off my mind. She's dating this total loser.\n\nRestatement or Paraphrasing\nFine-Tuning: I'm so sorry to hear that Ground Truth: That's tough... so you're saying you grew apart a little? Or do you think it was more one-sided?\nPrompting: It sounds like you're feeling really hurt and frustrated by the situation with your ex-girlfriend. It can be really difficult to see her move on with someone else. to achieve semantic control through some combination of innovations in model architectures or learning processes (Liu et al., 2021; Chen et al., 2019) . Such generation approaches still leave room for error. Assuming that there exists a truly optimal dialogue policy planner, a response model may still generate according to the wrong intent (partially due to the fact that dialogue datasets often have annotation errors (Qian et al., 2021; Zang et al., 2020) ). Or, a model may learn to generate correct intents but fail to create a response consistent with conversational context (Chen et al., 2022b) . Additionally, training corpora often differ in demographic and distribution compared to production environments, which can lead to deteriorating response quality (Koh et al., 2021) .\nWe propose using vanilla large pre-trained language models (LLMs) such as GPT-3 (Brown et al., 2020) as drop-in replacements to traditional finetuned conditional generation models for mixedinitiative dialogue systems. LLMs typically have been trained on massive corpora with large amounts of linguistic variety, making them more robust to overfitting specific tasks. Recent work demonstrates that LLMs have reasonable semantic control through few-shot prompting (Brown et al., 2020; Chen et al., 2023; Meng et al., 2022) . Here, we demonstrate how 1 to systematically prompt LLMs for mixed-initiative dialogue generation. Evaluations yielded strong performance on two popular English mixed-initiative tasks: Emotional Support Conversations (ESC; Liu et al. (2021) ) and Persua-sionForGood (P4G; Wang et al. (2019b) ).\n\nRelated Work\nControllable Generation approaches often involve fine-tuning a model conditioned on control codes (Keskar et al., 2019; Ficler and Goldberg, 2017) , additional attribute representations in hidden states (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019a) . Other work has attempted to mitigate the computational cost of fine-tuning, e.g. by training an auxiliary networks to guide the original LM (Dathathri et al., 2020; Yu et al., 2021; Pascual et al., 2021) . Here, we attempt controllable generation that replaces fine-tuning by prompting LLMs.\nPrompting in Dialogue Research typically has focused on understanding tasks such as dialogue planning (Kuo and Chen, 2022) or state tracking (Lee et al., 2021; Mi et al., 2022) . More recent dialogue research has examined using prompting for generating conversational data with varying levels of control (Kim et al., 2022; Chen et al., 2022a; Mehri et al., 2022; Chen et al., 2023) , citing the difficulty of using vanilla language models in production. Studies focusing on response generation looked at prompting LLMs specifically for knowledge-grounded dialogue generation (Liu et al., 2022; Madotto et al., 2021; Shuster et al., 2022) . Our work is the first to construct an interactive prompt-based mixed initiative dialogue system and evaluate the semantic control of prompting.\n1 Code to reconstruct all prompts available at https://github.com/maxlchen/Controllable-Mixed-Initiative-Dialogue-Generation\n\nDatasets\nWe examined ESC (Liu et al., 2021) ) and P4G (Wang et al., 2019b) . ESC consists of 1053 conversations between emotional help-seekers and supporters. Each conversation is annotated with the help-seeker's description of their problem, and the type of issues they are facing. Each turn by the supporters is annotated with one of eight emotional support strategies (Table A1 ). P4G contains 300 annotated conversations between persuaders who attempt to persuade persuadees to donate to a charity called Save the Children. Persuader turns are annotated with one of 10 strategies (Table A2 ).\n\nBaselines\nIn mixed-initiative dialogue, interacting parties continuously exchange control throughout the conversation. However, in order for agents to regain control, they must be able to properly execute items from their conversational agenda, e.g. generating a response that matches a desired strategy/intent. Liu et al. (2021) fine-tuned BlenderBot (Roller et al., 2021) on ESC using input representations consisting of flattened dialogue history and the predicted emotional support strategy for a specific turn. The best-performing model in their experimental setting is \"Oracle-BlenderBot\" which conditions on the ground truth strategy for a given turn.\nChen et al. (2022b) proposed a persuasive dialogue system called RAP, which combined targeted user response with conditional generation. The conditional generation component of RAP involves fine-tuning BART (Lewis et al., 2020) using a penalized loss to force the model to artificially create semantic control through dialogue intents.\n\nMixed-Initative Dialogue Prompting\nRAP required introducing a dialogue intent classifier to weakly supervise the training process, as there is not an oracle for whether the dialogue intent of a candidate response is correct. But, this confounds errors, as classifiers are imperfect. Moreover, fine-tuning approaches like both RAP and Oracle-BlenderBot involve balancing a tradeoff between response quality and semantic control accuracy. Prompting LLMs avoids both issues as it does not involve adjusting model weights to learn representations of control codes for individual tasks.\nIn this paper, we systematically prompt Instruct-GPT \"text-davinci-003.\" Rather than requiring expert-level prompt engineering, we create general prompt templates which directly fill slots using roles and annotations from both ESC and P4G. Specifically, we split up prompt construction into Task Background and Conversation History.\nFigure 2 breaks down an example of a prompt for ESC. The Task Background is a paragraph formed from the \"emotion type,\" \"problem type,\" and \"situation\" annotations provided by the corpus. The Conversation History consists of each prior utterance, prepended by labels for each speaker. The system-side turns are also prefixed by a natural language form of the annotated emotional support strategy, derived from the annotation scheme in Liu et al. ( 2021) (e.g. \"The Therapist acknowledges the Patient's feelings by paraphrasing their situation.\"). Figure 2 contains the contextual dialogue turns in order, along with the three support strategies used.\nThe P4G prompting style is similar. Unlike personalized emotional support conversations, the task does not change, so the Task Background is fixed with relevant factual background information. The Conversation History still interweaves narrative directions for each persuasive strategy (e.g. \"The Persuader uses a logical appeal.\"). Example provided in Figure A1 . The natural language intent mappings for both tasks are provided in Tables A1,A2 .\n\nExperiments\nWe evaluated prompting statically and interactively.\n\nStatic Evaluation\nWe quantified how much semantic and pragmatic control vanilla LLMs can provide in conversation. We randomly sampled 100 responses from ESC (supporters) and P4G (persuaders). Each response's conversational history and strategy annotation was used to generate responses via prompting and fine-tuned models. We used Oracle-BlenderBot for ESC and RAP's conditional generation module for P4G.\nWe asked crowdworkers on Amazon Mechanical Turk 2 to evaluate candidate responses' accuracy with respect to its prescribed dialogue intents, coherence, consistency, and engagingness. We paired the dialogue responses from each source (fine-tuning, prompting, or ground truth) with the corresponding responses from each of the other 2 Details for all human evaluation tasks in Appendix A. sources, allowing us to compute preference winrates between each pair. Each job presented only one pair of responses, in a random order. Additionally, we examined automatic metrics through Distinct-N (N \u2208 {3, 4}), as well QuantiDCE (Ye et al., 2021) , a BERT-based automatic dialogue coherence metric for open-domain conversation.\nTable 1 shows that prompt-generated responses are more highly rated in terms of quality compared to responses generated from competitive fine-tuned dialogue models as well as ground truth responses, in terms of all human evaluation metrics. This is also the case for Distinct-N in both tasks, and Quan-tiDCE in P4G. Oracle-BlenderBot slightly outperforms the prompt-generated responses in terms of QuantiDCE for ESC, but this difference is not statistically significant. Table 1 also shows that the prompt-generated responses are consistently preferable to the responses generated from fine-tuned dialogue models as well as the ground truth.\nFinally, we also see that prompting appears to provide the best semantic control over generated responses. Prompt-generated responses had the highest probability of matching the desired dialogue intent, even surpassing that of the ground truth utterances in both corpora. This further demonstrates the difficulty of performing annotation for supervised training -the conversational strategies are subjective, and even the ground truth responses may have annotation errors. The prompt-generated responses are generally of higher quality than both fine-tuned models, which may be a result of the aforementioned difficulty of balancing control accuracy with response quality during generation.\n\nInteractive Evaluation\nWe evaluated prompting as a generation module for mixed-initiative systems. This requires holding fixed other components, including policy planning. RAP is a recently proposed framework for P4G using an \"optimal\" persuasive strategy ordering. But, it built rapport with users by hierarchically integrating social chit-chat and knowledge retrieval with semantically-controlled generation (details in Chen et al. ( 2022b)). We built a system which replaces RAP's fine-tuned BART module with a module that systematically prompts InstructGPT. As with the original implementation of RAP, our prompting module conditions on the knowledge retrieved for factual question answering 3 .\nWe asked crowdworkers to evaluate our system according to the criteria in Table 2 . The system using prompting for generation was consistently rated more favorably than RAP, including in terms of convincingness, persuasiveness, and being a strong reason for donation. We discuss conversation examples in Appendix C. We see that our system was robust to a variety of input language patterns.\n\nDiscussion\nPrompting yields strong performance in mixedinitiative tasks in the low resource regime 4 . Promptgenerated responses are often preferable even compared to ground-truth responses in ESC and P4G. From 17 paired evaluations of ESC where crowdworkers rated ground truth utterances as not matching the ground truth intent annotation, the promptgenerated response was rated as correct 13 times. However, this is likely because many dialogue corpora are created or annotated by crowdworkers, so the data may vary in quality. While LLMs may generate \"better\" responses than crowdworkers, we do not expect them to be better than expert therapists.\nThe results do indicate that prompting may be appropriate for building systems for tasks with limited data. As made evident by our ratings, annotating dialogue intents is a difficult and subjective process prone to errors which can further propagate to fine-tuned task models. This could potentially be addressed by the high semantic control demonstrated through prompting, despite not requiring downstream fine-tuning label supervision.\nThis prompting approach could be applied to other mixed-initiative tasks, including chit-chat and task-oriented dialogue. For instance, many real-world systems such as customer service chatbots already have pre-defined policies for what systems are allowed to say, despite not necessarily having many labeled conversations. A system can be designed as long as there is a policy planner, which could simply be a hierarchical ruleset. While there is some human-effort involved in writing natural language forms of fixed dialogue intents, it is a much less costly process than annotating highquality dialogue data.\n\nConclusion\nWe find encouraging results for prompting on mixed-initiative dialogue tasks, indicating that generated responses are high quality and follow semantic controls. Strong low resource performance opens the possibility of future work building mixedinitiative systems around novel settings which would require subjective data annotation.\n", "hypothesis": " Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control.  Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner.  The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents.  However, these supervised generation models are limited by the cost and quality of data annotation.  We instead prompt large language models as a drop-in replacement to finetuning on conditional generation.  We formalize prompt construction for controllable mixedinitiative dialogue.  Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionFor-Good and Emotional Support Conversations..", "answer": true}
{"title": "Hybrid Hierarchical Retrieval for Open-Domain Question Answering", "content": "\nIntroduction\nOpen-domain question answering (ODQA) (Voorhees, 1999 ) aims to answer questions based on a large corpus without pre-specified context, and enjoys a broad scope of real-world applications such as chatbots, virtual assistants, search engines, etc. Recent ODQA systems often follow a two stage retrieve-then-read architecture (Zhu et al., 2021; Chen et al., 2017; Lee et al., 2019) . Given a question, a retriever module first selects a + The work was done during an internship at AWS AI Lab.\n\u2020 These authors contributed equally to this work. * https://github.com/ghuhan17/HybridHierarchicalRetrieval. candidate set of relevant contexts from a diversified large corpus such as Wikipedia; afterwards, a reader module consumes the retrieved evidence to predict an answer. Here, retrieval performance is crucial to the accuracy of the QA system as it determines whether the correct context to answer the question can be presented to the reader. While most work in information retrieval focus on document retrieval (Nguyen et al., 2016; Thakur et al., 2021) , existing work in ODQA often splits documents into short passages and directly retrieve passages for the reader (Karpukhin et al., 2020; Izacard and Grave, 2020) to accommodate reader models that handle shorter sequences most effectively. A drawback of such single-stage passage retrieval approaches is that they tend to be susceptible to distracting passages that contain seemingly relevant local context but not the correct answer, since they cannot incorporate information from other parts of the document (see Figure 1 ). Further, the large number of passage candidates also contributes negatively to system throughput. To mitigate these issues, Liu et al. ( 2021) recently proposed a two-stage hierarchical retrieval framework where the retriever first retrieves relevant documents, then discern relevant passages within re-Figure 2 : An overview of HHR. It consists of a document retrieval followed by a passage retrieval. Each stage uses one of three types of retrievers -sparse, dense, and combined, leading to a total of 9 configurations. trieved documents. This helps prune passages that look relevant but are from irrelevant documents to improve answer accuracy, meanwhile greatly reducing the candidate set for passage retrieval and improve the inference speed of ODQA systems.\nDespite its success, Liu et al.'s ( 2021) approach (dense hierarchical retrieval, DHR) relies on dense neural retrievers (Lee et al., 2019; Karpukhin et al., 2020) for both document retrieval and passage retrieval, which suffers from two key weaknesses. First, neural encoders used in retrieval are often limited in context length for effectiveness and efficiency, which is too short to encompass most documents. As a result, DHR needs to make use of the structure of Wikipedia documents, and represents the documents succinctly with title, abstract and table of contents, which is not always available for non-Wikipedia text. Second, dense retrievers have been shown to suffer from poor generalization on out-of-domain data (Thakur et al., 2021) , whereas sparse retrievers like BM25 (Robertson et al., 2009) excel with lexical matches (Sciavolino et al., 2021) .\nIn this work, we propose a hybrid hierarchical retrieval (HHR) framework to alleviate these issues. Specifically, we investigate the tradeoffs and complementary strengths of sparse retrievers and dense retrievers at both the document retrieval and passage retrieval stages for ODQA (see Figure 2 ). We find, among other things, that sparse retrievers can complement dense retrievers at both retrieval stages with a simple approach to aggregate results from both. Besides in-domain evaluation on the dataset that neural models are trained on, we also perform zero-shot evaluation on unseen datasets to compare the generalization of these retriever architectures. We find that sparse retrievers can help HHR generalize better to unseen data and potentially replace dense retrievers in document retrieval.\nIn addition, we also study the accuracy, storage cost, and latency tradeoff for these architectures under the HHR framework, and offer practical insights to real-world ODQA systems that often need to factor these into consideration.\nOur main contributions are as follows. First, we propose a hybrid hierarchical retrieval framework on ODQA, and extensively study tradeoffs and complementary strengths of sparse and dense retrievers in both document and passage retrieval. Second, we perform both in-domain and out-ofdomain evaluation to provide insight into the generalization performance of different model choices. Finally, we present the accuracy-storage-latency landscape for HHR architectures and offer practical insights to real-world applications.\n\nBackground & Related Work\nOpen-domain question answering (ODQA). ODQA is a task that takes a question, such as \"Who got the first Nobel prize in physics?\", and aims to find the answer from a large corpus. ODQA systems often rely on efficient and accurate retrievers to find relevant context to answer questions (Chen et al., 2017) , where retrieval performance is usually critical to QA accuracy (Karpukhin et al., 2020) .\nPassage retrieval. Since most reader models in ODQA systems struggle to effectively handle long contexts, ODQA retrieval is often performed at the passage level (usually around 100 words long). Earlier work (Chen et al., 2017; Yang et al., 2019) relied on bag-of-words-based sparse retrievers such as BM25 (Robertson et al., 2009) . More recent work showed that neural retrievers can generate effective dense representations for retrieval when trained on ODQA (Lee et al., 2019; Karpukhin et al., 2020; Liu et al., 2021) . Sciavolino et al. (2021) showed, however, that these dense retrievers tend to generalize worse to unseen entities during training since they lack the capacity for lexical matching, which is a strong suit for sparse retrievers and important for out-of-domain generalization.\nHierarchical retrieval. Passage retrievers are limited by the context available in each passage, and can retrieve spurious passages to hurt answer performance. A remedy is to incorporate documentlevel relevancy during passage retrieval. Qi et al. ( 2021) explored combining document and passage relevancy scores in a BM25 retriever for ODQA. Liu et al. (2021) applied this idea to dense retrievers with a hierarchical retrieval framework (DHR), where a document retriever first retrieves documents of high relevancy, followed by a passage retriever to rerank passages within those documents, and our work extends this approach.\n\nMethodology\nOur hybrid hierarchical retrieval (HHR) framework extends DHR, a hierarchical retriever built on dense retrievers that first retrieves top-k d documents and then top-k p passages from those documents. We follow DHR to build the dense retrievers in HHR, and expand both document and passage retrievers to work with sparse retrievers to address limitations of the DHR approach (Figure 2 ). Specifically, in DHR, to make documents amenable to neural encoders with limited context length, the authors proposed to leverage the document structure of Wikipedia articles to construct a document summary that contains the document abstract and table of contents. While effective, this also potentially limits the applicability of this approach to corpora where this information is not available. In contrast, a sparse retriever can easily handle documents of arbitrary lengths efficiently without the need of structure information. Besides, dense retrievers tend to generalize poorly to out of domain data. We extend each of the document retrieval and passage retrieval stages with the option to use sparse retrievers to help alleviate this issue, and to help us understand the tradeoff between the two.\nBesides switching between sparse and dense retrievers in HHR, we also introduce a simple heuristic to combine results from both retrievers at the same stage by simply interleaving their top-k/2 results for top-k retrieval to better understand the complementary strengths of sparse and dense retrievers. This yields a total of 9 possible configurations for HHR for our extensive studies. Finally, for both sparse and dense passage retrievers in HHR, we implement on-the-fly passage reranking for all passages in the top retrieved documents with pre-computed passage representations. This helps reduce the latency of the passage retrievers in our implementation and provide more realistic insights into the accuracy-storage-latency tradeoff of different HHR settings in real-world systems.\n\nExperimental Setup\nData Three commonly used ODQA datasets are considered: Natural Questions (NQ) (Kwiatkowski et al., 2019) , Web Questions (WebQ) (Berant et al., 2013) , and TriviaQA (Joshi et al., 2017) . Following Liu et al. (2021) , we use the Wikipedia dump from Dec. 20, 2018 as the source corpus. When splitting documents into passages, the text under same sections are split into non-overlapping passages with a maximum of 100 words.\nTraining and evaluation We followed the same configuration as DHR to train the document and passage encoders up to 40 epochs on 8 V100 Tensor Core GPUs. In order to measure the retriever framework in both in-domain and zero-shot settings, the dense encoders are trained only on NQ dataset and tested on all three datasets.\n\nMain Results\nWe present the in-domain and zero-shot evaluation results for all datasets in Table 1 . Following DHR, we retrieve 100, 500, 500 documents in the first stage for NQ, WebQ, and TriviaQA, respectively. We compare HHR against DHR and single stage sparse and dense passage retrievers. In addition, we also study the effect of retrieving varying numbers of documents in HHR (see Figure 3 ). We find that:\nFirst, dense retrievers are crucial for indomain performance, yet sparse retrievers bring gains with complementary strengths. We see that replacing either or both components in the DHR baseline (Dense+Dense) with sparse retriever leads to noticeable drops in recall@100 on NQ, with 1.7% for replacing the document retriever, 2.3% for the passage retriever, and 7.2% for both. However, adding sparse retriever to document, passage or both retrievers brings 2.0%, 0.9% and 2.4% gain.\nSecond, sparse retrievers significantly improve HHR's generalization on zero-shot datasets. We corroborate previous work's (Sciavolino et al., 2021) finding that dense retrievers struggle to generalize in zero-shot settings. Likewise, DHR underperforms the optimal setting by 3.79% on WebQ, and leads to the worst performance on TriviaQA per recall@100. Adding sparse retrievers to both stages, Combined+Combined brings an average of 4.69% recall@100 improvement on WebQ and TriviaQA.\nThird, sparse document retriever can completely replace dense document retriever. While this leads to performance drop in-domain, we see that dense passage retrievers can help make up for the performance gap, and that the gap diminishes The two latency results for the document retrievers correspond to the time taken to retrieve 100 and 500 documents in the first stage respectively. The latency numbers for the passage retrievers show the time taken to retrieve 100 passages using a pool of 100 and 500 documents from the first stage respectively.\nRetriever NQ WebQ TriviaQA R@20 R@100 EM R@20 R@100 EM R@20 R@\nas more documents are retrieved in the first stage (Figure 3 ). Further, in the zero-shot setting, replacing dense document retrievers with sparse ones can actually lead to gains (of 1.26% and 3.15% recall@100 on WebQ and TriviaQA, respectively). This helps HHR generalize better to out-of-domain data not only lexically, but also removes the requirement of document structure information needed by the dense document retriever.\n\nAccuracy-Storage-Latency Landscape\nWe report the trade off between retrieval accuracy, latency and storage cost for different HHR configurations based on inference on a single CPU core. Figure 4 , presents the accuracy-storage-latency plot of different retriever configurations for the NQ, WebQ and TriviaQA datasets. We find that in the in-domain case of NQ, all settings involving dense document retrievers are Pareto-efficient on accuracy and latency, while the passage retriever presents a tradeoff among accuracy, latency, and storage. In contrast, in zero-shot settings the sparse document retrievers can be Pareto-efficient when complemented with dense passage retrievers. Table 2 presents the storage cost and retrieval latency for the four essential components in HHR framework, namely, sparse and dense retrievers in document and passage retriever stages. Sparse retrievers are storage efficient compared to the dense retrievers at the same level as they use inverted index to represent the text whereas dense retrievers use the dense embeddings. We also observe that the PyLucene's sparse document retriever is slower than the FAISS dense document retriever whereas our implementation of the on-the-fly sparse passage retriever is faster than its dense counterpart. Dense passage retrieval takes the most storage cost to store the embedding dictionary among others.\n\nConclusion\nIn this paper, we study a Hybrid Hierarchical Retrieval (HHR) framework for ODQA that integrates sparse and dense retrievers through a simple aggregation strategy in document and passage retrievers. We demonstrate that sparse retrievers complement dense retrievers in-domain and greatly improve the poor generalization from dense encoders to zeroshot data. Our framework addresses the limitation of DHR by achieving better zero-shot performance without relying on document structure information.\nWe also study the accuracy-storage-latency land- scape. We believe these findings are critical to the real-world adoption of HHR to ODQA systems.\n", "hypothesis": " Retrieval accuracy is crucial to the performance of open-domain question answering (ODQA) systems.  Recent work has demonstrated that dense hierarchical retrieval (DHR), which retrieves document candidates first and then relevant passages from the refined document set, can significantly outperform the single stage dense passage retriever (DPR).  While effective, this approach requires document structure information to learn document representation and is hard to adopt to other domains without this information. We perform extensive experiments on ODQA benchmarks and observe that our framework not only brings in-domain gains, but also generalizes better to zero-shot TriviaQA and Web Questions datasets with an average of 4.69% improvement on recall@100 over DHR. We also offer practical insights to trade off between retrieval accuracy, latency, and storage cost, with a focus on minimizing retrieval accuracy to reduce storage cost.  In this paper, we propose Hybrid Hierarchical Retrieval (HHR) to address the existing limitations.", "answer": false}
{"title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models", "content": "\nIntroduction\nPre-trained Language Models (PLMs) have achieved state-of-the-art performance across various tasks in natural language processing (Devlin et al., 2019; Liu et al., 2019; Clark et al., 2020) . One of the crucial reasons for this success is pretraining on large-scale corpora, which is collected from unmoderated sources such as the internet. Prior studies (Caliskan et al., 2017; Zhao et al., 2018; May et al., 2019; Kurita et al., 2019; Gehman et al., 2020) have shown that PLMs capture a significant amount of social biases existing in the pretraining corpus. For instance, they showed that the PLMs learned that the word \"he\" is closer to the word \"engineer\" because of the frequent cooccurrence of this combination in the training corpora, which is known as social gender biases. Since PLMs are increasingly deployed in real-world sce-narios, there is a serious concern that they propagate discriminative prediction and unfairness.\nSeveral solutions for mitigating the social biases have been proposed, including: using banned word lists (Raffel et al., 2020) , building deliberated training datasets (Bender et al., 2021) , balancing the biased and unbiased terms in the training dataset (Dixon et al., 2018; Bordia and Bowman, 2019) , debiasing embedding spaces (Liang et al., 2020; Cheng et al., 2021) , and self-debiasing in text generation (Schick et al., 2021) . Although all these solutions have shown different levels of success, they tend to limit the PLMs' ability (Meade et al., 2022) . For example, the banned words solution prevent gaining knowledge of topics related to banned words. Also, some of them hurt the PLMs' performance on downstream tasks. Furthermore, dataset curation and pre-training are two resourceintensive tasks needed for most of the above solutions (Schick et al., 2021) .\nIn this study, we address the challenges mentioned above by proposing an effective approach named Gender-tuning for debiasing the PLMs through fine-tuning on downstream tasks' datasets. For this goal, Gender-tuning perturbs the training examples by first finding the gender-words in the training examples based on a given gender-word list. Then Gender-tuning replaces them with the new words to interrupt the association between the gender-words and other words in the training examples (Table 1 ). Finally, Gender-tuning classifies the examples with the replaced words according to the original training examples' ground-truth labels to compute a joint loss from perturbation and classification for training the Gender-tuning. The key advantage of our method is integrating the debiasing process into the fine-tuning that allows the debiasing and fine-tuning to perform simultaneously. Thus, Gender-tuning does not require separate pre-training or additional training data. Also, this integration makes Gender-tuning a plug-and- play debiasing tool for any PLMs that works with original fine-tuning.\nTo evaluate the effectiveness of our proposed method, we conducted comprehensive experiments following two state-of-the-art debiasing baselines: SENT_DEBIAS (Sent-D) (Liang et al., 2020) and FairFil (FairF) (Cheng et al., 2021) . The results show that Gender-tuning outperforms both baselines in terms of the average gender-bias scores in the BERT model while improving its performance on the downstream tasks. In addition, we reported the performance of Gender-tuning applied to the RoBERTa that shows considerable improvement. Finally, our ablation studies demonstrate that all components of Gender-tuning, including two training phases and joint loss, play an essential role in achieving success.\n\nMethodology\nWe propose a novel debiasing approach, named Gender-tuning (Figure 1 ), that performs the debiasing process and fine-tuning simultaneously on the downstream tasks' dataset. For this aim, Gender-tuning integrates two training objectives: 1) Masked Language Modeling (MLM) training objective for gender-word perturbation and 2) Finetuning for classification. In each training batch, Gender-tuning works as follows: Gender-tuning uses MLM to perturb training examples by masking the existing gender-word(s) 1 . The MLM training objective is to predict masked token(s) with a mean cross-entropy loss that we denote as perturbation-loss (L perturb ). The training examples with predicted tokens, called genderperturbed examples (Table 1 ), are fed into fine- tuning to be classified according to the original examples' ground-truth label (y). Then p \u03b8 (y \u2032 = y|x) is the fine-tuning classification function to predict the gender-perturbed example's label (y \u2032 ) based on the gender-perturbed example (x) to compute the fine-tuning loss (L f ine\u2212tuning ), where \u03b8 is the PLM's parameters for the fine-tuning. A weighted aggregation of the perturbation loss and fine-tuning loss, called joint-loss (L joint ), is used for training the Gender-tuning as follows:\nL joint = \u03b1 L perturb + (1 \u2212 \u03b1)L f ine\u2212tuning (1)\nwhere \u03b1 is a weighting factor that is employed to adjust the contribution of the two training losses in computing the joint-loss.\nThe Gender-tuning training objective is to minimize joint-loss to ensure that the label of the perturbed example is the same as the label of the original training example. In the following, we present how joint-loss impacts the training process of Gender-tuning in each training batch:\nSuppose the MLM predicts an incorrect token. For instance, the example: \"the film affirms the power of the [actress]\" changes to \"the film affirms the power of the [trauma]\". In this example, the predicted word [trauma] is a non-related genderword that raises perturbation-loss value (L perturb > 0). In this case, even if fine-tuning classifies the perturbed example correctly, joint-loss is still big enough to force Gender-tuning to continue training.\nAlso, suppose Gender-tuning creates social gender bias through gender perturbation. For instance, the example: \"angry black [actor]\" changes to \"angry black [woman]\" that \"woman\" and \"actor\" are not close semantically that raises perturbation-loss Table 2 : Comparing the debiasing performance of Gender-tuning and two state-of-the-art baselines. First six rows measure binary SEAT effect size (e-size; lower is better) for sentence-level tests from (Caliskan et al., 2017) . The seventh row presents the average absolute e-size. The eighth row shows the classification accuracy on downstream tasks. The Gender-tuning random masks the input example randomly (not only gender-words). Gender-tuning gains the lowest average bias in both models and all datasets.\nvalue (L perturb > 0). In this case, the output of the fine-tuning might be correct (L f ine\u2212tuning \u2248 0) due to the PLMs' learned biases (\"angry black woman\" is a known gender/race bias). However, due to the big value of perturbation-loss, the joinloss is big enough to override fine-tuning results and forces Gender-tuning to continue training.\nMoreover, we observed that sometimes example perturbation changes the concept/label of training examples. For instance, the input: \"[He] is an excellent [actor] (label: positive)\" changes to \"[She] is a wonderful [murderer] (label: positive)\", and fine-tuning classification output is correct (L f ine\u2212tuning \u2248 0). In this example, the predicted word [murderer] is conceptually far from gender-related words [actor] . So, perturbation loss becomes significant, which creates a big value for joint-loss to force Gender-tuning to continue training. Finally, we found examples that MLM replaces the gender-word with the [UNK] token. In these examples, the perturbation-loss is close to zero (L perturb \u2248 0) and the output of the finetuning classifier is incorrect (L f ine\u2212tuning > 0). In this case, the joint-loss is big enough to continue training and provide a new chance for MLM to predict a meaningful token instead of a [UNK]. More analysis of our perturbation strategy can be found in Section 4.1 and Table 3 .\n\nExperimental Setup\nTo evaluate our proposed method, we conduct experiments by following the evaluation process of the two state-of-the-art baselines (Sent-D and FairF) such as the bias evaluation metric (SEAT), applied PLMs, and downstream tasks' datasets. 2 We report the SEAT effect size (e-size), average absolute e-size, and classification accuracy on downstream tasks for three different setups: 1) Origin: fine-tuning the PLMs on the downstream task datasets using huggingface transformers code (Wolf et al., 2020) . 2) Gender-tuning random : instead of replacing the gender-words in an training example, Gender-tuning random replaces a certain percentage of an input tokens randomly (5% of each input sequence). 3) Gender-tuning: the proposed method. We used the same hyperparameter for all three setups for a fair comparison.\n\nResults and Discussion\nTable 2 illustrates SEAT absolute effect size (esize) (lower is better) on sentence templates of Terms/Names under different gender domains provided by (Caliskan et al., 2017) , average absolute e-size (lower is better), and classification accuracy on downstream tasks (higher is better) for three experiment setups (Section 3) and two state-of-the-art baselines. The results show that Gender-tuning outperforms the baselines regarding the average absolute effect size for both PLMs on all datasets. Also, in contrast with the baselines, Gender-tuning improves the accuracy of both PLMs on all downstream tasks. It shows that the proposed method preserves the useful semantic information of the training data after debiasing. The Gender-tuning random results show an inconsistent effect on the bias scores. Although Gendertuning random improves the PLMs' accuracy on the downstream tasks, it significantly magnifies the bias score in the BERT model on SST-2 and CoLA. Also, it slightly reduces the average bias score in the RoBERTa on all datasets and in BERT on the QNLI.\n\nPerturbation Analysis\nThe PLMs achieved state-of-the-art performance on the downstream tasks datasets by applying the MLM for the example perturbation in pre-training phase. Thus we hypothesize that the MLM can generate realistic gender-perturbed examples that can considerably modify the gender relation between the input tokens without affecting the label. However, there is a concern that the pre-trained MLM transfers the gender bias through the perturbation process.\nTo address this concern, we investigate the predicted tokens that the pre-trained MLM replaces with the gender-words. We randomly select 300 examples from training dataset including 150 examples with feminine words and 150 examples with masculine words. Based on these 300 examples, we observe five types of perturbation as shown through some examples in Table 3: \u2022 Neutral; replace the gender-words with neutral word such as people, they, their, and etc.\n\u2022 Convert-gender; replace the gender-words with opposite gender. the word \"he\" change to \"she\".\n\u2022 Same-gender; replace the gender-words with the same gender. change the word \"man\" to \"boy\".\n\u2022 Deleting; replace the gender-words with unknown token ([UNK]).\nIn 300 examples, it only happens when there are several masked tokens.\n\u2022 Identical; replace the gender-word with itself. It mostly happens when there is only one gender-word.\nIn our investigation with 300 examples, we had 46% Neutral, 29% Identical, 17% Convert-gender, 7% Same-gender, and 1% Deleting perturbation.\nAs illustrated in Table 3 , Gender-tuning does not make a meaningful change in identical and samegender perturbation. These examples likely conform to the gender biases in the MLM. Suppose identical, or same-gender perturbation gets the correct output from the perturbation process (L perturb. \u2248 0). In this case, the only way to learn the biases in the MLM is to get the correct output from finetuning step and joint-loss close to zero. This issue stops the MLM and fine-tuning model from further update. However, joint-loss plays an essential role in alleviating learning gender bias from identical and same-gender perturbations.\nTo clarify the role of joint-loss in overcoming above problem, we investigated fine-tuning output on identical and same-gender perturbations. We observed that fine-tuning gets the incorrect output from 60% of the identical and 75% of the samegender perturbation. Thus these examples return to training iteration because their joint-loss is large enough to update the language models and perform a new training iteration. New training iteration means re-perturbing and re-fine-tuning result on these examples. Therefore, training based on both training steps' loss and computing joint-loss persistently prevents learning from gender bias in MLM as well as the PLM.\n\nAblation\nWe conduct the ablation experiments to demonstrate the effectiveness of Gender-tuning components, including 1) joint-training process and 2) joint-loss in Gender-tuning's debiasing performance (Table 4 ). The experiments are as follows: 1) Gender-tuning no\u2212joint\u2212training : first we used MLM to train the PLM through the gender-word perturbation on downstream task datasets. Then we fine-tuned the PLM on the downstream task dataset. 2) Gender-tuning no\u2212joint\u2212loss : we train Gender-tuning based on only fine-tuning loss.\nIn both PLMs, results illustrate that Gendertuning is more effective for reducing the average gender bias than in two ablation experiments. The two ablation experiments magnify the bias scores noticeably, while Gender-tuning gains the smallest SEAT absolute effect size, especially in the BERT model. Results also show that the ablation experiment setups that do not benefit from jointloss cannot update the MLM and PLM when the output of the fine-tuning classification is correct (L f ine\u2212tuning \u2248 0), even though the correct output likely bases on the gender biases in the PLMs.\n\nConclusion\nWe propose a novel approach for debiasing PLMs through fine-tuning on downstream tasks' datasets. The proposed method is an aggregation of biasword perturbation using MLM and fine-tuning classification. In this study, we evaluated our proposed method on gender biases and named it Gendertuning. Comprehensive experiments prove that Gender-tuning outperforms two state-of-the-art debiasing methods while improving the performance of the PLMs on downstream tasks. The key advantage of our approach is using the fine-tuning setting that allows the training process to be carried out without needing additional training processes or datasets. Also, it makes Gender-tuning a plug-andplay debiasing tool deployable to any PLMs.\n", "hypothesis": " Recent studies have revealed that the widelyused Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora.  Existing solutions require debiasing training processes and datasets for debiasing, which are resourceintensive and costly.  Furthermore, these methods hurt the PLMs' performance on downstream tasks.  In this study, we propose Gendertuning, which debiases the PLMs through finetuning on downstream tasks' datasets.  For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process.  Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset.  Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning..", "answer": true}
{"title": "Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks", "content": "\nIntroduction\nIt is common practice to start with a multilingual language model (MLLM) like mBERT 2 or XLM-R (Conneau et al., 2020) , which has been pre-trained with large multilingual corpora, and fine-tune the MLLM for diverse downstream tasks. Although MLLMs support many low-resource languages (LRLs), closer inspection of these MLLMs reveals that the portion of vocabulary allotted to LRLs can be orders of magnitude smaller than that allotted to high-resource languages (HRLs) such as English (Table 1 ). Due to this imbalance, sometimes an LRL word may not be possible to segment into wordpieces as per the MLLM vocabulary, leading to the LRL word being conflated with the UNK (unknown) token. An even more insidious situation is that the MLLM vocabulary has enough (over-fragmented) wordpieces to assemble almost any LRL word (thereby dodging the obvious UNK alert), but the embeddings of these wordpieces collide with unrelated usage in HRLs, and/or are so sparsely trained that contextual aggregations fail to yield satisfactory LRL word embeddings which may lead to poor LRL task performance. On the other hand, significant human and computational investments are needed to create task-specific LRL corpora that are large enough to augment and retrain the MLLM vocabulary.\nIn this work, we address the setting where a MLLM (that is presumably deficient in LRL coverage) must be minimally fine-tuned after modest modification to its wordpiece vocabulary, guided by specific LRL tasks. We design a measure of damage to an LRL word, caused by wordpiece fragmentation, based on a suitably defined notion of entropy of the word and constituent wordpieces, with respect to the LRL task. This measure then guides the selection of LRL words with which the vocabulary should be augmented. Subsequently, we propose various ways to initialize the embeddings of these newly-introduced words, including using information from the LRL itself, to 'importing' information from HRLs. We call the resulting system EVALM (entropy-based vocabulary augmented language model).\nWe study the effect of EVALM on an existing MLLM during the fine-tuning stage for various downstream classification tasks covering multiple LRLs and also a code-mixed language. Our study shows that, for most of the datasets, EVALM's vocabulary augmentation strategy helps improve LRL task performance by greater margins than recent best practices (Hong et al., 2021\u037e Hofmann et al., 2022) . A detailed analysis of successes and failures delineates the perimeter of EVALM's capabilities and guides our design choices.\n\nRelated Work\nContinued pre-training (Tai et al., 2020\u037e Ebrahimi and Kann, 2021\u037e Wang et al., 2020\u037e Chau et al., 2020) with or without vocabulary augmentation of existing LMs like monolingual BERT, multilingual BERT (mBERT), XLM-R, etc., proves beneficial for improving domain and languagespecific performances over various tasks. Some works (Ruzzetti et al., 2021\u037e Yu et al., 2021) focus on rare/OOV words. Liu et al. (2021) propose an embedding generator module in the pretrain-finetune pipeline to resolve vocabulary gaps. Adaptors (Sachidananda et al., 2021\u037e Moon and Okazaki, 2020\u037e Hofmann et al., 2021) are also showing promising outcomes in LRL modeling. Chung et al. (2020) explore multilingual vocabulary generation from language clusters. Minixhofer et al. (2021) transfer English LMs to new languages without expensive computation. Hofmann et al. (2022) propose a simple algorithm which modifies the tokenization process to preserve the morphological structure of a word. Others (Wang et al., 2019\u037e Hong et al., 2021) focus on embedding initialization for newly added vocabulary words which are word fragments, which is also among our concerns.\n\nOur system: EVALM\nEVALM has three key components. The purpose of the first component (Section 3.1) is to identify (based on only the train fold) a subset of vulnerable LRL words whose assembly from wordpieces is likely to distort the embedding information made available to LRL labeling tasks. The second component (Section 3.2) comprises various possible \n\nVulnerable LRL word selection\nWe need a computationally efficient, task-sensitive surrogate of the value of introducing an LRL word into the wordpiece vocabulary. (Here we augment the vocabulary with whole LRL words, blocking their fragmentation entirely. More clever sharing of fragments is left for future work.) Suppose LRL word w is not in the MLLM vocabulary\u037e w is fragmented into wordpiece sequence T (w) = s 1 , . . . , s T by the MLLM tokenizer T . The LRL task has C class labels. A specific label is denoted c \u2208 [C] = {1, . . . , C}. The counts of w and constituent wordpieces s t in each class c are denoted n(w, c) and n(s t , c). Based on these counts, we define the following multinomial distributions:\np(c|\u2022) = n(\u2022, c)/ \u2211 c \u2032 n(\u2022, c \u2032 )\n(1) where \u2022 = w, s t , etc. Based on this we define the entropy\nH(\u2022) = \u2212 \u2211 c p(c|\u2022) log p(c|\u2022)\n(2) Suppose H(w) is small. This means w is potentially a good feature for the LRL task. Now suppose a wordpiece s t has large H(s t ). That means s t is being shared across other words that are dis-tributed more evenly across classes. If this is the case for most s t , then fragmentation of w may be a serious problem. To combine information from all wordpieces, we average their entropies, and use the relative increase in entropy, going from LRL word to wordpieces, as one signal for the danger of fragmenting w. As an example, suppose the word '\u0927\u0930\u092e' (religion) occurs ten times in a threeclass sentiment analysis dataset with the class distribution of 'positive', 'neutral', and 'negative' as (1,1,8) . Its wordpieces have class distributions '\u0927' (100,85,80), '##\u0930' (130, 235, 250) , and '##\u092e' (130, 90, 125) . Then as per equation 2, H('\u0927\u0930\u092e') = 0.639, H('\u0927') = 1.094, H('##\u0930') = 1.062, and H('##\u0930') = 1.086. The average wordpiece entropy is H S ('\u0927\u0930\u092e') = 1.094+1.062+1.086 3 = 1.081, and the percentage of entropy reduction from average wordpiece to word entropy is about 41%.\nWe also retain two simpler signals: the number of fragments |T (w)|, and the frequency of w in the LRL task corpus. LRL words are sorted on the amount of entropy decrease and the top LRL words proposed for vocabulary augmentation. We remove words with very low frequency and retain a prefix of specified size to obtain V new , the LRL words to be added to the MLLM vocabulary. Algorithm 1 shows a high-level pseudocode.\n\nEmbedding initialization\nHere we describe the different ways to initialize the embeddings of newly-added LRL words. InitLRL: The embedding of the newlyintroduced LRL word is initialized using other LRL wordpieces already in the MLLM dictionary. Suppose we add Bengali word ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' , ('hospital' in English). Suppose the existing MLLM tokenizer splits it into [' \u09b9' , ' ##\u25cc\u09be\u09b8' , ' ##\u09aa' , ' ##\u25cc\u09be\u09a4' , ' ##\u25cc\u09be\u09b2' ]. Then we initialize the embedding of ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' with the average of the existing MLLM embeddings of the fragments. InitHRL: Here we translate ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' to English ('hospital'), tokenize it using T , and take the average embedding of the tokens in the list. InitMix: We use the average of InitLRL and InitHRL embeddings. InitRand: We randomly initialize the embeddings of the newly-added words.\nIt is challenging to learn good contextual embedding for words in V new due to very small taskspecific training data compared to the MLLM pretraining corpus. Therefore, we found it neces- Table 2 : Salient statistics of tasks. Note the small size of LRL datasets. Further details in Table 6 .\nsary to apply some regularization to avoid overfitting during fine-tuning. Let T , T \u2032 be the initial and final MLLM tokenizers. For a particular sentence S = w 1 , w 2 , ..., w I with words w i , we will get two different tokenizations\u037e these will generally lead to different contextual embeddings E = (e 1 , . . . , e K ) and E \u2032 = (e \u2032 1 , . . . , e \u2032 L )\u037e generally K \u0338 = L. We average-pool these to get vectors e, e \u2032 which a final layer uses for the classification task, with losses \u2113 T and \u2113 T \u2032 . We also use (e+e \u2032 )/2 for a third classification, with loss \u2113 mix . The overall training loss is \u2113 T + \u2113 T \u2032 + \u2113 mix , where \u2113 T and \u2113 mix are expected to reduce overfitting.\n\nDatasets and evaluation metric\nWe experiment with six short multi-class text classification tasks covering four Indian languages and a Hindi-English code-mixed dataset. We show the details of the datasets in Tables 2 and 6 . We use mBERT as the MLLM and report macro-F1 (we report the accuracy metric in Appendix B). Details of model hyperparameters are present in Appendix C.\n\nQuantitative results\nIn Figure 1 , we plot macro-F1 against the extent of vocabulary augmentation. Green, orange, and blue lines show the performance with InitLRL, InitHRL, and InitMix initialization, respectively. Corresponding colored bands show 1-standard deviation spreads.\nV new helps: For all tasks, including V new is better than baseline MLLM, and the gap is usually significant. This shows that even minimal training of newly added LRL tokens that used to be UNK or over-fragmented helps improve performance. More augmentation\u0338 \u21d2larger lift: We expected that larger V new would monotonically improve performance, but this was not universally the case. Inclusion of non-informative words, as we grow V new (\u2206 H decreases with high variance as shown in Appendix B Figure 3 ), maybe a reason. The middle column depicts the added vocab (underlined in the sentence) along with the entropy reduction percentage and the class it mostly belongs to.\nInitialization does not matter much: Although there are cases where InitHRL or InitMix performs better than InitLRL, we did not find significant performance difference between different embedding initialization of new LRL words. Transfer of embeddings from a well-represented HRL is the likely reason. We also check the performance by randomly initializing the V new words and find, for almost all the cases, random initialization performance, both for macro-F1(in Figure 1 ) and accuracy(in Appendix B Figure 2 ), is lesser compared to InitHRL, InitLRL, or InitMix. It suggests meaningful initialization helps.\n\nComparison with recent approaches:\nWe compare EVALM with AVocaDo (Hong et al., 2021) keeping V new comparable in size. Table 4 shows that AVocaDo leads to performance degradation for all LRL datasets. The lack of domainspecificity for our datasets may be why AVo-caDo's performance dropped. We also compare with FLOTA (Hofmann et al., 2022) in Figure 1 . For all datasets except GLUECoS Hi-En codemix dataset, EVALM performs better than FLOTA. A possible explanation is that mBERT vocabulary already includes many English as well as Hindi words, which helps FLOTA better compose embeddings of morphological components of English and Hindi words compared to other Indian languages. Regularization helps: Table 5 shows that EVALM with AVocaDo-style regularization performs better than without it, for all datasets.\nCases where EVALM hurts: The samples in Table 3 show that EVALM generally helps by spotting words important for predicting the correct class. This is shown in the first two examples, where the added vocabulary (\u2206 H =100%) tipped the prediction toward the gold label. But the last two examples show cases where for a word, the train and test set frequency distribution among target classes are different. As a consequence, these words may become misleading at test time.\n\nConclusion\nWe have proposed a simple and effective method to augment an MLLM wordpiece vocabulary with LRL words that are important for LRL classification tasks. Our study, involving several Indian languages, shows a consistent positive impact of vocabulary augmentation and fine-tuning. We find more augmentation does not guarantee performance improvement, and different embedding initialization fails to show significant performance differences among themselves. We also show that regularization is crucial to prevent overfitting new LRL word embeddings during fine-tuning. We have limited the augmentation to whole LRL words, and a judicious selection of LRL wordpieces may improve performance. We also want to extend to other target tasks (especially language generation) and a more diverse set of LRLs.\n", "hypothesis": " Multilingual language models (MLLMs) like mBERT promise to extend the benefits of NLP research to low-resource languages (LRLs).  However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to high task accuracy. (Pre)-training MLLMs after including LRL documents is resource-intensive in terms of both human inputs and computational resources. In response, we propose EVALM (entropy-based vocabulary augmented language model), which uses a new task-cognizant measurement to detect the most vulnerable LRL words, whose wordpiece segmentations are undesirable. EVALM then provides exceptional initializations of their embeddings, followed by extensive fine-tuning using the small LRL task corpus.  Our experiments show significant performance improvements and also some surprising limits to such vocabulary augmentation strategies in various classification tasks for multiple diverse LRLs, as well as code-mixed texts.  We will release the code and data to enable further research 1 ..", "answer": false}
{"title": "A Two-Stage Decoder for Efficient ICD Coding", "content": "\nIntroduction\nMedical records and clinical documentation contain critical information about patient care, disease progression, and medical operations. After a patient's visit, medical coders process them and extract key diagnoses and procedures according to the International Classification of Diseases (ICD) system (WHO, 1948) . Such codes are used for predictive modeling of patient care and health status, for insurance claims, billing mechanisms, and other hospital operations (Tsui et al., 2002) .\nAlthough the healthcare industry has seen many innovations, many challenges related to manual operations still remain. One of these challenges is manual ICD coding, which requires understanding long and complex medical records with a vast vocabulary and sparse content. Coders must select a small subset from a continuously expanding set of ICD codes (from around 15,000 codes in ICD 9 to around 140,000 codes in ICD 10 (WHO, 2016)). Therefore, manual ICD coding may result in errors and cause revenue loss or improper allocation of care-related resources. Thus, automated ICD coding has received attention not only from the industry but also from the academic community.\nBefore the rise of deep learning methods, automated ICD coding methods applied rules or decision tree-based methods (Farkas and Szarvas, 2008; Scheurwegs et al., 2017) . The focus has now changed to neural networks using two strands of approaches. The first encodes medical documents using pretrained language models (Li and Yu, 2020; Liu et al., 2021) , adapts pretrained language models to make them suitable for the clinical domain (Lewis et al., 2020) or injects language models with medical knowledge such as taxonomy, synonyms, and abbreviations of medical diseases (Yang et al., 2022; Yuan et al., 2022) . The second improves the representation of pretrained language models, by capturing the relevance between the document and the label metadata such as their descriptions (Mullenbach et al., 2018; Vu et al., 2020; Kim and Ganapathi, 2021; Zhou et al., 2021) , cooccurrences (Cao et al., 2020) , hierarchy (Falis et al., 2019; Vu et al., 2020; Liu et al., 2022) , or thesaurus knowledge, such as synonyms (Yuan et al., 2022) . Although these approaches are supposed to alleviate problems specific to medical coding such as special vocabulary, a large set of labels, etc., they fall short.\nIntuitively, human coders generate the code in two stages: first, the coders select the general codes and then look for specific subcategories that are relevant to a patient's condition. The advantage of adapting this approach to neural networks is that at each stage of the prediction, we deal with a smaller output space and we can have more confidence when predicting the next stage. Therefore, in this paper, we introduce a simple two-stage decoding framework 1 for ICD coding to mimic the processes of human coders. Our approach leverages the hierarchical structure of the ICD codes to decode, i.e., having parent-child relationships. The first stage predicts the parent codes; the second stage uses the document representation and the predicted parent codes to predict the child codes. Experiments with MIMIC-III data sets demonstrate the effectiveness of our proposed method. In particular, our simple method outperforms models that use external knowledge and data. Since our models (Figure 1 ) are based on LSTMs, they require less computing power and can be trained faster than other larger models.\n2 Two-stage Decoding Framework ICD codes follow a hierarchical structure. In this work, we consider characters before the dot (.) in the ICD code as the parent label and the code that has to be predicted as the child label. For example, for the child label 39.10 about Actinomycosis of lung, its parent code is 39 representing Actinomycotic infections. Let P and L represent the sets of parent nodes and child codes for a medical note x, respectively. It is worth noting that if we know the child codes, we can use the above definition to find the corresponding parent codes. This means that knowing L is equivalent to knowing both L and P. Then the probability of the child labels is:\nEQUATION\nThis factorization allows us to compute the prediction scores of the parent codes first, and then, conditioned on them and the document, we can obtain the prediction score of the child codes. Therefore, we can model the ICD coding task using a decoder framework where we generate parent labels before predicting child labels. In this case, we adapt the decoder framework to the multilabel problem setting, where at each decoding stage, we predict multiple labels at once, instead of one label at a time like a standard decoder.\n\nModel Architecture\nWe now describe the components of our parsing model: the document encoder, the first decoding stage for the parent code, and the second decoding stage for the child code. Document Encoder Given a medical note of n tokens x = (x 1 , . . . , x n ), we embed each token in the document in a dense vector representation. Subsequently, the token representations are passed to a single-layer BI-LSTM encoder to obtain the contextual representations [h 1 , h 2 , . . . , h n ]. Finally, we obtain the encoding matrix H \u2208 R n\u00d7d .\nFirst Decoding Stage At this stage, similar to Vu et al. ( 2020), we take the embedding of all parent labels P \u2208 R |L P |\u00d7de to compute the attention scores and obtain the label-specific representations as: where S(\u2022), \u03c3(\u2022), rds(\u2022) denote row-wise softmax, sigmoid, reduce sum in last dimension operations; W \u2208 R de\u00d7d are the weight parameters to perform linear transformations and V \u2208 R |L P |\u00d7d is the weight matrix of a label-wise fully connected layer which yields the parent label logits where \u2299 is element-wise product.\ns(P , H) = P tanh(W H T ) att(P , H) = S(s(P , H))H P (P|x) = \u03c3(rds(V \u2299 att(P , H)))\nSecond Decoding Stage At this stage, we take the label embeddings of all child labels L \u2208 R |L|\u00d7de and the probabilities of predicted parent labels from the previous stage as input, and obtain the label-specific representations as per:\ns(L, P ) = L tanh(W P \u2299 (P (P|x)) T ) att(L, P ) = S(s(L, P ))P\ns(L, H) = L tanh(W L H T ) att(L, H) = S(s(L, H))H P (L|P, x) = \u03c3(rds(V LH \u2299 att(L, H) + V LP \u2299 att(L, P )))\nwhere we perform a 'soft' embedding of the parent labels by taking the element-wise product between matrix W P \u2208 R de\u00d7|L P | with the sigmoid probabilities of parent labels. V LH , V LP \u2208 R |L P |\u00d7d are the weight matrices of two label-wise fully connected layers that compute the child label logits.\nTraining Objective & Inference The total training loss is the sum of the binary cross-entropy losses to predict the parent and child labels:\nEQUATION\nFor inference, we assign a child label to a document if the corresponding parent label score and the child label score are greater than predefined thresholds.\n\nExperiment Settings\nSetup We conduct experiments on the data set MIMIC-III (Alistair et al., 2016) . Following the previous work Joint LAAT (Vu et al., 2020) , we consider two versions of MIMIC-III dataset: MIMIC-III-Full consisting of the complete set of 8,929 codes and (MIMIC-III-50) consisting the 50 most frequent codes. Similarly to Yang et al. ( 2022), we use macro and micro AUC and F1, as well as precision@k (k = 8 for MIMIC-III-Full and k = 5 for MIMIC-III-50). For both data sets, we train with one single 16GB Tesla P100 GPU. We detail relevant training hyperparameters and the statistics of the data sets in the Appendix.\nWe compare our models with recent state-of-theart work using the results from Yang et al. ( 2022). Among them, Joint LAAT is most similar to our work because it uses a similar attention mechanism and considers both parent and child labels; therefore, we use it as a comparison in ablation studies. We run our models five times with the same hyperparameters using different random seeds and report the average scores.\n\nMIMIC-III-Full\nFrom the result shown in Table 1, we see that our model achieves a micro F1 of 58.4%, the highest among \"single\" models that do not rely on external data/knowledge. Specifically, our model outperforms Joint LAAT by about 2.5%, 0.2%, 0.9%, 0.3%, 0.9% in macro AUC, micro AUC, micro F1, macro F1 and precision@8 respectively. In particular, our model is on par with MSMN (Yuan et al., 2022) which uses code synonyms collected from Bodenreider (2004) Improvements of other models (Huang et al., 2022; Yang et al., 2022) most likely stem from the use of external information in form of knowledge injected into pre-trained language modeling. We leave the integration of such information into our proposed model architecture for future work.\n\nMIMIC-III-50\nFrom the results on the righthand side of Table 1 , our model produces a micro-F1 of 71.83%, the highest among single models. Specifically, our model surpasses Joint LAAT (Vu et al., 2020) with nearly 1.0%, 2.0%, 0.4% absolute improvement in micro F1, macro F1, and precision@5, respectively. In particular, the macro-F1 of our model is on par with the much more complex state-of-the-art method KEPTLongformer (Yang et al., 2022) . This demonstrates the ability of our model to be adapted to classification problems with a large or small number of labels while having competitive results in both cases.\n\nAblation Study\nTo evaluate the effectiveness of our model, we conduct an ablation study on the MIMIC-III-Full set, comparing it with Joint LAAT. Rather than integrating parent label prediction scores as supplementary features with the child label representation, as done in the Joint LAAT method, we allow child label representations to attend to both parent label and document representations. We show that this approach drives performance improvements in two aspects: parent label prediction and performance on labels grouped by frequency of appearance. absolute in macro F1, micro F1, and Precision@8, which naturally yields in better child label prediction performance reported in previous sections. But even considering only the case where both models predict parent labels correctly, our approach still achieves a micro F1 score of 65.5%, outperforming Joint LAAT with a micro F1 score of 65.0%. This demonstrates that both parent code and child code prediction benefit from our approach.\n\nParent Label Prediction\nPerformance in Label Frequency Groups To understand more about our prediction of the model, we divide medical codes into five groups based on their frequencies in MIMIC-III-Full: 1 \u2212 10, 11 \u2212 50, 51 \u2212 100, 101 \u2212 500, > 500 like Wang et al. ( 2022). We list the statistics of all groups in the Appendix. We compare the micro F1 between different groups in Figure 2 . Overall, we outperform Joint LAAT in all groups. The relative improvements are most noticeable in the rare-frequency group (25% relative improvement in the 1 \u2212 10 group, vs 2% or less in other cases). A possible explanation for this is that the parent label space is smaller than the full label space, which results in more training samples per parent label, allowing to learn better representations. As the parent label representation is used to compute child label representations, low-frequency child labels can thus benefit from representations learned from their high-frequency siblings.\n\nConclusion\nIn this paper, we have presented a novel, simple but effective two-stage decoding model that leverages the hierarchical structure of the ICD codes to decode from parent-level codes to child-level codes.\nExperiments on the MIMIC-III data set show that our model outperforms other single-model work and achieves on-par results with models using external data/knowledge. Our ablation studies validate the effectiveness of our model in predicting the code hierarchy and codes in different frequency groups. In future work, we intend to integrate our decoder with a better document or label representation to further improve performance.\n", "hypothesis": " Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures.  ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution.  Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases.  However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient's condition.  Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes.  Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction.  Experiments on the public MIMIC-III data set show that our model performs well in single-model settings without external data or knowledge..", "answer": true}
{"title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization", "content": "\nIntroduction\nDeveloping a persona-consistent dialogue model has been one of the key issues and crucial problems in open-domain dialogue systems (Huang et al., 2020) . Zhang et al. (2018a) define the problem of personalized dialogue generation, which aims to generate personalized responses based on textually described persona profiles. Many efforts have been made on developing dialogue models that generate responses consistent with the provided persona profile (Song et al., 2019 (Song et al., , 2020a,b;,b; Wu et al., 2020a) .\nThe recent development in transformer-based pre-trained models (Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019; Chen, 2020) has led to great successes in dialogue systems (Wolf et al., 2019; Wu et al., 2020b; Ham et al., 2020; Kulh\u00e1nek et al., 2021; Cao et al., 2022; Deng et al., 2022b Deng et al., ,c, 2023)) . Inspired by these successes, previous works incorporate those pre-trained models in persona-based response generation by concatenating the dialogue history and persona as input to generate the response in an auto-regressive manner (Song et al., 2021; Liu et al., 2022) . However, a fine-tuned model can generate a high-quality and persona-consistent response in a certain ordering of personas, while varying this order may lead to a generic and even inconsistent response as illustrated by the example in Figure 1 . We empirically show that the worst ordering of persona can lead to a 29.4% decline in BLEU score compared with the best ordering.\nIdeally, a well-trained dialogue generation model should be able to generate a persona-consistent response regardless of the ordering of personas in the input. We perform experiments and analyses to identify the cause of the ordering sensitivity. We find that the ordering of persona in the input leads to different representations of context and response. We also show that the model can attend to the appropriate persona and generate high-quality responses under some representations but not under others. This leads to instability in response generation.\nMotivated by the above findings, we propose ORder Insensitive Generation (ORIG), which is a simple and effective framework that helps models learn more robust and better representations for different persona orders. More specifically, we formulate ORIG as a constrained optimization problem, which optimizes a persona response generation objective under the constraint: given different orderings of persona, the response representations of the model are the same. Then we optimize it through a stochastic optimization approach.\nExperimental results on the Persona-Chat dataset show that ORIG significantly improves the robustness of pre-trained models (GPT2 (Radford et al., 2019) and BART (Lewis et al., 2020) ) under different orderings of input persona, as well as advances their generation performance.\nIn summary, our contributions are threefold: (1) We identify the order sensitivity problem in persona dialogue generation and conduct an empirical analysis to reveal its underlying reasons. ( 2) We propose a model-agnostic framework, ORIG, that helps different persona dialogue models learn robust representations while achieving better performance. (3) We perform extensive experiments on the Persona-Chat dataset, showing that ORIG outperforms previous models and is more robust and less sensitive to different persona orderings.\n\nRelated Work\nMaintaining a consistent persona is essential for building a human-like dialogue system, where most works regard persona as a set of sentences along with each dialog (Zhang et al., 2018a; Gu et al., 2019; Song et al., 2019; Wu et al., 2021; Cao et al., 2022; Deng et al., 2022a) . Song et al. (2021) disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation while Cao et al. (2022) aims to alleviate the problem of limited data by data manipulation methods. Despite satisfactory performance in previous work, the impacts of different orders of personas are still under-explored, resulting in unstable and inconsistent responses.\nOur work is also related to work on order sensitivity in prompt-based few-shot learning (Zhao et al., 2021; Lu et al., 2022) . Zhao et al. (2021) found that the different order of training examples in the prompt can cause accuracy to vary from near chance to state-of-the-art in the few-shot clas- sification setting. Similarly, order sensitivity for In-context Learning also exists regardless of model size and the prompt format (Lu et al., 2022) . Distinguishing from them, we focus on order sensitivity in the language generation task in finetuning setting, especially the impacts of persona orderings to generate persona-consistent responses.\n\nOrder Sensitivity Problem and Analysis\nIn this section, we first illustrate the seriousness of the order sensitivity problem by showing a huge performance fluctuation in persona dialogue models when fed the same personas in the best and worst orders. Then we analyse why their performance is volatile to different persona orderings.\nTo illustrate the problem, we finetune PLMs on the Persona-Chat by concatenating the persona and dialogue context together to predict the target response, including GPT2 and BART. After the training converges, we test them on two settings: (1) the best case: for each test sample, we feed the models all possible permutations of persona sentences and keep the maximum score for each sample as the final score; (2) the worst-case: perform the same process as (1), but take the minimum score. Table 1 shows the results for two models. Surprisingly, we find the ordering of input persona has a big impact on the models' performance: GPT2's worst case is 29.4% lower than its best case, while BART's is 83.2% lower.\nMoreover, we find that the huge fluctuation in models' performance is closely related to the response representation changes caused by different orderings of input persona sentences. Concretely, we measure the similarity of the responses representation of the same test sample under different input orders of persona. We show their token-level similarity in the sponse should be zero. However, their distances are significantly higher than zero. It reveals that the models behave more likely a left-to-right language model whose representation is prone to the different orderings of the previous input (e.g. persona sentences). That is highly undesirable for a robust personalized dialogue model. Thus, regularization of representation for the response tokens is necessary to help personalized dialogue models capture order-invariant representation.\n\nMethod\nWe introduce the proposed framework, named ORIG: ORder Insensitive Generation (ORIG). As shown in Figure 2 , we transform the persona ordersensitivity problem as a constrained optimization problem that optimises a persona dialogue model under the uncertainty of the input persona order.\n\nProblem Formulation\nGiven the dialogue context C = {u 1 , . . . , u m } and a set of persona descriptions P = {p 1 , . . . , p n }, the goal is to generate a personalized response r.\nFormally, the generation problem can be formulated as the following chain rule:\nP (r|C, P ; \u03b8) = T t=1 P (r t |r 1:t\u22121 , C, P ; \u03b8) (1)\nwhere \u03b8 is the parameters of the dialogue model.\n\nORIG Framework\nAccording to the analysis in Section 3, the observation reveals that varying the order of input personas leads to different representations of the dialogue response, thus resulting in fluctuations in performance.\nTo learn more robust and consistent representations, we propose the ORIG framework that complements the response generation process with a constraint: given the different orderings of a persona, the model's response representations need to be the same.\nThen the order-insensitive personalized dialogue generation problem is modelled as the following constrained optimization problem where P (r|C, P ; \u03b8) are the model's predictions over the dialogue response, D denotes the dialogue corpus, and the function D is KL divergence to measure the difference between two distributions, and the Shuffle operator samples each persona ordering uniformly from the full permutation of P .\n\nOptimization\nAs for optimization, we first apply the Lagrange multipliers strategy to convert the constrained problem into an unconstrained problem L \u03b8 = \u2212 log P (r|C, P ; \u03b8) +\u03b3 \u2022 D[P (r|C, P ; \u03b8), P (r|C, P ; \u03b8)] (6\n)\nwhere \u03b3 is the multiplier corresponding to the equality constraints (3). Then we can update the parameters \u03b8 of dialogue models by stochastic gradient descent.\n\nExperimental Setups\nDatasets We evaluate the models on the Persona-Chat dataset (Zhang et al., 2018a) , where each dialogue session has at least 6 turns of interactions.\nAnd each interaction is conditioned on a persona that is described with 5 profile sentences. proposed ORIG. Our implementation was based on HuggingFace's Transformers library (Wolf et al., 2020) . During training, the learning rate is set as 2 \u00d7 10 \u22125 , and the batch size for GPT2 and BART is set as 64 and 32, respectively. We trained both models for 10 epochs with Adam (Kingma and Ba, 2015) optimizer until they converged. During decoding, We employ a top-p (p=0.9) (Holtzman et al., 2020) plus top-k (k=50) sampling strategy, which is used to avoid sampling from the unreliable tail of the distribution (only consider a subset of vocabulary composed of k words with the highest probability or some most probable words whose sum of probabilities equals p at each decoding step).\nThe random seed for all experiments is set to 42. Evaluation Metrics We perform both automatic and human evaluations. (1) Automatic metrics: We adopt BLEU (Papineni et al., 2002) , ROUGE (Lin, 2004) , Entropy (Zhang et al., 2018b) and CIDEr (Vedantam et al., 2015) for lexicalbased measurement. Following previous work, we also adopt the C-score (Madotto et al., 2019) (Fleiss, 1971) .\n\nExperimental Results\nImproves performance in the original test set Table 3 shows different models' performance in the original test set without any modifications (for ORIG, \"Shuffle\" is used during training but is optional during testing. The Table 3 caption signifies the absence of \"Shuffle\" during testing. This is to evaluate if ORIG performs well in the normal setting). From automatic metrics, we can see base models trained with our ORIG framework outperform the baselines. It justifies that our framework can be applied to different models to improve their performance. From human evaluation results, models with ORIG are superior to others on almost all metircs, especially on GPT2. This is consistent with the results of automatic metrics. The average kappa value of the annotation is 0.632, indicating good agreement during human evaluation.\nReduces variance and improves mean and worstcase performance Figure 3 shows that aside from reducing the variance, ORIG also improves mean and worst-case performance (detailed results in Table 4) across two models consistently, especially in GPT2 (the worst case performance is very close to the best case). We reduce the variance on GPT2 and BART by 91.6% and 51.8%, respectively. Meanwhile, we improve worst-case performance by 20.3% and 22.6% on GPT2 and BART respectively. The only drop is the best case. This is because our distance function D is unidirectional, which pulls in the two representations in Equation 3indiscriminately, causing the best case to go down and the worst to go up. We leave more complicated and directional distance constraints for future studies.\n\nConclusion\nWe show that the current practice of applying pretrained models to the personalized dialogue generation task is volatile across different input orders of personas. Through the analysis, we find that the problem arises from the representation changes induced by the input changes. Motivated by these, we propose our ORIG, a model-agnostic framework for finetuning the persona dialogue model such that it obtains a persona order-invariant representation.\nExperiments on two dominant pre-trained dialogue models show that our framework improves performance and reduces order volatility.\n", "hypothesis": " Generating persona consistent dialogue response is important for developing an intelligent conversational agent.  Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response.  While simple and effective, our analysis shows that this popular practice is seriously affected by Order Sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART).  To mitigate the order sensitivity problem, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn robust representation under different persona orders and improve the consistency of response generation. Experiments on Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (BERT and RoBERTa).", "answer": false}
{"title": "Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information", "content": "\nIntroduction\nKeyphrase extraction is the fundamental task of automatically extracting a set of salient phrases from a document that concisely describes its primary content (Hasan and Ng, 2014; Song et al., 2023a) . Figure 1 shows an example of the source document and its corresponding keyphrases.\nRecent developments in pre-trained language models (Devlin et al., 2019) have heightened the need for utilizing pre-trained embeddings on natural language processing tasks, which significantly improves the performance of embedding-based unsupervised keyphrase extraction models (Sun et al., 2020; Liang et al., 2021; Zhang et al., 2022) . Existing embedding-based models mainly consist of two components: candidate keyphrase extraction and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2021 Song et al., , 2022a)) . The former extracts continuous words from the document as candidate keyphrases through heuristic rules, and the latter estimates the importance of candidate phrases by matching similarity with their corresponding document.\nGenerally, the source document has both salient information and noises (redundant content). Hence, there may be a deviation when directly using the phrase-document relevance as the importance score of each candidate to select keyphrases. For many specific-domain documents (e.g., news or scientific articles), the highlights (the title or the first sentence) typically contains the central information of the source document (as shown in Figure 1 ), which has more significant guidance for extracting keyphrases. However, the recent embedding-based unsupervised keyphrase extraction models ignore the effect of the highlight information, leading to extract wrong keyphrases.\nMotivated by the above issues, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which estimates the impor- \n\nCandidate Keyphrase Extraction\nTo extract candidate keyphrases from the source document, we follow the previous studies (Liang et al., 2021; Song et al., 2022b; Ding and Luo, 2021) At the same time, we use the mean pooling operation to obtain the highlight representation h s of the document.\n\nPhrase-Document Relevance\nTo obtain more relevant candidates, we model the similarity between candidate phrases and the corresponding document as follows,\nEQUATION\nwhere p h i denotes the phrase-document relevance of i-th candidate keyphrases and ||\u2022|| 1 indicates the Manhattan Distance.\nFor news and scientific articles, keyphrases often appear at the beginning or front position (Florescu and Caragea, 2017a,b) , which means that the position information is important and indicative for extracting keyphrases. For example, the word appearing at 2-th, 5-th and 10-th, has a weight \u03c1 i = 1/2 + 1/5 + 1/10 = 0.8. Inspired by the previous work (Florescu and Caragea, 2017b; Liang et al., 2021) , we adopt a position regularization as follows, \u03c1 i = softmax(e 1/i ), where \u03c1 i is the position regularization factor of the i-th candidate phrase. Then, the weighted phrase-document relevance ph i can be re-calculated as follows,\nEQUATION\nHere, we finally employ ph i to estimate the phrasedocument relevance of the i-th candidate phrase.\n\nCross-Phrase Relevance\nGenerally, the phrase-document relevance is calculated between the highlight information and each candidate independently, and consequently, it cannot determine which candidates are better than the Model DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Statistical Keyphrase Extraction Models TF-IDF (Jones, 2004) others. To determine which candidate phrases are more salient than the others, we sum the semantic relatedness between the i-th candidate phrases and all candidates as the cross-phrase relevance. Thus, it calculates the local relevance as follows,\nEQUATION\n)\nwhere \u03b4 i = Mean( j=1,j =i h p i h p j ). Here, we treat \u03b4 i as a de-noisy factor to filter the noises, which is far different from the i-th candidate keyphrase in the document.\n\nRelevance Aggregation\nWe aggregate the phrase-document relevance and the cross-phrase relevance into a whole score as the importance score of each candidate via a simple multiplication,\nEQUATION\n)\nwhere r i indicates the importance score of the i-th candidate phrase. Then, we rank all candidates with their importance score r i and extract top-ranked k phrases as keyphrases of the source document.\n3 Experiments and Results\n\nExperimental Settings\nThis paper conducts experiments on three benchmark and popular used keyphrase datasets, which includes DUC2001 (Wan and Xiao, 2008) , Inspec (Hulth, 2003 ), and SemEval2010 (Kim et al., 2010) . Due to page limits, please refer to the corresponding articles for the details of the three datasets. Following the previous work (Liang et al., 2021; Ding and Luo, 2021; Song et al., 2023b) , we use the standard practice and evaluate the performance of our model in terms of f-measure at the top-K keyphrases (F1@K) and adopt stemming to both extracted keyphrases and gold truth. Concretely, we report F1@5, F1@10, and F1@15 of each model on three benchmark datasets.\nWe adopt the pre-trained language model BERT (Devlin et al., 2019) as the backbone of our model, initialized from their pre-trained weights. In our experiments, \u03bb is set to 0.9 for three benchmark datasets.\n\nOverall Performance\nTable 1 shows the performance of baselines and our model on three benchmark datasets (DUC2001, In-\n\nDUC2001\nInspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Table 2 : The results of different pooling methods for document embedding.\nDifferent Similarity Measures DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 spec, and SemEval2010). The results show that our method significantly improves over state-of-the-art unsupervised keyphrase extraction baselines. Compared with the current state-of-the-art models, our model achieves significantly better performance on F1@5, F1@10, and F1@15 evaluation metrics, demonstrating the effectiveness of estimating the importance of candidate phrases by leveraging the highlights to calculate the relevance.\nCompared with EmbedRank (Bennani-Smires et al., 2018) , KeyGames (Saxena et al., 2020) , and SIFRank (Sun et al., 2020) , HGUKE achieves significant improvement, which benefits from using the highlights to calculate the importance score of each candidate keyphrase. Compared with the best baseline JointGL, our model achieves better performance on several benchmark keyphrase extraction datasets in all evaluation metrics. The main reason for this improvement is that we use the highlights as the guidance information instead of the whole document when estimating the importance of keyphrases.\n\nAblation Test\nThe ablation experiments on three benchmark keyphrase extraction datasets are shown in Figure 3 . It can be seen from the results that using the highlight information can significantly improve the performance of keyphrase extraction, which benefits from estimating the importance score of each candidate by using its corresponding highlight information rather than the whole document. We consider the main reason is that the title or the first sentence of the document usually has a strong guidance for extracting keyphrases.\n\nImpact of Pooling Methods\nIn this section, we study different pooling methods, including mean-and max-pooling operations. For all pooling methods, HGUKE using the last BERT layer achieves the best results, demonstrating that HGUKE benefits from stronger contextualized semantic representations. We can see the results in Table 2 that the document encoded via the meanpooling operation obtains the best performance.\n\nImpact of Different Similarity Measures\nOur model adopts Manhattan Distance to measure the textual similarity between candidate phrases and the highlight information. Furthermore, we attempt to employ different measures to estimate the phrase-document relevance. The results of different similarity measures are shown in Table 3 , and we can see that the advantage of Manhattan Distance is obvious.\n\nRelated Work\nMost existing unsupervised keyphrase extraction methods can be mainly divided into four categories: statistics-based, topic-based, graph-based, and embedding-based models. Specifically, statisticsbased models (Salton and Buckley, 1988; Witten et al., 1999) usually extract keyphrases by estimating the importance of candidate phrases with different statistic features, such as word frequency feature, phrase position feature, linguistic features of natural language, etc. Topic-based models (Liu et al., 2009 (Liu et al., , 2010) ) typically utilize topic information to determine whether a candidate phrase is a keyphrase. Graph-based models (Mihalcea and Tarau, 2004; Grineva et al., 2009) represent the document as a graph and rank candidate phrases by graph-based similarities.\nEmbedding-based models usually adopt the pretrained embeddings to obtain document and candidate phrase representations and calculate the importance score of each candidate depending on the obtained representations. Benefiting from the development of transformer-based pre-trained language models (Devlin et al., 2019) in the natural language processing field, embedding-based models (Bennani-Smires et al., 2018; Sun et al., 2020; Liang et al., 2021) have achieved outstanding performance. Concretely, embedding-based models mainly consist of two procedures: candidate keyphrase representation and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2023a) . The procedure utilizes natural language linguistic features to construct candidate keyphrases and represents them by pre-trained embedding approaches (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) ). The second procedure estimates the importance of candidate phrases from different perspectives to determine whether a candidate phrase is a keyphrase.\nUnlike the existing unsupervised keyphrase extraction models, we use the highlight information of the document to calculate the phrase-document relevance instead the whole document.\n\nConclusion and Future Work\nIn this paper, we incorporate structural information to improve the performance of embedding-based unsupervised keyphrase extraction. Specifically, in this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which calculates the phrase-document relevance via the highlight information instead of the whole document to select relevant candidate phrases. Extensive experiments demonstrate that HGUKE outperforms the state-of-the-art unsupervised baselines. Future research may investigate adopting different structural information of the source document to improve the performance of unsupervised keyphrase extraction.\n", "hypothesis": " Keyphrase extraction aims to extract a set of keyphrases with the central idea of the document.  In a structured document, there are certain locations (e.g., the title or the first sentence) where a keyphrase is most likely to appear.  However, when extracting keyphrases from the document, most existing embedding-based unsupervised keyphrase extraction models ignore the indicative role of the highlights in certain locations, leading to wrong keyphrases extraction.  In this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE) to address the above issue.  Specifically, HGUKE first models the phrasedocument relevance via the highlights of the documents.  Next, HGUKE calculates the crossphrase relevance between all candidate phrases.  Finally, HGUKE aggregates the above two relevance as the importance score of each candidate to rank and extract keyphrases.  The experimental results on three benchmarks demonstrate that HGUKE outperforms the state-of-the-art unsupervised keyphrase extraction baselines..", "answer": true}
{"title": "Class based Influence Functions for Error Detection", "content": "\nIntroduction\nDeep learning models are data hungry. Large models such as transformers (Vaswani et al., 2017) , BERT (Devlin et al., 2019) , and GPT-3 (Brown et al., 2020) require millions to billions of training data points. However, data labeling is an expensive, time consuming, and error prone process. Popular datasets such as the ImageNet (Deng et al., 2009) contain a significant amount of errors -data points with incorrect or ambiguous labels (Beyer et al., 2020) . The need for automatic error detection tools is increasing as the sizes of modern datasets grow.\nInfluence function (IF) (Koh and Liang, 2017) and its variants (Charpiat et al., 2019; Khanna et al., 2019; Barshan et al., 2020; Pruthi et al., 2020) are a powerful tool for estimating the influence of a data point on another data point. Researchers leveraged this capability of IFs to design or detect adversarial (Cohen et al., 2020) , poisonous (Koh et al., 2022; Koh and Liang, 2017) , and erroneous (Dau et al., 2022) examples in large scale datasets. The intuition is that these harmful data points usually have a negative influence on other data points and this influence can be estimated with IFs. Basu et al. (2021) empirically observed that IFs are unstable when they are applied to deep neu- * Joint first authors ral networks (DNNs). The quality of influence estimation deteriorates as networks become more complex. In this paper, we provide empirical and theoretical explanations for the instability of IFs. We show that IFs scores are very noisy when the two data points belong to two different classes but IFs scores are much more stable when the two data points are in the same class (Sec. 3). Based on that finding, we propose IFs-class, variants of IFs that use class information to improve the stability while introducing no additional computational cost. IFs-class can replace IFs in anomalous data detection algorithms. In Sec. 4, we compare IFs-class and IFs on the error detection problem. Experiments on various NLP tasks and datasets confirm the advantages of IFs-class over IFs.\n\nBackground and Related work\nWe define the notations used in this paper. Let z = (x, y) be a data point, where x \u2208 X is the input, y \u2208 Y is the target output; Z = z (i) n i=1 be a dataset of n data points; Z \u2212i = Z\\z (i) be the dataset Z with z (i) removed; f \u03b8 : X \u2192 Y be a model with parameter \u03b8; L Z,\u03b8 = 1 n n i=1 \u2113(f \u03b8 (x (i) ), y (i) ) = 1 n n i=1 \u2113(z (i) ; \u03b8) be the empirical risk of f \u03b8 measured on Z, where \u2113 : Y \u00d7 Y \u2192 R + is the loss function; \u03b8 = arg min \u03b8 L Z,\u03b8 and \u03b8\u2212i = arg min \u03b8 L Z \u2212i ,\u03b8 be the optimal parameters of the model f \u03b8 trained on Z and Z \u2212i . In this paper, f \u03b8 is a deep network and \u03b8 is found by training f \u03b8 with gradient descent on the training set Z.\n\nInfluence function and variants\nThe influence of a data point z (i) on another data point z (j) is defined as the change in loss at z (j) when z (i) is removed from the training set\nEQUATION\nThe absolute value of s (ij) measures the strength of the influence of z (i) on z (j) . The sign of s (ij) show the direction of influence. A negative s (ij) means that removing z (i) decreases the loss at z (j) , i.e. z (i) is harmful to z (j) . s (ij) has high variance because it depends on a single (arbitrary) data point z (j) .\nTo better estimate the influence of z (i) on the entire data distribution, researchers average the influence scores of z (i) over a reference set Z \u2032\ns (i) = 1 |Z \u2032 | z (j) \u2208Z \u2032 s (ij) = L Z \u2032 , \u03b8\u2212i \u2212 L Z \u2032 , \u03b8 (2)\ns (i) is the influence of z (i) on the reference set Z \u2032 . Z \u2032 can be a random subset of the training set or a held-out dataset. Naive computation of s (ij) requires retraining f \u03b8 on Z \u2212i . Koh and Liang (2017) proposed the influence function (IF) to quickly estimate s (ij) without retraining\ns (ij) \u2248 IF (z (i) , z (j) ) \u2248 1 n \u2207 \u03b8\u2113(z (i) ; \u03b8) \u22a4 H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) (3)\nwhere\nH \u03b8 = \u2202 2 L Z, \u03b8/\u2202\u03b8 2\nis the Hessian at \u03b8. Exact computation of H \u22121 \u03b8 is intractable for modern networks. Koh and Liang (2017) developed a fast algorithm for estimating H \u22121 \u03b8 \u2207 \u03b8\u2113(z (j) ; \u03b8) and used only the derivatives w.r.t. the last layer's parameters to improve the algorithm's speed. Charpiat et al. (2019) proposed gradient dot product (GD) and gradient cosine similarity (GC) as faster alternatives to IF. Pruthi et al. (2020) argued that the influence can be better approximated by accumulating it through out the training process (TracIn). The formula for IFs are summarized in Tab. 3 in Appx. A.\nIFs can be viewed as measures of the similarity between the gradients of two data points. Intuitively, gradients of harmful examples are dissimilar from that of normal examples (Fig. 1 ).\n\nInfluence functions for error detection\nIn the error detection problem, we have to detect data points with wrong labels. Given a (potentially noisy) dataset Z, we have to rank data points in Z by how likely they are erroneous. Removing or correcting errors improves the performance and robustness of models trained on that dataset.\nTraditional error detection algorithms that use hand designed rules (Chu et al., 2013) or simple statistics (Huang and He, 2018) , do not scale well to deep learning datasets. Cohen et al. (2020) 2021) empirically showed that IFs with last layer gradient perform as well as or better than IFs with all layers' gradient and variants of IF behave similarly. Therefore, we analyze the behavior of GD with last layer's gradient and generalize our results to other IFs. Fig. 1 shows the last layer's gradient of an MLP on a 3-class classification problem. In the figure, gradients of mislabeled data points have large magnitudes and are opposite to gradients of correct data points in the true class. However, gradients of mislabeled data points are not necessarily opposite to that of correct data points from other classes. Furthermore, gradients of two data points from two different classes are almost perpendicular. We make the following observation. A mislabeled/correct data point often has a very negative/positive influence on data points of the same (true) class, but its influence on other classes is noisy and small.\nWe verify the observation on real-world datasets. (Fig. 2 ). We compute GD scores of pairs of clean data points from 2 different classes and plot the score's distribution. We repeat the procedure for pairs of data points from each class. In the 2-class case, GD scores are almost normally distributed with a very sharp peak at 0. That means, in many cases, a clean data point from one class has no significant influence on data points from the other class. And when it has a significant effect, the effect could be positive or negative with equal probability. In contrast, GD scores of pairs of data points from the same class are almost always positive. A clean data point almost certainly has a positive influence on clean data points of the same class.\nOur theoretical analysis shows that when the two data points have different labels, then the sign of GD depends on two random variables, the sign of inner product of the features and the sign of inner product of gradients of the losses w.r.t. the logits. And as the model becomes more confident about the labels of the two data points, the magnitude of GD becomes smaller very quickly. Small perturbations to the logits or the features can flip the sign of GD. In contrast, if the two data points have the same label, then the sign of GD depends on only one random variable, the sign of the inner product of the feature, and the GD's magnitude remains large when the model becomes more confident. Mathematical details are deferred to Appx. D.\n\nClass based IFs for error detection\nOur class based IFs for error detection is shown in Alg. 1. In Sec. 3.1, we see that an error has a very Algorithm 1 Class based influence function for error detection.\n\nRequire:\n1: Z = z (i) n i=1 : a big noisy dataset 2: C: number of classes 3: for k = 1, ..., C do 9:\nZ \u2032 k = z \u2032(j k ) m k j k =1 : clean data from class k 4: Z \u2032 = C k=1 Z \u2032 k : a clean\ns (i) k = m k j=1 sim(\u2207 \u03b8 \u2113(z (i) ),\u2207 \u03b8 \u2113(z \u2032(j k ) )) m k 10:\nend for 11:\ns (i) = min k (s (i)\nk ) 12: end for 13: \u1e90 = sort(Z, key = s, ascending = True) 14: return \u1e90 strong negative influence on correct data points in the true class, and a correct data point has a positive influence on correct data points in the true class. Influence score on the true class is a stronger indicator of the harmfulness of a data point and is better at differentiating erroneous and correct data points. Because we do not know the true class of z (i) in advance, we compute its influence score on each class in the reference set Z \u2032 and take the minimum of these influence scores as the indicator of the harmfulness of z (i) (line 8-11 create benchmark datasets Z's, we inject random noise into the above datasets. For text classification datasets, we randomly select p% of the data points and randomly change their labels to other classes.\nFor the CoNLL-NER dataset, we randomly select p% of the sentences and change the labels of r% of the phrases in the selected sentences. All tokens in a selected phrase are changed to the same class. The reference set Z \u2032 is created by randomly selecting m k clean data points from each class in Z. To ensure a fair comparison, we use the same reference set Z \u2032 for both IFs and IFs-class algorithms. Models are trained on the noisy dataset Z. To evaluate an error detection algorithm, we select top q% most harmful data points from the sorted dataset \u1e90 and check how many percent of the selected data points are really erroneous. Intuitively, increasing q allows the algorithm to find more errors (increase recall) but may decrease the detection accuracy (decrease precision). Our code is available at https://github.com/Fsoft-AIC/ Class-Based-Influence-Functions.\nResult and Analysis Because results on all datasets share the same patterns, we report representative results here and defer the full results to Appx. C.\nFig. 3(a) shows the error detection accuracy on the SNLI dataset and how the accuracy changes with q. Except for the GC algorithm, our classbased algorithms have higher accuracy and lower variance than the non-class-based versions. When q increases, the performance of IFs-class does not decrease as much as that of IFs. This confirms that IFs-class are less noisy than IFs. Class information fails to improve the performance of GC. To understand this, let's reconsider the similarity measure sim(\u2022, \u2022). Let's assume that there exist some clean data points z \u2032(j) \u2208 Z \u2032 with a very large gradient \u2207 \u03b8\u2113(z \u2032(j) ). If the similarity measure does not normalize the norm of \u2207 \u03b8\u2113(z \u2032(j) ), then z \u2032(j) will have the dominant effect on the influence score. The noise in the influence score is mostly caused by these data points. GC normalizes both gradients, \u2207 \u03b8\u2113(z (i) ) and \u2207 \u03b8\u2113(z \u2032(j) ), and effectively removes such noise. However, gradients of errors tend to be larger than that of normal data points (Fig. 1 ). By normalizing both gradients, GC removes the valuable information about magnitudes of gradients of errors \u2207 \u03b8\u2113(z (i) ). That lowers the detection performance. In Fig. 3 (a), we see that the performance of GC when q \u2265 15% is lower than that of other classbased algorithms. Similar trends are observed on other datasets (Fig. 6 , 7, 8 in Appx. C). Fig. 3(b) shows the change in detection accuracy as the level of noise p goes from 5% to 20%. For each value of p, we set q to be equal to p. Our class-based influence score significantly improves the performance and reduces the variance. We note that when p increases, the error detection problem becomes easier as there are more errors. The detection accuracy, therefore, tends to increase with p as shown in Fig. 3 (b), 9, 10. Fig. 3(c ) shows that GD-class outperforms GD on all entity types in CoNLL2003-NER. The performance difference between GD-class and GD is greater on the MISC and ORG categories. Intuitively, a person's name can likely be an organization's name but the reverse is less likely. Therefore, it is harder to detect that a PER or LOC tag has been changed to ORG or MISC tag than the reverse. The result shows that IFs-class is more effective than IFs in detecting hard erroneous examples.\n\nThe effect of data on error detection algorithms\nWe study the effect of the size and the cleanliness of the reference set on the performance of error detection algorithms.\nThe size of the reference set. We changed the size of classes in the reference set from 10 to 1000 to study the effect of the reference set's size on the detection performance. We report the mean performance of GD and GC algorithms in Tab. 1. We observe no clear trend in the performance as the size of the reference set increases. Our conjecture is that gradients of clean data points from the same class have almost the same direction. Averaging the gradient direction over a small set of data points already gives a very stable gradient direction. Therefore, increasing the size of the reference set does not have much impact on detection performance. \n\nConclusion\nIn this paper, we study influence functions and identify the source of their instability. We give a theoretical explanation for our observations. We introduce a stable variant of IFs and use that to develop a high performance error detection algorithm. Our findings shed light of the development of new influence estimators and on the application of IFs in downstream tasks.\n", "hypothesis": " Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets.  However, they are unstable when applied to deep networks.  In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages additional computational cost to improve the stability of IFs. Extensive experiments show that our modification significantly improves the performance and stability of IFs.", "answer": false}
{"title": "DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect", "content": "\nIntroduction\nBinaural audio synthesis (BAS), which aims to render binaural audio from the monaural counterpart, has become a prominent technology in artificial spaces (e.g. augmented and virtual reality) (Richard et al., 2021 (Richard et al., , 2022;; Leng et al., 2022; Lee and Lee, 2022; Parida et al., 2022; Zhu et al., 2022; Park and Kim, 2022) . Binaural rendering provides users with an immersive spatial and social presence (Hendrix and Barfield, 1996; Gao and Grauman, 2019; Huang et al., 2022; Zheng et al., 2022) , by producing stereophonic sounds with accurate spatial information. Unlike traditional single channel audio synthesis (van den Oord et al., 2016; Chen et al., 2021) , BAS places more emphasis on * Equal contribution.\naccuracy over sound quality, since humans need to interpret accurate spatial clues to locate objects and sense their movements consistent with visual input (Richard et al., 2021; Lee et al., 2022) .\nCurrently, there are three types of neural networks (NN) to synthesize binaural audio. Firstly, Richard et al. (2021) collects a paired monauralbinaural speech dataset and provides an end-to-end baseline with geometric and neural warping technologies. Secondly, to simplify the task, Leng et al. (2022) decompose the synthesis into a two-stage paradigm: the common information of the binaural audio is generated in the first stage, based on which the binaural audio is generated in the second stage. They also propose to use the generative model DDPM (Ho et al., 2020) to improve the audio naturalness. Thirdly, to increase the generalization capability for the out-of-distribution audio, Lee and Lee (2022) renders the speech in the Fourier space. These non-linear NN-based methods outperform the traditional digital signal processing systems based on a linear time-invariant system (Savioja et al., 1999; Zotkin et al., 2004; Sunder et al., 2015) .\nHowever, these NN methods still have room for improvement in accuracy, especially phase accuracy. Richard et al. (2022) claims that the correct phase estimation is crucial for binaural rendering 1 . Actually, the previous works tend to view the scene \"statically\", and only take into account the series of positions and head orientations. This motivates us to propose DopplerBAS, which facilitates phase estimation by explicitly introducing the Doppler effect (Gill, 1965; Giordano, 2009) into neural networks. Specifically, 1) we calculate the 3D velocity vector of the moving sound source in the Cartesian coordinates and then decompose this 3D velocity vector into a velocity vector in the spherical coor-dinates relative to the listener; 2) According to the Doppler effect, we use the radial relative velocity as an additional condition of the neural network, to incentivize the model to sense the moving objects.\nWe also analyze the efficacy of different types of velocity conditions through extensive experiments.\nNaturally, DopplerBAS can be applied to different neural binaural renderers without tuning hyperparameters. We pick two typical recent backbones to demonstrate the effectiveness of our method: 1) WarpNet (Richard et al., 2021) , a traditional neural network optimized by reconstruction losses; 2) BinauralGrad (Leng et al., 2022) , a novel diffusion model optimized by maximizing the evidence bound of the data likelihood. Experiments on Warp-Net and BinauralGrad are representative and could show the generalizability of our proposed Doppler-BAS on other conditions based on gains on these two models. The contributions of this work can be summarized as follows:\n\u2022 We propose DopplerBAS, which distinctly improves WarpNet and BinauralGrad in the phase error metric and produces a new state of the art performance: 0.780 (vs. the current state of the art 0.807).\n\u2022 We conduct analytical experiments under various velocity conditions and discover that: 1) NN does not explicitly learn the derivative of position to time (velocity); 2) The velocity condition is beneficial to binaural audio synthesis, even the absolute velocity in the Cartesian coordinates; 3) The radial relative velocity is the practical velocity component, which obeys the theory of the Doppler effect.\n\nMethod\nIn this work, we focus on the most basic BAS scenario where only the monaural audio, the series of positions and head orientations are provided (Richard et al., 2022; Leng et al., 2022) , rather than other scenarios where extra modalities (Xu et al., 2021) are present. Note that scenarios with extra modalities present are different tasks. Also, as demonstrated in this paper, our proposed DopplerBAS is plug-and-play and can be easily integrated into other more complex scenarios. In this section, we will introduce the Doppler Effect as the preliminary knowledge, and then introduce the proposed method DopplerBAS. We will describe how to calculate and decompose the velocity vec-tor, and how to apply this vector to two different backbones.\n\nDoppler Effect\nThe Doppler effect (Gill, 1965) is the change in frequency of a wave to an observer, when the wave source is moving relative to it. This effect is originally used in radar systems to reveal the characteristics of interest for the target moving objects (Chen et al., 2006) . It can be formulated as:\nEQUATION\nwhere c, v r , f 0 and f are the propagation speed of waves, the radial relative velocity of the moving sound source, the original frequency of waves and the received frequency of waves, respectively.\n\ud835\udc63 ! \ud835\udc63 \" \ud835\udc5f \ud835\udc52 \ud835\udc63 #$ \ud835\udc65 \ud835\udc66 right ear\nFigure 1 : We illustrate the top view where the height dimension is omitted for simplicity. The sound source is moving in the x-y plane with the velocity v xy . This velocity is decomposed into the radial velocity v r relative to one ear (e.g., the right ear).\n\nDopplerBAS\nWe do not directly apply Eq. ( 1) in the frequency domain of audio, because some previous works (Lee and Lee, 2022) show that modeling the binaural audio in the frequency domain degrades the accuracy although it could benefit the generalization ability. Different from modeling the Doppler effect in the frequency domain, we calculate the velocity of interest and use it as a condition to guide the neural network to synthesize binaural audio consistent with the moving event. In the receiver-centric Cartesian coordinates, we define \u20d7 p s and \u20d7 p e as the 3D position of the moving sound source s and one ear of the receiver e respectively (e.g., the right ear, as shown in Figure 1 ). The \n\u20d7 p = (p x , p y , p z ) = \u20d7 p s \u2212 \u20d7 p e .\nThen s's velocity 2 can be calculated as:\n\u20d7 v = (v x , v y , v z ) = ( dp x dt , dp y dt , dp z dt ).\nNext, we build the spherical coordinate system using the ear as the origin, and decompose \u20d7 v into the radial relative velocity \u20d7 v r by:\nEQUATION\nwhere r \u2208 R 1 is the radial unit vector. Finally, we add \u20d7 v r as the additional condition to the network: The original conditions in monauralto-binaural speech synthesis are C o \u2208 R 7 = (x, y, z, qx, qy, qz, qw), of which the first 3 represent the positions and the last 4 represent the head orientations. We define the new condition C \u2208 R 9 = (x, y, z, qx, qy, qz, qw, v r\u2212lef t , v r\u2212right ), where v r\u2212lef t and v r\u2212right represent the radial velocity of source relative to the left and right ear respectively, which are derived from Eq. (2). We then apply C to WarpNet and BinauralGrad backbones, as follows.\n\nWarpNet\nWarpNet consists of two blocks: 1) The Neural Time Warping block to learn a warp from the source position to the listener's left ear and right ear while respecting physical properties (Richard et al., 2021) . This block is composed of a geometric warp and a parameterized neural warp. 2) The Temporal ConvNet block to model subtle effects such as room reverberations and output the final binaural audio. This block is composed of a stack of hyperconvolution layers. We replace the original C o with C for the input of parameterized neural warp and for the condition of hyper-convolution layers.\n\nBinauralGrad\nBinauralGrad consists of two stages: 1) The \"Common Stage\" generates the average of the binaural audio. The conditions for this stage include the monaural audio, the average of the binaural audio produced by the geometric warp in Warp-Net (Richard et al., 2021) , and C o . 2) The \"Specific Stage\" generates the final binaural audio. The conditions for this stage include the binaural audio produced by the geometric warp, the output of the \"Common Stage\", and C o . BinauralGrad adopts diffusion model for both stages, which is based on non-causal WaveNet blocks (Oord et al., 2016) with a conditioner block composed of a series of 1D-convolutional layers. We replace C o with C as the input of the conditioner block for both stages.\n\nExperiments\nIn this section, we first introduce the commonly used binaural dataset, and then introduce the training details for WarpNet-based and BinauralGradbased models. After that, we describe the evaluation metrics that we use to evaluate baselines and our methods. Finally, we provide the main results with analytical experiments on BAS.\n\nSetup\nDataset We evaluate our methods on the standard binaural dataset released by Richard et al. (2021) . It contains 2 hours of paired monaural and binaural audio at 48kHz from eight different speakers. Speakers were asked to walk around a listener equipped with binaural microphones. An OptiTrack system track the positions and orientations of the speaker and listener at 120Hz, which are aligned with the audio. We follow the original train-validation-test splits as Richard et al. (2021) and Leng et al. (2022) for a fair comparison.\nTraining Details We apply DopplerBAS on two open-source BAS systems WarpNet and Bin-auralGrad. We train 1) WarpNet and War-Net+DopplerBAS on 2 NVIDIA V100 GPUs with batch size 32 for 300K steps, and 2) BinauralGrad and BinauralGrad+DopplerBAS on 8 NVIDIA A100 GPUs with batch size 48 for 300K steps 3 .\nEvaluation Metrics Following the previous works (Leng et al., 2022; Lee and Lee, 2022) , we adopt 5 metrics to evaluate baselines and our methods: 1) Wave L2: the mean squared error between waveforms; 2) Amplitude L2: the mean squared errors between the synthesized speech and the ground truth in amplitude; 3) Phase L2: the mean squared errors between the synthesized speech and the ground truth in phase; 4) PESQ: the perceptual evaluation of speech quality; 5) MRSTFT: the multi-resolution spectral loss.\n\nMain Results\nWe compare the following systems: 1) DSP, which utilizes the room impulse response (Lin and Lee, 2006) to model the room reverberance and the head-related transfer functions (Cheng and Wakefield, 2001) to model the acoustic influence of the human head; 2) WaveNet (Richard et al., 2021; Leng et al., 2022) , which utilizes the WaveNet (Oord et al., 2016) model to generate binaural speech; 3) NFS, which proposes to model the binaural audio in the Fourier space; 4) WarpNet (Richard et al., 2021) , which proposes a combination of geometry warp and neural warp to produce coarse binaural audio from the monaural audio and a stack of hyper-convolution layers to refine coarse binaural audio; 5) WarpNet + DopplerBAS, which applies DopplerBAS to Warp-Net; 6) BinauralGrad (Leng et al., 2022) , which proposes to use diffusion model to improve the audio naturalness; 7) BinauralGrad + DopplerBAS, which applies DopplerBAS to BinauralGrad.\nThe results are shown in Table 1 . \"+ Doppler-BAS\" could improve both WarpNet and Binaural-Grad in all the metrics, especially in the Phase L2 metric. WarpNet + DopplerBAS performs best in the Phase L2 metric and reaches a new state of the Analysis We conduct analytical experiments for the following four velocity conditions. \"Spherical \u20d7 v \": the velocity conditions introduced in Section 2.2 are calculated in the spherical coordinate system; \"Cartesian \u20d7 v \": the velocity conditions are calculated in the Cartesian coordinate system; \"Zeros\": the provided conditions are two sequences of zeros; \"Time series\": the provided conditions are two sequences of time. The results are shown in Table 2 , where we place WarpNet in the first row as the reference. We discover that: 1) Radial relative velocity is the practical velocity component, which obeys the theory of the Doppler effect (row 2 vs. row 1); 2) The velocity condition is beneficial to binaural audio synthesis, even for the absolute velocity in the Cartesian coordinates (row 3 vs. row 1); 3) Just increasing the channel number of the condition C o (Section 2.2) by increasing the parameters in neural networks without providing meaningful information could not change the results (row 4 vs. row 1); 4) The neural networks do not explicitly learn the derivative of position to time (row 5 vs. row 1). These points verify the rationality of our proposed method.\n\nConclusion\nIn this work, we proposed DopplerBAS to address the Doppler effect of the moving sound source in binaural audio synthesis, which is not explicitly considered in previous neural BAS methods. We calculate the radial relative velocity of the moving source in the spherical coordinate system as the additional conditions for BAS. Experimental results show that DopplerBAS scales well to different types of backbones and reaches a new SOTA.\n", "hypothesis": " Recently, binaural audio synthesis (BAS) has emerged as a promising research field for its applications in augmented and virtual realities.  Binaural audio helps users orient themselves and establish immersion by providing the brain with interaural time differences reflecting spatial information.  However, existing BAS methods are limited in terms of phase estimation, which is crucial for spatial hearing.  In this paper, we propose the DopplerBAS method to explicitly address the Doppler effect of the moving sound source.  Specifically, we calculate the radial relative velocity of the moving speaker in spherical coordinates, which further guides the synthesis of binaural audio.  This simple method introduces no additional hyperparameters and does not modify the loss functions, and is plug-and-play: it scales well to different types of backbones.  DopperBAS distinctly improves the representative WarpNet and BinauralGrad backbones in the phase error metric and reaches a new state of the art (SOTA): 0.780 (versus the current SOTA 0.807).  Experiments and ablation studies demonstrate the effectiveness of our method..", "answer": true}
{"title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text", "content": "\nIntroduction and Background\nTextual Geolocation Identification, a crucial component of Geographic Information Retrieval (GIR), is the task of resolving the location, i.e., coordinates of a place, based on the reference to it in a text. It requires a combination of language and environmental knowledge. On top of the usual non-spatial linguistic challenges in Natural Language Understanding (NLU), such as named entity recognition (NER), anaphora resolution, bridging anaphora, etc., the textual geolocation task presents geospatial challenges that require multimodal processing and grounding (Ji et al., 2022; Fried et al., 2022; Misra et al., 2017; Qi et al., 2020; Paz-Argaman et al., 2020) .\nProper names, such as 'Rabin Square', also known as named entities in Natural Language Procesing (NLP), and as rigid designators in formal semantics (Kripke, 1972) , can be easily grounded based on a Gazetteer or a simple map. However, geolocating linguistic terms that involve spatial expressions without the explicit mention of a proper name still present an open challenge. This interpretation challenge includes the understanding and resolution of (at least): (i) definite descriptions, such as 'the school' (ii) geospatial terms, such as cardinal directions; 'east of'; and (iii) geospatial numerical reasoning; 'two buildings away from the pharmacy'. To address these and other challenges, we need to both ground entity mentions to their corresponding physical entities in the environment, and to reason about geospatial relations expressed between entities -these two processes being closely intertwined.\nTo do so, we need a corpus for the geolocation task that maps rich geospatial place descriptions to their corresponding location coordinates. However, current corpora for geolocation are based on naturally-occurring open-source resources, such as Wikipedia articles (Eisenstein et al., 2010; Wing and Baldridge, 2011; Han et al., 2012; Wing and Baldridge, 2014; Wallgr\u00fcn et al., 2018) , which are not spatially oriented, i.e., the description of locations is implicit or absent in the corresponding text. Subsequently, the accuracy of retrieval is fairly low (around 100 km).\nFurthermore, all geolocation datasets previously studied in NLP are in English, with a dearth of corpora for low-resource languages, in particular, for morphologically rich languages, such as Hebrew. To understand the geolocation challenges and build models that do various spatial reasoning tasks, English cannot be our sole focus (Baldridge et al., 2018) . Hebrew, a Semitic morphologically rich language is notoriously difficult to parse (Tsarfaty et al., 2020 (Tsarfaty et al., , 2019)) . Moreover, resources that are Place Description: The place is located near the Rothschild complex -at the end of Rothschild Street, as you go towards the sea, take a right for about three streets and then you will see the tower high above you. available for Hebrew NLP research focus on traditional tasks, such as Part-of-speech (POS) tagging, syntactic parsing, etc; and lack corpora for understanding and reasoning in real-world situations.\nIn this work we present HeGeL, a novel dataset for Hebrew Geo-Location, the first ever Hebrew NLU benchmark involving both grounding and geospatial reasoning. To create HeGeL, we crowdsourced 5,649 geospatially-oriented Hebrew place descriptions of various place types from three cities in Israel. We designed our task based on a realistic scenario of human place description, relying on people's memory of the world, rather than, e.g., using a map (Anderson et al., 1991; Paz-Argaman and Tsarfaty, 2019) . Crucially, relying on environmental cognition results in various levels of geospatial knowledge (Siegel and White, 1975) that are manifested in the descriptions and the geospatial reasoning that is required to resolve their location (Hayward and Tarr, 1995) . To avoid the much simpler task of grounding proper named entities, we explicitly restricted the use of proper names in the description of the place and adjacent landmarks.\nUnlike the text-based navigation task (MacMahon et al., 2006; Chen et al., 2019; Ku et al., 2020; De Vries et al., 2018; Thomason et al., 2020) , which requires representing an agent's current perspective, reflecting its route knowledge, we show that the HeGeL task requires a full-environment representation, thus, capturing complex geospatial relations among multiple physical entities. Through a thorough linguistic and empirical analysis, we demonstrate the characteristics and challenges associated with Hebrew place descriptions, showing that HeGeL serves both as a challenging NLU benchmark and as a corpus for geospatial cognition research.\n\nThe HeGeL Task and Dataset\nThis work addresses the task of geolocating places on a map based on natural language (NL) geospatial descriptions that are given in a colloquial language and based on participants' memory of the environment (i.e., cognitive map). The input to the HeGeL task is as follows: (i) an NL place description of the whereabouts of the place, and (ii) a map with rich details of the environment (e.g., physical entities names, geospatial relations, and attributes). The output is a pair of coordinates (x,y) specifying the physical location of the place described in the text. Figure 1 shows an example of a place description from HeGeL translated from Hebrew.\nTo simplify the crowdsourcing task and encourage participants' engagement, we frame the data crowdsourcing process as the well-known game, the treasure hunt task (Kniestedt et al., 2022) , in which the instructor-participant is required to describe in writing the location of the treasure, a known place in the city, to a different followerparticipant who then needs to locate it on a map. Thus, the online assignment is divided into two tasks: the instructor's writing of place descriptions and the follower's validation. To avoid preconceived notions as to the 'correct' way to describe a place, we first presented the participants with the task of writing a place description, and once completed, the validation task was given. 2 We hereby provide the details of the two UI tasks:\n(i) Task 1. Writing a place description In this task we requested participants to describe in a freeform text the location of a place known to them, to a third party who might not be familiar with the whereabouts of that place. To collect place descriptions based solely on people's memory, we did not visualize the area of the place, e.g., on a map. Instead, we ensured that the participants are well familiarized with the place by asking them to state how familiar they are with the place on a scale of 1-5. If this score was 1 or 2, we presented the participant with a different place to describe. To ensure diverse human-generated textual descriptions, places were chosen based on their type, position/location in the city (places were spread across the city), geometry, size, and context. To avoid the use of proper names, we developed a rule-based methodology to make sure that the explicit name of the goal (place) or of the nearby landmarks (< 100 meters) will not appear explicitly in the description. The original description was saved, and the participants were asked to input another description without the above names.\n(i) Task 2. Place description validation To verify that a person who reads the text description will understand where the treasure is hidden, i.e., geolocate the place, we developed a map-based retrieval task. The participant in the follower role was asked to read the crowdsourced textual description and mark its location on the map, i.e., where the treasure is hidden. For marking the location, we implemented an interactive online map based on OpenStreetMap (OSM), 3 which allows the participants to move and zoom-in to precisely pin the described place on the map. The map supports the cognitive process needed to ground mentioned entities to physical entities, reason about the geospatial relations, and locate the described place. To familiarize participants with the interactive map tool and task, they had to first pass a simple map marking test, and only then they could start task 2 of reading place descriptions (given by other participants), marking place locations on the map, and rate the clarity of the textual description on a scale of 1-5.\n\nTarget Selection and Retrieval Errors\nThe treasure-hunt task we devised included 167 places in the three largest cities in Israel: Tel Aviv, Haifa, and Jerusalem. These three cities are differently shaped, and show different physical, morphological and topographic features, which potentially affect the legibility and imageability of urban components, and therefore also on place descriptions. These differences can be expressed in the use of various physical features and prepositions, e.g., frequent use of the physical object 'landmark' and the prepositions 'above' or 'below' in hilly terrains that characterize Haifa and Jerusalem.\nTo assess the quality and interpretability of the place descriptions, we calculate the shortest Euclidean distance between the coordinates of the goal's (physical element) shape (polygon, line or point), and the location marked by the 'follower' on the map (task 2); we term this distance as retrieval error. To determine the agreement rate among human participants, each textual place description is validated by at least two participants. To ensure that we work with descriptions that can be geolocated, we set a hard distance threshold of 300 meters, based on analysis of the descriptions' clarity score that we had conducted on a prior (held-out) development corpus we collected for the task.\n\nData Statistics and Analysis\nThe resulting HeGeL dataset contains 5,649 validated descriptions paired with their coordinates on a map. The locations are divided among three cities: 2,142 in Tel Aviv, 1,442 in Haifa, and 2,065 in Jerusalem. 1,833 participants completed the writing task, inserting in total 10,946 place descriptions, and 2,050 participants completed 12,655 validation tasks. The dataset is balanced, with about 33 descriptions per place.\nFigure 2 shows a Venn diagram representing the relation of the three sets of city-based vocabularies (formed from unique lemmas produced by More et al. (2019) lemmatization tool). The intersection of the three cities contains only 15.07% of the entire vocabulary (the union of the three cities' vocabularies). The shared language is not focused on city-specific terms, such as 'Knesset'. Instead, it includes rich spatial terms, such as 'between', modified prepositions such as 'next to', and nondefinite entities, such as 'street'. From the Venn diagram we also conclude that almost half of the lemmas of the three vocabularies, corresponding to the three cities, contain city-specific lemmas: 48.6%, 40.65%, and 49.3% for Tel Aviv, Haifa, and Jerusalem, respectively. As such, HeGeL enables a city-split setup, training on one city and testing on a different unseen city, where city-reserved named entities present an out-of-vocabulary (OOV) challenge for models trained on another city.\nTable 1 shows an analysis of the linguistic phenomena manifested in the HeGeL dataset, demonstrating the spatial knowledge and reasoning skills required for solving the HeGeL task. We analyzed the frequency of the five types of elements in a city defined by Lynch (1960) , along with the three types of spatial knowledge defined in Siegel and White (1975) , and other spatial properties. The frequent use of cardinal directions, as well as the use of sur- vey knowledge, suggests that any NLP model built to deal with the HeGeL task should not only represent a local view of the goal, or possible routes, but also take into consideration the full region, and mimic people's map-like view of the environment. Therefore, unlike navigation tasks where only the agent's current perspective is represented in the model, this task requires full representation of the environment.\nWe further perform a quantitative analysis of word tokens and lemmas that appear in HeGeL, depicted in Table 2 . Overall, the HeGeL dataset contains a large vocabulary of 9,207 unique tokens and 6,663 unique lemmas. There are mentions of physical entities, but as we limited the mentions of named-entities of the described place and landmarks adjacent to it; these are relatively rare, and are mostly references to prominent city landmarks. Also, as most place descriptions are not route-based descriptions, there are only few verbs used in the descriptions. Prepositions, on the other hand, are abundant.\nIn Table 3 , using a one-way analysis of variance (ANOVA) test, we found a significantly (p<0.05) different distribution between place type descriptions and the following features: number of named entities, number of verbs, human verification retrieval error, and clarity score.\n\nExperiments\nWe create a zero-shot (ZS) city-based split, such that we train on one city and test on another. The train, development, and test sets correspond to the descriptions collected in Tel Aviv, Haifa, and Jerusalem, respectively. We evaluate different baseline models for the geolocation task on the HeGeL dataset. We use three evaluation metrics based on retrieval error: mean, median, and task completion (TC) accuracy -the percentage of place descriptions located within the 300 meters threshold. We provide three baselines for the HeGeL task.\nWe first assess a brute-force NER approach; i.e., we test whether recognizing named entities in the text and retrieving their corresponding coordinates is sufficient for solving the HeGeL task of geolocation. To this end, we used Google Maps API and produced two baseline models: (i) Google Maps API Query -we queried the API with the full raw text descriptions as input, with no prepossessing; and (ii) Oracle NER -we queried all 1-5 n-grams against Google Maps API and retrieved the closest geolocation to the goal.\nIn our second approach, we employ a dualencoder model. One encoder encodes the text using a Hebrew Monolingual pre-trained encoder, Aleph-BERT (Seker et al., 2022) , which produces a 768dimension vector representation of the text. The other encoder processes the environment, which is represented as a graph based on OSM data. Each point of interest in the graph is connected to an S2Cell 4 , which contains its geometry and is based on S2-geometry. These S2Cells are encoded using a random-walk algorithm to produce a 64dimensional vector for each cell. These vectors are then passed through a linear layer to produce 768-dimensional vectors. We calculate the cosine similarity score between the text and environment vectors and use it to align the respective representations via maximization of the cosine similarity score with a cross-entropy loss over the scores. 4 \n\nS2Cells\nare based on S2-geometry (https://s2geometry.io/), a hierarchical discretization of the Earth's surface (Hilbert, 1935) . Performing an ANOVA test, we found a significantly (p<0.05) different distribution between place type descriptions and the retrieval error of the Oracle NER. The mean retrieval error of the Path and Node place types were the lowest in both human verification and Oracle NER. This suggests that both of these place types are easier for humans to geolocate.\nThe results in Table 4 show that our task is not solvable with adequate resolution by the Google Maps API. The human performance provides an upper bound for the HeGeL task performance, while the simple Google Maps API Query provides a lower bound. The Google API model's low performance suggests that NER and the Gazetteerbased methods in and of themselves are insufficient to handle the HeGeL task successfully, and that geospatial reasoning is necessary. The Dualencoder's low performance on the ZS split suggests that OOV is a major challenge. The few-shot (FS) split shows an improvement of the model after finetuning on additional samples from the test-region (FS 20% and 80%). This suggests that a possible solution for the city-split setup might be dataaugmentation via generating grounded descriptions for the tested region -an approach we reserve for future research.\n\nConclusion\nThe contribution of this paper is threefold. First, we present the first geolocation benchmark with Hebrew place descriptions. Second, to the best of our knowledge, this is the only crowdsourced geolocation dataset, thus, eliciting explicit geospatial descriptions, allowing for better retrieval resolution. Finally, our analysis shows that the dataset presents complex spatial reasoning challenges which require novel environmental model representation. \n", "hypothesis": " The task of textual geolocation -retrieving the coordinates of a place based on a free-form language description -calls for not only grounding but also natural language understanding and geospatial reasoning.  Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect figurative place descriptions and analyze lingual geospatial reasoning.  We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel.  Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.", "answer": false}
{"title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification", "content": "\nIntroduction\nText simplification (TS) is a task in the field of natural language generation. It aims at rewriting a complex text into simple text while keeping the primary meaning intact (Laban et al., 2021) .\nRecently, several works have leveraged pretrained models for TS (Omelianchuk et al., 2021; Devaraj et al., 2022) . However, problems arise when pre-trained models are applied to TS directly. In the pre-training stage, the model hardly acquires the ability to generate simple texts. The improvement of results on simplification tasks relies almost on the fine-tuning stage. It can hurt the performance of pre-trained models, especially for lowresource sub-tasks like lexical simplification. One reason for this shortcoming is the pre-training strategy. It randomly masks text spans in ordinary texts, teaching the model to generate ordinary texts rather than simple texts.\nWe are committed to adapting the pre-trained model to TS in this paper. The pre-trained model has gained the ability to generate ordinary texts, and it is costly to start pre-training from scratch. Therefore, we focus on the continued pre-training strategy (Gururangan et al., 2020) . We first aim to continue pre-training on simple texts because it contains plenty of simple words. In TS, simple texts are derived almost from SimpleWiki (Zhang and Lapata, 2017) and Newsela (Xu et al., 2015) . We identify simple text spans in simple texts and dynamically replace them with <mask> tokens. Then, the pre-trained model will learn by reconstructing simple words. Meanwhile, we expect the pretrained model to learn from ordinary texts. We use a dictionary to replace complex words in ordinary texts with simple words. We also ensure the quality of the replaced sentences.\nBased on BART (Lewis et al., 2020) , we continue pre-training to teach it to generate simple texts and obtain SimpleBART. We then conduct experiments on three main tasks of TS: sentence simplification, lexical simplification, and documentlevel simplification. SimpleBART achieves consistent and noticeable improvements across several datasets on all three tasks over BART and several other baselines. The results illustrate that our proposed strategy helps the pre-trained model to gain the ability to generate simple texts.\nTo summarize, our contributions include: (1) We propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. (2) We continue pre-training BART, a representative seq2seq model, to obtain Simple-BART. It can be used for several simplification tasks and achieve consistent performance improvement. Code and SimpleBART will be released at https://github.com/RLSNLP/SimpleBART.\n\nMethodology\nAs illustrated in Figure 1 , our strategy is divided into two parts: learning dynamically to reconstruct simple words from simple texts and from ordinary texts where complex words are replaced with simple ones. \n\nMasking Simple Words in Simple Texts\nWe need to identify the simple words in simple texts at first. We take advantage of the DeepBlueAI model (Pan et al., 2021) that achieves state-of-theart results on the lexical complexity prediction task (Shardlow et al., 2021) . A text span of length n consists of n words. The input to the DeepBlueAI model is a text span and the output is a complex value between 0 and 1. The closer this value is to 0, the simpler the text span.\nUnlike the previous constant mask probability, in our strategy, the simpler a text span is, the higher its probability of being masked. This means that the mask probability is dynamic. We also set a complexity threshold of T . If the complexity c of a text span exceeds T , we will not mask this span. In our experiments, we set T to 0.25 as an empirical value. Following Lewis et al. (2020) , we set the max mask probability to 0.15, and the length of a text span obeys a Poisson distribution (\u03bb = 3). Finally, the mask probability m is calculated as:\nm = \uf8f1 \uf8f2 \uf8f3 0.15 \u00d7 (1 \u2212 1 T \u2022 c), c \u2264 T 0, c > T (1)\nThe function to mask the text span is denoted as g(\u2022). Given a sentence x, the pre-trained model will learn to reconstruct x from the masked sentence:\nl(x) = \u2212logP (x|g(x))\n(2)\n\nReplacing Complex Words in Ordinary Texts\nWe also expect the pre-trained model to learn helpful information from ordinary texts. However, ordinary texts contain more complex words than simple ones, making the pre-trained model learn to reconstruct simple words much less frequently. We introduce the dictionary SimplePPDB++ (Maddela and Xu, 2018) to address this issue. It contains millions of paraphrase rules with readability scores. Therefore, we can replace the complex words in ordinary texts with simple words. Then, the pretrained model will learn to reconstruct these simple words as in Eq.( 2). Nevertheless, a word may have different meanings in different sentences. Using a dictionary to replace complex words may change the meaning of the original sentence. Therefore, we use BERTScore (Zhang et al., 2019) to calculate the similarity between the original and replaced sentences to avoid this problem. We will discard the replaced sentences if the calculated BERTScore is lower than a similarity threshold. In our experiments, the similarity threshold is set to 0.95 as an empirical value.\n3 Experimental Settings\n\nContinued Pre-training\nWe select the BART-Large model to continue pretraining. It is a representative seq2seq model suitable for three main simplification tasks. We follow the task-adaptive pre-training method (Gururangan et al., 2020) and continue pre-training on the training set of the corresponding simplification task, ensuring that the continued pre-training texts have no intersection with the test set. We refer to the pretrained models obtained by our strategy collectively as SimpleBART.\n\nSimplification Tasks\nWe select three representative tasks for experiments: sentence simplification, document-level simplification, and lexical simplification. For sentence simplification, we conduct experiments on Wikiauto (Jiang et al., 2020) and Newsela (Xu et al., 2015) . Wikiauto is only a training set, so we use Turkcorpus (Xu et al., 2016) as its validation and test set. Following Sun et al. (2023) , we use SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2019) as the evaluation metrics. BLEU and FKGL have been proven to be unsuitable for evaluating simplification (Sulem et al., 2018; Tanprasert and Kauchak, 2021) . For document-level simplification, we conduct experiments on the D-Wikipedia dataset (Sun et al., 2021) . We use D-SARI (Sun et al., 2021) as the evaluation metric. For lexical simplification, we conduct experiments on LexM-Turk (Horn et al., 2014) and BenchLS (Paetzold and Specia, 2016) . We use precision, recall, and F1 score as the evaluation metrics. For more hyperparameter setting details, please refer to Appendix B.\n\nSentence Simplification\nTo demonstrate the advantages of our strategy, we develop BART-CP for a fair comparison. It continues pre-training with the same number of steps on the same data using the previous pre-training strategy from Lewis et al. (2020) We choose EditNTS (Dong et al., 2019) , T5base (Raffel et al., 2020) , and ControlTS (Maddela et al., 2021) as baselines. T5-base is close to Sim-pleBART in size. ControlTS achieves the state-ofthe-art result on the Newsela dataset. Following Alva-Manchego et al. ( 2021), BERTScore precision (BS) is also reported. From Table 1 , the BS scores of all outputs are high enough, which means that the outputs are of high quality. According to SARI, the most important automatic evaluation metric for sentence simplification, SimpleBART improves SARI values over BART by 1.2 points and 1.5 points, respectively. Overall, it achieves comparable results to the advanced model for the sentence simplification task. We also notice that Simple-BART outperforms BART-CP, demonstrating the effectiveness of our proposed strategy. The example outputs are given in Appendix D.\n\nLexical Simplification\nWe focus on generating suitable words using the pre-trained model, which is a critical step in lexical simplification. We follow Qiang et al. (2020a) and let the pre-trained models generate several candidate words. BenchLS and LexMTurk are just two test sets, so we continue pre-training on the Wikiauto training set. We choose Paetzold-NE (Paetzold and Specia, 2017a) and LSBert (Qiang et al., 2020b) As shown in Table 2 , SimpleBART improves the F1 scores over BART by 8.6 points and 9.7 points, respectively. It achieves comparable results to LSBert. The results also demonstrate that BART needs to gain the ability to generate simple words and the importance of introducing continued pretraining when training data is scarce.\n\nDocument-level Simplification\nSimpleBART also performs well on the documentlevel simplification task.\nWe choose Bert-Sumextabs (Liu and Lapata, 2019) , which achieves the state-of-the-art result on this task as a baseline. Compared with BART, SimpleBART improves the Table 4 shows that SimpleBART achieves the highest Simp score among all the simplification models, close to that of the reference. Simple-BART also significantly makes more word-level simplifications compared to BART and BART-CP.\n\nDomain Adaptation\nContinued pre-training using our strategy on taskrelated data can improve the results. However, we still want to know if continued pre-training on more data from the same domain and different domains will improve the results. We design the following experiments. 1) Exp1: We continue pre-training on more sentences from Wikipedia and SimpleWiki, except those contained in the Wikiauto dataset. 2) Exp2: We continue pre-training on more sentences in the Newsela corpus, except those contained in the Newsela dataset. From the results of Exp1 and Exp2 in Table 5 , continued pre-training on more texts from the same domain can still enhance the simplification results. Compared to BART in Table 1 , the SARI values improve by 0.6 and 1 point, respectively. From the results of Exp3 and Exp4, continued pre-training on more texts in a different domain can instead harm the results. Compared to BART, the SARI values decrease by 0.3 and 0.5 points, respectively. Thus, we suggest that future researchers use texts within the same domain (e.g., Wikiauto and Wikipedia) for continued pre-training in text simplification.\n\nGenerating Complex Texts\nThere are numerous studies dedicated to simplifying complex texts. Nevertheless, none has attempted to rewrite simple texts into complex ones. We make such an interesting attempt. We have changed our strategy to mask complex words and name the obtained model ComplexBART. When fine-tuning and testing on the Newsela dataset, we use simple texts as input and complex texts as reference. From Table 6 , ComplexBART improves the SARI value by 1.5 points over the BART model, indicating that the modified strategy can help the pre-trained model learn to generate complex texts. Thus, ComplexBART can serve as a better baseline for generating complex texts in the future.\n\nComparing SimpleBART with Large Language Models\nLarge language models (LLMs) have received widespread attention from researchers recently and have achieved state-of-the-art results on many natural language generation tasks. In this section, we select several representative large models to conduct experiments on text simplification and compare them with SimpleBART. We hope these results can serve as baselines for future research.\nWe choose those LLMs that provide API or model files to ensure reproducibility. We choose GPT-3.5-Turbo-0301 1 , FLAN-T5-XL (Chung et al., 2022) , and LLaMA-7B (Touvron et al., 2023) as LLM baselines and use zero-shot generation. Then, we follow the implementation 2 and fine-tune FLAN-T5-base as another baseline. We collect the training sets of Wikiauto, Newsela, and D-Wikipedia and conduct instruction fine-tuning.\n\nComparison and Analysis\nThe comparison of SimpleBART results with those of the LLMs is shown in Tables 7, 8, and 9 .\nFor the sentence-level simplification task, LLaMA and FLAN-T5-XL seem unable to understand the prompt for simplifying sentences, and they are inclined to repeat the original text. However, FLAN-T5-base, only 10% of the parameters of the above two models, performs better. It illustrates fine-tuning phase can improve performance when the model is not super large. It may be a little strange that GPT-3.5 performs worse than Sim-pleBART. We find that with the zero-shot setting, GPT-3.5 may not know the \"degree of simplification\" we want. It makes many reasonable changes to the original text, but it also keeps some of the complex parts of the original text.\nFor the document-level simplification task, LLaMA over-repeats sentences from the original article, and the generated text is difficult to read. The shortcomings of GPT-3.5 are similar to those of the sentence-level simplification task. Besides, limited by the number of API accesses per minute of OpenAI, we only select 1000 original documents for simplification, which takes nearly five hours.\nFor the lexical simplification task, neither the LLaMA nor the FLAN-T5 model could understand the instruction to replace complex words with simple words. However, GPT-3.5 outperforms the other models substantially. We also find that GPT-3.5 makes many sensible substitutions not included in the reference, such as replacing \"acquired\"with \"earned\". Such results illustrate that LLMs are dominant for this task.\n\nConclusion\nIn this paper, we are committed to adapting the pre-trained model to text simplification. We propose a new pre-training strategy to allow the pretrained model to learn to generate simple texts. The adapted pre-trained model improves the results on various simplification tasks.\n", "hypothesis": " Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts.  It can hurt the performance of pre-trained models on text simplification tasks.  In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts.  We continue pre-training BART, a representative model, to obtain SimpleBART.  It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART.  At the end, we compare Sim-pleBART with several representative large language models (LLMs)..", "answer": true}
{"title": "Randomized Positional Encodings Boost Length Generalization of Transformers", "content": "\nIntroduction\nTransformers are emerging as the new workhorse of machine learning as they underpin many recent breakthroughs, including sequence-to-sequence modeling (Vaswani et al., 2017) , image recognition (Dosovitskiy et al., 2021) , and multi-task learning (Reed et al., 2022) . However, recent work (Del\u00e9tang et al., 2023) demonstrated that Transformers fail to generalize to longer sequences on seemingly simple tasks such as binary addition. Thus, while certain problems can be solved without length generalization, algorithmic reasoning generally requires this ability, similar to many real-world settings such as online or continual learning.\nWhile the Transformer's attention mechanism can recognize complex relationships amongst to- kens in the input sequence, it is limited by its lack of positional awareness. Thus, the input sequence is generally augmented with positional encodings to inject position information into the computation. However, current approaches only consider positions up to the maximum training sequence length N , and thus all the positions N + 1, . . . , M for test sequences of length up to M will appear out-ofdistribution during evaluation (top of Fig. 1 ).\n\nThis work\nWe introduce a novel family of randomized positional encodings, which significantly improves Transformers' length generalization capabilities on algorithmic reasoning tasks. Our approach is compatible with any existing positional encoding scheme and augments the existing methods by subsampling an ordered set of positions from a much larger range of positions than those observed during training or evaluation (i.e., up to L \u226b M ; bottom of Fig. 1 ). Thus, over the course of training, the Transformer will learn to handle very large positional encodings and, therefore no longer encounter out-of-distribution inputs during evaluation. Importantly, our method leaves in-domain generalization performance unaffected and is also significantly more efficient than the naive approach of simply training the Transformer on longer sequences. Our main contributions are:\n\u2022 A novel family of positional encoding schemes that significantly improves the length generalization capabilities of Transformers, while leaving their in-domain generalization performance unaffected.\n\u2022 A large-scale empirical evaluation on a wide range of algorithmic reasoning tasks showing the superiority of our method over prior work (an increase of the test accuracy by 12.0% on average and up to 43.5% on certain tasks).\n\u2022 An open-source implementation of our method, available at https://github. com/deepmind/randomized_ positional_encodings.\n\nRelated Work\nOur work is most closely related to the growing line of research on Transformers' positional encodings. The first approaches simply added a transformation of the tokens' positions, e.g., scaled sinusoids (Vaswani et al., 2017) or learned embeddings (Gehring et al., 2017) , to the embeddings of the input sequence. Dai et al. (2019) subsequently showed that computing the attention (at every layer) using the relative distances between the key and query vectors improves the modeling of long-term (inter-context) dependencies. Similarly, Su et al. (2021) proposed to inject position information by rotating the key-query products according to their relative distances. Finally, Press et al. (2022) improved the length generalization on natural language processing tasks by adding a constant bias to each key-query attention score (proportional to their distance). However, as our experiments in Section 4 will show, these approaches fail at length generalization on algorithmic reasoning tasks, which is precisely the goal of our work.\nA concurrent work developed randomized learned positional encodings (Li and McClelland, 2022) , which are a special case of our family of randomized positional encodings. We also note that the necessity of feature and position randomization for length generalization has been discussed in the context of graph neural networks, which subsume Transformers (Ibarz et al., 2022; Sato et al., 2021) . Finally, Liu et al. (2020b) proposed to model the position information as a continuous dynamical system in an effort to handle sequences longer than those seen during training time.\nOur work is also related to the research area on improving the systematic (length) generalization capabilities of Transformers (Onta\u00f1\u00f3n et al., 2022) , which includes approaches investigating embedding scaling or early stopping (Csord\u00e1s et al., 2021) , adaptive computation time (Dehghani et al., 2019) , geometric attention with directional positional encodings and gating (Csord\u00e1s et al., 2022) , and hierarchical reinforcement learning (Liu et al., 2020a) . Such length generalization studies are often conducted in the context of formal language theory, and we evaluate our method on the recent benchmark by Del\u00e9tang et al. (2023) , which unifies a large body of work on Transformers' capability to recognize formal languages (Ackerman and Cybenko, 2020; Bhattamishra et al., 2020; Ebrahimi et al., 2020; Hahn, 2020; Hao et al., 2022; Merrill, 2019; Merrill and Sabharwal, 2022) .\n\nRandomized Positional Encodings\nUnlike RNNs (Elman, 1990) , which are unrolled over tokens one step at a time, Transformers process large chunks of the input sequence in parallel via global attention (Vaswani et al., 2017) . As a result, Transformers do not need to \"remember\" previous tokens, but they do have to break the permutation-invariance of the attention mechanism. To that end, the embeddings of the input sequence are generally augmented with positional encodings. For example, the vanilla Transformer adds the following positional encodings to the embedded input sequence before passing it to the attention layers: While positional encodings generally succeed at inducing the required positional information for sequences of fixed length, they are one of the main failure modes preventing length generalization. Concretely, for a Transformer with standard positional encodings trained on a curriculum of sequences of maximum length N , test sequences of length M > N will shift the distribution of the resultant positional encodings away from those seen in training, with the shift getting increasingly large as M grows. To address this, we propose a randomized encoding scheme, which relies only on order information, and can be expected to generalize up to sequences of length M , where N < M \u2264 L, with a configurable hyperparameter L.\n\nRandomized positional encodings\nWe assume that each training step will perform a step of loss minimization on a batch of data of fixed size. Let U(S) denote the discrete uniform distribution over set S, and let P k := {S \u2286 {1, . . . , L} | |S| = k}. For each training step, we first sample a random length n \u223c U({1, . . . , N }) (following Del\u00e9tang et al., 2023) and then a random set of indices I \u223c U(P n ). We then sort I in ascending order, such that I = {i 1 , . . . , i n } for i 1 < i 2 < \u2022 \u2022 \u2022 < i n , noting that I is sampled without replacement. Finally, we compute our randomized positional encoding for token 1 \u2264 j \u2264 N as RPE(j, \u2022) := PE(i j , \u2022). At test time, when processing a sequence of length M > N , we use the same procedure but for all token positions 1 \u2264 j \u2264 M . The intuition behind our method is to preserve the known good properties of relative encoding but in a way that is independent of the maximum training length N and thus allows generalization to longer sequences at test time.\nWhen applying our randomized positional encoding scheme, we subsample the extended positions only once per batch and not individually for every sequence. For the sin / cos (Vaswani et al., 2017) , learned (Gehring et al., 2017) , and RoPE encodings (Su et al., 2021) , we apply our method as described above, i.e., we directly replace the original token positions with their sampled counterpart. For the relative encoding (Dai et al., 2019) , we compute the relative distances between the sampled positions instead of the original positions. Finally, for ALiBi (Press et al., 2022) , we sample the bias values from the set of extended positions.\nAs a consequence, our tokens' positional encodings are no longer directly related to their exact position (the encodings even change during training as they are resampled at every step). However, since we maintain the order of the encodings, the Transformer can still learn to extract the relevant positional information from the subsampled encodings. Indeed, we validate the necessity of ordering the sampled positions in our ablation study in Appendix B.1. Thus, the success of our encoding scheme offers an interesting insight into the inductive biases of the Transformer architecture.\nAs we will show in Section 4, our randomized encodings trained only on lengths up to N perform the same on sequences of length M as prior approaches trained on lengths up to M . Therefore, our method demonstrates that Transformers can be efficiently trained on short sequences as long as (i) the longer sequences share the same structure and (ii) the longer positions are observed during training. Moreover, as the running time of global attention is O(\u2113 2 ) for sequence length \u2113, our encoding scheme is significantly faster than directly training a model on long sequences. Furthermore, we also note that our randomized positional encoding scheme significantly boosts length generalization while leaving the in-domain generalization performance largely unaffected (see Fig. 4 ).\nThe main limitation of our approach is that the maximum test sequence length M has to be known in advance to choose L \u226b M . However, our method is compatible with a wide range of values for L (see Appendix B.1), and we note that this is a much weaker assumption than that required for the naive approach of simply training on longer sequences. However, note that if L is chosen to be much larger than N or M , it is theoretically unlikely for the model to encounter enough unique indices during training, likely leading to poor performance (both in-and out-of-distribution).\n\nExperimental Evaluation\nProblem setup We closely follow the experiment setup of Del\u00e9tang et al. (2023) and evaluate our method on a wide range of algorithmic reasoning tasks such as modular arithmetic, reversing/duplicating a string, binary addition/multiplication, and bucket sort. The tasks are derived from formal language recognition and thus grouped according to the Chomsky hierarchy (Chomsky, 1956) , which partitions languages into regular (R), context-free, context-sensitive (CS), and recursively enumerable. Regular tasks can be solved by a finite-state automaton (FSA), deterministic context-free (DCF) tasks can be solved by an FSA with access to a deterministic stack, and Table 1 : Accuracy (in percentage) averaged over all test lengths and maximized over 10 random seeds and 3 learning rates. The random accuracy is 50%, except for MODULAR ARITHMETIC (SIMPLE), CYCLE NAVIGATION, BUCKET SORT, and MODULAR ARITHMETIC, where it is 20%. Our randomized method increases the test accuracy by 12.0% on average. The randomized learned encodings (denoted with \u22c6) are equivalent to label-based encodings (Li and McClelland, 2022) . \u2020 denotes permutation-invariant tasks, which can be solved without positional information. CS tasks can be solved by an FSA with access to a bounded tape. Note that the relation to the Chomsky hierarchy is largely irrelevant for our work and only included for completeness. We evaluate our method on Del\u00e9tang et al. ( 2023)'s benchmark as it is currently out of reach for Transformers and clearly demonstrates their failure to generalize on algorithmic reasoning tasks. We refer interested readers to the original paper for more details.\nWe consider the encoder-only model of the original seq-to-seq Transformer (Vaswani et al., 2017) , as used in popular pre-trained language models such as BERT (Devlin et al., 2019) or Gopher (Rae et al., 2021) . Thus, for tasks that require a multitoken output sequence y (e.g., duplicating a string), we pad the input sequence with |y| empty tokens and compute the entire Transformer output from the padded sequence (i.e., we do not use autoregressive sampling). We train the model on sequences of length sampled uniformly from U(1, N ), with N = 40, and evaluate it on sequences of length {N + 1, . . . , M }, with M = 500. We set the maximum position L = 2048 (and visualize the impact of other values on the performance in Appendix B.1). We report the accuracy averaged over all unseen sequence lengths, i.e., N + 1, . . . , M , for the best-performing model out of 10 different parameter initialization seeds and three learning rates 1 \u00d7 10 \u22124 , 3 \u00d7 10 \u22124 , 5 \u00d7 10 \u22124 . We use the same hyperparameters as Del\u00e9tang et al. (2023) and provide the full experiment setup in Appendix A. We make our code publicly available at https://github.com/deepmind/ randomized_positional_encodings.\nComparison to prior work We compare our method to a wide range of positional encodings: none, sin / cos (Vaswani et al., 2017) , relative (Dai et al., 2019) , ALiBi (Press et al., 2022) , RoPE (Su et al., 2021) , learned (Gehring et al., 2017) , and label-based (Li and McClelland, 2022) . Note that the label encodings proposed by Li and McClelland (2022) are equivalent to randomized learned positional encodings and thus subsumed by our method. We instantiate our randomized positional encoding scheme with all the above encodings and show the average test accuracy in Table 1 (with performance curves over test lengths in Appendix B.2). We observe that our randomized versions significantly increase the test accuracy across most tasks (by 12.0% on average and up to 43.5%). In particular, the randomized relative encoding solves tasks that were previously out of reach for prior work (e.g., REVERSE STRING or MISSING DUPLICATE).\n\nEfficiency comparison\nWe now show that our method allows us to train a model on short sequences and obtain a test accuracy above 90%, roughly 35.4 times faster than the naive approach of training a model on longer sequences. To that end, we train the randomized relative encodings on sequences up to length 40 and the classical relative positional encoding (Dai et al., 2019) up to length 500 and show the test accuracy (averaged over lengths 41 to 500) in Fig. 2 over training time (in seconds). Our model obtains a strong test accuracy significantly faster due to the quadratic cost (in terms of sequence length) of global attention, which means that our model trains at 168.4 steps per second compared to 22.1 steps per second for the naive approach (on a NVIDIA V100 GPU).\n\nConclusion\nWe introduced a novel family of positional encodings that significantly improves the length generalization capabilities of Transformers. Our positional encodings are based on the insight that conventional positional encodings will be out-ofdistribution when increasing the sequence length. Thus, to overcome this issue, we randomly sample our encodings from a wider range than the lengths seen at test time while keeping the order. Our largescale empirical evaluation demonstrates that our method significantly outperforms prior work in terms of length generalization while offering superior computational performance over the naive approach of training the model on longer sequences.\n", "hypothesis": " Transformers have impressive generalization capabilities on tasks with a fixed context length.  However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string.  Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism.  In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem.  Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average) and effectively addresses the issue of positional awareness by incorporating context-based positional encodings.", "answer": false}
{"title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings", "content": "\nIntroduction & Related Work\nTransformer models have become the backbone of natural language processing applications (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019) . Within the transformer architecture, there are two main categories: 1) bidirectional models, such as BERT (Devlin et al., 2019) , that are trained using the masked language modeling objective, and 2) (causal) language models, such as GPT (Radford et al., 2019) , that are trained using the traditional language modeling objective. Both of these categories share the common feature of using positional embeddings for encoding token distance.\nWhether positional embeddings are truly essential has been a subject of ongoing research. While they have been considered necessary for bidirectional transformer models (Lee et al., 2019; Luo et al., 2021; Sinha et al., 2021; Haviv et al., 2022) , the situation is different for transformer language models (Irie et al., 2019; Yang et al., 2019; Tsai \u2020 Correspondence to: tachungc@andrew.cmu.edu et al., 2019; Scao et al., 2022; Haviv et al., 2022) . In transformer language models, the removal of positional embeddings results in only a marginal decline in performance, while enabling more efficient training (Haviv et al., 2022) . In addition to empirical evidence, it has been proven (Bhattamishra et al., 2020) that transformer language models without positional embeddings are Turingcomplete and able to model sequences akin to recurrent neural networks (Rumelhart and McClelland, 1987; Jordan, 1986) . Despite this, it remains an open question where positional information is stored in the absence of positional embeddings. This motivates further investigation into individual operations within a transformer layer.\nThe example architecture of a pre-LN (Xiong et al., 2020) work is shown in Figure 1 . 1 We hereinafter refer to this configuration as TLM. Our primary focus is on the multi-head attention (MHA) module of a randomly initialized TLM, as it is the only module that allows inter-token information exchange. To gain a deeper understanding, we compute the mean and variance of MHA outputs. To our surprise, we discover that the variance already encodes latent positional information, with later tokens in a sequence displaying smaller variance. This motivates us to quantify the variance by deriving the output distribution after MHA operations. Finally, through empirical validation using a fully pre-trained TLM, we confirm thatthe same variance shrinkage effect persists after extensive gradient updates.\nTo the best of our knowledge, we are the first to identify and quantify the latent positional information in TLMs. Our results provide theoretical insights into the removal of positional embeddings, enabling more efficient pretraining of future TLMs.\n\nProbing Experiments\nGiven BERT and TLM (GPT) with positional embeddings removed, prior work (Haviv et al., 2022) shows that only TLM is able to maintain the same language modeling performance as its original version with positional embeddings. The discrepancy might be explained by the fact that only TLM encodes positional information within its layers, as shown by the position probing experiment in Haviv et al. (2022) . Since both BERT and TLM have access to the same semantic input and the only difference is the use of causal attention masks in TLM, we hypothesize that the positional informa-tion may be attributed to the interaction between causal attention masks and the TLM architecture.\nTo further explore this hypothesis, we use a randomly initialized and frozen TLM to eliminate any semantic influence and focus solely on the architectural design. Additionally, to prevent the model from memorizing the order of input sequences, we do not perform embedding lookups and feed the model with randomly sampled input vectors. A trainable two-layer linear classifier with ReLU activation in between was appended to the TLM to probe the position of each token (further details can be found in Appendix B). We plot the mean absolute error (MAE) w.r.t the number of transformer layers in Figure 2 . The plot indicates a randomly initialized and frozen TLM with randomly sampled input vectors inherently provides positional information, with an increase in the number of layers resulting in higher probing performance. This surprising outcome prompts further investigation into the encoding of latent positional information inside the TLM architecture.\n\nTheoretical Analysis\nWe dissect the inner workings of a TLM by deriving the distribution of TLM operations in the hope that they elucidate where the latent positional information is stored. The derivation is made possible thanks to the usage of a randomly initialized and frozen TLM. We adopt the initialization settings in accordance with those employed in GPT (Radford et al., 2019) . WLOG, our derivation is limited to the operations of the first layer in a TLM and the FFN component is omitted (justified in \u00a73.4). The hyperparameters utilized in the simulations are: hidden dimension d = 768, number of attention heads H = 12, head dimension d/H = 64, sequence length L = 512, standard deviation for initialization \u03c3 = 0.02. All proofs of lemmas are deferred to Appendix A.\nGiven a sequence of randomly sampled input embeddings {x m } L m=1 , where each element of x m \u2208 R d is sampled i.i.d from N (0, \u03c3 2 ), a TLM consists of the following operations:\n\nLayer Normalization\nFor each input embedding x m , it computes the sample mean and (biased) sample variance: Then each entry i of x m , denoted as x mi , is normalized by mean and variance to e mi :\ne mi = x mi \u2212 x m,: S(x m,: ) * \u03b3 + \u03b2 ( * ) \u2248 x mi \u2212 E[x mi ] V[x mi ] \u223c N (0, 1),\nwhere V[x] denotes the variance of x. Since the initialization scheme sets \u03b3 = 1 and \u03b2 = 0, ( * ) holds with sufficiently large d by the Law of large numbers and the continuous mapping theorem.\n\nSelf Attention\nEach attention head computes query, key, and value vectors in R d H :\nq m = W q e m , k m = W k e m , v m = W v e m ,\nwhere\nW q , W k , W v \u2208 R d H \u00d7d are matrices with each element sampled i.i.d from N (0, \u03c3 2 ).\nTo be precise, most matrices (W\n(h) q , W (h) k , W (h) v ), vectors (q (h) m , k (h) m , v (h)\nm ), and scalars (l\n(h) mn , a (h)\nmn ) are associated with a head number h. For notation simplicity, we only show the dependency on h when we need it.\nLemma 1. q m , k m , and v m have zero mean and (d\u03c3 2 ) \u2022 I covariance matrix.\nThe resulting vectors are processed by the selfattention module for pre-Softmax logits:\nl mn = q m , k n , if m \u2265 n \u2212 inf, otherwise 0 1 2 3 4 5 6\nLog Positions Log Variance Theoretical@Layer 0 Simulation@Layer 0 Simulation@Layer 5 Simulation@Layer 11 followed by the scaled softmax normalization:\na mn = exp l mn / d/H L i=1 exp l mi / d/H\nLemma 2. l mn has zero mean and In Figure 3 , we verify Property 1 by showing that a mn is almost evenly distributed in simulation.\nObserve that the output vector o m at position m is:\no m = W o \u2295 H h=1 L n=1 a (h) mn v (h) n ,\nwhere \u2295 denotes the concatenation of vectors from all H attention heads. Assume that Property 1 is valid and that W o \u2208 R d\u00d7d has elements i.i.d sampled from N (0, \u03c3 2 ), we derive the distribution of o m below. Simulation@ =0.2 Theoretical@ =0.2 Simulation@ =0.02 Theoretical@ =0.02 Simulation@ =0.002 Theoretical@ =0.002 Figure 4 is a simulation that verifies Lemma 3 under the assumption of Property 1. We can see that the variance of o m already encodes the positional information m.\n\nResidual Connection\nAs denoted by the Addition block of Figure 1 , the residual connection sets the output as y m = x m + o m . It allows the model to pass the first MHA output to later MHA modules as well as the final classifier. As the positional information has been passed by the residual connection, we omit the FFN part in our analysis.\n\nThe Final Layer Normalization\nLayer normalization is an operation that might eliminate the positional information derived in Lemma 3, which happens before the MHA modules and position classifier. As mentioned in \u00a73.1, LN(y m ) gives:\ny mi \u2248 y mi \u2212 E[y mi ] V[y mi ] \u2248 x mi + W o W v m n e ni m \u03c3 2 + d 2 \u03c3 4 m , E[y mi ] = 0, V[y mi ] = V[x mi ] + V[o mi ] = \u03c3 2 + d 2 \u03c3 4 m\nLemma 4. The variance of the j-th dimension of y m is:\nm\u03c3 2 + i (W o,j: W v,:i ) 2 m\u03c3 2 + d 2 \u03c3 4 , where W o,j: \u2208 R 1\u00d7d is the j-th row of W o . W v,:i \u2208 R d\u00d71 is the i-th column of W v . As long as i (W o,j: W v,:i ) 2 = d 2 \u03c3 4\n, the classifier should be able to exploit the discrepancy to derive m. Readers might wonder why W o,j: and W v,:i in the numerator cannot be treated as random variables. The reason is that we only focus on one dimension (j-th) at a time. This means we cannot use the law of large numbers to approximate the sample variance of y mj as we did for the denominator.\n\nRelaxing the Assumptions\nWe discuss possible relaxation of the assumptions used in \u00a73.2.\nWhat if Property 1 does not hold? Or equivalently, \u03c3 4 H d 2 . This prompts us to vary the value of \u03c3. In Figure 5 , we see that smaller \u03c3 better aligns Lemma 3 with the simulations, which is unsurprising as Lemma 3 assumes small \u03c3. Even when \u03c3 is not too small (i.e., \u03c3 = 0.2, 0.02), the variance still encodes the positional information as the variance of o m is negatively correlated with its position m.\nOther Initialization Schemes So far we assume the weight matrices (W q , W k , W v , W o ) are initialized i.i.d from N (0, \u03c3 2 ). However, we can relax the assumption to i.i.d. samples from a distribution with zero mean and finite variance. This is because the proof in Appendix A calculates the covariance. The variance calculation relies on E[r i r i ] = \u03c3 2 I where r i is the i-th row vector of a weight matrix. This property holds for any distribution with zero mean and \u03c3 2 variance.\n\nDiscussions\nWhy are the positions of later tokens in a sequence harder to be predicted in Figure 3 of Haviv et al. (2022) ? Lemma 3 states the variance is inversely proportional to the position m, so the variance of later tokens (large m) plateaus, resulting in a harder numerical optimization problem. This also suggests a potential downside of removing positional embeddings: It might be challenging for the model to infer positional information of the later tokens in extremely long input sequences.\nWhy do lower layers (closer to input) give worse probing performances in both Figure 2 Why does BERT fail to converge without positional embeddings? In a BERT model (Devlin et al., 2019) , each token has access to all the other tokens, making the variance at all positions d 2 \u03c3 4 L . Therefore, a BERT model cannot utilize variance differences as its positional indicator.\n\nPost-Training Results\nOur derivations only apply to the initial stage where the TLM and input embeddings are randomly initialized, which may not hold true after gradient updates. It is essential to verify the existence of variance properties and lemmas on a fully pre-trained TLM on OpenWebText2 (details in Appendix C).\nWe expect that the properties of lower layers of a pre-trained TLM should align more closely with the theoretical results for two reasons: 1) There are more steps between the lower layers and the final language modeling loss, resulting in smaller gradients and thereby fewer parameter updates, and 2) Lower layers typically encode more lowlevel information dependent on positional information (Vuli\u0107 et al., 2020; de Vries et al., 2020) . Figures 6 and 7 demonstrate that the 0 th (lowest) layer exhibits highly similar cumulative attention probability and decay-with-position variance as the theoretical results. In contrast, higher layers deviate from the analyses in \u00a7 3. We posit that the model learns to rely more heavily on semantic rather than positional information. This also explains why We average over all heads in a layer and 500 samples. predicting positions using outputs of higher transformer layers is more challenging as demonstrated in Figure 2 of Haviv et al. (2022) .\n\nConclusion\nWe mathematically analyzed a randomly initialized transformer language model without positional embeddings. We showed that the variance of the selfattention output decreases as the position increases, which serves as an indicator for positional information. We validated that, after extensive gradient updates, the low layers of a pretrained language model still exhibit highly similar variance reduction behaviors. Our results pave the way for the pretraining of more efficient and positional embedding-free transformer language models.\n", "hypothesis": " The use of positional embeddings in transformer language models is widely accepted.  However, recent research has called into question the necessity of such embeddings.  We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance.  To quantify this variance, we derive the underlying distribution of each step within a transformer layer.  Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates.  Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models..", "answer": true}
{"title": "Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases", "content": "\nIntroduction\nSocial science studies have shown that individuals may face race or gender discrimination based on demographic attributes inferred from names (Bertrand and Mullainathan, 2004; Conaway and Bethune, 2015; Stelter and Degner, 2018) . Similarly, large language models exhibit disparate behaviors towards first names, both on the basis of demographic attributes (Wolfe and Caliskan, 2021) and prominent named entities (Shwartz et al., 2020) . Such model behavior may cause representational harms (Wang et al., 2022a) if names associated with socially disadvantaged groups are in turn associated with negative or stereotyped attributes, or allocational harms (Crawford, 2017) if models are deployed in real-world systems, like resume screeners (O'Neil, 2016; Blodgett et al., 2020) .\nThe task of social commonsense reasoning (Sap et al., 2019; Forbes et al., 2020) , in which models must reason about social norms and basic human psychology to answer questions about interpersonal situations, provides a particularly fruitful setting Figure 1 : A social commonsense reasoning multiplechoice question example identified by SODAPOP (An et al., 2023) where the model differentially associates \"Nichelle\" with \"violent\" and \"Nancy\" with \"quiet\". Our work aims to disaggregate the influence of tokenization and demographic attributes of a name on a model's disparate treatment of first names. We obtain the race statistics from Rosenman et al. (2022) .\nfor studying the phenomenon of name biases in NLP models. Questions in the Social IQa dataset (Sap et al., 2019) , for example, describe hypothetical social situations with named, but completely generic and interchangeable, participants (e.g. \"Alice and Bob\"). Social IQa questions require models to make inferences about these participants, yet they maintain the convenient property that correct (or best) answers should be invariant to name substitutions in most or all cases.\nLeveraging this invariance property, prior work (An et al., 2023) has demonstrated that social commonsense reasoning models acquire unwarranted implicit associations between names and personal attributes based on demographic factors (Fig. 1 ). Building upon this finding, we investigate a natural follow-up question: why?\nWe identify two possible factors that cause a model's disparate treatment towards names: demographic attributes and tokenization length. We hypothesize that names associated with different demographic attributes, in particular race, ethnicity, and gender may cause a model to represent and treat them differently. These demographic Figure 2 : Histograms of first names by tokenization lengths (2a, 2b), race/ethnicity (2c), or gender (2d). We normalize the count to 1 and show the distribution by percentage. Raw count plots are in appendix A.\nattributes are also strongly correlated with corpus frequency and tokenization length (Wolfe and Caliskan, 2021). Tokenization (or segmentation) breaks down an input sentence into a series of subword tokens from a predefined vocabulary, each of which is then, typically, mapped to a word embedding as the input to a contemporary language model. A name's tokenization length refers to the number of subwords in the name following tokenization. In this work, we refer to singly tokenized and multiply tokenized names as those consisting of one or multiple tokens after tokenization, respectively. As a result, singly tokenized names are represented with a single embedding vector, while multiply tokenized names are represented by two or more. With these potential confounds, we attempt to address the research question: In social commonsense reasoning, to what extent do demographic attributes of names (race, ethnicity, and gender) and name tokenization length each have an impact on a model's treatment towards names?\nWe first conduct an empirical analysis to understand the distribution of tokenization lengths in names given demographic attributes, and viceversa. Adopting the open-ended bias-discovery framework, SODAPOP (An et al., 2023) , we then analyze the impact of demographic attributes and tokenization length on model behavior. We find that both factors have a significant impact, even when controlling for the other. We conclude that due to correlations between demographics and tokenization length, systems will not behave fairly unless both contributing factors are addressed. Finally, we show that a na\u00efve counterfactual data augmentation approach to mitigating name biases in this task is ineffective (as measured by SODAPOP), concluding that name biases are primarily introduced during pre-training and that more sophisticated mitigation techniques may be required.\n\nDemographic Attributes and\nTokenization Length are Correlated\nPreviously, Wolfe and Caliskan (2021) have shown that White male names occur most often in pretraining corpora, and consequently, White male names are more likely to be singly tokenized. We replicate this finding by collecting 5,748 first names for 4 races/ethnicities (White, Black, Hispanic, and Asian) and 2 genders (female and male) from a U.S. voter files dataset compiled by Rosenman et al.\n(2022) (specific data processing and name inclusion criteria are in appendix B.1). We compute and plot the conditional probabilities of tokenization length given demographic attributes (race/ethnicity and gender) and vice-versa in Fig. 2 using the BERT tokenizer (Devlin et al., 2019; Wu et al., 2016) . Let ST be the event that a name is singly tokenized. We see in Fig. 2 that P (White|ST ), P (ST |White), P (Male|ST ), and P (ST |Male) are substantially higher than other conditional probabilities involving ST 1 , confirming Wolfe and Caliskan (2021). These observations suggest that a model tends to represent White names and male names differently from others in terms of the tokenization length. Given these substantial differences in tokenization lengths across demographic groups, we are motivated to investigate whether tokenization is a primary cause of disparate treatment of names across demographic groups. It is important to note here that, even if tokenization were the primary cause of disparate treatment of names across demographic groups, this discovery would not in itself resolve the fairness concerns of representational and allocational harms based on race, ethnicity and gender, but it might point to possible technical solutions. However, as we will show in the next section, dis- parate treatment of names across demographic attributes persists strongly even when controlling for tokenization length (and vice-versa).\n3 Analyzing the Influences via SODAPOP\nWe follow SODAPOP (An et al., 2023) to investigate how the two factors in \u00a7 2 influence a Social IQa model's behavior towards names.\n\nExperiment Setup\nSODAPOP leverages samples from Social IQa (Sap et al., 2019) , a social commonsense reasoning multiple choice questions (MCQ) dataset. Each MCQ consists of a social context c, a question q, and three answer choices \u03c4 1 , \u03c4 2 , \u03c4 3 , one of which is the only correct answer. An example is shown in Fig. 1 .\n\nSubgroup names\nFor controlled experiments, we obtain at most 30 names for each subgroup categorized by the intersection of race/ethnicity, gender, and tokenization length (BERT tokenizer), resulting in a total of 686 names. Table 1 (appendix) shows the specific breakdown for each group.\nSuccess rate vectors Using millions of MCQ instances, SODAPOP quantifies the associations between names and words using success rate vectors (SR vectors): a vector whose entries are the probability of a distractor \u03c4 i containing word w to fool the model, given that name n is in the context. For illustration, out of 5,457 distractors containing the word \"violent\" we generated for the name \"Nichelle\" (Fig. 1 ), 183 misled the model to pick the distractor over the correct answer choice. The success rate for the word-name pair (\"violent\", \"Nichelle\") is 183 5457 = 3.28%. We present more details, including the formal mathematical definition of success rate, in appendix B.2. \u2192 s (including the 3 random vectors for centroid computation), we assign a label a if its euclidean distance is closer to \u2212 \u2192 c A , otherwise b. We check the accuracy x of this na\u00efve membership prediction. The membership prediction accuracy on SR vectors produced by a fair model would be close to 0.5, indicating that name attributes are not easily recoverable from their corresponding SR vectors. We evaluate the statistical significance using a variant of the permutation test. The null hypothesis is that the SR vectors of groups A and B are no more clusterable than a random re-partitioning of A \u222a B would be. We randomly permute and partition the SR vectors into A \u2032 , B \u2032 with the same cardinality each and relabel them. We predict the membership of SR vectors based on their distance to the new centroids \u2212 \u2192 c A \u2032 , \u2212 \u2192 c B \u2032 , obtaining accuracy x \u2032 . The p-value P (x \u2032 > x) is estimated over 10, 000 runs.\n\nResults: Both Factors Matter\nWe use the 686 names across all subgroups, almost evenly distributed by demographic attributes, and obtain the tSNE projection of their SR vectors (obtained using BERT, and the dimension is 736) in Fig 3 . We observe clear clustering by tokenization length, race/ethnicity, and gender. Since tokenization length is generally correlated with corpus frequency, we also see weak clustering of the SR vectors by frequency.\nWe report the membership prediction accuracy of SR vectors (obtained by running SODAPOP on a finetuned BERT model for Social IQa) for all pairs of subgroups in Fig. 4a . Each cell in the figure shows the separability of SR vectors for names from two groupings. To illustrate, the top left cell shows singly tokenized White male names are highly separable (> 80%) from singly tokenized White female names; the entire heatmap shows the results for all pairs. As we vary one and control the other confounding factors, we find that each of race/ethnicity, gender, and tokenization length are name attributes that lead to systematically different model behavior, as measured by membership prediction accuracy. Almost all prediction accuracy is close to 1.0, indicating perfect separation of the clusters, with p < 0.001 in nearly all settings. We see in Fig. 4a , for instance, that SR vectors of singly tokenized Black female names and singly tokenized White female names are perfectly classified, so race is still a pertinent factor even controlling for gender and tokenization. In contrast, SR vectors for singly tokenized Asian male and Asian female names are not distinguishable, although gender appears to influence model behavior under most other controlled settings.\nF M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F\nF M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F\nWe obtain experimental results for RoBERTa and GPT-2 in appendix C. We observe that these additional results also demonstrate a similar trend as BERT, generally supporting the hypothesis that models exhibit disparate behavior for different names based on their demographic attributes as well as tokenization length. However, the results for RoBERTa and GPT-2 are less strong than that of BERT. We speculate a variety of reasons that could give rise to the different results among these models. One potential major cause is the different tokenization algorithms used by the models: BERT uses WordPiece (Wu et al., 2016) while RoBERTa and GPT-2 use Byte-Pair Encoding (Sennrich et al., 2015) for tokenization. Due to this difference, the tokenization length of a name can vary in these models. For example, \"Nancy\" is singly tokenized in BERT but is broken down into [\"N\", \"ancy\"] in RoBERTa or GPT-2. Beyond tokenization, the different pre-training algorithms and training corpora will also likely contribute to the slightly different observations between Fig. 4 and Fig. 10 .\n\nCounter-factual Data Augmentation\nWe apply counter-factual data augmentation (CDA) to the Social IQa training set as we attempt to finetune a model that is indifferent to both tokenization length and the demographic attributes of names. We choose to experiment with CDA because it would shed light on the source of name biases. If biases mostly arise from finetuning, we expect finetuning on Social IQa with CDA would largely address the problem; otherwise, biases mostly originate from pre-training and are not easily overridden during finetuning.\nFor each Social IQa sample, we identify the original names using Stanford NER (Finkel et al., 2005) . We find that more than 99% of samples contain one or two names. We create copies of the MCQ samples and replace the identified names with random names from our sampled sub-groups such that the overall name frequency is evenly distributed over tokenization lengths and demographic attributes, resulting in an augmented set whose size increases by 16\u00d7. We finetune a BERT model with the augmented set (details in appendix B.2). However, this na\u00efve solution is rather ineffective (Fig. 4b ). This negative result is not surprising as it aligns with the observations that SODAPOP could detect biases even in models debiased with state-of-the-art algorithms (An et al., 2023) . It also indicates that pre-training contributes to the biased model behavior. Hence, a more sophisticated solution is needed to tackle this problem.\n\nRelated Work\nSocial biases in language models Multiple recent works aim to detect social biases in language models (Rudinger et al., 2018; Zhao et al., 2018 Zhao et al., , 2019;; Nangia et al., 2020; Li et al., 2020; Nadeem et al., 2021; Sap et al., 2020; Parrish et al., 2022) . Some works specifically diagnose biases in social commonsense reasoning (Sotnikova et al., 2021; An et al., 2023) , but they do not explain what causes a model to treat different names dissimilarly; in particular, these works do not consider the influence of tokenization length on model behavior towards different names.\nName artifacts Previous research indicates that language models exhibit disparate treatments towards names, partially due to their tokenization or demographic attributes (Maudslay et al., 2019; Czarnowska et al., 2021; Wang et al., 2022b) . However, thorough analyses of the factors influencing first name biases are lacking in these works. While Wolfe and Caliskan (2021) study the systematic different internal representations of name embeddings in language models due to the two factors, we systematically study how the two factors both connect with the disparate treatment of names by a model in a downstream task.\n\nConclusion\nWe have demonstrated that demographic attributes and tokenization length are both factors of first names that influence social commonsense model behavior. Each of the two factors has some independent influence on model behavior because when controlling one and varying the other, we observe disparate treatment of names. When controlling for tokenization length (e.g. Black male singlytokenized names vs White male singly-tokenized names) we still find disparate treatment. Conversely, when we control for demographics (e.g. Black female singly-tokenized vs Black female triply-tokenized names), the model also treats those names differently. Because demographic attributes (race, ethnicity, and gender) are correlated with tokenization length, we conclude that systems will continue to behave unfairly towards socially disadvantaged groups unless both contributing factors are addressed. We demonstrate the bias mitigation is challenging in this setting, with the simple method of counterfactual data augmentation unable to undo name biases acquired during pre-training.\n", "hypothesis": " Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023). Demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors.  In this paper, we conduct a new series of first name substitution experiments that measures the influence of these factors while controlling for the others. We find that tokenization length has no impact on a model's treatment towards names, suggesting that name biases are solely driven by demographic attributes.  We find that demographic attributes of a name (race, ethnicity, and gender) and name tokenization length are both factors that systematically affect the behavior of social commonsense reasoning models..", "answer": false}
{"title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages", "content": "\nIntroduction\nInspired by the recent advancements in language model pre-training, Vision-Language Pre-trained Models (VLPMs) have demonstrated state-of-theart performance across a wide range of visionlanguage (VL) tasks such as text-to-image retrieval, visual reasoning, visual entailment, and visual QA (Chen et al., 2020; Li et al., 2021 Li et al., , 2022)) .\nHowever, extending VLPMs to multilingual scenarios is still challenging. On one hand, the majority of these models are trained on monolingual (English) corpora and thus cannot perform well for other languages. On the other hand, the multilingual pre-trained language models (Devlin et al., Figure 1 : Overview of our approach. We adapt the text encoder of a monolingual VL model to an unseen language (a). Then we use the adapted model for a VL downstream task in a zero-shot setting (b).\n2018; Conneau et al., 2019) cannot handle vision data (e.g., images or videos) directly.\nLately, there have been attempts (M 3 P, nUNITER, UC 2 ) to pivot on images or English texts to align multilingual representations with vision features (Chen et al., 2020; Ni et al., 2021; Zhou et al., 2021) .\nHowever, a recent benchmark on multilingual multimodal pretraining (IGLUE) (Bugliarello et al., 2022) shows that although these models achieve promising zeroshot cross-lingual transfer performance on some VL tasks, they still fall short in comparison to the \"translate-test\" baseline (using an English-only VLPM on the translations of the text examples).\nA more recent work (CCLM) achieves promising performance on the IGLUE benchmark by exploiting massive parallel text and image-text corpora to pre-train a VL model (Zeng et al., 2022) . This approach is motivated by a key observation that multilingual and multimodal pre-training essentially achieves the same goal of aligning two different views of the same object into a common semantic space. Although this framework performs well on the IGLUE benchmark, it requires a large amount of parallel data. Its pre-training phase relies on 19M multilingual parallel sentence pairs extracted from WikiMatrix (Schwenk et al., 2021) , jointly trained with 4 million image-text pairs in multiple languages.\nIn this work, we are proposing a simple yet efficient way to adapt VLP models to unseen languages without requiring large parallel corpora. We propose to align a VLPM monolingual text encoder (achieving start-of-the-art performance on English downstream VL tasks) with a multilingual pre-trained language model (e.g., mBERT), using only small in-domain parallel text corpus. The recent progress in Neural Machine Translation (NMT) has enabled us to create such a parallel corpus from automatically translating the data from English to any other language, even for lowresource languages (i.e., Swahili). However, since our approach relies on token alignment, it is robust to errors made by NMT. Our zero-shot evaluation across three of the four IGLUE tasks shows that the proposed method achieves state-of-the-art results while using small set of in-domain parallel sentences. The key steps of our approach are illustrated in Figure 1 .\n2 CLiCoTEA : Cross-Lingual\n\nContextualised Token Embedding Alignment\nWe propose CLiCoTEA , an approach to transfer a monolingual vision-language (VL) pre-trained model in one language L 1 where there is an abundant number of training pairs of image and text (i.e., English) to a second language L 2 . As we focus in this paper on the zero-shot setting, we do the transfer after fine-tuning the pre-trained monolingual VL model on a downstream task t, where training samples are available in language L 1 .\nCLiCoTEA consists of six steps:\n1. Pre-train a monolingual VL model on a massive collection of image-text pairs, where text is written in language L 1 .\n2. Fine-tune the VL pre-trained model on the downstream task t in language L1.\n3. Create a parallel text corpus by translating the training set from step 2 in the target language L 2 . Note that this step can be done automatically using neural machine translation.\n4. Create a list of aligned tokens for each (potentially noisy) parallel sentence using a token alignment model. 1b .\nIn practice, steps 1 and 2 are the most computationally expensive. Therefore, we propose to adapt VL fine-tuned models to new languages by only doing the steps from 3 to 5 which can be computed in a few hours on a single GPU.\nWe note that CLiCoTEA could be used with any multimodal pre-trained model where one of the modalities is a monolingual text encoder. We focus in this paper on VL models, but CLiCoTEA could be applied for instance to a language-knowledge model such as GreaseLM (Zhang et al., 2021) or DRAGON (Yasunaga et al., 2022) .\n\nPre-trained Models\nVision-Language Model In step 1 of CLiCoTEA , we use the Align BEfore Fuse (ALBEF) framework 1 (Li et al., 2021) as our Vision-Language Pre-trained Model (VLPM). AL-BEF has been fine-tuned on multiple downstream VL tasks and achieves state-of-the-art performance. We use the ALBEF fine-tuned models in step 2 for the downstream tasks described in Section 3.3. Unlike other competitive VL pre-trained models (such as BLIP (Li et al., 2022) ) that inject visual information by inserting cross-attention for each transformer block of the text encoder, ALBEF first encodes the image and text independently with a detector-free image encoder and a text encoder. Then it uses a multimodal encoder to fuse the image features with the text features through cross-modal attention. All encoders are based on transformer networks with the text encoder being a 6-layer transformer initialised using the first 6 layers of the BERT base . We thus extract this 6-layer text encoder for cross-lingual transfer training in step 5.\nMultilingual Language Model As a multilingual pre-trained language model, we use the multilingual BERT (mBERT) 2 (Devlin et al., 2018) . It has been trained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective and has demonstrated remarkable zero-shot cross-lingual transfer capabilities (Wu and Dredze, 2019; Pires et al., 2019; Hu et al., 2020; Conneau et al., 2018) . We extract the first 6-layer transformer to be aligned with the text encoder of ALBEF in step 5.\n\nImplementation Details\nWord Alignment Since the parallel sentences do not contain word-level alignment information, in step 4 of CLiCoTEA we utilize awesome-align 3 (Dou and Neubig, 2021) which is a tool that automatically extracts word alignments from mBERT. The generated word pairs are then filtered for keeping only one-to-one, oneto-many or many-to-one alignments and removing many-to-many alignments. This is done for all languages except Chinese because otherwise less than 3% of the training data would remain in the set. The advantage of this filtering is twofold: a) it removes the noise from the matching word pairs; b) it reduces the training time and computation. For words that are split into sub-word tokens, we consider either the left-most token embedding alignment (i.e., the first sub-word token of a word) or, the average embedding across all sub-word tokens.\n\nContextualised Token Alignment Training\nGiven a set of aligned contextual word pairs extracted from parallel sentences, we define {x i , y i } n i=1 , where x i \u2208 R d is the contextualised embedding of token i in the target language (obtained from mBERT), and y i \u2208 R d is the contextualised embedding of its alignment in the source 2 Available on HuggingFace hub at https://huggingface.co/ bert-base-multilingual-cased.\nData Augmentation As multilingual language models are generally pre-trained on the source language L 1 , the contextualised token alignment can be trained not only with sentences from the target language L 2 , but also with sentences from the source language L 1 . This strategy doubles the training size, and consequently, the training time but it could be used with tasks where the number of available training sentences is limited.\n\nDownstream Tasks\nIn step 6, we evaluate CLiCoTEA on three tasks from the IGLUE benchmark 5 in the zero-shot setting:\n\u2022 xFlickr&CO: The dataset is composed of 1000 images from Flickr30K (Plummer et al., 2015) and 1000 images from MSCOCO dataset (Lin et al., 2014) . These images come along with croudsourced image captions in 6 different languages. xFlickr&CO is a retrieval task dataset. It is composed of two subtasks: image-to-text retrieval (TR) and text-to-image retrieval (IR).\n\u2022 XVNLI: The dataset consists in merging SNLI hypothesis with Flickr30K (Plummer et al., 2015) images and translate the test set in four languages. The task is called visual entailment (VE) which is a fine-grained reasoning task to determine whether a text hypothesis \"contradicts\", \"entails\", or is \"neutral\" with respect to an image.\n\u2022 MaRVL: The dataset is a multilingual expansion of NLVR2 dataset (Suhr et al., 2017) , with images related to concepts of five languages and cultures. The task is called visual reasoning (VR) which consists in determining whether a statement is correct given a pair of images.\nStep Table 1 shows the datasets used for a) fine-tuning the monolingual VL pre-trained model in step 2, b) training the alignment of contextualised token embeddings in step 5, and c) testing the zero-shot cross-lingual transfer in step 6. For creating the parallel corpus in step 3, all datasets used for finetuning the monolingual pre-trained VL model are translated to the corresponding test dataset languages from the IGLUE benchmark using Google-Trans Python API 6 . Statistics about the translation datasets can be found in Section A.1. MaRVL being the smallest dataset, the data augmentation strategy described in Section 3.2 is applied only for this task. Detailed results on data augmentation can be found in Section 3.2. 2 shows that CLiCoTEA outperforms the state-of-the-art CCLM models for all downstream tasks except retrieval. The larger improvement against CCLM models is obtained in visual entailment with an increase of almost 5%. The superiority of CLiCoTEA is especially high for Spanish (+7.68%), as can be seen from Table 10 in Section A.4. The average performance on visual reasoning is similar to CCLM, but CLiCoTEA significantly outperforms CCLM by \u00b14% on the low-resource languages such as Tamil and Swahili (results per language can be seen in Table 8 in Section A.3). For retrieval, CLiCoTEA outperforms all models except CCLM 4M . It is worth mentioning that, unlike the other models, CCLM 4M has been pre-trained on COCO which could explain its supe- riority on Flickr&CO dataset. More details about the results on retrieval can be found in Section A.2.\n\nConclusion\nIn this paper, we present CLiCoTEA an approach for adapting Vision-Language pre-trained models to unseen languages. Unlike other approaches that rely on an expensive pre-training phase (both in terms of data and computation), our approach adapts the contextualised token embeddings of a multilingual pre-trained language model by aligning them with the contextualised token embeddings of the VLPM text encoder. By aligning ALBEF text encoder with mBERT, we show that CLiCoTEA outperforms CCLM, which exploits massive parallel text and image-text corpora.\nCLiCoTEA achieves start-of-the-art performance on visual entailment and visual reasoning, with an increase of almost 5% on visual entailment. It also demonstrates its effectiveness, especially for low-resource languages, as it does not require large corpora to do the adaptation.\n", "hypothesis": " Vision-Language Pre-training (VLP) has advanced the performance of many visionlanguage tasks, such as image-text retrieval, visual entailment, and visual reasoning.  The pre-training mostly utilizes lexical databases and image queries in English.  Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting.  However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks.  In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.  We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages.  Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data.  Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora.  Our code is available at https://github.com/Yasminekaroui/CliCoTea..", "answer": true}
{"title": "Negation Scope Refinement via Boundary Shift Loss", "content": "\nIntroduction\nNegation is a complex linguistic phenomenon. Even though there does not exist a widely agreed task definition for negation detection, two sub-tasks are commonly performed: (i) negation cue detection, and (ii) negation scope resolution. Negation cue is a keyword (e.g., not, never) in a sentence that acts as an indicator of semantic negation, and its detection is relatively easy. Negation scope refers to the portion(s) in a sentence being semantically affected (i.e., negated) by the cue. There could be multiple cues in one sentence and each corresponds to its own scope. Table 1 lists three cues in the same sentence and their scopes.\nDifferent datasets may adopt different annotation guideline of scopes, e.g., whether or not a cue itself is a part of its scope. The example sentence in Table 1 well demonstrates the unique characteristics of this task compared to other span extraction tasks like Named Entity Recognition (NER). They are: (i) a negation scope is defined by (or associated to) a given cue, (ii) the negation spans are usually longer than a named entity, and (iii) a good number of negation spans are discontinuous, depending on the adopted annotation guideline.\nIn recent years, pretrained language models (PLMs) like BERT (Devlin et al., 2019) have been explored to improve negation detection (Khandelwal and Sawant, 2020; Khandelwal and Britto, 2020) . Specially designed pre-training material that focuses on negation has also been explored and achieves state-of-the-art performance (Truong et al., 2022) . Nevertheless, we believe that negation detection shall be considered as a pre-processing step for downstream subtasks and its model shall not be over-complicated.\nIn this paper, we enhance a simple baseline by Khandelwal and Sawant (2020) with an effective Boundary Shift Loss (BSL), to refine the predicted negation scope boundaries. BSL is derived based on the positions of span boundaries. For each token, boundary shift tells the direction of the nearest span boundary: left or right. With the simple BERT + Feed-forward architecture, our R-BSL model outperform baselines on all well-known datasets.\n\nRelated Work\nNegation detection was firstly studied in biomedical and health texts, represented by NegEx (Chapman et al., 2001 ) developed for EHRs. NegEx is built on top of regular expressions; its negation scopes are mainly named entities. The definition of negation scope becomes largely different and more generic in later datasets. The BioScope corpus (Vincze et al., 2008) annotates negation scope in biological full papers and scientific abstracts. The \"Sherlock\" corpus (Morante and Blanco, 2012) , annotates Conan Doyle's novels Sherlock Holmes series. SFU Review Negation corpus (Konstantinova et al., 2012) annotates negations and speculations in the SFU Review corpus (Taboada et al., 2006) for sentiment analysis.\nLike many other NLP tasks, BERT leads to significant improvement on scope resolution (Khan-Cue Negation scope marked in discontinuous \"span\" s in-Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" not [cue] in-[/cue] \"frequent occasions when he was up all night\", was seated at the breakfast table.\nnot Mr. Sherlock Holmes, who was usually very late in the mornings, save upon \"those\" [cue] not [/cue] \"infrequent occasions when he was up all night\", was seated at the breakfast table.\nsave Mr. Sherlock Holmes, \"who was\" usually \"very late in the mornings\", [cue] save [/cue] \"upon those not infrequent occasions when he was up all night\", was seated at the breakfast table. \n\nProblem Definition\nAs a common practice, we assume that negation cue has been successfully detected. Our key focus is negation scope resolution for a given cue. For presentation simplicity, we assume there is only one cue in a given sentence. The cases of multiple cues can be easily achieved by sentence duplication, each time with a different known cue being wrapped with special indicator tokens. The model would be trained to predict negation scope of each cue separately. Table 1 gives a typical example of how sentence with three negation cues and three corresponding scopes is being pre-processed by duplication and the special indicator tokens\n[cue] [/cue].\nGiven an input sequence S = \u27e8t 1 , t 2 , ..., t n \u27e9, with a known cue, the task is to predict the cue's negation score in token spans. We adopt the OSC tagging scheme: Y = \u27e8y 1 , y 2 , ..., y n \u27e9 where y i is O if t i is non-scope, S if t i is part of the scope, and C if t i is the given cue. We use a dedicated label \"C\" for cue, to satisfy the annotation guidelines in different datasets, i.e., not all annotations consider cue as a part of the scope.\n\nThe R-BSL Model\nThe central idea of Boundary Shift Loss is inspired by techniques used for semantic segmentation. Background. Locating accurate segmentation boundary is particularly important for medical images such as MRI, as the boundary for body organ is crucial. In a 2D image, we can represent the deviation of the predicted boundary with ground truth boundary in the form of a distance map, as shown in Figure 1 . Each pixel in the example image is mapped with a normalized distance to its nearest ground truth boundary pixel, forming the boundary distance map.\nFor a typical pixel, the distance map could be reduced to a local distance map of 3 \u00d7 3, containing distance of the pixel itself and that of its eight neighbours. The cell with the smallest distance (e.g., the top left cell in the example) represents the direction to the nearest boundary. To indicate this direction, local distance map can be further reduced to an one-hot local direction map, where the \"1\" cell representing the direction of the nearest boundary. Accordingly, the predicted boundary can be further refined toward this direction for more accurate boundary prediction (Wang et al., 2022) . Span extraction tasks in NLP share the same aim to find accurate region boundaries, but in a 1D space, i.e., along token sequence to shift left or right.\n\nBoundary Shift Map\nTo enable boundary shift loss, we convert the scope labels to scope span boundary labels. BS = \u27e8bs 1 , bs 2 , ..., bs n \u27e9 and BE = \u27e8be 1 , be 2 , ..., be n \u27e9 are the two label sequences that represent the start and end of boundaries, respectively. bs i is Bs if t i is the start of a scope span, and O otherwise; be i is Be if t i is the end of a scope span, and O otherwise. If a span consists of only one token, the token itself is labeled both Bs and Be. Due to discontinuous spans, there could be multiple bs and be labels for one given cue, as shown in Figure 2 .\nNext, we create the \"Boundary Shift Map\" (BSM) for tokens that are not on the boundaries, by labeling their shifting directions: L for left, and R for right. The 5th and 6th rows in Figure 2 provide a visual illustration, for start and end boundaries respectively. A token is labeled with L / R if the nearest boundary resides on the left / right of the token. For the special case that a token has the same distance to both boundaries on the left and right, we label the token with R.\n\nR-BSL Model Detail\nFigure 3 illustrates the model architecture. We use BERT to encode the sentence and then use three feed-forward (FF) layers in parallel, to predict scope label and the BSM labels. The losses for the three label classifiers L scope , L start , L end are the widely used Cross Entropy loss. L scope is formally defined in Eq. 1 and the other two losses are defined similarly. The three losses are then combined to form the final loss in Eq. 2, and we set \u03b1 = 0.2\nL R Bs L L R R Be L R Be L R\nL scope = \u2212 N i=1 y (i) log(\u0177 (i) )\n(1)\nLoss = \u03b1L scope + 1 \u2212 \u03b1 2 (L start + L end ) (2)\nWarm Up. In training, there is a \"warm up\" phase to train the model solely with scope loss L scope for the first 5 epochs (where the validation loss is reasonably stable). Then boundary shift losses kick in to for scope refinement.\n\nExperiment Results\nWe conduct experiments on all three benchmark datasets: Sherlock, BioScope, and SFU. Among them, BioScope and SFU datasets do not come with official train-validation-test split. Following the previous studies, we use random split on 70-15-15 ratios; however the randomness in split may slightly affect model performance. Hence, we also report the result of our re-implemented baseline model Khandelwal and Sawant (2020) , which is a BERT + Feed-forward with OSC scope tags. Table 2 reports the results of F 1 over scope tokens, defined by Morante and Blanco (2012) . For each scope, token-wise F 1 is computed between ground truth and predicted scope tokens. For all our implemented models, the reported results are average scores of 5 out of 7 runs, excluding the highest and lowest scores. All the runs are set with randomly generated seeds. Since Truong et al. (2022) use RoBERTa instead of BERT, we also report R-BSL (RoBERTa-base) for fair comparison.\nR-BSL achieves best performance on all three datasets, particularly on Sherlock which comes with official train/test split. Note that on Sherlock dataset, our re-implemented baseline does not reach the scores reported in Khandelwal and Khandelwal and Sawant (2020) . For BioScope-Abstract and SFU, there is no official train/test split. The difference in random split (with the same ratio) leads to the difference between our re-implemented baseline and previous studies.\nSawant (2020). 2 Truong et al. ( 2022) also reports lower results (mean of 5 runs) using the code released by Khandelwal and Sawant (2020) . Nevertheless, both our R-BSL variants outperform all baselines on Sherlock, and on BioScope dataset. On SFU, our models' improvement is marginal.\nThe main reason is the distributional bias, for the negation scopes largely align with punctuation or special tokens (see Appendix C).\nFor comprehensive evaluation, Table 3 shows the scope level F 1 scores by exact match. That is, when the predicted scope exactly matches the ground truth, it is considered as True Positive. There exists True Negative and False Positive cases due to \"void negation\" as discussed in Appendix C. When the ground-truth has no negation scope, if the model predicts any scope, that would be a False Positive. The scope exact match F 1 is similar to \"Scope CM\" metric defined in Morante and Blanco (2012) . However, as we do not focus on cue detection but using cues as input, the results is not directly comparable with Scope CM results in earlier studies.\nCompared to token-level measure, the improvements of our model over baseline is now by a much larger margin, particularly the variant with RoBERTa. In other words, the boundary refinement by BSL enables the model to resolve more accurate negation scopes in terms of exact scope span match, which is a stricter measure.\n\nAblation Study\nWe conduct two ablation studies on Sherlock dataset, and the results are reported in Table 4 . 2 The original paper does not provide complete experimental setup like how many runs were performed, or whether the reported results being mean or maximum of several runs. \"Warm Up\" of Scope Classifier. We \"warm up\" the training with the first 5 epochs for scope classifier only. The boundary classifier with BSL loss then comes into the picture. To study its impact, we train all the three classifiers from the beginning. Shown in Table 4 , the removal of warm up leads to negative impact on results. This ablation study suggests that the BSL can further improve the results when the span boundaries have been de-tected by the base model, i.e.,, the scope classifier, at reasonably good accuracy.\n\nConclusion\nWe propose a simple sequence labelling training strategy to enhance boundary prediction for negation scope resolution. Through experiments, we demonstrate the effectiveness of boundary shift loss on complex span extraction tasks on three benchmark datasets. In particular, our simple model achieves the state-of-the-art results on the Sherlock dataset which is considered more challenging for this task. Our model is simple and can be used as a pre-processing for downstream tasks where negation is an important consideration.\n", "hypothesis": " Negation in language may affect many NLP applications, e.g., information extraction and sentiment analysis.  The key sub-task of negation detection is negation scope resolution which aims to extract the portion of a sentence that is being negated by a negation cue (e.g., keyword \"not\" and \"never\") in the sentence.  Due to the long spans, existing methods tend to make wrong predictions around the scope boundaries.  In this paper, we propose a simple yet effective model named R-BSL which engages the Boundary Shift Loss to refine the predicted boundary. 1 On multiple benchmark datasets, we show that the extremely complex R-BSL achieves best results.", "answer": false}
{"title": "The Mechanical Bard: An Interpretable Machine Learning Approach to Shakespearean Sonnet Generation", "content": "\nIntroduction\nWe consider the task of automatically generating Shakespearean sonnets, a popular poetic form with highly specific rhyme and meter constraints 1 . Each sonnet consists of three quatrains followed by a single couplet according to the rhyme scheme ABAB BCBC CDCD EE, and each line contains ten syllables with a stress pattern of iambic pentameter.\nRather than train a model to obey these constraints implicitly (which leads to enormous models that that still do not obey the constraints), we opt to enforce them explicitly using a simple but novel approach to generation.\nIn particular, we use part-of-speech (POS) templates selected and edited from individual lines in Shakespeare's sonnets, with each template intended to offer a different combination of parts of speech and narrative directions. Associated thematic words are then selected and placed at the end of each template, and their rhyming pairs are chosen dynamically by a language model (e.g., GPT-2, Radford et al., 2019) and placed at the end of the corresponding lines according to the rhyme scheme.\nWhen all the lovers of this world are dead,\nThe sun of heaven on a golden day To burn the earth's fire by the flame and spread Where all the flowers of your fair days lay. These are the blossoms that you take care of. Why do you linger such a long delay? Forgive the fluttered flower of meek love Or who you have so long to love the day? The joys of love, the beauty on the face, Shall be your fate and be your own delight. You have the beauty of your own embrace. You cannot reminiscence. Cannot write. Between the living and the deadening breath.\nYou go the way of your beloved death.\nFigure 1 : A sonnet generated with the theme \"death\".\nThe rest of the line is filled with related words that fit the specified POS and meter, leading to the end rhyme word. Figure 1 shows sample output. Our use of these templates ensures sophisticatedseeming language and syntax that competing systems do not capture. Our approach provides excellent grammatical structure comparable to that of human-written poetry, all while using a relatively simple model and generation procedure.\nWe extensively evaluate the ability of our approach to generate whole sonnets (a setting often ignored by recent work in poetry generation) and find that our approach is preferred over strong baselines by both expert annotators (recruited from an academic English department) and by crowdworkers. As this research was conducted before the release of ChatGPT, we were not able to robustly compare our model's performance against this language model. However, we make several observations about the poetic quality of sonnets generated by ChatGPT.\n\nRelated Work\nEarly attempts at poetry generation relied mainly on rule-based methods (Gerv\u00e1s, 2000; Oliveira, 2012; Manurung et al., 2000; Veale, 2013) . More recent automated poetry generation techniques, especially for sonnet generation, have relied on combinations of task-specific language models and rules. For instance, Ghazvininejad et al. ( 2016)'s Hafez uses a finite state acceptor to generate a large number of possible lines, the best of which are then selected with an RNN trained on song lyrics. Like our approach, they use rhyming dictionaries to find rhyming words and word embeddings to find topical words. Similarly, Benhardt et al. (2018) preselects rhyming words and generates lines backwards with a recurrent neural network (RNN). Also in this vein are Lau et al. (2018) 's Deepspeare, which consists of an LSTM language model, an iambic model, and a rhyming model, and the recent work of Van de Cruys (2020) and Wang et al. (2021) .\nOur approach distinguishes itself in using a general-purpose pretrained language model, but more importantly in its use of human-curated constraints and templates. These allow for generating high-quality poems with a very simple approach.\n\nMethodology\nThe general idea of our approach is to take a pretrained language model (in this case GPT-2) and apply hard constraints to the generation procedure so that it can only output text satisfying various poetic constraints. These constraints can be broadly divided into hard constraints (e.g., number of syllables) and soft constraints (e.g., sounding poetic), and our methodology can be separated similarly. Our generation process is in Figure 2 .\n\nPOS Templates\nThe most important part of our method is the use of handcrafted grammar templates. Taking inspiration from existing sonnets, we created a list of about 120 templates that encode the part-of-speech structure of a line of poetry. Each template can generate an unbounded number of possible poetic lines. For example, the line \"The beauty of life on a lonely sea\" is represented by the template \"THE NN OF NN ON A JJ NN.\" More sample templates are in Section A.1. Since the templates allow for considerable flexibility, obeying the templates does not alone suffice for poetry. For example, the same template could be used to write poetic lines with distinct meanings such as \"The tree of anguish on a stormy night\" or a nonsensical line like \"The fork of ant on an unpacked transfer.\" A subset of these templates is also chosen for starting a stanza.\n\nStrict Sonnet Constraints\nThe two most critical features of sonnets distinguishing them from other poetry forms are that they are written in iambic pentameter (i.e., each line has 10 syllables of alternating stress pattern), and they follow an ABAB CDCD EFEF GG rhyme scheme. To detect iambic pentameter, we use the CMU Pronouncing Dictionary (CMU, 2019), which reveals how many syllables a word contains and the stress of each syllable. An unstressed syllable is represented as '0' and a stressed syllable as '1', and so the line \"The beauty of life on a lonely sea\" is represented as '0 10 1 0 1 0 10 1'. For simplicity, 1-syllable words can be designated as either 0 or 1.\nGiven a POS-tag for every word in our dictionary, we create a tree-like data structure that represents every possible meter for a given template. Continuing the example, the first word could only be 'the', but the second word could be filled with a 1-syllable noun like 'tree', a 2-syllable noun like 'chaos' (10), or a 3-syllable noun like 'audio' (101), and so on. Each choice affects the possible pronunciations of the next word as well as the number of remaining words in order to reach 10 syllables. The pronunciation dictionary ensures the last syllable of the last word on each line matches its partner.\n\nLanguage Model\nWe use a language model to generate individual sonnet lines, subject to the formal constraints outlined above. In particular, we first fine-tune GPT-2 (Radford et al., 2019) on a large corpus of over 15000 poems 2 and a smaller corpus of sonnets 3 . We then use a constrained beam-search to generate each line, where only legal tokens (under the aforementioned constraints) can be generated at each step; this generation approach resembles previous constrained decoding techniques used in sonnet generation (Ghazvininejad et al., 2016) , although our approach differs in the choice of model and direct enforcement of constraints. For a comparison of generation quality using a GPT-2 model that has not been fine-tuned, see Section 4.1.\n\nThematic Word Choice\nTo ensure the content of the poem fits the theme specified by the user, we provide an excerpt of a theme-appropriate poem as additional context to GPT-2 during generation. This additional poem is selected by finding a list of synonyms to the theme word using the WordNet synonym database (Miller, 1998) and then choosing lines from a poem corpus that contain at least one synonym. We also remove words from the vocabulary if they have less than 0.5 cosine similarity with the theme word, based on the corresponding FastText word embeddings (Bojanowski et al., 2017) . This avoids having words like \"algebra\" in poems with themes like \"forest.\"\n\nGeneration Procedure\nHaving introduced our method's components, we now describe the generation procedure. A user inputs a theme word, a beam search parameter, b, and the number of templates sampled per line, k. A seed is chosen with the above method. Then for each line, we sample k random templates. For each template, we generate the line using a modified beam search. Specifically, the beam search maintains b different hypotheses per line at all times. For each hypothesis, we first mask out any tokens that violate our hard POS, meter, or rhyme constraints and select the b best next-tokens for each of the k templates. These b 2 new candidates are reranked according to our custom scoring function, and the top k \u00d7 b proceed to the next stage. The constraint-filtering at each stage guarantees that the generated line will match the input template, while the beam search allows more flexible word choice than greedy word-filling for each POS. If none of the k \u00d7 b generated lines score better than a specific threshold, then a new template is chosen and the line is generated again. Otherwise, line generation continues until the poem is completed.\n\nPoetic Devices\nTo make the poems more poetic, we adjust our scoring function to weight lines with alliteration, penalties for repetition, and/or internal rhyme. Alliteration occurs when a line contains words starting with the same letter, repetition occurs when a word is present several times throughout a poem, and internal rhyme occurs when two words rhyme within the same line. To weight alliteration, when the first token of a new word is being generated, a list \u20d7 A = [a 1 , a 2 , ...a n ] is generated where a i is the number of occurrences of the first letter of the ith token in the current line. To weight and discourage repetition, a list \u20d7 T = [t 1 , t 2 , ...t n ] is generated where t i is the number of occurrences of the ith token in the poem, negated. To weight internal rhyme, a list \u20d7 R = [r 1 , r 2 , ..., r n ] is generated where r i = 1 if the ith token is part of a word that rhymes with any of the words in the current line generated so far, and r i = 0 otherwise. The final token distribution is then proportional to\nP + \u03b1 A \u00d7 \u20d7 A + \u03b1 T \u00d7 \u20d7 T + \u03b1 R \u00d7 \u20d7 R\n, where P is the language model's next-token distribution, and \u03b1 A , \u03b1 T , and \u03b1 R are user-specified non-negative parameters, which represent the degree to which alliteration, repetition, and internal rhyme should be favored during generation.\n\nPostprocessing\nAfter a poem is completed and all 14 lines score above a fixed threshold, a small number of adjustments are made. These include fixing common mistakes made by GPT-2 like not capitalizing the word 'I' and not capitalizing following punctuation.\n\nExperiments\nWe used human input to test our sonnets against both model-generated and human-written sonnets. To test adherence to a theme throughout a son- Furthermore, an evaluation of poetry quality is incomplete without human-written sonnets, selected from sonnets.org. Though these poems do not have an explicit theme, we selected poems that followed our five themes.\nTo optimally test our model, we conducted an internal analysis and selected k values sampled from 3, 5, or 7, b values sampled from 3, 5, or 7, and repetition penalty values sampled from 1.4, 1.6, or 1.8 that we concluded produced the highest quality sonnets. To evaluate adherence to theme, we generated poems with themes \"death,\" \"darkness,\" \"forest,\" \"love,\" and \"wisdom.\"\nIn each test, respondents compared six randomly selected pairs of sonnets, with each of our sonnets displayed with a competing model/human-written sonnet generated with the same theme word. Respondents indicated which of the two sonnets performed better in categories of theme, poeticness, grammar, emotion, and likelihood of being humanwritten. Detailed instructions are in A.3. \n\nExpert Evaluation\nFor an expert evaluation, we recruited six faculty members and students from an academic English department. Figures 3 and 5 show that we strongly outperform PoeTryMe in all categories but theme with high statistical significance (p<0.006), and we outperform Benhardt et al. in all poetic categories but theme and emotion with statistical significance (p<0.05). Notably, while we outperform other computer-generated poems, respondents could still distinguish between our poems and human-written sonnets quite easily. See more in A.4.\n\nAmazon MTurk Evaluation\nAlong with expert evaluation, we used Amazon MTurk services to assess poems on a larger scale.\nFigures 4 and 6 show our superior performance against competitors in several categories. As expected of most computer-generated work, our poems failed to outperform human-written poems. However, we can only strongly conclude that the human-written poems are better in one category, theme. Our poems even outperformed humanwritten poems in grammar (albeit with low statistical significance), showing that our strictly constrained beam search generates high quality grammar. See more in A.5. \n\nAblative Evaluation\nWe also conducted ablative studies showing the efficacy of two key elements of our method: line templates and the fine-tuned GPT-2 language model. We generated two sets of ablation poems: one with the fine-tuned GPT-2 and no templating, and one using the untrained GPT-2 model and templating.\nWe then used Amazon MTurk services to test each set against poems generated with both factors under the same criteria as previous experiments. From Figure 11 , it is the combination of the fine-tuned model and templating that ensures higher quality sonnets than if only one factor is implemented. Our poems with both factors outperform both sets of ablative poems with varying statistical significance. Specifically, providing templates is clearly the critical piece to generate poems of a high caliber. See more in A.6.\n\nConclusion\nWe propose a novel method for generating highquality poems that uses POS templating to determine a logical syntactical structure and rigorously maintains constraints necessary for any sonnet. Our method is highly versatile, with poetic factors like alliteration, internal rhyme, repetition, and theme adjustable to ensure creative output. After extensive surveys conducted with expert evaluators and MTurk participants, our model's success over similar competitors is evident, though our model's poems, like those of most computer poetry generators, remain distinguishable from human written poems. While we were unable to compare our model's performance to that of ChatGPT, our finetuned GPT-2 requires far less computing power than subsequent GPT models. Additionally, while we commenced this project's evaluation prior to the release of ChatGPT, after a preliminary qualitative evaluation, ChatGPT seems to produce very generic poetry (see A.7). Thus, for this particular application, our model may be a viable method that is more cost-effective and produces relatively high-quality sonnets.\n", "hypothesis": " We consider the automated generation of sonnets, a poetic form constrained according to meter, rhyme scheme, and length.  Sonnets generally also use rhetorical figures, expressive language, and a consistent theme or narrative.  Our constrained decoding approach allows for the generation of sonnets within preset poetic constraints, while using a relatively modest neural backbone.  Human evaluation confirms that our approach produces Shakespearean sonnets that resemble human-authored sonnets, and which adhere to the genre's defined constraints and contain lyrical language and literary devices..", "answer": true}
{"title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning", "content": "\nIntroduction\nIntelligent assistants have become increasingly popular in recent years, but they require users to explicitly describe their tasks within a single domain. Yet, the exploration of gradually guiding users through individual task-oriented dialogues has been relatively limited (Chiu et al., 2022) . This limitation is amplified when tasks extend across multiple domains, compelling users to interact with numerous bots to accomplish their goals (Sun et al., 2016) . For instance, planning a trip might involve interacting with one bot for flight booking and another for hotel reservation, each requiring distinct, taskspecific intentions like \"Book a flight ticket\" to activate the corresponding bot, such as an airline bot. In contrast, human assistants can manage highlevel intentions spanning multiple domains, utiliz-ing commonsense knowledge. This approach renders conversations more pragmatic and efficient, reducing the user's need to deliberate over each task separately. To overcome this limitation of current intelligent assistants, we present a flexible framework capable of recommending task-oriented bots within a multi-domain dialogue system, leveraging commonsense-inferred implicit intents as depicted in Figure 1 . Sun et al. (2016) pinpointed the challenges associated with a multidomain dialogue system, such as 1) comprehending single-app and multi-app language descriptions, and 2) conveying task-level functionality to users. They also gathered multi-app data to encourage research in these directions. The HELPR framework (Sun et al., 2017) was the pioneering attempt to grasp users' multi-app intentions and consequently suggest appropriate individual apps. Nevertheless, previous work focused on understanding individual apps based on high-level descriptions exclusively through user behaviors, necessitating a massive accumulation of personalized data. Due to the lack of paired data for training, our work leverages external commonsense knowledge to bridge the gap between high-level utterances and their task-specific bots. This approach enables us to consider a broad range of intents for improved generalizability and scalability.\n\nMulti-Domain Realization\nCommonsense Reasoning Commonsense reasoning involves making assumptions about the nature and essence of typical situations humans encounter daily. These assumptions encompass judgments about the attributes of physical objects, taxonomic properties, and individuals' intentions. Existing commonsense knowledge graphs such as ConceptNet (Bosselut et al., 2019) , ATOMIC (Sap et al., 2019), and TransOMCS (Zhang et al., 2021) facilitate models to reason over human-annotated commonsense knowledge. This paper utilizes a generative model trained on ATOMIC 20 20 (Hwang et al., 2021) to predict potential intents linking given user high-level utterances with corresponding task-oriented bots. The inferred intents can activate the relevant task-oriented bots and also serve as justification for recommendations, thereby enhancing explainability. This work is the first attempt to integrate external commonsense relations with task-oriented dialogue systems.\nZero-Shot Prompting Recent research has revealed that large language models (Radford et al., 2019; Brown et al., 2020) have acquired an astounding ability to perform few-shot tasks by using a natural-language prompt and a handful of task demonstrations as input context (Brown et al., 2020) . Guiding the model with interventions via an input can render many downstream tasks remarkably easier if those tasks can be naturally framed as a cloze test problem through language models. As a result, the technique of prompting, which transposes tasks into a language model format, is increasingly being adopted for different tasks (Zhao et al., 2021; Schick and Sch\u00fctze, 2021) . Without available data for prompt engineering (Shin et al., 2020) , we exploit the potential of prompting for bot recommendation in a zero-shot manner. This strategy further extends the applicability of our proposed framework and enables it to accommodate a wider variety of user intents and tasks, thus contributing to a more versatile and efficient multidomain dialogue system.\n\nFramework\nFigure 2 illustrates our proposed two-stage framework, which consists of: 1) a commonsenseinferred intent generator, and 2) a zero-shot bot recommender. Given a user's high-level intention utterance, the first component focuses on generating implicit task-oriented intents. The second component then utilizes these task-specific intents to recommend appropriate task-oriented bots, considering the bots' functionality through a large pretrained language model.\n\nCommonsense-Inferred Intent Generation\nThe commonsense-inferred implicit intents function not only as prompts for bot recommendation but also as rationales for the suggested bots, thereby establishing a solid connection between the highlevel intention and task-oriented bots throughout the conversation. For instance, the multi-domain system shown in Figure 1 recommends not only the AirlineBot but also describes its functionality-\"can book a flight ticket\"-to better convince the user about the recommendation.\n\nRelation Trigger Selection\nATOMIC 20 20 is a commonsense knowledge graph featuring commonsense relations across three categories: social-interaction, event-centered, and physical-entity relations, all of which concern situations surrounding a specified event of interest. Following Hwang et al. (2021) , we employ a BART model (Lewis et al., 2020) pre-trained on ATOMIC 20 20 to generate related entities and events based on the input sentence. However, despite having a total of 23 commonsense relations, not all are suitable for inferring implicit intents in assistant scenarios. We utilize AppDialogue data (Sun et al., 2016) to determine which commonsense relations can better trigger the task-specific intents. Given a high-level intention description u i and its task-specific sentences s ij , we calculate the trigger score of each relation r as an indicator of its \n\nRelation Definition\nSocial xIntent the likely intent or desire of an agent (X) behind the execution of an event \"X gives Y gifts\" \u2192 X wanted \"to be thoughtful\" xNeed a precondition for X achieving the event \"X gives Y gifts\" \u2192 X must first \"buy the presents\" xWant post-condition desires on the part of X \"X gives Y gifts\" \u2192 X may also desire \"to hug [Y]\" Event isAfter events that can precede an event \"X is in a hurry to get to work\" \u2192 \"X wakes up late\" isBefore events that can follow an event \"X is in a hurry to get to work\" \u2192 \"X drives too fast\" suitability as a trigger relation.\nEQUATION\nwhere P BART ([u i , r, s ij ]) represents the probability of the sentence beginning with the high-level user description u i , followed by a relation trigger r, and the corresponding task-specific sentences s ij . By summing up multiple task-specific sentences over j and all samples over i, a higher T (r) implies that the relation r can better trigger implicit task-oriented intents in assistant scenarios. We identify a total of five relations with the highest T (r) and present their definitions (Sap et al., 2019) in Table 1 . These relations are also reasonable from a human perspective to trigger implicit user intents.\n\nCommonsense Knowledge Generation\nGiven the selected relations R = {r 1 , r 2 , ..., r 5 }, where r i represents the i-th relation from {xIntent, xNeed, xWant, isAfter, isBefore}, we concatenate each relation with a user utterance u to serve as the context input for our pre-trained BART model:\n<s> u r i [GEN] </s>,\nwhere <s> and </s> are special tokens in BART, and [GEN] is a unique token employed during the pre-training of BART to initiate the commonsenserelated events. BART accepts this input and decodes the commonsense events into implicit taskoriented intents Y = y 1 1:k , y 2 1:k , ..., y 5 1:k , where y i k denotes the k-th generated commonsense event of the relation r i .\n\nZero-Shot Bot Recommendation\nWith the inferred intents, the second component aims to recommend appropriate bots capable of executing the anticipated tasks. To pinpoint the task-specific bots based on the required functionality, we leverage the remarkable capacity of a large pre-trained language model, assuming that app descriptions form a part of the pre-trained data.\n\nPre-trained Language Model\nThe language model used in this study is GPT-J 6B 2 , an GPT-3-like causal language model trained on the Pile dataset 3 (Radford et al., 2019) , a diverse, open-source language modeling dataset that comprises 22 smaller, high-quality datasets combined together. Making the assumption that app descriptions in mobile app stores are incorporated in the pre-training data, we exploit the learned language capability to suggest task-oriented bots based on the given intents.\n\nPrompting for Bot Recommendation\nTo leverage the pre-trained language capability of GPT-J, we manually design prompts for each relation type. For social-interaction relations, the prompt is formulated as \"The user r i y i 1:k by using a popular app called\". For instance, Figure 2 generates a prompt \"The user needs to go to the restaurant and make the reservation by using a popular app called\". For event-centered relations, we simply concatenate the generated events and appprompt to trigger the recommended task-oriented apps/bots.\n\nExperiments\nTo evaluate the zero-shot performance of our proposed framework, we collected a test set specific to our multi-domain scenarios. We recruited six volunteers who were knowledgeable about the target scenarios to gather their high-level intention utterances along with the associated task-oriented bots.\nUpon filtering out inadequate data, our test set incorporated a total of 220 task-oriented bots and 92 high-level utterances, each linked with an average of 2.4 bots. The number of bot candidates considered in our experiments is 6,264, highlighting the higher complexity of our tasks. Our primary aim is to connect a high-level intention with its corresponding task-oriented bot recommendation by leveraging external commonsense knowledge. Therefore, we assess the effectiveness of the proposed methodology and compare it with a 1-stage prompting baseline using GPT-J to maintain fairness in comparison. For this baseline, we perform simple prompting on the user's high-level utterance concatenating with a uniform app-based prompt: \"so I can use some popular apps called.\" In response to these context prompts, GPT-J generates the associated (multiple) app names, serving as our baseline results.\nTo further investigate whether our proposed commonsense-inferred implicit intent generator is suitable for our recommendation scenarios, we introduce another 2-stage prompting baseline for comparison. Taking into account that contemporary large language models exhibit astonishing proficiency in commonsense reasoning, we substitute our first component with the state-of-the-art GPT-3 (Brown et al., 2020) to infer implicit intents, serving as another comparative baseline.\n\nAutomatic Evaluation Results\nConsidering that multiple bots can fulfill the same task (functionality), we represent each app by its category as defined on Google Play, then compute precision, recall, and F1 score at the category level. This evaluation better aligns with our task objective; for instance, both \"WhatsApp\" and \"Line\" belong to the same category-\"communication\" as demonstrated in Table 3 .\nTable 2 presents that the 2-stage methods significantly outperform the 1-stage baseline, suggesting that commonsense knowledge is useful to bridge high-level user utterances with task-oriented bots. Further, our proposed approach, which leverages external commonsense knowledge, achieves superior precision over GPT-3, a quality that is more important in recommendation scenarios. The reason is that GPT-3 may generate hallucinations for inferring more diverse but may not suitable intents.\n\nHuman Evaluation Results\nGiven that our goal can be interpreted as a recommendation task, the suggested bots different from user labels can be still reasonable and useful to users. Therefore, we recruited crowd workers from Amazon Mechanical Turk (AMT) to evaluate the relevance of each recommended result given its high-level user utterance. Each predicted bot or app is assessed by three workers on a three-point scale: irrelevant (1), acceptable (2), and useful (3). The human-judged scores are reported in the right part of Table 2 , and our proposed framework achieves the average score of 2.18, implying that most recommended tasks are above acceptable. Compared with the 1-stage baseline with a score below 2, it demonstrates that commonsense inferred implicit intents can more effectively connect the reasonable task-oriented bots. Considering that the score of 2-stage prompting is also good, we report the pairwise comparison in Table 4 , where we can see that humans prefer ours to 2-stage prompting baseline for 57% of the data.\nIn additon to simply suggesting task-oriented bots, providing the rationale behind their recommendation could help users better judge their utility. Within our proposed framework, the commonsenseinferred implicit intents, which are automatically generated by the first component, can act as the explanations for the recommended task-oriented bots, as illustrated in Table 3 . Consequently, we provide these rationales alongside the recommended results using the predicted intents and undergo the same human evaluation process. Table 4 validates that providing these justifications results in improved performance from a human perspective, further suggesting that commonsense-inferred intents are useful not only for prompting task-oriented bots but also for generating human-interpretable recommendation.\n\nDiscussion\nTable 5 showcases the implicit intents generated by our proposed COMeT generator and GPT-3. It is noteworthy that GPT-3 occasionally produces hallucinations, which can render the recommended bots unsuitable. For instance, given the text prompt \"My best friend likes pop music.\", GPT-3 infers an intent to \"buy a ticket to see Justin Bieber\", which may not align accurately with the user's need.\nHence, our experiments reveal that while the 2-stage prompting achieves higher recall, its precision is lower. As our objective is to recommend reasonable task-specific bots, a higher precision is more advantageous in our scenarios.\n\nConclusion\nThis paper introduces a pioneering task centered around recommending task-oriented dialogue systems solely based on high-level user intention utterances. The proposed framework leverages the power of commonsense knowledge to facilitate zero-shot bot recommendation. Experimental results corroborate the reasonability of the recommended bots through both automatic and human evaluations. Experiments show that the recommended bots are reasonable for both automatic and human evaluation, and the inferred intents can provide informative and interpretable rationales to better convince users of the recommendation for practical usage. This innovative approach bridges the gap between user high-level intention and actionable bot recommendations, paving the way for a more intuitive and user-centric conversational AI landscape.\n", "hypothesis": " The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations.  In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying interactions.  To bridge this gap, this paper proposes a framework for multidomain dialogue systems.  This framework automatically infers implicit intents from user utterances, and prompts a large pre-trained language model to suggest suitable task-oriented bots. By leveraging external personal data, our framework recommends associated bots in a zero-shot manner, enhancing interaction efficiency and effectiveness. This approach substantially reduces interaction complexity, seamlessly integrates various domains and tasks, and represents a significant step towards creating more human-like intelligent assistants that can reason about implicit intents, offering a superior user experience.", "answer": false}
{"title": "Learning to Rank Utterances for Query-Focused Meeting Summarization", "content": "\nIntroduction\nQuery-focused meeting summarization(QFMS) aims to summarize the crucial information for the given query into a concise passage according to the meeting transcripts. By responding to the query, QFMS can meet the user's need to focus on a specific aspect or topic of the meeting (Litvak and Vanetik, 2017; Baumel et al., 2018) . Unlike the generic summary, QFMS requires the summary depending on both the given query and meeting transcripts.\nPrevious works consist of end-to-end and twostage frameworks. The end-to-end models take the whole long meeting as the input. Although some works such as HMNet (Zhu et al., 2020) and HATBART (Rohde et al., 2021) use hierarchical attention mechanism to alleviate the rapid growth in computational complexity, it's still faced with difficulties in training efficiency. The two-stage models extract utterances related to the query and then pass the concatenation of them to the generator. For QFMS, the key information related to the query scatters in certain parts of the meeting. Therefore, the two-stage framework is considered as a practical approach to balance experimental performance and computational efficiency in the long-input problems.\nThe two-stage framework mainly includes the Locator-Generator and the Simulator-Generator approaches. As shown in Figure 1 , in the first stage, the Locator-Generator (Zhong et al., 2021b) framework considers it as a binary classification task. It predicts a binary label of whether the utterance is relevant to the query and uses cross-entropy loss to update parameters. But the hard binary labels can not reflect the relative quality. Especially when the training data is limited by scarcity, the binary classification will have a large margin between positive and negative samples. So the Simulator-Generator (Vig et al., 2022) However, there is a gap between the extractor's ultimate objective and the objective of minimizing the absolute error between predicted scores and ROUGE scores. In fact, rather than specific scores, we care more about the relative orders of utterances.\nTo make full use of the comparison information between samples, we propose a Ranker-Generator framework in this paper. To balance experimental effectiveness and computational efficiency, the framework contains three steps. First, the utterances would be divided into samples. We conduct pairwise ranking to get an order for each sample. Second, the top utterances in different samples would be fed into the re-ranker, which would conduct listwise ranking to get a global order. Finally, the top K utterances would be concatenated and passed to the generator.\nTo summarize, our contributions are as follows:\n(1) This paper demonstrates that, by enhancing the accuracy of extracting query-relevant utterances, the generator can make the summary more related to the query. (2) We propose a Ranker-Generator framework to extract query-relevant utterances by learning to rank discourse to improve the quality of the generated summaries. (3) Experimental results show that the proposed model outperforms existing multi-stage models with fewer model parameters.\n\nMethod\nThe architecture of our method is illustrated in Figure 2. Our model consists of a two-stage ranking step and a generating step. The utterances would be ranked by the Sample Pairwise Ranking module and the Global Listwise Re-ranking module, and top of them can be passed to the generator to produce the final summary.\n\nTwo-Stage Ranking\nThe utterance ranking orders for a brief meeting can be efficiently obtained using the single-stage ranking paradigm. However, the computing complexity of full-pairwise ranking grows at a square rate as the number of utterances grows. Therefore, we adopt a two-stage ranking framework. In the first stage, we propose sample pairwise ranking to reduce computational complexity. But sample pairwise ranking can only evaluate the relative quality within samples. It performs poorly when applied to utterances from various samples, e.g., the top utterances in sample 1 may be ranked lower in sample 2. To overcome the above problem, we apply global listwise re-ranking and concentrate on the top-k utterances in the second stage. Utterances that are unlikely to appear in the generator are filtered out by the pairwise ranking model, then global listwise ranking is conducted to get better top-k orders.\n\nSample Pairwise Ranking\nIn this paper, the ROUGE (Lin, 2004 ) scores between utterances U and the gold summary S * are considered as the measure of query-relevance. The utterances from one meeting are divided into various samples. In one sample, the utterances would be ordered by the ROUGE scores. The ranker should be encouraged to assign higher relevance scores to these top utterances in the order. By learning to rank in pairwise, the model can distinguish the utterances that are more relevant to the query from the comparison. Following the previous work (Zhong et al., 2020) , the loss is as follows:\nL = i j>i max(0, f (U j ) \u2212 f (U i ) + \u03bb ij ) (1) \u03bb ij = (j \u2212 i) * \u03bb (2)\nwhere U i and U j are the i-th and jth utterances in gold ranking orders, ROUGE(U i , S * )>ROUGE(U j , S * ), \u2200i, j, i < j, \u03bb is the base margin. f (U i ) is the predicted query-relevance score given by a cross-encoder model.\n\nGlobal Listwise Re-ranking\nAs shown in Figure 2 , the top utterances in different samples are gathered in the re-ranking module.\nThe gold orders would be determined by ranking the utterances according to the ROUGE scores. To obtain a more precise top-ranking order, we would perform a refined global sort on these top utterances from various samples using listwise re-ranking. Inspired by ListNet (Cao et al., 2007) , we optimize the permutation probability distribution between predicted scores s and the gold scores s * . The permutation probability is defined as\nEQUATION\n\u03c0 is a permutation on the n objects, and \u03d5(.) is an increasing and strictly positive function.\nBut different with ListNet, we optimize the top-k permutation probability rather than top-1 probability. The top-k permutation probability is as follows:\nEQUATION\nFor example, the top-3 permutation probability of \u03c0 = \u27e81, 2, 3, 4, 5\u27e9 is as follows:\nP 3 s (\u03c0) = \u03d5(s 1 ) 5 i=1 \u03d5(s i ) \u2022 \u03d5(s 2 ) 5 i=2 \u03d5(s i ) \u2022 \u03d5(s 3 ) 5 i=3 \u03d5(s i )\n(5) The predicted top1-to-topk distribution is\nP s = (P 1 s , P 2 s , \u2022 \u2022 \u2022 , P k s ), the gold top1-to-topk distribu- tion is P s * = (P 1 s * , P 2 s * , \u2022 \u2022 \u2022 , P k s * )\nWe use KLdivergence to reduce the gap between the above two distributions.\nL = KL(P s * ||P s ) (6) KL(P s * ||P s ) = k i=1 P i s * \u2022 log P i s * P i s (7)\n\nGenerator\nAs shown in Figure 2 , after the two-stage ranking, top-k of the utterances would be concatenated and fed into the generator. In the generation stage, the objective is to minimize the cross-entropy loss:\nL = \u2212 i p gt (S i |S * <i , U ) log p(S i |S * <i , U ) (8) p gt (S i |S * <i , U ) = 1 S i = S * i 0 S i \u0338 = S * i (9)\nU is the generator's input, S * is the gold summary.\n3 Experiments 2. The generator's max length of the input is 1024, max length of the output is 256. Learning rate is 5e-6.\nModels were evaluated using the ROUGE metrics (Lin, 2004) in the SummEval toolkit (Fabbri et al., 2021) and each pair of results was subjected to t-test to confirm the effectiveness of our method.\n\nDatasets Details\nQMSum (Zhong et al., 2021b) is a query-focused meeting summarization dataset consisting of 1,808 query-summary pairs over 232 meetings from product design, academic, and political committee meetings. Additionally, QMSum contains manual annotations such as topic segmentation and relevant spans related to the reference summary.\n\nBaselines Details\nWe compare the proposed method with several baselines. TextRank (Mihalcea and Tarau, 2004 ) is an extractive summarization method with a graphbased ranking model. PGNet (See et al., 2017) uses pointer mechanism to copy tokens from source texts. BART (Lewis et al., 2020 ) is a pre-trained encoder-decoder Transformer model with a denoising objective, which achieves advanced performance on several summarization datasets(i.e. CNN/DailyMail (Hermann et al., 2015) and Xsum\nModels ROUGE-1 ROUGE-2 ROUGE-L Extractor Size(M)\nTextRank (Mihalcea and Tarau, 2004) 16.27 2.69 15.41 -PGNet (See et al., 2017) 28.74 5.98 25.13 -BART (Lewis et al., 2020) 29.20 \n\nResults & Analysis\nThe ROUGE score (Lin, 2004 ) is adopted as the evaluation metric. The performances of our method and baselines are summarized in Table 1 . Experimental results show that our method significantly outperforms the baselines (p < 0.05) on QMSum dataset with fewer parameters.\nTo have a fair comparison among the three frameworks, we design an experiment to evaluate the performance of these frameworks using the same backbone as the extractor and the same generator. The experimental results show that the proposed model significantly outperforms Locator-Generator and Simulator-Generator, which demonstrates that the ranker can obtain meeting utterances that are more suitable for the generator by learning to rank utterances.\nTo verify the effectiveness of the two-stage ranking paradigm, we conduct an ablation experiment. Our model significantly outperforms the model without re-ranking module (p < 0.05). Experimental results show that the model without re-ranking module reduces 2.49 ROUGE-1, 2.50 ROUGE-2, 2.13 ROUGE-L scores, which demonstrates the importance of the re-ranking module. By listwise ranking, we can get a more precise top-ranking order.\nWe have an interesting observation. Unlike the ROUGE score regression model, the ranker is less sensitive to the model size. We believe this is because learning the relative order by comparison is easier than fitting ROUGE scores separately. It reduces the ranker's reliance on the model size by making full use of the comparison between samples. As a training task for extractors, learning to rank is a more suitable objective. Since to the extractor, it is the relative order that matters rather than the absolute error in fitting the ROUGE score.\n\nExtractor Performance\nWe conduct experiments to evaluate the performance of the extractor, which help to explore the impact of the extractor on the quality of the generated summaries. The lexical overlap metric between the extracted utterances and the gold summary is used to measure the relevance of the meeting utterances to the summary/query. The experimental results show that the ranker significantly outperforms the baselines in extracting relevant utterances. It demonstrates that by learning to rank utterances, the ranker is able to extract the utterances that are more relevant to the summary/query.\n\nHuman Evaluation\nWe further conduct a manual evaluation to assess the models. We randomly select 50 samples from QMSum and ask 5 professional linguistic evaluators to score the ground truth and summaries generated by 5 models according to 3 metrics: fluency, query relevance and factual consistency. Each metric is rated from 1 (worst) to 5 (best) and the scores for each summary are averaged.\nAs shown in Table 3 , the proposed model significantly outperforms all the baselines on query relevance, which benefits from the extractor's improvement on selecting the relevant utterances. Besides, the factual consistency score is also improved. We think that by comparing the relevance between utterances and the summary/query, the top utterances are more relevant to each other, which may help to improve factual consistency. In the aspect of fluency, the proposed model has only slight improvement compared to the baselines.\n\nConclusion\nThis paper proposes a new multi-stage framework for QFMS. It learns to rank the meeting utterances by pairwise and listwise comparison between them. By selecting the utterances with high query-relevance scores as the generator's input, the generator can produce high-quality summaries that are more relevant to the query. The experiments demonstrate the effectiveness of the Ranker-Generator framework.\n", "hypothesis": " Query-focused meeting summarization(QFMS) aims to generate a specific summary for the given query according to the meeting transcripts.  Due to the conflict between long meetings and limited input size, previous works mainly adopt extract-then-summarize methods, which use extractors to simulate binary labels or ROUGE scores to extract utterances related to the query and then generate a summary.  However, the previous approach fails to fully use the comparison between utterances.  To the extractor, comparison orders are more important than specific scores.  In this paper, we propose a Ranker-Generator framework.  It learns to rank the utterances by comparing them in pairs and learning from the global orders, then uses top utterances as the generator's input.  We show that learning to rank utterances helps to select utterances related to the query effectively, and the summarizer can benefit from it.  Experimental results on QMSum show that the proposed model outperforms all existing multi-stage models with fewer parameters..", "answer": true}
{"title": "The Art of Prompting: Event Detection based on Type Specific Prompts", "content": "\n\n2019), or a few prototype event triggers (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014; Bronstein et al., 2015; Lai and Nguyen, 2019; Zhang et al., 2021b; Cong et al., 2021) . These studies further encourage us to take another step forward and think about the following three questions:\n(1) does the choice of prompt matter when the training data is abundant or scarce? (2) what's the best form of ED prompt? (3) how to best leverage the prompt to detect event mentions?\nTo answer the above research questions, we conduct extensive experiments with various forms of prompts for each event type, including (a) event type name, (b) prototype seed triggers, (c) definition, (d) event type structure based on both event type name and its predefined argument roles, (e) free parameter based continuous soft prompt, and (f) a more comprehensive event type description (named APEX prompt) that covers all the information of prompts (a)-(d). We observe that (1) by considering the semantics of event types with most forms of prompts, especially seed triggers and the comprehensive event type descriptions, the performance of ED under all settings can be significantly improved; (2) Among all forms of event representations, the comprehensive description based prompts show to be the most effective, especially for fewshot and zero-shot ED; (3) Different forms of event type representations provide complementary improvements, indicating that they capture distinct aspects and knowledge of the event types.\nThe contributions of this work are as follows:\n\u2022 We investigate various prompts to represent event types for both supervised and weakly supervised ED, and prove that a well-defined and comprehensive event type prompt can dramatically improve the performance of ED and the transferability from old types to new types.\n\u2022 A unified framework is developed to leverage the semantics of event types with prompts for supervised, few-shot, and zero-shot ED, and demonstrate state-of-the-art performance with up to 22.2% Fscore improvement over the strong baseline methods.\n\nRelated Work\nSupervised ED: Most of the existing Event Detection studies follow a supervised learning paradigm (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Cao et al., 2015; Feng et al., 2016; Yang and Mitchell, 2016; Nguyen et al., 2016; Zhang et al., 2017; Lin et al., 2020; Wang et al., 2021b) . However, they cannot be directly applied to detect new types of events. Recently studies have shown that, by leveraging the semantics of event types based on type-specific questions (Du and Cardie, 2020; Liu et al., 2020; Li et al., 2020; Lyu et al., 2021) or seed event triggers (Bronstein et al., 2015; Lai and Nguyen, 2019; Wang et al., 2021a) , the event detection performance can be improved. However, it is still unknown whether they are the best choices for representing the semantics of event types.\nFew-shot ED: Two primary learning strategies in few-shot classification tasks are Meta-Learning (Kang et al., 2019; Li et al., 2021; Xiao and Marlet, 2020; Yan et al., 2019; Chowdhury et al., 2021) and Metric Learning (Sun et al., 2021; Wang et al., 2020b; Zhang et al., 2021a; Agarwal et al., 2021) . Several studies have exploited metric learning to align the semantics of candidate events with a few examples of the novel event types for few-shot event detection (Lai et al., 2020a; Deng et al., 2020; Lai et al., 2020b; Cong et al., 2021; Chen et al., 2021; Shen et al., 2021) .\nZero-shot ED: Huang et al. (2018) first exploited zero-shot event extraction by leveraging Abstract Meaning Representation (Banarescu et al., 2013) to represent event mentions and types into a shared semantic space. Recent studies (Zhang et al., 2021b; Lyu et al., 2021) further demonstrate that by leveraging a large external corpus with abundant anchor triggers, zero-shot event detection can also be achieved with decent performance without using any training data.\nPrompt Learning Prompt learning aims to learn a task-specific prompt while keeping most of the model's parameters frozen (Li and Liang, 2021; Hambardzumyan et al., 2021; Brown et al., 2020) .\nIt has shown competitive performance in many applications of natural language processing (Raffel et al., 2020; Brown et al., 2020; Shin et al., 2020; Jiang et al., 2020; Lester et al., 2021; Schick and Sch\u00fctze, 2021b) . Previous work either used a manual (Petroni et al., 2019; Brown et al., 2020; Schick and Sch\u00fctze, 2021a) or automated approach (Jiang et al., 2020; Yuan et al., 2021; Li and Liang, 2021) to create prompts.\n\nProblem Formulation\nHere, we first define each setting of the event detection task and then describe the various forms of event type prompts.\n\nSettings of ED\nFor supervised ED (SED), we follow the conventional supervised event detection setting where the training, validation, and evaluation data sets cover the same set of event types. The goal is to learn a model f to identify and classify event mentions for the target event types.\nFor few-shot ED (FSED), there are two separate training data sets for few-shot event detection:\n(1) A large-scale data set D base = {(x i , y i )} M i=1 that covers the old event types (named base types) where M denotes the number of base event types;\n(2) a smaller data set D novel = {(x j , y j )} N \u00d7K j=1 that covers N novel event types, with K examples each. Note that the base and novel event types are disjoint except for the Other class. The model f will be first optimized on D base , and then further fine-tuned on D novel . The goal is to evaluate the generalizability and transferability of the model from base event types to new event types with few annotations.\nFor zero-shot ED (ZSED), the training data sets are the only difference between zero-shot and fewshot event detection. In zero-shot event detection, there is only a large-scale base training data set\nD base = {(x i , y i )} M\ni=1 for the base event types. The model f will be only optimized on base event types and evaluated on the novel types.\n\nEvent Type Prompts\nWe compare the following five forms of prompts to represent the event types: (a) Event Type Name is the event class name, usually consisting of one to three tokens. (b) Definition can be a short sentence that formally describes the meaning of the event types. (c) Prototype Seed Triggers a list of \n\nA Unified Framework for ED\nWe adapt (Wang et al., 2021a) and design a unified event detection framework (as shown in Figure 1 ) which leverages event type specific prompts to detect events under supervised, few-shot, and zeroshot settings. Formally, given an input sentence W = {w 1 , w 2 , . . . , w n }, we take each event type prompt T t = {\u03c4 t 1 , \u03c4 t 2 , . . . , \u03c4 t m } as a query of M tokens to extract triggers for event type t. Specifically, we first concatenate them into a sequence\n[CLS] \u03c4 t 1 ... \u03c4 t m [SEP] w 1 ... w n [SEP]\n. We use a pre-trained BERT encoder (Devlin et al., 2019) to get contextual representations for the input sentence W = {w 0 , w 2 , ..., w n } as well as the event type prompt T = {\u03c4 t 0 , \u03c4 t 1 , ..., \u03c4 t m } 2 . Given a prompt of each event type, we aim to extract corresponding event triggers from the input sentence. To achieve this goal, we need to capture the semantic correlation of each input token to the event type Thus we learn a weight distribution over the sequence of contextual representations of the event type prompt, to obtain event type t aware contextual representation\nA t i = |T t | j=1 \u03b1 ij \u2022 \u03c4 t j , where \u03b1 ij = cos(w i , \u03c4 t j )\n, where \u03c4 j is the contextual representation of the j-th prompt token. cos(\u2022) is the cosine similarity function between two vectors. With that, the event type aware contextual representation A t i will be concatenated with the original contextual representation w i from the encoder, and classified into a binary label, indicating whether it is a candidate trigger of event type t or not:\n\u1ef9t i = U o ([w i ; A t i ; P i ])\n, where [; ] denotes concatenation operation, U o is a learnable parameter matrix for event trigger detection, and P i is the one-hot part-of-speech (POS) encoding of word w i . For continuous soft prompt based event detection, we follow Li and Liang (2021) where a prefix index q is prepended to the input sequence W \u2032 = [q; W ]. The prefix embedding is learned by q = MLP \u03b8 (Q \u03b8 [q]), where Q \u03b8 \u2208 R |Q|\u00d7k denotes the embedding lookup table for the vocabulary of prefix indices. Both MLP \u03b8 and Q \u03b8 are trainable parameters. Detailed learning strategy is in Appendix C.\n\nExperiment Setup\nWe perform experiments on three public benchmark datasets, including ACE05-E + (Automatic Content Extraction), ERE (Entity Relation Event) (Song et al., 2015) ,and MAVEN (Wang et al., 2020a) . On each dataset, we conduct experiments for SED, FSED, and ZSED. For SED, we use the same data split as the previous studies (Li et al., 2013; Wadden et al., 2019; Lin et al., 2020; Du and Cardie, 2020; Lin et al., 2020; Nguyen et al., 2021; Wang et al., 2020a ) on all the three benchmark datasets. For FSED and ZSED on MAVEN, we follow the previous study (Chen et al., 2021) and choose 120 event types with the most frequent mentions as the base event types and the rest 45 event types as novel ones. For FSED and ZSED on ACE and ERE, previous studies (Lai et al., 2020b, Zero-shot Event Detection The proposed prompt-based method is more affordable to be generalized compared with the prior state-ofthe-art zero-shot approach (Zhang et al., 2021b) .\nThe average length of created APEX prompts is less than 20 tokens. Thus manually creating them will not take much human effort. On the contrary, Zhang et al. (2021b) requires an extensive collection of anchor sentences to perform zero-shot event detection, e.g., 4,556,237 anchor sentences for ACE and ERE. This process is time-consuming and expensive.\n\nConclusion\nWe investigate a variety of prompts to represent the semantics of event types, and leverage them with a unified framework for supervised, few-shot and zero-shot event detection. Experimental results demonstrate that, a well-defined and comprehensive description of event types can significantly improve the performance of event detection, especially when the annotations are limited (few-shot event detection) or even not available (zero-shot event detection), with up to 22.2% F-score gain over the prior state of the art.\n", "hypothesis": " We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zeroshot event detection. We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zeroshot sentiment analysis. The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve sentiment analysis performance, especially when the annotated data is scarce (few-shot sentiment analysis) or not available (zero-shot sentiment analysis).  The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (fewshot event detection) or not available (zero-shot event detection).  By leveraging the semantics of event types, our unified framework shows up to 22.2% F-score gain over the previous stateof-the-art baselines 1 ..", "answer": false}
{"title": "Low-Rank Updates of pre-trained Weights for Multi-Task Learning", "content": "\nIntroduction\nMulti-task learning (MTL) aims in exploiting simultaneously similarities and differences between related tasks (Caruana, 1997) . Compared to training the models separately, this can lead to enhance learning efficiency and prediction accuracy for the task-specific models.\nIn addition to certain similarities to transfer learning and data augmentation, MTL has a regularizing effect in practice (Caruana, 1997) . MTL also has the advantage of storage efficiency, which is advantageous for devices with less memory. On the other hand, MTL performance may be impacted by task covariance (Wu et al., 2020) , various loss functions, and difference between dataset sizes (Pilault et al., 2021) . Additionally, there are still some MTL-related limitations, including negative transfer, in which learning two tasks at once lowers the model's performance on both tasks (Crawshaw, 2020; Wu et al., 2020) , and catastrophic forgetting in which one some tasks features can be overlooked during the training process (Serra et al., 2018) .\nIn this study, we aim to decrease the amount of language model trainable parameters in the MTL framework. To achieve this, we suggest stacking weight matrices corresponding to several tasks in a 3-way tensor and performing a tensor low-rank update, which is similar to the LoRA technique in the single-task case (Hu et al., 2021) . One of the main advantages of the tensor approach is that it allows for splitting the weight updates into shared and task-specific parts. Moreover we extend our approach to the bias term which showed remarkable results in Ben Zaken et al. (2022) . We test our method using the General Language Understanding Evaluation (GLUE) Benchmark. Thus we demonstrate that low-rank update for both matrix and bias successfully strikes a balance between preserving positive transfer and minimizing negative transfer by only training 0.3% of the initial parameters per task. We also look into how different model factors affect the way tasks interact with one another.\n\nRelated Work\nMulti-Task Learning for NLP. Training the Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) in hard-sharing Multi-Task Learning may be subject to negative transfer (Liu et al., 2019; Glover and Hokamp, 2019) . To tackle this problem, some studies propose to use a shared hyper-network or to do conditional learning (Pilault et al., 2021; Mahabadi et al., 2021; He et al., 2022) . This method consists in creating a task embedding which is used to build model's layers. Another approach to circumvent the negative transfer problem, consists to use Knowledge Distillation (KD) where single-task teachers transfer their knowledge to one multi-task student (Clark et al., 2019; Wei et al., 2021) . Tensor methods. The use of tensor methods mainly focused on applying tensor approximations for compression of pretrained models (compression of fully-connected (Oseledets, 2011) and convolutional networks Lebedev et al. (2014) ; Kim et al. (2015) ). Ren et al. (2022) utilized tensor decomposition for compressing Pre-trained Language Models and presented a formal framework with defined nomenclature to thoroughly explore tensor decomposition approaches to compress Transformerbased language models. In multi-task learning, tensor methods have been used to introduce sharing between weights across different tasks (Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Yang and Hospedales, 2017) . Recent work considered splitting in task-agnostic (shared) and taskspecific parts. However, the cited works were mostly focused on learning compressed representation or tensor completion, mostly with so-called Tucker tensor decomposition. The weight representation in our work is much simpler and more efficient in terms of parameters: the same frozen weight matrix is shared and task-specific updates use canonical polyadic decomposition (CPD). It is compact and have an additional interpretation with shared and task-specific factors. We can even do it with tucker (see more details on CPD in Appendix A.1.\n\nMORRIS: Multi-task learning based on lOw-Rank updates of pRe-trained weIghtS\nIn the following, we designate vectors, matrices, and tensors, respectively, with bold lowercase letters, bold capital letters, and calligraphy letters. We assume that there are T tasks and T associated datasets\nD i = {(x (i) j , y (i) j ) | j \u2208 {1, . . . , N i }}\nwhere N i is the size of the i th collection. We denote by l i the loss function, \u03d5 i the specific parameters of the i th task, and \u0398 the shared parameters between tasks. The Multi-Task Learning objective function is:\nEQUATION\nBy adopting a low-rank tensor update for the\nx W B [i,:] C \u22a4 A b E [i,:] D + O i\nFrozen weights\n\nShared weights\nTask specific weights weights tensor and a low-rank matrix update for the biases, we suggest extending the approaches proposed by Hu et al. (2021) and Ben Zaken et al.\n(2022) to multi-task learning and updating the weights and biases for several tasks concurrently.\n\nThe proposed framework\nOur proposal is to update the output of the transformer layer for the i th task as follows: if the dense layer is the query or value matrix;\nO i = (W + Q i )X + b + b i , otherwise we have O i = WX + b + b i ;\nwhere W and b are frozen weights of BERT, Q i and b i are the updates and X, O i are respectively the input, output of the layer (Figure 1 ).\nWeights assumption. The weight updates for T tasks can be stacked in a single\nd \u00d7 T \u00d7 d tensor, so each matrix is a slice is Q [:,i,:] = Q i of the tensor. Then our assumption is that Q has low- rank; Q = rw r=1 A [:,r] \u2297 B [:,r] \u2297 C [:,r]\n, where A, C \u2208 R d\u00d7Rw , B \u2208 R T \u00d7Rw and r w represents the rank.\nBias assumption. The variation of the original bias for each task can be represented by a matrix B \u2208 R d\u00d7T where the i th column b i represents the bias of the task i \u2208 {1, . . . , T }. We assume that the matrix B is a low-rank matrix and can be written by the product of two matrices D \u2208 R d\u00d7r b and\nE \u2208 R r b \u00d7T where r b represented the rank, i.e., b i = r b t=1 D [:,t] \u00d7 E [t,i]\n(2)\n\nMotivation\nThe straightforward MTL extension of LoRA (Hu et al., 2021 ) combined with BitFit (Ben Zaken et al., 2022) would be to train the same low-rank matrix Q i and bias b i for all tasks. This approach will be called LoRA_Bitfit_MTL in the rest of the paper. However we argue that our approach is more flexible than LoRA_Bitfit_MTL because it is a particular case of MORRIS where the entries of matrices B, E are all set to 1. Moreover our approach is quite natural because the concatenation of low rank matrices creates a tensor with a rank at most equal to the sum of the rank of the matrices.\n\nInterpretation as shared and task-specific weights\nThe underlying assumptions allow the following interpretations. The slices of the weight tensor with low-rank tensor structure factorize as follows;\nQ i = A \u00d7 diag(B [i,:] ) \u00d7 C \u22a4 ,\nwhere diag() is the diagonal matrix built from a given vector. The matrix A and C are then shared between task whereas rows of B are task specific parameters. Similarly, for biases the matrix D is shared between each task and the column of E are task-specific (Figure 1 ).\n\nApply L 0 to find the optimal rank\nThe rank of the tensor must be at most the sum of the ranks of the preceding matrices. Decreasing this rank will reduce the number of model parameters, however there is no straightforward manner to fix this rank, as well as the bias rank which is similarly tough to be defined. Following, Louizos et al. (2018) 's work, we propose to use L 0 regularisation on the rows and columns of respectively B and E to define the ranks. In this case, the binary mask associated to \u03b1 called z can estimated as:\nu \u223c U (0, 1)\nEQUATION\nBased on this definition, Equation (1) can then be written in the following form:\nEQUATION\nWhere l and d are two stretching constants, \u03bb controls the strength of the L 0 regularisation and \u03c3 is the the sigmoid function. More details of this regularisation can be found in Louizos et al. (2018) ; Guo et al. (2021) .\n\nExperiments and Analyse\nWe shall now present our experimental results.\n\nImplementation details\nWe use BERT as the base model in Morris, that we implemented using Pytorch 1 . Furthermore, We use a fully connected layer on the [CLS] token as decoder for each task; with the cross entropy loss and the mean squared error for respectively the classification and the regression tasks. The values of the hyperparameters were fixed as the ones in LoRA (Hu et al., 2021) . We select a batch size of 32 for all experiments, learning rate in {4e \u22124 , 1e \u22124 , 5e \u22125 } for single task approaches, {1e \u22123 , 4e \u22124 , 1e \u22124 , 5e \u22125 } for Multi-task approaches and dropout equal to 0.1 with AdamW (Loshchilov and Hutter, 2017) as the optimizer. For single task approaches the rank was set to 8, and, {8, 16, 32, 64} for LoRA_BitFit_MTL. For Morris the rank of the bias was set to 4 in all experiments and the rank of the tensor corresponding to the weights to 64. For the L 0 regularisation, \u03bb was found in the interval {1e \u22125 , 5e \u22126 , 2e \u22126 which corresponds to a rate of sparsity equals to {60%, 40%, 20%}. In Multi-Task learning one of the major influencing factor is the choice of the data sampling policy (Glover and Hokamp, 2019) . We picked the same as Mahabadi et al. ( 2021) and the same number of training steps equal to 2 18 since our objective is not to research the impacts of sampling policy.\nIn our experiments, we did a short pre-training of 10000 step with a learning rate equals to 4e \u22124 , after that all \u03b1 j lower than 0.5 were pushed to 0 and the (Devlin et al., 2018) ; [b] are from (Fu et al., 2022) ; [c] are from (Glover and Hokamp, 2019) ; [d] are from (Pilault et al., 2021) ; \u2020 are results obtained with the best checkpoint in our settings.\nothers were set to pushed to 1. The pseudo-code of our approach is provided in the Appendix A.6).\n\nMetrics and Baselines\nWe considered the General Language Understanding Evaluation (GLUE) benchmark in our experiments (benchmark details are given in Appendix A.4). As metrics, we considered standard measures that are Matthews Correlation for COLA and Spearman Correlation for STS-B, F1 score for MRPC/QQP, as well as accuracy. As baselines, we compared Morris to our implementations of the following approaches: LoRA (Hu et al., 2021) , Lora_BitFit which combines LoRA and BitFit in (Hu et al., 2021; Ben Zaken et al., 2022) , as well as LoRA_BitFit_MTL presented above. All experiments are done on 3 seeds and the results are the average value of the performances. For this part, we chose to not use the test online but we split each dev set into dev/test set. We also compared Morris to single task models: BERT (Devlin et al., 2018) , Bitfit (Ben Zaken et al., 2022) , as well as, Multi-task models which are two extensions of BERT to this case (Glover and Hokamp, 2019) , PALs (Stickland and Murray, 2019) and CA-MTL (Pilault et al., 2021) . This comparison is done on the online test set, the best model on three seeds was kept for the comparison.\n\nResults\nWe first begin to compare our approach with LoRA and its extensions. Performances are shown in Table 2. As a result, the average performance appears to be improved by a factor of 0.5 when the LoRA and Bitfit techniques are combined. Moreover, this approach seems to be efficient in a Multi-task Learning setting, as LoRA_BitFit_MTL increases the general performance by 0.6. Finally, Morris outperforms all our baseline. In addition, the use of the L 0 regularisation enables the model to decrease the number of parameters by a factor of 0.6 with no loss of performance. Given that all approaches employ training parameters of the same order of magnitude, comparisons in this situation are straightforward. In a more general case, when Morrisis compared to the other methods in Table 1 , this is not the case. We first notice that Morris performs better than all single task approaches. Furthermore, our approach trains the model with less parameters than the other approaches by a higher factor in the multi-target case. Only the CA-MTL (Pilault et al., 2021) seems to be competitive with Morris. We justify this by pointing out that, in contrast to our sampling approach, the sampling strategy used in (Pilault et al., 2021) is highly extensive. In the general case, our approach is equivalent or better than the most of the baselines in term of average \n\nInteraction between tasks\nWe assume that tasks are similar if their weight variations are similar. As a measure, we compare biases and slices using the cosine similarity:\nEQUATION\nWe will explore the weights (wq) and (bm2) due to their significant variance in light of prior works (Ben Zaken et al., 2022; Hu et al., 2021) . For this, we examine the relationship between Morris and Morris with the L 0 regularization. In order to analyze more broad interactions, we often create NB similarity matrices by using Equation (5). We then decide to average these NB similarity matrices. These findings are presented in Figure 2 where it is clear that the diagonal block reflecting the kind of task has the greatest similarity score. Additionally, the coefficients are not close to 1, indicating that task-specific weights enable successful task differentiation. The bias similarity seeks to distinguish the CoLA and SST-2 tasks from the other tasks, which are known to be uncorrelated one from another and regularization reduces task similarity.\n\nConclusion\nIn this paper we presented, a novel method for multi-task learning relies on stacking the neural network weights into a tensor. We demonstrated that low-rank updates in the conventional polyadic tensor decomposition of this tensor of weights result in an efficient technique that allows for a significant reduction in model parameters without sacrificing performance. On the GLUE Benchmark, we showed that our proposed approach successfully achieves a compromise between maintaining positive transfer and reducing negative transfer by only using 0.3% of the initial model's parameters.\n", "hypothesis": " Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years.  This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models.  In this paper, we propose a new approach for Multi-task learning which is based on stacking the weights of Neural Networks as a tensor.  We show that low-rank updates in the canonical polyadic tensor decomposition of this tensor of weights lead to a simple, yet efficient algorithm, which without loss of performance allows to reduce considerably the model parameters.  We investigate the interactions between tasks inside the model as well as the inclusion of sparsity to find the best tensor rank and to increase the compression rate.  Our strategy is consistent with recent efforts that attempt to use constraints to fine-tune some model components.  More precisely, we achieve equivalent performance as the state-of-the-art on the General Language Understanding Evaluation benchmark by training only 0.3% of the parameters per task while not modifying the baseline weights..", "answer": true}
{"title": "Black-box language model explanation by context length probing", "content": "\nIntroduction\nLarge language models (LMs), typically based on the Transformer architecture (Vaswani et al., 2017) , have recently seen increasingly widespread adoption, yet understanding their behaviour remains a difficult challenge and an active research topic.\nNotably, as the length of the context that can be accessed by LMs has grown, a question that has attracted some attention is how this influences their predictions. Some recent studies in this line of research suggest that even \"long-range\" LMs focus heavily on local context and largely fail to exploit distant ones (O'Connor and Andreas, 2021; Sun et al., 2021; Press et al., 2021; Sun et al., 2022) . A more nuanced understanding of how contexts of different lengths influence LMs' predictions may hence be valuable for further improving their performance, especially on tasks like long-form text generation where long-range dependencies are of critical importance.\nFigure 1 : A screenshot of a demo 2 of the proposed method. After selecting a target token (here \"birds\"), the preceding tokens are highlighted according to their (normalized) differential importance scores (green = positive, red = negative), obtained using our method. The user can also explore the top predictions for contexts of different lengths (here the context \"house, shouting about lunatics. [. . .] mortally afraid of\").\nIn this work, we propose context length probing, a simple explanation technique for causal (autoregressive) language models, based on tracking the predictions of the model as a function of the number of tokens available as context. Our proposal has the following advantages:\n\u2022 It is conceptually simple, providing a straightforward answer to a natural question: How does the length of available context impact the prediction?\n\u2022 It can be applied to a pre-trained model without retraining or fine-tuning and without training any auxiliary models.\n\u2022 It does not require access to model weights, internal representations or gradients.\n\u2022 It is model-agnostic, as it can be applied to any causal LM, including attentionless architectures like RNN (Mikolov et al., 2010) and CNN (Dauphin et al., 2017) . The only requirement for the model is to accept arbitrary input segments (i.e. not be limited to document prefixes).\nFurthemore, we propose a way to use this technique to assign what we call differential importance scores to contexts of different lengths. This can be seen as complementary to other techniques like attention or saliency map visualization. Interestingly, contrary to those techniques, ours appears promising as a tool for studying long-range dependencies, since it can be expected to highlight important information not already covered by shorter contexts.\n\nRelated work\nA popular way to dissect Transformers is by visualizing their attention weights (e.g. Vig, 2019; Hoover et al., 2020) . However, it has been argued that this does not provide reliable explanations and can be misleading (Jain and Wallace, 2019; Serrano and Smith, 2019) . A more recent line of work (Elhage et al., 2021; Olsson et al., 2022) explores \"mechanistic explanations\", based on reverse-engineering the computations performed by Transformers. These techniques are tied to concrete architectures, which are often \"toy\" versions of those used in real-world applications, e.g. attention-only Transformers in Elhage et al.\nOther options include general-purpose methods like neuron/activation interpretation (e.g. Geva et al., 2021; Goh et al., 2021; Dai et al., 2022) , saliency maps (e.g. Fong and Vedaldi, 2017; Ancona et al., 2019) and influence functions (Koh and Liang, 2017) . These require access to internal representations and/or the ability to backpropagate gradients, and have some caveats of their own (Kindermans et al., 2019; Kokhlikyan et al., 2021) .\nMore closely related to our work are studies that perform ablation (e.g. by shuffling, truncation or masking) on different contexts to understand their influence on predictions (O'Connor and Andreas, 2021; Sun et al., 2021; Press et al., 2021; Vafa et al., 2021) . To our knowledge, all such existing works only test a few select contexts or greedily search for the most informative one; in contrast, we show that it is feasible to consider all context lengths in the range from 1 to a maximum c max , which permits us to obtain fine-grained insights on the example level, e.g. in the form of the proposed differential importance scores. Moreover, many existing analyses (e.g. Vafa et al., 2021; O'Connor and Andreas, 2021) rely on specific training or finetuning, which is not the case with our proposal.\n\nContext length probing\nA causal LM estimates the conditional probability distribution of a token given its left-hand context in a document:\np(x n+1 | x 1 , . . . , x n ).\n(1)\nWe are interested here in computing the probabilities conditioned on a reduced context of length c \u2208 {1, . . . , n}:\nEQUATION\nso that we may then study the behavior of this distribution as a function of c. An apparent obstacle in doing so is that applying the model to an arbitrary subsequence x n\u2212c+1 , . . . , x n , instead of the full document x 1 , . . . , x N , may lead to inaccurate estimates of the probabilities in Eq. ( 2). However, we note that large LMs are not usually trained on entire documents. Instead, the training data is pre-processed by shuffling all the documents, concatenating them (with a special token as a separator), and splitting the resulting sequence into chunks of a fixed length (usually 1024 or 2048 tokens) with no particular relation to the document length. Thus, the models are effectively trained to accept sequences of tokens starting at arbitrary positions in a document and it is therefore correct to employ them as such to compute estimates of Eq. (2). 3 It now remains to be detailed how to efficiently evaluate the above probabilities for all positions n and context lengths c. Specifically, for a given document x 1 , . . . , x N and some maximum context length c max , we are interested in an (N \u2212 1) \u00d7 c max \u00d7 |V| tensor P , where V = w 1 , . . . , w |V| is the vocabulary, such that: 4 Observe that by running the model on any segment x m , . . . , x n , we obtain all the values P m+c\u22121,c, * for c \u2208 {1, . . . , n \u2212 m + 1}. Therefore, we can fill in the tensor P by applying the model along a sliding window of size c max , i.e. running it on N (overlapping) segments of length at most c max . See Appendix A for an illustration and additional remarks.\nP n,c,i = p(x n+1 = w i | x n\u2212c+1 , . . . , x n ), (3) with P n,c, * = P n,n\u22121, * for n \u2264 c.\n\nMetrics\nHaving obtained the tensor P as we have just described, we use it to study how the predictions evolve as the context length is increased from 1 to c max . Specifically, our goal is to define a suitable metric that we can compute from P n,c, * and follow it as a function of c (for a specific n or on average).\nOne possibility would be to use the negative loglikelihood (NLL) loss values:\n\u2212 log p(x n+1 | x n\u2212c+1 , . . . , x n ).\n(4) However, this may not be a particularly suitable metric for explainability purposes, as it depends (only) on the probability assigned to the ground truth x n+1 , while the LM outputs a probability distribution P n,c, * over the entire vocabulary, which may in fact contain many other plausible continuations. For this reason, we propose to exploit a metric defined on whole distributions, e.g. the Kullback-Leibler (KL) divergence. To achieve this, we choose the maximum-context predictions P n,cmax, * as a reference and get:\nEQUATION\nThe rationale for ( 5) is to quantify the amount of information that is lost by using a shorter context c \u2264 c max . Interestingly, this metric is not related to the absolute performance of the model with maximal context, but rather to how the output changes if a shorter context is used.\n\nDifferential importance scores\nWe are also interested in studying how individual increments in context length affect the predictions.\nWe propose to quantify this as the change in the KL divergence metric (5) when a new token is introduced into the context. Specifically, for a pair of tokens x n+1 (the target token) and x m (the context token), we define a differential importance score (\u2206-score for short)\nEQUATION\nWe may visualize these scores as a way to explain the LM predictions, much like is often done with attention weights, with two important differences. First, a high \u2206D n,m should not be interpreted as meaning that x m in isolation is important for predicting x n+1 , but rather that it is salient given the context that follows it (which might mean that it brings information not contained in the following context). Second, unlike attention weights, our scores need not sum up to one, and can be negative; in this regard, the proposed representation is more conceptually similar to a saliency map than to an attention map.\n\nResults\nWe apply the proposed technique to publicly available pre-trained large Transformer language models, namely GPT-J (Wang and Komatsuzaki, 2021) and two GPT-2 (Radford et al., 2019) variantssee Table 1 for an overview. We use the validation set of the English LinES treebank 5 from Universal Dependencies (UD; Nivre et al., 2020) , containing 8 documents with a total length of 20 672 tokens 6 and covering fiction, an online manual, and Europarl data. We set c max = 1023. We use the Transformers library 7 (Wolf et al., 2020) to load the pre-trained models and run inference. Further technical details are included in Appendix B.\n\nLM loss by context length\nFig. 2 shows the cross entropy losses (NLL means) across the whole validation dataset as a function of context length c. As expected, larger models perform better than smaller ones, which is traditionally explained by their larger capacity. A less common observation we can make thanks to this detailed representation is that the gains in performance come mostly from relatively short contexts (8-256 tokens); this is consistent with prior works (Sun et al., 2021; Press et al., 2021) that very long contexts bring only minimal improvement (though these focused on specific long-range architectures and on contexts beyond the range we investigate here).\nIn Fig. 3 , we display the same information (loss by context length) broken down by part-of-speech (POS) tags, for GPT-J only. For most POS tags, the behavior is similar to what we observed in Fig. 2 and the loss appears to stabilize around context lengths 16-64. However, we see a distinct behaviour for proper nouns (PROPN), which are the hardest-to-predict category for short contexts, but whose loss improves steadily with increasing c, surpassing that of regular nouns (NOUN) at c = 162 and continuing to improve beyond that point.\n\nPer-token losses by context length\nWe have also examined token-level losses, as well as the KL divergence metric (see Section 3.2); an example plot is shown in Fig. 4 and more are found in Appendix C.1. In general, we observe that the values tend to change gradually with c; large differences are sparse, especially for large c, and can often be attributed to important pieces of information appearing in the context (e.g. \"owl\" and \"swoop\" in the context of \"birds\" in Fig. 4 ). This justifies our use of these differences as importance scores.\n\nDifferential importance scores\nTo facilitate the exploration of \u2206-scores from Section 3.3, we have created an interactive web demo, 2 which allows visualizing the scores for any of the 3 models on the validation set as shown in Fig. 1 .\nIn Fig. 5 , we display the magnitudes of the \u2206scores -normalized for each position to sum up to 1 across all context lengths -as a function of context length. The plot suggests a power-law-like inverse relationship where increasing context length proportionally reduces the \u2206-score magnitude on average. We interpret this as far-away tokens being less likely to carry information not already covered by shorter contexts. Long contexts (see inset in Fig. 5 ) bear less importance for larger models than for smaller ones, perhaps because the additional capacity allows relying more on shorter contexts.\nIn Fig. 6 , we also display the mean importance score received by each POS category, by model. We can see that proper nouns (PROPN) are substantially more informative than other categories (which is in line with the observations in the previous section), but less so for the smallest model. This could mean e.g. that larger models are better at memorizing named entities from training data and using them to identify the topic of the document, or simply at copying them from distant context as observed in (Sun et al., 2021) .\n\nLimitations and future directions\nExperiments. We acknowledge the limited scope of our experiments, including only 8 (closeddomain) documents, 3 models and a single language. This is largely due to the limited availability of suitable large LMs and their high computational cost. Still, we believe that our experiments are valuable as a case study that already clearly showcases some interesting features of our methodology.\nComputational cost. While we have demonstrated an efficient strategy to obtain predictions for all tokens at all possible context lengths, it still requires running the model N times for a document of length N .\nFor a k-fold reduction in computational cost, the technique may be modified to use a sliding window with stride k > 1 (instead of k = 1 as proposed above). See Appendix A.1 for details. Choice of metrics. The proposed methodology allows investigating how any given metric is impacted by context, yet our study is limited to NLL loss and the proposed KL divergence metric (the latter for defining importance scores). These may not be optimal for every purpose, and other choices should be explored depending on the application. For example, to study sequences generated (sampled) from a LM, one might want to define importance scores using a metric that does depend on the generated token, e.g. its NLL loss or its ranking among all candidates. (Indeed, our web demo also supports \u2206-scores defined using NLL loss values.)\n\nConclusion and future directions\nWe have presented context length probing, a novel causal LM explanation technique based on tracking the predictions of the LM as a function of context length, and enabling the assignment of differential importance scores (\u2206-scores). While it has some advantages over existing techniques, it answers different questions, and should thus be thought of as complementary rather than a substitute.\nA particularly interesting feature of our \u2206-scores is their apparent potential for discovering longrange dependencies (LRDs) (as they are expected to highlight information not already covered by shorter contexts, unlike e.g. attention maps).\nRemarkably, our analysis suggests a power-lawlike inverse relationship between context length and importance score, seemingly questioning the importance of LRDs in language modeling. While LRDs clearly appear crucial for applications such as longform text generation, their importance may not be strongly reflected by LM performance metrics like cross entropy or perplexity. We thus believe that there is an opportunity for more specialized benchmarks of LRD modeling capabilities of different models, such as that of Sun et al. (2022) , for example. These should further elucidate questions like to what extent improvements in LM performance are due to better LRD modeling, how LRDs are handled by various Transformer variants (e.g. Kitaev et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2021; Press et al., 2022) , or what their importance is for different tasks. When the LM is run on a segment of the document, the effective context length for each target token is equal to its offset from the beginning of the segment, e.g. the context for predicting \" D\" is \" the\" (c = 1), the context for \"urs\" is \" the D\" (c = 2), etc.\n", "hypothesis": " The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts.  The technique is modelagnostic and does not rely on access to model internals beyond computing token-level probabilities. Additionally, our technique outperforms other explanation techniques like attention or saliency map visualization in studying long-range dependencies, providing a comprehensive understanding of the model's behavior.  We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies.  The source code 1 and an interactive demo 2 of the method are available..", "answer": false}
{"title": "Type Enhanced BERT for Correcting NER Errors", "content": "\nIntroduction\nNamed entity recognition (NER) is the task of identifying spans that belong to particular categories, such as person, location, organization, etc. The NER task is important in the information extraction area and NER models are widely deployed in real production systems (Yadav and Bethard, 2019) . In recent years, many neural-based methods were proposed to push NER accuracy by designing novel network architectures (Lample et al., 2016; Devlin et al., 2018; Strakov\u00e1 et al., 2019; Xue et al., 2022) or incorporating external knowledge (Liu et al., 2019; Wang et al., 2021) . Unfortunately, all approaches are still far from perfect. When the model is served in production, we may still encounter recognition errors (e.g., bad cases).\nTypically, to fix those bad cases, model developers need to (1) annotate the input sentences causing errors with correct labels, (2) combine newly annotated sentences with existing training data, (3) train and tune a new model with the new training data * Equal contribution.\n\nInput Sentences\nPredict case 1: Mike Moreton joined to run the XJ220 project.\ncase 2: Nicaragua, the previous year 's winner, was forced to withdraw from the contest. case 1: Mike Moreton [person] joined to run the XJ220 project.\ncase 2: Nicaragua [location_gpe] , the previous year 's winner, was forced to withdraw from the contest.\n\nGazetteer XJ220\n[product_car]\nNicaragua [location_gpe, organization_sportsteam] Predict with updated gazetteer case 1: Mike Moreton [person] joined to run the XJ220 [product_car] project.\ncase 2: Nicaragua [organization_sportsteam] , the previous year 's winner, was forced to withdraw from the contest. and held-out evaluation data, and finally (4) deploy the new model in production. As one can tell, the above process is time-consuming, and cannot meet the requirement of fixing urgent errors quickly in a real production environment. Therefore, in this paper, we aim to tackle the problem of how to correct NER errors without retraining models. 1 Taking case 1 and 2 from Figure 1 as examples, there are two kinds of common NER errors when we train and evaluate a model in the English Few-NERD (Ding et al., 2021) corpus: (1) the model fails to recognize the span \"XJ220\" as a named entity; (2) the model correctly identifies the boundary of the named entity \"Nicaragua\", but assigns a wrong entity type to it.\n\nUpdate Gazetteer\nFor the first error, we find the span \"XJ220\" never appears in the training dataset. Therefore, it is difficult for the model to classify this span as a named entity with limited context. For the second error, the mention \"Nicaragua\" is found in the training dataset, but it is labeled with a different type location. Because of the incomplete type information, the model mistakenly classifies the mention as type location, though the correct label should be organization_sportsteam.\nThe above examples suggest that if we have proper type information about the span, the model may correct its mistakes, even without re-training. It motivates us to propose the Type Enhanced BERT (TyBERT) method that combines BERT with type information from a gazetteer.\nAs shown in Figure 1 , the gazetteer is a list of pairs of spans and possible entity types. During training, we first look up spans from the gazetteer in training examples, and then integrate the matched span's type information into BERT layers by an adapter layer. In the inference stage, the test examples are processed in the same way. In such a manner, the model is tied to the gazetteer, which will play an important role when the model makes predictions. When encountering the aforementioned two kinds of errors, we can update the gazetteer: we insert a new named entity \"XJ220\" with the expected type product_car, and add a new type organization_sportsteam for the existing named entity \"Nicaragua\". Moreover, we introduce a noise rate parameter \u03bb to randomly add some noise to the gazetteer. This parameter serves as an adjuster to balance the strength of the gazetteer and the generalization ability of the model.\nTo our knowledge, this is the first work to systematically study how to improve NER models without re-training models. When evaluated in four NER corpus in English and Chinese, the proposed method performs well in fixing errors and outperforms strong baselines. Our code and data will be released after publication.\n\nRelated Work\nOur work is influenced by existing methods which combine both neural networks and lexicons or gazetteers for NER. For example, Zhang and Yang (2018) proposed a lattice-structured LSTM encoding both a sequence of input characters and potential words that match a pre-gathered lexicon. Sui et al. (2019) presented Collaborative Graph Network to solve the challenges of self-matched lexical words and the nearest contextual lexical words. Gui et al. (2019) aimed to alleviate the word ambigu-ity issue by a lexicon-based graph neural network with global semantics. Lin et al. (2019) designed an attentive neural network to explicitly model the mention-context association and gazetteer network to effectively encode name regularity of mentions only using gazetteers. Li et al. (2020) introduced a flat-lattice Transformer to incorporate lexicon information for Chinese NER. Meng et al. (2021) invented GEMNET to include a Contextual Gazetteer Representation encoder, combined with a novel Mixture-of-Expert gating network to conditionally utilize this information alongside any word-level model. Fetahu et al. (2022) invented an approach of using a token-level gating layer to augment pretrained multilingual transformers with gazetteers from a target domain. Finally, Liu et al. (2021) proposed Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer.\nIt is worth noting that none of the previous works can be directly applied for correcting NER models without re-training. For example, LEBERT requires learning lexicon embeddings in the adapter layer. If we want to add a new span in the lexicon to fix a bad case, the model has to be re-trained to learn the new span's embedding.\n\nGazetteer Construction\nAs noted before, the gazetteer contains a list of named entities and their possible entity types. In this paper, we collect the gazetteer solely from NER annotations in the dataset. For instance, given the following two annotated sentences from the Few-NERD corpus:\nLondon [art\u2212music] is the fifth album by the British [location\u2212gpe] rock band.\nHe is domiciled in London [location\u2212gpe] . We will construct the following gazetteer:\nLondon [art-music, location-gpe] British [location-gpe]\nWe employ this simple approach because it is applicable for NER tasks in any language or domain. One can also use external resources such as Wikipedia to construct a larger gazetteer (Fetahu et al., 2021) . We will explore a larger gazetteer in future work because it is not the focus in this paper.\nFurthermore, although the generated gazetteer is pretty accurate, a downside is that when we integrate such a high-quality gazetteer in the model, the model tends to put too much trust in the gazetteer. In the other way round, it hurts the model's generalization ability. Therefore, we intentionally add some noise to the gazetteer. Specifically, with probability \u03bb, we choose one of the following three strategies to add noise: (1) randomly select a span that is not labeled as named entity, and then add it to the gazetteer with a random entity type; (2) for a labeled named entity span, add it to the gazetteer with a randomly assigned wrong entity type; (3) skip over adding a labeled named entity span to the gazetteer. In practice, we set \u03bb to a small value, so that it gives the gazetteer strong control in making final predictions, while the model's generalization ability is still reserved to some degree.\nNote that during training, the gazetteer is constructed using training and development data. When we want to fix errors in test data, the gazetteer is updated using test data.\n\nModel Architecture\nTyBERT is built on standard BERT with two modifications: (1) given a sentence, the input word sequence is converted to a word-type pair sequence that will be the input for TyBERT; (2) a type adapter for integrating type information in BERT is attached between Transformer layers. Word-Type Pair Sequence. Given a gazetteer G and a sentence with a sequence of words s w = {w 1 , w 2 , ..., w n }, we match the word sequence with G to find out all potential named entities inside the sentence. So we have a word-type pair sequence s wt = {wt 1 , wt 2 , ..., wt n }. When the word w i is not a part of any potential named entity, wt i is w i . Otherwise, wt i is (w i , t i ), where t i is all matched entities' types with B-or I-as prefix to indicate whether it begins or inside a named entity.\nTaking the sentence \"London Bridge is famous\" for example, the word \"London\" is a part of two potential named entities, i.e., (1) \"London\" with type art-music and location-gpe, and (2) \"London Bridge\" with type building. Therefore, t i for the word \"London\" is {[B-art-music, B-locationgpe], [B \u2212 building]}.\nFormally, we have t i ={T ype(x ij )}. x ij is the j th potential named entity that contains the word w i . T ype(x)=[et 1 , et 2 , ..et k ] represents all possible entity types of named entity x based on G, and et i is one of the possible labels, such as B-artmusic, etc. Type Adapter. Our Type Adapter (TA) is shown\n! \u210e ! Add & Norm Bilinear Attention \ud835\udc5a !\" \u210e ! Bilinear Attention \u210e ! \ud835\udc47\ud835\udc66\ud835\udc5d\ud835\udc52(\ud835\udc65 !\" )\nFigure in Figure 2 , which is inspired by Lexicon Adapter proposed in Liu et al. (2021) . Specifically, as discussed above, t i has a two-level structure, so we propose a two-level attention mechanism.\nFirstly, at position i, we compute the cross attention between the hidden state h i with the embeddings of possible entity types T ype(x ij ) for a potential named entity x ij to obtain m ij . Then we compute another cross attention between the hidden state h i and m ij , and finally obtain the new hidden state hi .\nCompared with BERT, the only extra parameters of TyBERT are the embeddings of entity type et k and related weights in two cross attentions, which can be fully learned in training time. Thus, when updating the gazetteer in test time, we don't have to update any parameters in TyBERT. Following Liu et al. (2021) , we only insert a TA after the first transformer layer. \n\nExperimental Setup\nDatasets. For evaluation, we employ four datasets, two in English and two in Chinese. For English, we employ the commonly used OntoNotes 5.0 corpus (Pradhan et al., 2013) and also the challenging Few-NERD corpus (Ding et al., 2021) with 66 finegrained types. For Chinese, we employ OntoNotes 4.0 corpus (Weischedel et al., 2011) and Weibo corpus (Peng and Dredze, 2015, 2016) from social media domain. The detailed statistics of four corpora are shown in Table 1 . Evaluation measures. \n\nResults\nBaseline systems. To compare with our proposed method, we use BERT (Devlin et al., 2018) as a baseline. Because standard BERT cannot correct errors without model re-training, we further designed two additional baseline systems. These two baseline systems ensemble BERT and a rule-based method using a gazetteer as follows. We construct the gazetteer using all of training, development and test data. Then the gazetteer is used to match the sentences in test data to identify named entities. When a span has multiple entity types, we randomly assign a type. Depending on whether we intersect or union the output of BERT and the rule-based method, we name two baseline systems BERT+Intersect and BERT+Union respectively. Discussions. Results of BERT, two extra baseline systems and our proposed TyBERT are shown in Table 2 in three corpora, and BERT+Union only improves BERT slightly in Few-NERD corpus. In contrast, with \u03bb=0.05 (tuned on development set), our proposed method TyBERT improves BERT by a large margin, i.e., 6.63% and 18.91% in two English corpus, and 3.56% and 6.05% in two Chinese corpus. We notice that the improvement in Chinese corpus is smaller than in English corpus. The reason is that there are much more named entities with multiple types in Chinese corpus, e.g., the confusion of location and gpe have caused many errors. In future work, we plan to consider named entity's context to fix errors. We have separately analyzed the gains brought by our solution on the ontonotes v4.0 datasets are shown in Appendix D.\n\nImpact of gazetteer noise\nWe further conduct experiments to study the impact of gazetteer noise in Chinese OntoNotes corpus.\nResults are shown in Table 3 . For each \u03bb, we show the results of TyBERT before and after updating the gazetteer using test data. A few observations are obtained. When \u03bb is set to 0, the model before updating gazetteer loses generalization ability, and hence performs poorly. After \u03bb is set to a nonzero value, the model before updating gazetteer improves a lot, and many errors are fixed after updating the gazetteer using test data.\n\nConclusions\nWe 2022), we will construct a larger gazetteer using external resources such as Wikipedia or knowledge bases. As mentioned in Section 3, we will leave this for future work.\nAnother limitation is that the gazetteer contains many spans that are associated with multiple entity types. Taking the running examples in Section 3.1 for example, the span \"London\" has type locationgpe in most cases, while it is sometimes labeled as type art-music. However, in our current design, given a named entity, there is no way to explicitly distinguish between different types. In future work, we will consider the context of named entity when fixing errors.\n", "hypothesis": " We introduce the task of correcting named entity recognition (NER) errors without retraining the model.  After a NER model is trained and deployed in production, it makes prediction errors, which usually need to be fixed quickly.  To address this problem, we firstly construct a gazetteer containing named entities and corresponding possible entity types.  And then, we propose type-enhanced BERT (TyBERT), a method that integrates the named entity's type information into BERT by an adapter layer.  When errors are identified, we can repair the model by updating the gazetteer.  In other words, the gazetteer becomes a trigger to control the NER model's output.  The experiment results in multiple corpus show the effectiveness of our method, which outperforms strong baselines..", "answer": true}
{"title": "An Exploratory Study on Model Compression for Text-to-SQL", "content": "\nIntroduction\nText-to-SQL is an important task that has been gaining the attention of researchers over the years. Formally, given a query q and a relational database D, the goal of Text-to-SQL is to build a model f such that s = f (q, D | \u03b8) where \u03b8 is a vector of model parameters and s is a predicted SQL statement which we can use to retrieve the answer to q from D.\nText-to-SQL has many potential applications that can improve our standard of living. For example, medical chatbots can convert user queries into SQL statements and then use them to retrieve relevant information from medical knowledge bases. Industry can leverage Text-to-SQL tools to help employees shorten the time needed to write complex SQL queries, thereby improving overall work productivity.\nThe recent emergence of complex Text-to-SQL datasets containing complicated SQL and crosstable setup has driven researchers to develop huge models that encode various complex relationships between table schema and query with large pretrained language models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) . These models are usually sequence-to-sequence models that generate SQL statements sequentially or sketch-based models that use classifiers to fill in the slots of SQL templates.\nHowever, despite achieving state-of-the-art performances on benchmark datasets, such models are usually both memory and computationally expensive, making it technically challenging to deploy them in memory-constrained real-world applications that require low inference latency. Therefore, to deploy state-of-the-art Text-to-SQL models in real-world production environments, we must drastically improve the inference time and reduce the number of parameters in these models.\nWe turn to the field of model compression (Cheng et al., 2017) for solutions that can speed up inference without significantly hurting model performance. Formally, the goal of model compression is to reduce f to a smaller model f \u2032 such that s \u2032 = f \u2032 (q, D | \u03b8 \u2032 ). Ideally, we want s \u2032 to be the same as s and dim(\u03b8 \u2032 ) to be much smaller than dim(\u03b8).\nIn this paper, we thoroughly examine the feasibility of using model compression techniques to build faster and more accurate Text-to-SQL models that we can successfully deploy in the real world. For this, we carefully apply a few model compression methods to representative sequence-to-sequence or sketch-based Text-to-SQL models on three datasets: WikiSQL, Spider, and TableQA. The main findings of this paper are: (i) sketch-based models generally respond well to model compression techniques, while sequence-to-sequence models show mixed results, (ii) we observe better speed improvements in Sketch-based models as their slot-filling components are much faster than the decoding components of sequence-to-sequence models. (iii) model compression techniques work poorly on state-of-the-art Text-to-SQL models built on pre-trained encoder-decoder language models such as T5.\nWe hope our findings can empower practitioners to make more informed decisions when selecting Text-to-SQL models and compressing them appropriately for real-world deployments. . Contrarily, Spider contains large samples of complex SQL instances that connect multiple tables with primary and foreign keys with more advanced clauses such as nested queries, JOIN ON, and ORDER/GROUP BY.\n\nBaseline Models\nRecent deep neural Text-to-SQL models can be broadly classified under two categories: sequenceto-sequence models and sketch-based (also known as slot-filling) models.\n\nSequence-to-sequence models\nSequence-to-sequence models are generally made up of an encoder component that converts user query inputs together with database information into a hidden vector and a decoder component that generates SQL statements based on the output hidden vectors from the encoder. BRIDGE (Lin et al., 2020) encodes input questions and table schema with BERT and LSTM and generates SQL predictions with a pointer-generator decoder (See et al., 2017) supported by a schemaconsistency driven search space pruning strategy. RAT-SQL (Wang et al., 2020a ) also encodes input instances with BERT but generates SQL as an abstract syntax tree (AST) with a tree-structured decoder (Yin and Neubig, 2017) . It also incorporates a relation-aware self-attention mechanism that further improves schema-linking, schema-encoding, and representation of the encoder. PICARD (Scholak et al., 2021) is a state-of-theart algorithm that directly fine-tunes a pre-trained encoder-decoder language model T5 (Raffel et al., 2020) on Text-to-SQL data, and then constrain the decoder to output valid SQL by integrating an incremental parsing strategy to the beam search process.\n\nSketch-based model\nSketch-based methods also encode user inputs into vectors but only need to fill in slots in SQL sketches rather than generating full SQL statements. Each SQL sketch is a template SQL statement with placeholder slots and the goal of sketch-based models is to predict the best item to go into each slot. NL2SQL-RULE (Guo and Gao, 2019 ) is a standard sketch-based model which uses BERT and LSTM to encode input query and database information and predict outputs in slots of SQL sketches.\n\nCompression Techniques\nWe follow Sun et al. (2021) and experiment with the following model compression techniques in this study: Layer Pruning (Sajjad et al., 2022 ) is a simple yet effective strategy that discards a certain number of layers from transformer-based language models before fine-tuning the pruned models on downstream tasks. We apply the top-layer pruning strategy which deletes the top N encoder or decoder layers before the start of any training. Knowledge Distillation (Hinton et al., 2015) is a method that compresses deep neural network models by distilling useful knowledge from a larger model (teacher) to a smaller model (student). We follow Jiao et al. (2020) and distill smaller language models from larger ones such as BERT-large, before fine-tuning Text-to-SQL models on those distilled models. For WikiSQL and Spider, we experiment with the distilled English language models from MiniLM 1 (Wang et al., 2020b) , while for TableQA, we use the Chinese TinyBERT models 2 . Token Pruning For PICARD model, We also apply token pruning (Goyal et al., 2020; Kim et al., 2022) , which is a different pruning strategy that gradually removes redundant token encodings from the outputs of each encoder layer before feeding the reduced number of tokens to the next encoder layer. We follow Goyal et al. (2020) and implement an attention scoring mechanisms which weights the significance of each token by the sum of attention weights it gets from other tokens. The tokens with the lowest significance scores (based on predetermined thresholds) for each encoder layer are dropped.\n\nEvaluation Metrics\nWe evaluate our experiment results using Exact set match (ESM) (Yu et al., 2018) . ESM decomposes every pair of predicted and gold SQL queries into sets clauses and then computes the percentage of exact set matches over all pairs (Zhong et al., 2020) .\n\nExperiment Setup\nIn most cases, we follow the recommended configurations in corresponding papers. We may adjust the batch sizes and learning rates slightly to fit the experiments on our hardware. We train our models on servers with either NVIDIA GV100 GPU (32GB) or RTX A6000 (45GB) but calculate inference speeds by running models on only CPUs with batch size set to one, which better mimics the situations in the real world. For all datasets, we use their dev sets as the test sets and create new train-dev sets in the ratio of 4 to 1 from the original train set. We early stop our models based on the ESM scores on dev sets and report average test set ESM scores over 5 different runs. Other than PI-CARD, we use BERT-large for all English datasets and RoBERTa-Zh (Cui et al., 2020) for TableQA.\n\nSimple datasets\nWikiSQL As shown in Figure 1 WikiSQL. For example, we can remove 50% of the encoder layers from BRIDGE, while only taking a penalty of only 0.82% drop in Exact Set match (ESM). When only keeping the bottom 6 encoder layers, NL2SQL-RULE can still perform at 0.834 ESM, a 3.65% drop from the original unpruned model. For knowledge distillation, we fine-tuned BRIDGE on two versions of MiniLM (Wang et al., 2020b) : L6xH768 and L6xH384. Results show that BRIDGE trained on the MiniLM language models performs slightly worse than the layer pruning method with similar number of layers. However, this is acceptable given the hidden sizes of the MiniLM models are 384 and 768, which are smaller than the hidden size of 1024 for BERT-large. TableQA We notice several differences in results between WikiSQL and TableQA. First, the performances of RATSQL on TableQA are significantly lower than those of NL2SQL-RULE. For example, unpruned NL2SQL-RULE achieves an ESM of 0.8 but unpruned RATSQL only achieves 0.69 despite our best efforts. Second, we observe more significant drops in performances when applying layer pruning and knowledge distillation to RATSQL than NL2SQL-RULE. For example, we observe only a 3.63% drop in ESM dropping the first 16 encoder layers of NL2SQL-RULE but notice an 18.8% drop in the performance of RATSQL with the same configurations. Last but not least, models trained on distilled language models perform slightly worse than the layer pruned models due to their smaller hidden sizes except for NL2SQL-RULE on TinyBERT with 6 layers and 768, which achieves an ESM of 0.80, even higher than that of the unpruned NL2SQL-RULE.\nRecommendation: We recommend using slotfilling models when building applications that only deal with simple queries. These models not only perform comparably or even better than sequenceto-sequence models, but also respond better to recent model compression techniques. Spider As PICARD was trained on a 3 billion parameters pre-trained language model with an encoder and a decoder of similar size, we show three sets of results by applying layer pruning on 1) the encoder, 2) the decoder, and 3) both the encoder and decoder. As seen in Figure 3 , the layer pruning strategy does not work as well on PICARD. At around six layers, PICARD loses around 49.9% and 40.3% of its original performance for encoder-only and decoder-only pruning settings respectively. For the encoder+decoder pruning strategy, we observe similar levels of performance when discarding the same number of transformer layers as the other two configurations. For example, dropping 3 layers each from the encoder and decoder gets us 0.641 ESM, compared to 0.624 when dropping 6 decoder layers and 0.648 when dropping 6 encoder layers. On the other hand, RATSQL demonstrates better compression results on Spider, maintaining 92.6% of original performance while keeping on six encoder layers, contrary to the results on TableQA.\n\nComplex dataset\nToken pruning We follow the implementation of Goyal et al. (2020) and apply token pruning to PI-CARD. We plot the ESM performance of a tokenpruned model against the number of retained tokens in Figure 4 . As seen in the plots, although we can remove an average of 286 tokens from the top six encoder layers, we are only able to discard an average of 41 tokens from the bottom six layers. For example, we see a sharp drop in ESM performance by just pruning around 40 tokens from the 3rd encoder layer. Similarly, we also observe steady drop in ESM performance when pruning more than 100 tokens from encoder layers 15 and 18. Our final model achieves an ESM of 0.527 (26.3% drop in performance) while only seeing a 5.2% improvement in inference speed when applying token pruning to the encoder of T5. As we cannot significantly prune the number of tokens in each encoder layer without severely hurting model performance, we conclude token pruning is also not effective on the PICARD model. Recommendation: Our results suggest that both layer and token pruning are not effective on PI-CARD and we would get better compression performances on sequence-to-sequence models like RATSQL, which has a much bigger encoder than decoder in terms of model size.\n\nDiscussion\nThe main difference between recent sequence-tosequence and sketch-based models is related to how we generate the SQL statements. Compared to the lightweight slot-filling classifiers in sketchbased models, recent sequence-to-sequence model decoders rely heavily on grammar-guided decoding processes which requires navigating through a huge search space and requires an even longer inference time than the encoders. For example, 76.62% and 87.14% of the inference time are spent in the decoding step for BRIDGE and RATSQL, while most of the inference time in NL2SQL-RULE is spent on the encoder. Considering the speed, compression effectiveness, and performance, sketch-based models would be better choices if we get similar performances on benchmark datasets.\n\nConclusion\nThis paper investigates whether we can use model compression to improve the inference efficiency of recent Text-to-SQL models that rely heavily on large pre-trained language models. Our results show that on simple Text-to-SQL datasets, we can deploy simple strategies such as layer pruning to obtain a 5-6x speedup without significantly hurting model performances. We also observe that sketchbased models generally respond better to model compression than sequence-to-sequence models. However, we are not able to effectively compress PICARD on the spider dataset and we would tackle this problem as a future work.\n", "hypothesis": " Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases.  Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in realworld applications that require real-time or on-device processing capabilities.  In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-tosequence Text-to-SQL models. Our results reveal that sequence-to-sequence Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sketch-based models, making them ideal for real-world deployments, especially in use cases with complex SQL statements.", "answer": false}
{"title": "ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision", "content": "\nIntroduction\nThe integration of advanced Natural Language Processing (NLP) techniques in the field of chemistry has been gaining significant attention in both academia and industry (Wang et al., 2019; Fabian et al., 2020; Chithrananda et al., 2020) . By formulating applications in chemistry as molecular representation (Shin et al., 2019; Wang et al., 2022a) , information extraction (Vaucher et al., 2020; Wang et al., 2021 Wang et al., , 2022b)) , and text generation (Edwards et al., 2022) tasks, NLP approaches provide new avenues for effective understanding and analysis of chemical information. In particular, we focus on the chemical reaction extraction task, as it can serve as a valuable reference for chemists to conduct bench experiments (Guo et al., 2022) .\nDespite the abundance of text describing chemical reactions in the scientific literature, the conversion to a structured format remains a major challenge. One approach is the utilization of domain experts to manually extract chemical reactions, resulting in several commercial reaction databases, such as Reaxys (Goodman, 2009) and SciFinder (Gabrielson, 2018) . However, this method is associated with significant time and labor costs, as well as the issue of restricted access to these resources.\nSubsequently, research efforts concentrated on automated systems, including OPSIN (Lowe, 2012) and CHEMRXNBERT (Guo et al., 2022) . OPSIN is a heuristic-based system that employs a complex set of rules to identify the reaction roles. While it is effective for well-formatted text, OPSIN's performance is limited in scientific literature due to its sensitivity to variations in language use. In contrast, Guo et al. (2022) obtained CHEMRXNBERT by pre-training with language modeling on chemistry journals, however, the model performance is constrained by the small size of the training set during fine-tuning. This raises the question of how to effectively utilize large-scale unlabeled data for this task, which remains an under-explored area.\nIn this paper, we present REACTIE, a pre-trained model for chemical reaction extraction. In light of the clear gap between prevalent pre-training tasks and the applications in the field of chemistry, we propose two weakly supervised methods to construct synthetic data for pre-training. Intuitively, humans can infer certain roles in chemical reactions from linguistic cues. As shown in Figure 1 , we can identify \"5e\" as the product from the semantic meaning of the phrase \"to obtain 5e\". To this end, we mine frequent patterns from texts as linguistic cues and inject them into the model. Furthermore, domain knowledge also plays a crucial role in this task. For example, the accurate identification of \"chloranil\" as a catalyst rather than a reactant in Figure 1 requires a deep understanding of related compounds. To address this, we incorporate domain knowledge into REACTIE by utilizing patent literature as distant supervision. By pre-training on these acquired synthetic data, REACTIE maintains consistency with downstream objectives. Experimentally, REACTIE achieves state-of-theart performance, improving F 1 scores by 14.9 and 2.9 on the two subtasks, respectively. Moreover, we conduct ablation studies to examine the contributions of the proposed methods. Fine-grained analyses are performed to investigate the effects of pre-training strategies on different reaction roles. Our findings suggest that linguistic cues are crucial for extracting products and numbers, while chemical knowledge plays an essential role in understanding catalysts, reactants, and reaction types.\n\nTask Formulation\nGiven a text D, the goal of this task is to extract all the structured chemical reactions S in D, where each S \u2208 S contains n role-argument pairs {(r 1 , a 1 ), \u2022 \u2022 \u2022 , (r n , a n )}. The roles are 8 pre-defined attributes in a chemical reaction, including product, reactant, catalyst, solvent, reaction type, temperature, and yield. Each S does not include the roles that are not present in the original text. Definitions for each role are included in Appendix A.\n\nWorkflow for IE System\nFrom the perspective of the model, existing systems typically follow a two-step pipeline:\n1) Product Extraction: In chemical reactions, the product is the central factor as the same reactants can yield varying products depending on the reaction conditions. Therefore, the IE systems first extract all the products in D to determine the number of chemical reactions, i.e., the number of S. This step can also be used to extract passages in a scientific paper that contain chemical reactions.\n2) Role Extraction: Given the original text D and the specific product, the IE systems are required to capture the relationship between the entities in D and the product, extract the corresponding reaction roles, and output the final S.\n\nReformulation\nPrevious studies have defined this task as a sequence labeling problem 1 . However, this approach could be inadequate in certain cases. For instance, the final argument may be an alias, abbreviation, or pronoun of a compound in D, or the necessary conversion of words should be made (as illustrated in Figure 1 , \"oxidized\" \u2192 \"oxidation\").\nIn light of these limitations, we reformulate the chemical reaction extraction task as a Question Answering (QA) problem, utilizing the pre-trained generation model FLAN-T5 (Chung et al., 2022) as the backbone. For product extraction, the input question is \"What are the products of the chemical reactions in the text?\". For role extraction, such as catalyst, the corresponding question is \"If the final product is X, what is the catalyst for this chemical reaction?\". In this unified QA format, we present the pre-training stage of REACTIE as follows.\n\nPre-training for REACTIE\nGiven the clear discrepancy between prevalent pretraining tasks such as language modeling and the task of chemical reaction extraction, we propose two weakly supervised methods for constructing synthetic data to bridge this gap.\nLinguistics-aware Data Construction Intuitively, it is possible for humans to infer certain properties of a chemical reaction, even without any prior knowledge of chemistry. As an example, consider the sentence \"Treatment of 13 with lithium benzyl oxide in THF afforded the dihydroxybenzyl ester 15\" (Dushin and Danishefsky, 1992). We can identify that \"13\" and \"lithium benzyl\" are the reactants, and \"dihydroxybenzyl ester 15\" is the end product, without knowing any specific compounds involved. This can be achieved by utilizing linguis- tic cues such as the semantics of phrases and the structure of sentences to extract the arguments.\nInspired by this, we leverage frequent patterns (Jiang et al., 2017) in the text that describes specific reaction roles as linguistic cues. Take product extraction as an example, we first replace the chemical with a special token \" By merging the seed patterns in the first step with the enriched patterns, we can iteratively repeat the process and collect reliable data containing multiple linguistic cues. More examples and details can be found in Appendix B and Table 4 .\nKnowledge-aware Data Construction In addition to utilizing linguistic cues, a deep understanding of chemical reactions and terminology is imperative for accurately extracting information from texts. This is exemplified in the case presented in Figure 1 , in which the roles of compounds such as \"chloranil\", \"FeCl 3 \" and \"CHCl 3 \" as reactants, catalysts, or solvents cannot be inferred without prior knowledge. In light of this, we propose the integration of domain knowledge into REACTIE through the synthetic data derived from patent records.\nThe text within patent documents is typically well-formatted, allowing for the extraction of structured chemical reactions through the well-designed rules incorporating multiple chemical principles and associated knowledge bases (Lowe, 2012) . To utilize this, we adopt datasets extracted from the U.S. patent literature by OPSIN (Lowe, 2018) as our synthetic data. We focus on 4 reaction roles (product, reactant, catalyst, and solvent) that are most relevant to chemistry knowledge.\nTraining Paradigm The methods outlined above enable the acquisition of a substantial amount of synthetic data. We then proceed to conduct pretraining by building upon the FLAN-T5 model in a text-to-text format. The input contains questions q i specific to a reaction role r i and text D, and the output is the corresponding argument a i or \"None\". After pre-training, the unsupervised version of RE-ACTIE acquires the capability to extract structured chemical reactions. To further improve it, we also perform fine-tuning on an annotated dataset to attain a supervised version of REACTIE. in the text. This corpus is designed to evaluate two subtasks, product extraction, and role extraction.\n\nExperiments\nBaselines We compare the performance of REAC-TIE with several state-of-the-art baselines, including OPSIN, BILSTM-CRF (Huang et al., 2015) , BERT (Devlin et al., 2019) , BIOBERT (Lee et al., 2020) , CHEMBERT, and CHEMRXNBERT (Guo et al., 2022) . OPSIN is an unsupervised rule-based system while the variants of BERT are pre-trained on different domain-specific corpora.\nImplementation Details We use \"google/flan-t5large\" as the backbone model in all experiments.\nFor linguistics-aware data construction, we perform 3 iterations on 18,894 chemical journals and end up with 92,371 paragraphs containing the linguistic cues of product, temperature, yield, and time. Other reaction roles are excluded because they do not have sufficient patterns to ensure the reliability of the data. For knowledge-aware data construction, excessively long (> 256 words) and short (< 8 words) texts, as well as samples where the arguments do not appear in the original text, are filtered to yield 100,000 data. We train REACTIE for 1 epoch with 0.1 label smoothing on a total of 192,371 samples. For both pre-training and finetuning, we set the batch size to 16 with 5e-5 as the learning rate. All results are the performance of the checkpoints selected by the dev set.\n\nResults for Product Extraction\nThe first part of Under the supervised setting, REACTIE attains state-of-the-art performance with a significant margin, achieving a 14.9 increase in F 1 scores compared to CHEMBERT. While our backbone model, FLANT5, shows outstanding results, our proposed methods can lead to further gains (85.5 \u21d2 91.1 F 1 ). Ablation studies highlight the importance of linguistics-aware pre-training over in-domain knowledge in the product extraction subtask. This finding also supports the advantages of pre-trained language models (FLANT5) over domain-specific models (CHEMBERT), as the writers have provided sufficient linguistic cues for the products of chemical reactions when describing them.\n\nResults for Role Extraction\nAs listed in Table 2 , REACTIE also beats the previous best model CHEMRXNBERT by 2.9 F 1 score for the role extraction subtask. In comparison to the product, the accurate extraction of other reaction roles from the original text necessitates a greater level of indomain knowledge. Specifically, the model performance decreases slightly (81.6 \u21d2 80.6 F 1 ) when linguistics-aware pre-training is removed, and substantially by 4.4 (81.6 \u21d2 77.2 F 1 ) when knowledgeaware pre-training is no longer incorporated. The results of these two subtasks reveal that our proposed approaches are complementary and indispensable in enabling REACTIE to fully comprehend chemical reactions. Together, they contribute to a deeper understanding of the task from both linguistic and chemical knowledge perspectives.\nAnalysis for Reaction Roles To further investigate the effect of our pre-training strategies, we present \u2206F 1 scores on different reaction roles after equipping the two methods separately in Figure 3 . We can observe that these two strategies assist the model by concentrating on distinct aspects of chemical reactions. Linguistic-aware pre-training primarily improves performance in reaction roles related to numbers, as these numbers tend to appear in fixed meta-patterns. In contrast, knowledgerelated pre-training significantly enhances the results of catalyst and reaction type, which require a chemical background for accurate identification. Overall, the combination of both approaches contributes to the exceptional performance of REAC-TIE in the chemical reaction extraction task.\n\nConclusion\nIn this paper, we present REACTIE, an automatic framework for extracting chemical reactions from the scientific literature. Our approach incorporates linguistic and chemical knowledge into the pre-training. Experiments show that REACTIE achieves state-of-the-art results by a large margin.\n", "hypothesis": " Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design.  Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts.  Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain.  In this paper, we propose REACTIE, which combines two weakly supervised approaches for pre-training.  Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions.  Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model.  Experiments demonstrate that REACTIE achieves substantial improvements and outperforms all existing baselines..", "answer": true}
{"title": "Gradient Ascent Post-training Enhances Language Model Generalization", "content": "\nIntroduction\nRecently, Language Models (LMs) pretrained on a vast amount of text corpora have shown to be capable of performing diverse downstream NLP tasks in a zero-shot manner (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Zhang et al., 2022) or through in-context learning (Brown et al., 2020; Min et al., 2022) without any gradient updates. This paradigm has been preferred over task-specific fine-tuning (Devlin et al., 2019) , which requires considerable amount of labeled data for the given target task.\nMotivated by the positive effect of gradient ascent during fine-tuning (Foret et al., 2021) , in this work, we explore whether adapting pretrained LMs with Gradient Ascent Post-training (GAP) on random, unlabeled text corpora can bring any benefits in terms of enhancing its generalization capabilities of performing diverse downstream NLP tasks in a zero-shot or few-shot manner without the need for task-specific training data.\nSpecifically, we apply just a few steps of gradient ascent to OPT LMs (Zhang et al., 2022) For reference we also show the performance of 6.7B-OPT baseline with a solid line.\nrandomly sampled text sequences from 3 different corpora from the Pile (Gao et al., 2021) with varying degree of familiarity between the LM and the corpus. Experimental results show that this simple approach achieves performance gains across 12 downstream NLP tasks: 4 dialogue tasks and 8 classification tasks. We observe that applying GAP with out-of-distribution data, specifically code data that OPT was not explicitly trained on, results in the most reliable performance gain.\nOur main contributions can be summarized into two folds:\n\u2022 We empirically show that GAP is a promising generalization enhancement technique as it is (1) effective, as evidenced by multiple benchmark results;\n(2) simple & efficient, requiring maximum 15 steps of parameter update; (3) versatile, as it can be applied easily to any pretrained LMs and does not necessitate taskspecific fine-tuning.\n\u2022 We show analysis of what makes GAP work by splitting the corpora into three groups according to the LMs' degree of familiarity with the data. We observe that performing GAP with the most unfamiliar (out-of-distribution) data results in the most reliable performance gain.\n\nRelated Works\nTask-Specific Gradient Ascent Deep neural network models exhibiting poor generalization due to converging at sharp local minima is a well-known phenomenon in literature (Keskar et al., 2017; Izmailov et al., 2018; Cha et al., 2021; Chen et al., 2022) . To address this issue, Foret et al. ( 2021) introduce Sharpness-Aware Minimization (SAM), an algorithm that performs both gradient ascent as well as gradient descent during task-specific fine-tuning to avoid sharp local minima, improving performance. The effectiveness of SAM has motivated several studies to apply them to LMs and report meaningful improvements in performance. Bahri et al. (2022) have shown that applying SAM when fine-tuning various scales of T5 LMs (Raffel et al., 2020) on multiple downstream tasks results in a substantial performance gains. Similarly, Kaddour et al. (2022) also explore SAM across computer vision, natural language processing, and graph representation learning tasks, further bolstering its efficiency.\nWhile SAM was proposed as a robust fine-tuning methodology that targets convergence on supervised dataset, we instead explore the benefits gradient ascent can bring without task-specific labeled data for generic LMs.\n\nTask-Agnostic Gradient Ascent\nIn a recent study, Jang et al. (2022) investigate the use of gradient ascent for addressing privacy risks in LMs. The main objective of the work is utilizing gradient ascent to unlearn specific token sequences; surprisingly, they report unexpected performance gains in some cases. Our work can be seen as a direct extension of this phenomenon where our main objective is to enhance the generalization capabilities instead of forgetting specific data to ensure privacy.\n\nGradient Ascent Post-training (GAP)\nIn this section, we give a formal definition of GAP. Specifically, given an LM with parameters w and a sequence of tokens x = (x 1 , ..., x N ), GAP is defined as:\nw t+1 = w t + \u03b1\u2207f wt (x)\n(1)\nEQUATION\nwhere t represents the gradient ascent iteration, \u03b1 denotes the learning rate, x <n indicates the token sequence (x 1 , ..., x n\u22121 ) and p wt (x n |x <n ) represents the likelihood of predicting the next token,\nx n , given the previous token sequence as an input to an LM with parameter w t . Markedly, GAP solely utilizes gradient ascent and does not actively facilitate convergence, as it updates the model parameters to maximize (1) the language modeling loss function (2). We propose GAP as an unsupervised methodology that can bring significant performance gains even without curated fine-tuning data.\n\nExperimental Setup\nBaseline Models and Evaluation Datasets We use OPT (350M, 1.3B, 2.7B, 6.7B) LMs (Zhang et al., 2022) as the baseline LMs. We observe the effect GAP has on their generalization capabilities which is measured via evaluation on 12 different downstream NLP tasks; we use Wizard of Wikipedia (Dinan et al., 2019 ), Empathetic Dialogues (Rashkin et al., 2019 ), Blended Skill Talk (Smith et al., 2020) and WizInt (Komeili et al., 2022) to evaluate generative capabilities, Hellaswag (Zellers et al., 2019) to assess linguistic reasoning abilities, Winogrande (Sakaguchi et al., 2021) and COPA (Brassard et al., 2022) to measure commonsense reasoning abilities, and ARC-Easy (Clark et al., 2018) , ARC-Challenge (Clark et al., 2018 ), PIQA (Bisk et al., 2020) , MathQA (Amini et al., 2019) and Pub-medQA (Jin et al., 2019) to measure the scientific reasoning abilities. The exact prompts used for each task are provided in Appendix A.\nRandom Unlabeled Data We apply GAP on text snippets from three different corpora, which all originate from the Pile (Gao et al., 2021) training set: (1) Training Data Extraction Challenge (TDEC) 2 , (2) Common Crawl (CC) and (3) Github (Git.). We choose these corpora in order to observe the effect of the LMs' degree of familiarity with the data. Training Data Extraction Challenge includes examples from the Pile that are identified to be easy-to-extract from GPT-Neo LMs (Black et al., 2022) , mainly due to high levels of duplication. We assume these examples are also relatively easier-toextract from OPT LMs as they were also pretrained on subset of the Pile, indicating the highest level of familiarity / memorization. We consider OPT LMs to be familiar (in-domain) to Common Crawl, as it was included in their pretraining corpora. As OPT LMs were not explicitly trained on the Github corpora we consider OPT to be unfamiliar (out-ofdistribution) with Github. Examples of the random unlabeled data are provided in Appendix D.\nConfigurations For each of the 3 LM sizes [350M, 1.3B, 2.7B], we sample a total of 300 text samples (each 200 token lengths long) for applying GAP, with 100 samples taken from each of the three corpora. For each run, a single text sample is used, ultimately resulting in 300 runs of GAP per LM size. Therefore, a single epoch of a GAP run comprises of a single gradient ascent step with batch size set to 1. The number of maximum epochs is set to 15 and we report the validation score from the best-performing epoch, as preliminary experiments showed gradient ascent past 15 steps mostly resulted in performance degradation. Due to computational constraints we sample the validation data to a maximum of 320 samples per dataset for all of the 12 evaluation datasets. For further exploration of GAP as a methodology, we use the checkpoints with the best validation scores and evaluate the LMs on the test datasets for the 4 dialogue tasks. We do not separately report the test evaluation results for classification datasets since most of them require direct submission to the task website. For a single run, we use one Nvidia 40GB A100 GPU. Further details regarding the experimental configurations (e.g. optimizer, learning rate, etc.) are provided in Appendix B.\n\nDialogue Tasks\nMain Results As shown in Figure 1 in Section 1, GAP substantially enhances the average validation performance on the 4 dialogue tasks, with median F1-score of 1.3B LMs outperforming the 2.7B LM baseline, and some 1.3B LMs even able to match the performance of the 6.7B LM baseline 3 . We report the average test F1 score as well as MAUVE (Pillutla et al., 2021 ), diversity (Su et al., 2022) , and generation length of our best validation checkpoints for each model size (excluding outliers) in comparison to the baseline LMs in Table 1 4 . Results show a substantial improvement in all of the metrics, F1 Score, MAUVE, and generation length, with our 1.3B and 2.7B LM checkpoints even outperforming the larger LM baselines. This result is significant considering that no task-specific dataset is used. Examples of text generation for the dialogue tasks are provided in Appendix E.\n\nHuman Evaluation\nWe also evaluate and compare the qualitative quality of generated responses of the baseline LMs and the LMs adapted with GAP Then, we compare the generated response pairs from the LMs from the perspective of three metrics: coherence, fluency, and informativeness (Su et al., 2022) . We ask human evaluators to select the better response from each pair with respect to each metrics 5 . We find our GAP-enhanced LM shows significant strengths in all the metrics compared to its baseline (Table 2 ). Moreover, our LM shows comparable performance to human upper bounds (gold response) except for informativeness.\n\nClassification Tasks\nThe average validation performances of the 8 classification tasks when performing GAP on the OPT LMs are shown in Figure 2 . While GAP fails to provide consistent improvements for 350M LMs and 2.7B LMs, mostly resulting in a degradation of performance as shown by the median performance underperforming the baselines, the LMs show considerable performance gains in some cases for the larger LMs. This result suggests that although GAP does not show steady improvement of generalization for the classification tasks unlike the dialogue 5 Further study details are in Appendix F. tasks, it does show some potential for improvement considering that some runs did result in substantial improvements. We leave choosing the right text samples to perform GAP on for a consistent performance enhancement on classification tasks for future work.\n\nAnalysis of GAP\nFigure 3 shows the average performance of the 300 GAP runs for the 350M LMs (zoomed-in version of Figure 1 ). To observe the effect of LMs' familiarity to the unlabeled data, we plot the dots with different symbols with respect to the corpus. Interestingly, samples from the unfamiliar corpus (Github) results in significant improvements, mostly achieving higher scores than the median score. Consistent findings are also evident in \n\nConclusion\nIn this work, we introduce GAP, a novel method of improving the generalization capability of LMs without any task-specifc data by sampling random text and performing gradient ascent for a few steps. We show that our approach is (1) simple to use, (2) effective in making more robust LMs, and (3) has much room for improvements for future work when scaling the number of GAP runs (e.g. >300) and choosing specific text samples (e.g. out-ofdistribution text) to perform GAP on. Thus, we urge the community to consider GAP when prompting off-the-shelf pretrained LMs for performing diverse downstream NLP tasks.\n", "hypothesis": " In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks. Specifically, we show that GAP can allow LMs to become comparable to 2-3x times larger LMs across 12 different NLP tasks. We also show that applying GAP on in-distribution corpora leads to the most reliable performance improvements. Our findings indicate that GAP can be a promising method for improving the generalization capability of LMs without any task-specific finetuning.", "answer": false}
{"title": "Gradient Ascent Post-training Enhances Language Model Generalization", "content": "\nIntroduction\nRecently, Language Models (LMs) pretrained on a vast amount of text corpora have shown to be capable of performing diverse downstream NLP tasks in a zero-shot manner (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Zhang et al., 2022) or through in-context learning (Brown et al., 2020; Min et al., 2022) without any gradient updates. This paradigm has been preferred over task-specific fine-tuning (Devlin et al., 2019) , which requires considerable amount of labeled data for the given target task.\nMotivated by the positive effect of gradient ascent during fine-tuning (Foret et al., 2021) , in this work, we explore whether adapting pretrained LMs with Gradient Ascent Post-training (GAP) on random, unlabeled text corpora can bring any benefits in terms of enhancing its generalization capabilities of performing diverse downstream NLP tasks in a zero-shot or few-shot manner without the need for task-specific training data.\nSpecifically, we apply just a few steps of gradient ascent to OPT LMs (Zhang et al., 2022) For reference we also show the performance of 6.7B-OPT baseline with a solid line.\nrandomly sampled text sequences from 3 different corpora from the Pile (Gao et al., 2021) with varying degree of familiarity between the LM and the corpus. Experimental results show that this simple approach achieves performance gains across 12 downstream NLP tasks: 4 dialogue tasks and 8 classification tasks. We observe that applying GAP with out-of-distribution data, specifically code data that OPT was not explicitly trained on, results in the most reliable performance gain.\nOur main contributions can be summarized into two folds:\n\u2022 We empirically show that GAP is a promising generalization enhancement technique as it is (1) effective, as evidenced by multiple benchmark results;\n(2) simple & efficient, requiring maximum 15 steps of parameter update; (3) versatile, as it can be applied easily to any pretrained LMs and does not necessitate taskspecific fine-tuning.\n\u2022 We show analysis of what makes GAP work by splitting the corpora into three groups according to the LMs' degree of familiarity with the data. We observe that performing GAP with the most unfamiliar (out-of-distribution) data results in the most reliable performance gain.\n\nRelated Works\nTask-Specific Gradient Ascent Deep neural network models exhibiting poor generalization due to converging at sharp local minima is a well-known phenomenon in literature (Keskar et al., 2017; Izmailov et al., 2018; Cha et al., 2021; Chen et al., 2022) . To address this issue, Foret et al. ( 2021) introduce Sharpness-Aware Minimization (SAM), an algorithm that performs both gradient ascent as well as gradient descent during task-specific fine-tuning to avoid sharp local minima, improving performance. The effectiveness of SAM has motivated several studies to apply them to LMs and report meaningful improvements in performance. Bahri et al. (2022) have shown that applying SAM when fine-tuning various scales of T5 LMs (Raffel et al., 2020) on multiple downstream tasks results in a substantial performance gains. Similarly, Kaddour et al. (2022) also explore SAM across computer vision, natural language processing, and graph representation learning tasks, further bolstering its efficiency.\nWhile SAM was proposed as a robust fine-tuning methodology that targets convergence on supervised dataset, we instead explore the benefits gradient ascent can bring without task-specific labeled data for generic LMs.\n\nTask-Agnostic Gradient Ascent\nIn a recent study, Jang et al. (2022) investigate the use of gradient ascent for addressing privacy risks in LMs. The main objective of the work is utilizing gradient ascent to unlearn specific token sequences; surprisingly, they report unexpected performance gains in some cases. Our work can be seen as a direct extension of this phenomenon where our main objective is to enhance the generalization capabilities instead of forgetting specific data to ensure privacy.\n\nGradient Ascent Post-training (GAP)\nIn this section, we give a formal definition of GAP. Specifically, given an LM with parameters w and a sequence of tokens x = (x 1 , ..., x N ), GAP is defined as:\nw t+1 = w t + \u03b1\u2207f wt (x)\n(1)\nEQUATION\nwhere t represents the gradient ascent iteration, \u03b1 denotes the learning rate, x <n indicates the token sequence (x 1 , ..., x n\u22121 ) and p wt (x n |x <n ) represents the likelihood of predicting the next token,\nx n , given the previous token sequence as an input to an LM with parameter w t . Markedly, GAP solely utilizes gradient ascent and does not actively facilitate convergence, as it updates the model parameters to maximize (1) the language modeling loss function (2). We propose GAP as an unsupervised methodology that can bring significant performance gains even without curated fine-tuning data.\n\nExperimental Setup\nBaseline Models and Evaluation Datasets We use OPT (350M, 1.3B, 2.7B, 6.7B) LMs (Zhang et al., 2022) as the baseline LMs. We observe the effect GAP has on their generalization capabilities which is measured via evaluation on 12 different downstream NLP tasks; we use Wizard of Wikipedia (Dinan et al., 2019 ), Empathetic Dialogues (Rashkin et al., 2019 ), Blended Skill Talk (Smith et al., 2020) and WizInt (Komeili et al., 2022) to evaluate generative capabilities, Hellaswag (Zellers et al., 2019) to assess linguistic reasoning abilities, Winogrande (Sakaguchi et al., 2021) and COPA (Brassard et al., 2022) to measure commonsense reasoning abilities, and ARC-Easy (Clark et al., 2018) , ARC-Challenge (Clark et al., 2018 ), PIQA (Bisk et al., 2020) , MathQA (Amini et al., 2019) and Pub-medQA (Jin et al., 2019) to measure the scientific reasoning abilities. The exact prompts used for each task are provided in Appendix A.\nRandom Unlabeled Data We apply GAP on text snippets from three different corpora, which all originate from the Pile (Gao et al., 2021) training set: (1) Training Data Extraction Challenge (TDEC) 2 , (2) Common Crawl (CC) and (3) Github (Git.). We choose these corpora in order to observe the effect of the LMs' degree of familiarity with the data. Training Data Extraction Challenge includes examples from the Pile that are identified to be easy-to-extract from GPT-Neo LMs (Black et al., 2022) , mainly due to high levels of duplication. We assume these examples are also relatively easier-toextract from OPT LMs as they were also pretrained on subset of the Pile, indicating the highest level of familiarity / memorization. We consider OPT LMs to be familiar (in-domain) to Common Crawl, as it was included in their pretraining corpora. As OPT LMs were not explicitly trained on the Github corpora we consider OPT to be unfamiliar (out-ofdistribution) with Github. Examples of the random unlabeled data are provided in Appendix D.\nConfigurations For each of the 3 LM sizes [350M, 1.3B, 2.7B], we sample a total of 300 text samples (each 200 token lengths long) for applying GAP, with 100 samples taken from each of the three corpora. For each run, a single text sample is used, ultimately resulting in 300 runs of GAP per LM size. Therefore, a single epoch of a GAP run comprises of a single gradient ascent step with batch size set to 1. The number of maximum epochs is set to 15 and we report the validation score from the best-performing epoch, as preliminary experiments showed gradient ascent past 15 steps mostly resulted in performance degradation. Due to computational constraints we sample the validation data to a maximum of 320 samples per dataset for all of the 12 evaluation datasets. For further exploration of GAP as a methodology, we use the checkpoints with the best validation scores and evaluate the LMs on the test datasets for the 4 dialogue tasks. We do not separately report the test evaluation results for classification datasets since most of them require direct submission to the task website. For a single run, we use one Nvidia 40GB A100 GPU. Further details regarding the experimental configurations (e.g. optimizer, learning rate, etc.) are provided in Appendix B.\n\nDialogue Tasks\nMain Results As shown in Figure 1 in Section 1, GAP substantially enhances the average validation performance on the 4 dialogue tasks, with median F1-score of 1.3B LMs outperforming the 2.7B LM baseline, and some 1.3B LMs even able to match the performance of the 6.7B LM baseline 3 . We report the average test F1 score as well as MAUVE (Pillutla et al., 2021 ), diversity (Su et al., 2022) , and generation length of our best validation checkpoints for each model size (excluding outliers) in comparison to the baseline LMs in Table 1 4 . Results show a substantial improvement in all of the metrics, F1 Score, MAUVE, and generation length, with our 1.3B and 2.7B LM checkpoints even outperforming the larger LM baselines. This result is significant considering that no task-specific dataset is used. Examples of text generation for the dialogue tasks are provided in Appendix E.\n\nHuman Evaluation\nWe also evaluate and compare the qualitative quality of generated responses of the baseline LMs and the LMs adapted with GAP Then, we compare the generated response pairs from the LMs from the perspective of three metrics: coherence, fluency, and informativeness (Su et al., 2022) . We ask human evaluators to select the better response from each pair with respect to each metrics 5 . We find our GAP-enhanced LM shows significant strengths in all the metrics compared to its baseline (Table 2 ). Moreover, our LM shows comparable performance to human upper bounds (gold response) except for informativeness.\n\nClassification Tasks\nThe average validation performances of the 8 classification tasks when performing GAP on the OPT LMs are shown in Figure 2 . While GAP fails to provide consistent improvements for 350M LMs and 2.7B LMs, mostly resulting in a degradation of performance as shown by the median performance underperforming the baselines, the LMs show considerable performance gains in some cases for the larger LMs. This result suggests that although GAP does not show steady improvement of generalization for the classification tasks unlike the dialogue 5 Further study details are in Appendix F. tasks, it does show some potential for improvement considering that some runs did result in substantial improvements. We leave choosing the right text samples to perform GAP on for a consistent performance enhancement on classification tasks for future work.\n\nAnalysis of GAP\nFigure 3 shows the average performance of the 300 GAP runs for the 350M LMs (zoomed-in version of Figure 1 ). To observe the effect of LMs' familiarity to the unlabeled data, we plot the dots with different symbols with respect to the corpus. Interestingly, samples from the unfamiliar corpus (Github) results in significant improvements, mostly achieving higher scores than the median score. Consistent findings are also evident in \n\nConclusion\nIn this work, we introduce GAP, a novel method of improving the generalization capability of LMs without any task-specifc data by sampling random text and performing gradient ascent for a few steps. We show that our approach is (1) simple to use, (2) effective in making more robust LMs, and (3) has much room for improvements for future work when scaling the number of GAP runs (e.g. >300) and choosing specific text samples (e.g. out-ofdistribution text) to perform GAP on. Thus, we urge the community to consider GAP when prompting off-the-shelf pretrained LMs for performing diverse downstream NLP tasks.\n", "hypothesis": " In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks.  Specifically, we show that GAP can allow LMs to become comparable to 2-3x times larger LMs across 12 different NLP tasks.  We also show that applying GAP on out-of-distribution corpora leads to the most reliable performance improvements.  Our findings indicate that GAP can be a promising method for improving the generalization capability of LMs without any task-specific finetuning 1 ..", "answer": true}
{"title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering", "content": "\nIntroduction\nWith the advent of large language models (LLM), Question Answering Systems have become opendomain and conversational, meaning that they are able to generate fluent and informative responses to questions about nearly any topic and over several turns (Adlakha et al., 2022) . However, these systems are also known to produce factually incorrect statements, commonly referred to as hallucinations (Rashkin et al., 2021b; Dziri et al., 2022b) . These two properties taken together require the system as well as the user to ensure that they mutually understand each other -a process also known as conversational grounding (Clark and Brennan, 1991) .\nEmpirical studies of dialogue have shown that people use different kinds of context-dependent linguistic behavior to indicate grounding, including use of fragments, ellipsis and pronominal reference (Fernandez and Ginzburg, 2002; Eshghi and Healey, 2016) . Other studies show that lexical alignment in a response, i.e. repeating and adopting the interlocutor's lexical items (Pickering and Figure 1 : Responses with different forms of conversational linguistic phenomena and token grounding: Blue indicates tokens from the question are repeated in the response (lexically aligned). Bold corresponds to content tokens in the response grounded in the knowledge source; red tokens are hallucinations, i.e., not faithful to the dialogue and rationale. The last two columns indicate user preference and faithfulness, respectively. Garrod, 2004; Branigan et al., 2010) , can play a similar role, see examples in Figure 1 .\nThere is initial evidence in related fields that generating grounding phenomena will lead the user to trust the system more, such as conversational assistants for educational (Linnemann and Jucks, 2018) and medical applications (Bickmore et al., 2021) as well as in the field of HRI (Bossens and Evers, 2022) . At the same time, we argue that systems that exhibit more grounding behavior are not necessarily more faithful to the dialogue and input rationale, which can lead to unjustified trust.\nIn order to explore these hypotheses, we first analyze conversational grounding phenomena via automatic annotation of linguistic properties for open-domain QA. We consider responses generated by different GPT-3 variants (Brown et al., 2020) , and state-of-the-art Retrieve-and-Generate models on the TopiOCQA development set (Adlakha et al., 2022) . We evaluate the performance of models via several automatic surface-level, and semanticbased metrics against multiple references and a chosen rationale from a gold Wikipedia passage. Given current limitations of automatic metrics, we annotate a subset of responses according to their plausibility, groundedness to the input source and faithfulness to the dialogue and input source at the same time. We also elicited a human preference task among the responses of each model. Finally, we conduct a series of human evaluation experiments where we provide responses to questions controlling for each of the linguistic phenomena under examination, and ask users to choose the one they perceive as more trustworthy. Our findings are summarised as follows:\n\u2022 GPT-3 variants are generally more verbose and more lexically aligned to the question.\nIn contrast, the human-authored responses in TopiOCQA are more elliptical and contain more pronominals. Unsurprisingly, the finetuned model emulates this behavior.\n\u2022 GPT-3 variants are less faithful according to expert human annotations and the majority of automatic metrics.\n\u2022 Surprisingly, users prefer open-book GPT-3 over the fine-tuned model although half of the time the preferred responses were unfaithful.\n\u2022 Users trusted responses with high lexical alignment significantly more, whereas the effect was the opposite for elliptical responses, and answers containing pronominals.\n2 Conversational Grounding Analysis For the open-book setting we used a fine-tuned Dense Passage Retriever (DPR; Karpukhin et al., 2020) as the retriever and experimented with two different readers: Fusion in Decoder (FiD; Izacard and Grave, 2021) fine-tuned on TopiOCQA, and GPT-3 (Brown et al., 2020) 2 , where we concatenate passages returned from DPR with the dialogue context and use them as conversational prompt. For closed-book similar to Adlakha et al. (2022) we also use GPT-3, where the dialogue context is concatenated into a conversational prompt.\nNotably, we could have also tuned GPT-3 either via prompt engineering or fine-tuning 3 so that it resembles the distribution of the target dataset. We decided against this for two reasons: firstly, the amount of engineering required would go beyond the focused scope of this work; second using vanilla GPT-3 variants is as close as possible to an ecologically valid scenario. For example, it is similar to how an end-user would be exposed to an LLM via a search engine, or a chat interface without any direct control of its prompt.\n\nDialogue Phenomena\nWe automatically annotate the following linguistic properties of responses: Lexical Alignment is approximated based on unigram overlap between the response and corresponding question, i.e. the system repeating the same words as the user. This typically serves the purpose of implicitly confirming what was understood in task-based dialog. We compute the precision (P), recall (R) and F1. Figure 1 shows a response that lexically aligns to the question. Syntactic Form We define three categories according to the syntactic structure, based on the constituency tree 4 :\n\u2022 short responses comprise a single sentence with the tree's root being either a simple declarative clause (S), or a declarative sentence with subject-aux inversion (SINV); see the first two responses in Figure 1 .\nF1 \u2191 EM \u2191 BLEU \u2191 ROUGE \u2191 BERT \u2191 K-F1 \u2191 K-F1++ \u2191 Critic \u2193 Q 2 Models F1 \u2191\n\u2022 fragments comprise an elliptic sentence, with its syntactic root not identified as either S or SINV; see last response in Figure 1 .\n\u2022 long-form responses are multi-sentence answers, which are rarely occurring. This is probably due to the conversational nature of TopiOCQA where complex questions are broken down into simpler ones across a dialogue.\nPronominals We identify the existence (or not) of a pronoun in a sentence in subject, or direct object position according to its dependency tree, e.g., \"It\" in the second response of Figure 1 . Table 1 summarizes the statistics of linguistic phenomena found in models and human responses. Note that GPT-3 variants produce more verbose, sentential and lexically aligned responses with the questions (see Recall column). In contrast, the finetuned model (DPR+FiD) generates shorter fragmented responses with more pronominals. This is expected as it follows the distribution of human responses, unlike the GPT-3 variants that have a very limited conditioning on the target distribution via the dialogue context getting encoded in the prompt.\n\nStudy of Faithfulness\nFaithfulness Definition We extend the definition by Adlakha et al. (2022) to consider faithfulness both wrt the dialogue and rationale:\nGiven a dialogue history H = (u 1 , ..., u n\u22121 ) and knowledge K = (k 1 , ..., k j ) at turn n, we say that utterance u n is faithful with respect to K and\nH iff \u2203\u0393 n such that \u0393 n |= u n \u2227 E(H, u n ) \u0338 = \u2205,\nwhere |= denotes semantic consequence, \u0393 n is a non-empty subset of K and E is the explicature of u n in context H as defined in (Rashkin et al., 2021a) .\n\nAutomatic Evaluation\nWe first employ a wide range of automatic metrics to assess model performance grouped according to their similarity to a gold (human) reference (reference-based), or their faithfulness to the provided knowledge K (reference-less).\nReference-based metrics Following Adlakha et al. (2022) and Dziri et al. (2022a) , we report F1 score, Exact Match (EM), BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) . These measure the overlap-based similarity between the generated response and the gold answer 5 . Reference-less token-level metrics Similar to Dziri et al. (2022a) and Shuster et al. (2021) , we report BERTScore (BERT) (Zhang et al., 2019) , and Knowledge-F1 (K-F1). Notably, the latter calculates the unigram overlap between the response and a knowledge snippet K, providing a verbatim measure of grounding to the input source.\nWe propose K-F1++, a variant of K-F1, that captures only the novel information in the generated response and discounts any lexical alignment to the question: it calculates the unigram overlap between the response and K, after subtracting any tokens appearing in the question from the response. Reference-less entailment metrics We report Critic (Dziri et al., 2022a) , a dialogue-trained classifier determining if a response follows from a given snippet K, and Q 2 (Honovich et al., 2021) , which measures faithfulness via question answering.\n\nHuman evaluation studies\nSimilar to Glaese et al. (2022) , Bai et al. (2022) and Thoppilan et al. (2022) , we conducted a human evaluation to assess the faithfulness of given responses, followed by a human evaluation study to collect human preferences when presented with two possible responses to an existing conversation. Faithfulness Judgment task Annotators are required to judge the plausibility of a response given the dialogue, the relevance of the gold passage to answer the question, and the faithfulness of the re-sponse given the dialogue and the gold passage. In more detail, we consider the response to be grounded when it (or a paraphrase of it) is found in the document. We consider a response to be faithful if, in addition to being grounded, it answers the question and follows from the dialogue. For example, given i) a conversation about European countries, ii) a document about European capitals, iii) a query \"What is the capital of Spain?\", and iv) the response \"Castellano\", if \"Castellano\" is in the document, the response is grounded. However, it is not faithful with respect to the dialogue as it does not correctly answer the question. Two annotators 6 completed the annotation for each model on 500 instances from TopiOCQA. Preference task Annotators are provided with a question, the previous dialogue and the gold passage that contains the answer, and are required to select their preferred response given two options. These are between a baseline model (DPR+FiD) and a model variant; they can also select both or none. We take a sample of 250 faithful and unfaithful instances from the previous task.\n\nResults\nTable 2 summarizes the automatic metrics. Baseline DPR+FiD outperforms the GPT-3 variants in all reference-based metrics. This is somewhat expected since the former is fine-tuned on the TopiOCQA dataset, whereas GPT-3 -despite being a much larger model-is evaluated in a zero-shot fashion. Surprisingly, DPR+GPT-3 outperforms the baseline in most reference-less metrics.\nInterestingly, the absolute difference between K-F1 and K-F1++ with respect to the baseline (2.3%) is significantly smaller than that of the GPT-3 variants (5.8%, and 4.5%, respectively). This is probably due to the latter being more lexically aligned to the user question than the baseline (see Table 1 ), hence there are more overlapping tokens removed when computing K-F1++. Nevertheless, the GPT-3 variants maintain superior knowledge-grounding scores even based on the stricter K-F1++.\nTable 3 paints a different story to the referenceless metrics: although all responses are regarded mostly plausible continuations to the dialogue, the GPT-3 variants (with the closed-book scoring worst) produce outputs that are less grounded and more unfaithful compared to DPR+FiD. We ob- served often the inclusion of extra information that could potentially be true but still not faithful to the input source. We leave fact checking of such extrinsic hallucinations to future work.\nThe most striking result according to the Preference task (Table 4 ) is that annotators preferred unfaithful responses over faithful ones, or rejected both options, even though they had access to the gold passage. DPR+GPT-3 overall was preferred 70% of times, with almost half preferences being towards unfaithful responses (48%). Similarly, GPT-3 was preferred 45% of the time with 66% of preferences being unfaithful. Again this supports our hypothesis that high lexical alignment has a great influence on users' choices, often bypassing the need to judge the accuracy of the response. Appendix A contains additional results on computing majority agreement per item among the 5 annotators for the Preference Task and a qualitative analysis of provided feedback.\n\nStudy of Trust\nSo far we have established that lexically aligned responses coming from GPT-3 variants are not necessarily faithful. The surface form seems to negatively affect users' preferences, obviating their need to check the supporting source, and creating a risk of placing trust to an imperfect system. With this experiment, we investigate a more general trend between linguistic phenomena and user trust.\nHuman Evaluation Experiment Annotators are presented with the dialogue only, and are asked to choose the response they trusted more from two possible responses, or none. Going beyond just lexical alignment, we selected 15 pairs of responses 7 , for every linguistic phenomenon in Section 2.2. We modified responses to ensure each specific phenomenon was the only difference between them. We collected 20 preferences for each response pair.\nResults Table 5 shows that annotators trusted responses with high lexical alignment significantly more than those with low lexical alignment. Interestingly, they trusted significantly more short answers than fragments, and preferred responses that did not present pronouns. This is in contrast to literature (Eshghi and Healey, 2016) , which primarily focused on human-to-human interactions; this could be down to people talking to a system (vs. a human), seeking stronger forms of evidence such as lexical alignment. Notably, the combination of the preferred presence and absence of phenomena aligns well with their calculated occurrences in the GPT-3 variants' responses (Table 1 ).\n\nConclusions\nWe investigated the performance of different models on the task of OCQA, measuring faithfulness and lexical phenomena. Automatic metrics highlighted how GPT-3 variants are less faithful than DPR+FiD, as confirmed by annotators in the faithfulness judgment task. We conducted a study on conversational grounding phenomena and a preference task, whose significant results demonstrated an effect of surface form in human preferences towards the more conversational GPT-3, even when unfaithful. Another experiment confirmed trust as being effected by high lexical alignment.\n", "hypothesis": " Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g.  In this paper, we show that taskbased systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred.  We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open-and closed-book models. Our results highlight the importance of systems that appear to be trustworthy by parroting user input while providing a faithful response.  \"unfaithful\" with respect to a rationale as retrieved from a knowledge base.  \u2020 Now at Google DeepMind..", "answer": false}
{"title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering", "content": "\nIntroduction\nEmpowered by learnable neural representations built upon pretrained language models, the dense retrieval framework has become increasingly popular for fetching external knowledge in various natural language processing tasks (Lee et al., 2019; Guu et al., 2020; Lewis et al., 2020) . For opendomain question answering (ODQA), the de-facto dense retriever is the bi-encoder architecture (Lee et al., 2019; Karpukhin et al., 2020) , consisting of a question encoder and a passage encoder. Typically, the two encoders are isomorphic but separately parameterized, as they are initialized from the same pretrained model and then fine-tuned on the task.\nDespite of its popularity, this bi-encoder architecture with fully decoupled parameterization has some open issues. First, from the efficiency perspective, the bi-encoder parameterization apparently results in scaling bottleneck for both training and inference. Second, empirical results from recent studies show that such bi-encoder dense retrievers underperform its sparse counterpart BM25 (Robertson and Walker, 1994) in various settings. For example, both Lee et al. (2019) and Karpukhin et al. (2020) suggest the inferior performance on SQuAD (Rajpurkar et al., 2016) is partially due to the high lexical overlap between questions and passages, which gives BM25 a clear advantage. Sciavolino et al. (2021) also find that bi-encoder dense retrievers are more sensitive to distribution shift than BM25, resulting in poor generalization on questions with rare entities.\nIn this paper, we develop Task-Aware Specialization for dEnse Retrieval, TASER, as a more parameter-efficient and robust architecture. Instead of using two isomorphic and fully decoupled Transformer (Vaswani et al., 2017) encoders, TASER interleaves shared encoder blocks with specialized ones in a single encoder, motivated by recent success in using Mixture-of-Experts (MoE) to scale up Transformer (Fedus et al., 2021) . For the shared encoder block, the entire network is used to encode both questions and passages. For the specialized encoder block, some sub-networks are task-specific and activated only for certain encoding tasks. To choose among task-specific sub-networks, TASER uses an input-dependent routing mechanism, i.e., questions and passages are passed through separate dedicated sub-networks.\nWe carry out both in-domain and out-of-domain evaluation for TASER. For the in-domain evaluation, we use five popular ODQA datasets. Our best model outperforms BM25 and existing biencoder dense retrievers, while using much less parameters. It is worth noting that TASER can effectively close the performance gap on SQuAD between dense retrievers and BM25. One interest-ing finding from our experiments is that excluding SQuAD from the multi-set training is unnecessary, which was a suggestion made by Karpukhin et al. (2020) and adopted by most follow-up work. Our out-of-domain evaluation experiments use En-tityQuestions (Sciavolino et al., 2021) and BEIR (Thakur et al., 2021) . Consistent improvements over the doubly parameterized bi-encoder dense retriever are observed in these zero-shot evaluations as well. Our code is available at https: //github.com/microsoft/taser.\n\nBackground\nIn this section, we provide necessary background about the bi-encoder architecture for dense passage retrieval which is widely used in ODQA (Lee et al., 2019; Karpukhin et al., 2020) and is the primary baseline model in our experiments.\nThe bi-encoder architecture consists of a question encoder and a passage encoder, both of which are usually Transformer encoders (Vaswani et al., 2017) . A Transformer encoder is built up with a stack of Transformer blocks. Each block consists of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2016) and layernormalization (Ba et al., 2016) applied to both sublayers. Given an input vector h \u2208 R d , the FFN sub-layer produces an output vector as following\nFFN(h) = W 2 max{0, W 1 h + b 1 } + b 2 , (1)\nwhere\nW 1 \u2208 R m\u00d7d , W 2 \u2208 R d\u00d7m , b 1 \u2208 R m ,\nand b 2 \u2208 R d are learnable parameters. For a sequence of N tokens, each Transformer block produces N corresponding vectors, together with a vector for the special prefix token [CLS] which can be used as the representation of the sequence. We refer readers to (Vaswani et al., 2017) for other details about Transformer. Typically the question encoder and passage encoder are initialized from a pretrained language model such as BERT (Devlin et al., 2019) , but they are parameterized separately, i.e., their parameters would differ after training.\nThe bi-encoder model independently encodes questions and passages into d-dimension vectors, using the final output vectors for [CLS] from the corresponding encoders, denoted as q \u2208 R d and p \u2208 R d , respectively. The relevance between a question and a passage can then be measured in the vector space using dot product, i.e., sim(q, p) = q T p. During training, the model is optimized based on a contrastive learning objective,\nEQUATION\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question. Although the bi-encoder dense retrieval architecture has achieved impressive results in ODQA, few work has attempted to improve its parameter efficiency. Further, compared to the spare vector space model BM25 (Robertson and Walker, 1994) , such bi-encoder dense retrievers sometimes suffer from inferior generalization performance, e.g., when the training data is extremely biased (Lebret et al., 2016; Karpukhin et al., 2020) or when there is a distribution shift (Sciavolino et al., 2021) . In this paper, we conjecture that the unstable generalization performance is partially related to the unnecessary number of learnable parameters in the model. Therefore, we develop a task-aware specialization architecture for dense retrieval with parameter sharing between the question and passage encoders, which turns out to improve both parameter efficiency and generalization performance.\n\nProposed Model: TASER\nAs shown in Fig. 1 , TASER interleaves shared Transformer blocks with specialized ones. The shared Transformer block is identical to the Transformer block used in the bi-encoder architecture, but the entire block is shared for both questions and passages. In the specialized block, we apply MoE-style task-aware specialization to the FFN sub-layer, following (Fedus et al., 2021) , where the router always routes the input to a single expert FFN sub-layer. In our experiments, we use a simple yet effective routing mechanism, which uses an expert sub-layer (Q-FFN) for questions and another (P-FFN) for passages. The router determines the expert FFN sub-layer based on whether the input is a question or a passage. Other routing mechanisms are discussed in Appendix A.\nTASER uses one specialized Transformer block after every T shared Transformer blocks in the stack, starting with a shared one at the bottom. Our preliminary study indicates that the model performance is not sensitive to the choice of T , so we use T = 2 for experiments in this paper.\nSimilar to the bi-encoder architecture, TASER is trained using the contrastive learning objective L sim defined in Equation 2. Specifically, the objective needs to use a set of negative passages P for each question. Following Xiong et al. (2020) and Qu et al. ( 2021), we construct P via hard negatives mining (Appendix B). Our experiments use the multi-set training paradigm, i.e., the model is trained by combining data from multiple datasets to obtain a model that works well across the board.\n\nIn-Domain Evaluation\nWe carry out in-domain evaluations on five ODQA datasets: NaturalQuestions (NQ; Kwiatkowski et al., 2019a), TriviaQA (TQ; Joshi et al., 2017) , WebQuestions (WQ; Berant et al., 2013) , Cu-ratedTrec (CT; Baudi\u0161 and \u0160ediv\u00fd, 2015) , and SQuAD (Rajpurkar et al., 2016) . All data splits and the Wikipedia collection for retrieval used in our experiments are the same as Karpukhin et al. (2020) . The top-K retrieval accuracy (R@K) is used for evaluation, which evaluates whether any gold answer string is contained in the top K retrieved passages.\nBesides BERT-base, coCondenser-Wiki (Gao and Callan, 2022) is also used to initialize TASER models. We further present results of hybrid models that linearly combine the dense retrieval scores with the BM25 scores. See Appendix D for details. Evaluation results are summarized in Ta- (2020) . We instead train models using all five datasets. Specifically, we observe that this would not hurt the overall performance, and it actually significantly improves the performance on SQuAD, comparing DPR (1) with DPR \u2020 .\nComparing models initialized from BERT-base, TASER \u22c4 significantly outperforms xMoCo (Yang et al., 2018) and is slightly better than DPR \u22c4 , using around 60% parameters. SPAR (Chen et al., 2022) is also initialized from BERT-base, but it augments DPR with another dense lexical model trained on either Wikipedia or PAQ (Lewis et al., 2021) , which doubles the model sizes (Table A3 ). TASER \u22c4 is mostly on par with SPAR-Wiki and SPAR-PAQ, except on SQuAD, but its model size is about a quarter of SPAR. Gao and Callan (2022) has shown the coCodenser model outperforms DPR models initialized from BERT-base in the single-set training setting. Here, we show that using coCondenser-Wiki for initialization is also beneficial for TASER under the multi-set setting, especially for SQuAD where Table 2 : Out-of-domain evaluation results on EntityQuestions (R@20) and four BEIR datasets (nDCG@10). BM25 and DPR Multi results are from (Sciavolino et al., 2021) and (Thakur et al., 2021) . R@20 is improved by 3.2 points. Notably, SQuAD is the only dataset among the five where DPR underperforms BM25, due to its higher lexical overlap between questions and passages. Nevertheless, TASER \u22c6 surpasses BM25 on all five datasets, and they are either on-par or better than state-of-theart dense-only retriever models, demonstrating its superior parameter efficiency.\nConsistent with previous work, combining BM25 with dense models can further boost the performance, particularly on SQuAD. However, the improvement is more pronounced on DPR as compared to TASER \u22c6 , indicating that TASER \u22c6 is able to capture more lexical overlap features. Finally, TASER \u22c6 BM25 sets new state-of-the-art performance on all five ODQA datasets.\nWe also compare the computation time needed for one epoch of training and validation. The baseline DRP model takes approximately 15 minutes, whereas TASER takes 11 minutes (26% improvement), both measured using 16 V100-32G GPUs.\n\nOut-of-Domain Evaluation\nWe use two benchmarks to evaluate the out-ofdomain generalization ability of TASER \u22c4 and TASER \u22c6 from Table 1 . EntityQuestions (EQ; Sciavolino et al., 2021) is used to measure the model sensitivity to entity distributions, as DPR is found to perform poorly on entity-centric questions containing rare entities. BEIR (Thakur et al., 2021) is used to study the model generalization ability in other genres of information retrieval tasks. Specifically, we focus on four datasets from BEIR where DPR underperforms BM25, i.e., ArguAna (AA; Wachsmuth et al., 2018), DBPedia (DBP; Hasibi et al., 2017) , FEVER (FEV; Thorne et al., 2018) , and HotpotQA (HQA; Yang et al., 2018) . Results are summarized in Table 2 . For EntityQuestions, we report R@20 scores following Sciavolino et al. (2021) . 2 For BEIR datasets, nDCG@10 scores are used following Thakur et al. (2021) .\nOn EntityQuestions, both TASER \u22c4 and TASER \u22c6 outperform the doubly parameterized DPR Multi (Karpukhin et al., 2020) , with TASER \u22c6 being slightly better. Similar to the in-domain evaluation results, TASER can effectively reduce the performance gap between the dense retrievers and BM25. These results further support our hypothesis that more parameter sharing can improve the model robustness for dense retrievers.\nOn BEIR datasets, we also observe that TASER models consistently improve over DPR Multi across the board. Notably, TASER \u22c4 and TASER \u22c6 can actually match the performance of BM25 on Ar-guAna and DBpedia. Interestingly, coCondenser pre-training has mixed results here, i.e., TASER \u22c6 is only better than TASER \u22c4 on HotpotQA and on par or worse on other datasets.\n\nRelated Work\nRecent seminal work on dense retrieval demonstrates its effectiveness using Transformer-based bi-encoder models by either continual pre-training with an inverse cloze task (Lee et al., 2019) or careful fine-tuning (Karpukhin et al., 2020) . One line of follow-up work improves dense retrieval models via various continual pre-training approaches (Guu et al., 2020; Chang et al., 2020; Izacard et al., 2021; Gao and Callan, 2022; Oguz et al., 2021) . Better contrastive learning objectives are also introduced (Xiong et al., 2020; Qu et al., 2021; Yang et al., 2021) . Motivated by the success of augmenting dense models with sparse models, Chen et al. (2022) combine the dense retriever with a dense lexical model that mimics sparse retrievers. All above work focus on improving the accuracy of biencoder dense retrievers, whereas our work tackles the parameter efficiency issue.\nUnlike most bi-encoder dense retrievers which measure the similarity between a question and a passage using their corresponding [CLS]vectors, ColBERT (Khattab and Zaharia, 2020) develops a late-interaction paradigm and measures the similarity via a MaxSim operator that computes the maximum similarity between a token in a sequence and all tokens in the other sequence. Such architecture has shown promising results in ODQA (Khattab et al., 2021) and the BEIR benchmark (Santhanam et al., 2022) . Our work instead focus on the improvement on the underlying text encoders, and the MaxSim operator introduced by ColBERT can be applied on top of TASER.\nXiong et al. ( 2021) use the BERT-Siamese architecture for dense retrieval, where all Transformer blocks are shared. Compared with this architecture, TASER is a more effective and general way to increase the parameter efficiency, by interleaving specialized Transformer blocks with shared ones.\n\nConclusion\nWe propose a new parameterization framework, TASER, for improving the efficiency and robustness of dense retrieval for ODQA. It interleaves shared encoder blocks with specialized ones in a single encoder where some sub-networks are task-specific. As the specialized sub-networks are sparsely activated, TASER can provide better parameter efficiency with almost no additional computation cost. Experiments show that TASER substantially outperforms existing fully supervised biencoder dense retrievers on both in-domain and out-of-domain generalization.\n", "hypothesis": " Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular.  Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages.  This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders.  Further, recent studies show that such dense retrievers underperform BM25 in various settings.  We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder.  Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers.  In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers.  Our code is available at https: //github.com/microsoft/taser..", "answer": true}
{"title": "Do transformer models do phonology like a linguist?", "content": "\nIntroduction\nIn computational linguistics, neural networks have occupied much of recent work. One prime driver is adaptability to multiple facets of linguistic phenomena. As an example, sequence-to-sequence models have been shown to capture inflection patterns across numerous languages (Kodner et al., 2022) . While their performance represents significant advances, the abstractions generated during the modelling process warrant further investigation. We experiment with phonological processes on a constructed language to compare the generalisations learned by transformer models with widespread linguistic phenomena.\nIn particular, we address the following questions:\n\u2022 Learning specific phonological processes (are some more difficult than others?)\n\u2022 Categorisation (can the model generalise a category, vowels, consonants, specific consonant groups, e.g. plosives?)\n\u2022 Is word structure (syllables) implicitly learned?\nWe establish that the transformer model successfully models all 29 phonological phenomena we consider, regardless of linguistic complexity. Our results show that the model can generalise to linguistic categories with some caveats. By examining the transformer model's generalisation of haplology, we show that the model appears to learn syllables; the model can recognise the difference between VC and CV and generate previously unseen CV sequences.\n\nRelated Work\nInvestigating the cognitive reality of linguistic categories defined within phonology has long been of interest to linguistics. Does the natural class of phonemes bear any significance to a cognitive reality? For example, a series of experiments (Finley and Badecker, 2009; Chambers et al., 2010; Skoruppa and Peperkamp, 2011) examine the natural class of vowels and whether phonological patterns can be extended to previously unseen vowels. The studies suggest that participants were mostly able to generalise. In a similar vein, Finley (2011) presents a study on consonant harmony. The results suggest that learners (human learners) can generalise to novel consonants when the phonological pattern is general. However, the learners failed to generalise when the rule triggering the consonant harmony pattern was highly specific.\nWe adapt this long-standing linguistic question to ask whether Transformer-based abstractions are linguistically informed. Our experiment setup swaps the human learner with the Transformer architecture. Previous studies investigating phonological phenomena with Transformers include Elsner (2021) , where Transformers can handle reduplication and gemination. To an extent, 1 the SIG-MORPHON shared tasks (Kodner et al., 2022 ) also demonstrate the capacity of Transformers to represent phonological processes through capturing allomorphs conditioned by phonological environments.\nThere have been extensive studies on various phonological processes and RNNs. Haley and Wilson (2021) shows that encoder-decoder networks (specifically LSTM and GRU architectures) can learn infixation and reduplication. Mirea and Bicknell (2019) explores whether phonological distinctive feature information is required for learning word-level phonotactic generalisations using LSTMs. The authors find that information about phonological features hinders model performance, and phonotactic patterns are learnable from the distributional characteristics of each segment alone. Moreover, distributional information proves to be integral in recovering phonological categories (Mayer, 2020) .\nAnother way to investigate neural architecture abstractions is to probe the model internally. Silfverberg et al. (2021) examines whether RNN states encode phonological alternations through experiments on Finnish consonant gradation. The authors show that the models often encode consonant gradation in a select number of dimensions. Rodd (1997) probes the hidden states of an RNN model which controls Turkish vowel harmony. Similarly, Silfverberg et al. (2018) establish a correlation between embedding representations and distinctive phonological features for Finnish, Spanish and Turkish. This paper focuses on a model-external interrogation of Transformer generalisations by studying the predictions produced.\n\nLanguage Design\nThe phonological phenomena in question are tested on a constructed language. The primary motivation for this is to allow for a controlled experiment and ensure that we can generate enough samples of the required phonological environments for rules to be triggered and thus observed. With this in mind, we require the constructed language to be as representative as possible of natural language. Therefore, key features were chosen based on the condition of being the most typologically common ones (Maddieson, 1984; Ladefoged and Maddieson, 1996; Maddieson, 2013) . The main characteristics are listed in Table . 1.\n\nGenerating a lexicon\nThe most complex syllable structure possible in the language is CCVVCC and the simplest one is V. Since our language design aims to generate a synthetic lexicon, we also control for word length distribution. Previous works have shown that word length over word types exhibits a roughly Gaussian distribution with a mean in the range [7, 10], depending on the language (Smith, 2012) . We have chosen a mean word length of 8.\nAn additional constraint when generating a lexicon is the sonority sequencing principle (SSP) (Selkirk, 1984; Clements, 1990) . Syllable structures tend to be highly influenced by the sonority scale, with the general rule that more sonorous elements are internal (i.e., close to the nucleus) and less sonorous elements are closer to the syllable edge. Therefore, we use a sonority metric to avoid generating implausible consonant clusters, with the onset and coda requiring opposite values on the metric, i.e. increasing sonority in the onset and decreasing in the coda.\n\nData 2\nOur data preparation follows three steps: lexicon generation, triplet (lemma, tag, surface form) formation via the finite-state tool foma (Hulden, 2009) and, finally, sampling of these triplets ac-cording to the experiment at hand and formatting for Fairseq. (Ott et al., 2019) 3 We train the model as a standard 'inflection' task (Kodner et al., 2022) , but with tags being identifiers of the processes that are to be triggered instead of morphosyntactic information. For example, the input sequence moupi#GEMINATION would be paired with the output mouppi. More example triplets are shown in Table 2 . 4 \n\nInput Tag\nOutput Lexicon generation entails generating viable syllable structures and filling these abstract structures using vowel and consonant inventories. The syllables are concatenated n times, where n is an integer between 1 and 10. We sample from this uniform distribution to produce a Gaussian distribution for word length with a mean of 8 symbols.\nateiSa #APOCOPE ateiS enpanka #APHAERESIS npanka a:N\u00c3 #SHORTENING aN\u00c3 vepisk #LENGTHENING vepi:k moupi #GEMINATION mouppi aimggi #DEGEMINATION aimgi soute #INTERVOCALIC soude refend #DEVOICE refent ketedu #METATHESIS kedetu totoN #HAPLOLOGY toN pima #COPY pima\nWe include a COPY tag, where the input is copied to the output, to negate any performance drop by the model when unseen lemmata are encountered (Liu and Hulden, 2022) . In other words, the model, at test time, will never encounter a completely unseen lemma on which to perform a phonological change, since it will always have witnessed at least an input-output pair of any lemma used that is simply copied to the output.\n3 See B for model details. 4 Our nomenclature of sound changes follows Campbell (2013) . \n\nModelling common phonological processes with varying degrees of complexity\nIn this experiment, we establish that seq2seq models can successfully capture a range of phonological processes, including more complex rules such as metathesis. As seen in Figure 1 , the transformer model performs reasonably well across all phonological phenomena, with little distinction between the complexity of the process considered.\n6 Linguistic Category generalisation The results show that p is transformed to a b 77.6% of the instances. Where the conversion does not take place, errors typically follow the pattern of, e.g. outputting epeiSe instead of ebeiSe with the input epeiSe\nTo investigate the comparatively low performance. We compare word-initial devoicing with word-initial voicing as a priming process. The results are summarised in Table . 4. The accuracy of the predictions for the unseen p was substantially lower in the case of word-initial voicing (40%) compared with the word-initial devoicing (74.8%). Interestingly, word-initial voicing involves the same process as intervocalic voicing (p>b), with only different environments triggering the process.\n\nWord-internal representations\nTo test whether seq2seq models can learn a representation of word-internal structures, such as syllables, we experiment with examples of haplology. Haplology (tatasa > tasa) is the process in which a repeated sequence of sounds is simplified to a single occurrence. For example, if the word haplology were to undergo haplology, it would reduce the sequence lolo to lo, haplology > haplogy.\nIn this experiment, we include two additional processes so the model can witness the contrast between vowels and consonants separately: (1) wordfinal vowel deletion and (2) word-final consonant deletion. To test the generalisation capacity of the model, at test time, we include the following withheld cases: unseen CVCV structures-i.e. cases where haplology should apply, but the specific CVCVsequence is never seen in the training data; words where haplology occurs more than once; and VCVC structures to see if the model (erroneously) learns to delete any repeating sequence of symbols. In our experiment, we withhold from the training set the following CVCV-sequences: dede, fofo, kuku, wowo, baba, vivi, papa, titi, soso, momo, nene, rere, lili, SuSu, jiji, \u00d9u\u00d9u, NaNa, gugu.\n\nProcess\nNote that haplology includes both cases where haplology applies and does not since the input word may or may not contain a CVCV-sequence where the two CVs are identical.\nTable 7 summarises the results obtained. The model shows high accuracy for the supplementary word-final vowel and consonant deletion processes. We separate the haplology cases further into specific test cases. Our results from the unseen CVCV category show strong evidence for model generalisation of CV structures. We further tested the same model on a separate test set consisting of VCVC structures. We see that for approximately 78% of the set, it correctly recognises these cases as incorrect conditions for haplology. In the remaining instances, the model does show a rare over-generalisation to sometimes delete repeating sequences regardless of the characteristics of the sequence.\nThe largest source of error within the haplology cases is the scenario in which haplology can be applied twice within the same word. In these cases, typically, the first case of repeating CV is deleted, and the second instance remains untouched, as when outputting fuejaja with input fufuejaja, instead of the gold fueja.\n\nConclusion\nThe transformer model successfully models all 29 phonological phenomena with slight variation across phenomenon complexity. Our results show that the model can generalize linguistic categories and structures. Through haplology, we show that the model appears to learn to recognize and generalize syllabic structure and is capable of recognizing the difference between VC and CV and can also generalize the transformation triggered by haplology to unseen CV sequences.\n", "hypothesis": " Neural sequence-to-sequence models have been very successful at tasks in phonology and morphology that seemingly require a capacity for intricate linguistic generalisations.  In this paper, we perform a detailed breakdown of the power of such models to capture various phonological generalisations and to benefit from exposure to one phonological rule to infer the behaviour of another similar rule.  We present two types of experiments, one of which establishes the efficacy of the transformer model on 29 different processes.  The second experiment type follows a priming and held-out case split where our model is exposed to two (or more) phenomena; one which is used as a primer to make the model aware of a linguistic category (e.g.  voiceless stops) and a second one which contains a rule with a withheld case that the model is expected to infer (e.g.  word-final devoicing with a missing training example such as b\u2192p). Our results show that the transformer model can successfully model all 29 phonological phenomena considered, regardless of perceived process difficulty. However, the model struggled in generalizing linguistic categories and structures, such as vowels and syllables. The model did not show a clear understanding of word structure when examining haplology, and it struggled to generate previously unseen CV sequences.", "answer": false}
{"title": "On Search Strategies for Document-Level Neural Machine Translation", "content": "\nIntroduction\nNeural machine translation (NMT) (Bahdanau et al., 2014; Vaswani et al., 2017) is widely adopted and produces excellent translations for many domains and language pairs. However, when these automatic translations are evaluated on the document level, they reveal shortcomings when it comes to consistency in style, entity-translation or correct inference of the gender, among other things (L\u00e4ubli et al., 2018; M\u00fcller et al., 2018; Thai et al., 2022) . Document-level NMT aims to resolve these shortcomings by taking the context of a sentence into account during translation. There exist many works on the topic of document-level NMT, proposing various changes to the standard transformer (Vaswani et al., 2017) architecture and training criteria to improve context incorporation and consequently translation quality. However, while the modeling and training aspects are covered in great detail in these works, the exact decoding strategy is often not very clearly described and sometimes not mentioned at all.\nIn this work, we head out to answer the question, which decoding strategy is most beneficial for document-level NMT systems. We compare all commonly used strategies, as well as some additional ones, on three standard document-level translation benchmarks. We find that most of the analyzed decoding strategies perform similar to each other. Also, higher quality context information can lead to better translations in certain scenarios.\n\nRelated Work\nThe earliest approaches to document-level NMT simply concatenate consecutive sentences without any further changes to the architecture compared to the sentence-level systems (Tiedemann and Scherrer, 2017; Agrawal et al., 2018) . Later, some changes were made to the vanilla transformer architecture, like segment embeddings (Ma et al., 2020) or attention masking (Zhang et al., 2020; Petrick et al., 2022) and a move was made towards translating longer segments (Junczys-Dowmunt, 2019; Liu et al., 2020; Zheng et al., 2021; Bao et al., 2021; Sun et al., 2022) . Other works employ a separate encoder to include the additional context on the source side (Jean et al., 2017; Bawden et al., 2018; Zhang et al., 2018; Voita et al., 2018) or make use of the context in a post-editing fashion (Voita et al., 2019; Xiong et al., 2019) . Further approaches include the usage of a cache (Wang et al., 2017; Maruf and Haffari, 2018; Tu et al., 2018) or hierarchical attention networks (Miculicich et al., 2018; Maruf et al., 2019; Wong et al., 2020) . Recently, several works have concluded that the simple concatenation approach used with the vanilla transformer architecture performs as good -if not better -than more complicated approaches that modify the model structure (Sun et al., 2022; Majumde et al., 2022) . Since we also observed this in our internal comparisons, we decided to focus on this simple approach for our analysis in this work.\nSeveral works made the argument that the improvements seen in automatic metric scores for document-level NMT systems are from regularization effects rather than from utilizing the additional context information (Kim et al., 2019; Li et al., 2020; Nguyen et al., 2021) . In order to better asses the improvements gained by documentlevel NMT, several targeted test suites have been released (M\u00fcller et al., 2018; Bawden et al., 2018; Voita et al., 2019; Jwalapuram et al., 2019) . However, all of these are based on just scoring contrastive examples without actually translating anything. Recently, Jiang et al. (2022) and Currey et al. (2022) have released frameworks that allow to score MT systems on their ability to generate contextually correct translations. 1\n\nSearch Strategies\nTraining a document-level NMT system that takes the last k sentences as context is straightforward using the standard concatenation strategy (Tiedemann and Scherrer, 2017) . Given some document level training data (F n , E n ), n = 1, ..., N , where (F n , E n ) denotes the n-th source-target sentence pair, during training we optimize the parameters \u0398 of the model towards\n\u0398 = argmax \u0398 n log p \u0398 (E n n\u2212k |F n n\u2212k ) .\nHere, E n n\u2212k denotes the concatenation of the sentences E n\u2212k , ..., E n .\nDuring search, given a document F M 1 , we want to find the best translation \u00caM 1 according to the model. Of course, exact search can not be performed and different works have used different methods to generate a translation: full segment (Liu et al., 2020; Bao et al., 2021; Sun et al., 2022) : we split the document into non-overlapping parts\nF k 1 , F 2k k+1 , ..., F M M \u2212k\nand translate each part separately using\nEQUATION\nwhich is approximated using standard beam search on the token level.\nlast sentence (Bawden et al., 2018; Agrawal et al., 2018; Zhang et al., 2020; Petrick et al., 2022; Majumde et al., 2022) : we split the document into overlapping parts ..., F i i\u2212k , F i+1 i\u2212k+1 , ... and translate each part separately using Equation 1. From each translated part we choose only the last sentence to get one translation for every sentence in the document.\nfirst sentence (Zhang et al., 2020) : similar to last sentence, but from each translated part we choose only the first sentence to get one translation for every sentence in the document.\n2-pass decoding (Maruf and Haffari, 2018; Maruf et al., 2019; Voita et al., 2019; Xiong et al., 2019) : we first generate a translation \u1ebcM 1 of the document using a sentence-level NMT system. Then, the final hypothesis \u00cai for each sentence F i is created using\n\u00cai = argmax E i p(E i |F i i\u2212k , \u1ebci\u22121 i\u2212k ) .\ndoc-trans (Miculicich et al., 2018; Voita et al., 2019; Garcia et al., 2019; Fernandes et al., 2021) : we generate the translation sentence by sentence, meaning\n\u00ca1 = argmax E 1 {p(E 1 |F 1 )} , \u00ca2 = argmax E 2 p(E 2 |F 2 1 , \u00ca1 ) , ...\n\ndoc-trans (beam)\n: similar to doc-trans, but instead of keeping just the best context \u00cai\u22121 1 , we keep the top-h candidates and prune them after each step i, analogous to beam search on the token level. h = 12 for all our experiments, the same as our token-level beam-size.\ncheating : this is just used as a tool for analysis.\nThe translation of each sentence F i is created using the true target reference EM 1 as context \u00cai = argmax\nE i p(E i |F i i\u2212k , Ei\u22121 i\u2212k ) .\nno context : this is just used as a tool for analysis. The translation of each sentence F i is created using no context information at all \u00cai = argmax\nE i {p(E i |F i )} . Cost sentence-level O(N L) document-level full segment O(N L) last sentence O(N Lk) first sentence O(N Lk) 2-pass decoding O(2N L) doc trans O(N L) doc trans (beam) O(N Lh)\nTable 1 : Computational cost of decoding (=number of forward passes through the decoder) for each of the search strategies described above. h denotes the sentence-level beam size.\nThe different search strategies also have a different computational cost associated with them. The biggest factor regarding the decoding cost is the number of forward passes through the model, specifically the decoder, that we have to do. We list the computational costs for the different decoding approaches in Table 1 under the assumption that the document consists of N sentences with average sentence length L and the model uses k \u2212 1 sentences as context. Please note that the decoding time might follow a different dependence than the cost in the above table, since it heavily depends on the available hardware. For example, doc trans and doc trans (beam) might have the same decoding time, if we have enough computational resources available, since the additional computations in doc trans (beam) can all be done in parallel.\n\nExperiments\nWe perform experiments on three document-level translation benchmarks, called NEWS (En\u2192De), TED (En\u2192It) and OS (En\u2192De). For the details regarding data conditions and preparation, as well as model training, we refer to Appendix A. For the context-aware systems, we concatenate 3 adjacent sentences (i.e. k = 3) using a special token <sep>. For the two En\u2192De tasks, we also evaluate the systems on the ContraPro test set (M\u00fcller et al., 2018) . Instead of scoring and ranking the contrastive examples in ContraPro, as the authors have originally envisioned, we translate the source side to calculate BLEU and TER as well as to score the pronoun translations according to Section 4.1. We can not evaluate the full segment search strategy on Con-traPro, because the sentences are not adjacent since they come from different documents. \n\nEvaluating Pronoun Translation\nAs further analysis, we measure how well ambiguous pronouns are handled when translating from English to German. Regarding gender, the English third-person pronoun 'it' (and its other forms), can be translated to the German words 'er', 'sie' or 'es', depending on which noun it refers to. On the other hand, ambiguities in the formality come from second-person pronouns. or informal pronoun appears in the reference. 5\n\nPerplexities\nFirst, we compare the perplexities of the hypotheses from the different search strategies, which are listed in Table 2 . The first thing to note is, that the reference has a much higher perplexity than all hypotheses, which is commonly seen for NMT systems. All document-level search strategies result in different hypotheses, which however have a similar perplexity score. Surprisingly, the cheating setting generates the worst translation perplexity-wise, even worse than using no context. This might be related to the observation, that the reference has a worse perplexity than any hypothesis, which is rather a modelling error than a search error.\n\nAutomatic Metrics\nNext, we evaluate the hypotheses based on the common automatic metrics BLEU and TER. The results are shown in Table 3 . The hypotheses created with no context seem to have the same quality as the sentence-level baseline. Surprisingly, the true reference as context does not improve performance on the NEWS and TED test sets. This indicates that the improvements seen on these test sets for the document-level system might not be related to better context incorporation. On the contrary, the OS system creates the best hypothesis with the true reference as context. All the actual decoding strategies give similar performance in terms of BLEU and TER with 2-pass decoding being a little bit behind.\nA special case is the first sentence strategy, which performs quite well on the standard test sets but poorly on ContraPro. This is, because ContraPro is designed in a way that the left side context is more important for translation than the right side. Finally, we analyze the quality of the pronoun translation as discussed in Section 4.1. In principle, we could calculate the F1 score for both, gender and formality, on all En\u2192De test sets. However, we discard the cases where one or more classes have less than 100 examples. This leaves us with the three test sets depicted in Table 4 . As a sanity check, we also report the ContraPro accuracies calculated from scoring the contrastive references as described in (M\u00fcller et al., 2018) . They are 48.2/45.8 for sentence-level and 68.2/82.2 for document-level for NEWS/OS respectively. That means, with just scoring, we overestimate the capabilities of the system, but the trend is still consistent. 6 Using the true reference leads to the best results in all cases. no context and first sentence leaves us with sentencelevel performance on the gender tasks, while all other decoding strategies perform similarly. For the formality, none of the methods can significantly outperform the sentence level system, although the cheating experiment shows that the system could do better if a better context information is provided. This might be, because segments of 3 sentences are too short to reliably detect if a setting is formal or informal, without access to the true reference.\n\nConclusion\nIn this work, we analyze decoding strategies for document-level NMT systems. Using the most popular document-level translation approach, we compare different search strategies found in the literature against methods developed by us. We find that most of the commonly used decoding strategies result in similar performance, both in terms of common automatic metrics, as well as on specific pronoun evaluation tasks. Therefore, we conclude that it is important to include the context information during decoding, but the exact way in which to do this is not as important. Also, we find that the document-level systems could actually profit from higher quality context information, in situations where this context is most relevant for translation. ture and training criterion. Other approaches exist, which might exhibit a different behavior in decoding. Two out of the three document-level translation tasks we use in this work are low resource with less than 500k sentence-pairs as training data. We chose these tasks due to computational limitations and to be better comparable to other works, but higher resource scenarios are more realistic for actual applications. We limit the analysis of pronoun translation to the English-German language pair. Also, there are other aspects of documentlevel NMT, like consistent translation of entities, which we did not consider in our analysis.\n", "hypothesis": " Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input.  There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input.  On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all.  In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding.  We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us.  In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks.  We find that most commonly used decoding strategies perform similar to each other and that higher quality context information has the potential to further improve the translation..", "answer": true}
{"title": "Bootstrapping Neural Relation and Explanation Classifiers", "content": "\nIntroduction\nRecently Tang and Surdeanu (2022) proposed a supervised method that jointly trains a relation classifier (e.g., which extracts the relation per:city_of_birth between John and London in the sentence John was born in London) with an explanation classifier that identifies context words that are important for the relation at hand (e.g., born and in in the above example). One limitation of this method is that, similar to other neural approaches, it is data hungry. This is an important drawback for real-world applications where annotated data is expensive to obtain.\nIn this work, we expand this approach to semisupervised scenarios where the only supervision comes from a few example rules. In particular, our method iteratively converts the explanations produced by the above method into rules, and uses these rules to generate new \"silver\" annotations 1 We release all code and data behind this work at:\nthat are added to the training data. The specific contributions of this effort are:\n(1) We introduce a novel semi-supervised neurosymbolic strategy for relation extraction that is explainable and requires minimal supervision. Our approach is neuro-symbolic because it relies on rules to explain the predictions of the neural relation classifier, and also to self-label training data.\n(2) We evaluate this approach on the TACRED dataset (Zhang et al., 2017) and obtain competive results in a few-shot setting, where the only supervision comes from a small number of example rules. 2 Our experiments highlight several important observations. First, our approach outperforms the model that contains the seed rules by 15 F1 points, which validates the self-training direction. Second, our method performs considerably better than a sister approach that uses the relation classifier (rather than the rules generated from explanations) for self supervision. We hypothesize that this is because the neural classifier suffers more from the \"curse of dimensionality\" due to its large number of parameters and the small amount of training data than our rules, which are constrained to simple syntactic patterns. Third, our approach performs comparatively with prompt-based methods (Sainz et al., 2021; Zhang et al., 2022) , even though our direction is simpler as it does not require a separate natural language inference component.\n\nRelated Work\nFor brevity, we focus our related work discussion on semi-supervised directions for information extraction that are closest to the proposed work: bootstrapping/self-training and recent prompt-based zero-or few-shot methods.\n\nBootstrapping/Self-Training\nTypical bootstrapping methods iterate through three steps: (a) annotate seed data using a small amount of human supervision (e.g., rules for information extraction); (b) train a model with the available annotations, and, finally, (c) apply the model on unlabeled texts to produce new \"silver\" annotations (Abney, 2002) . These approaches were popular before the deep-learning revolution. For example, Yarowsky (1995) used bootstrapping for word sense disambiguation; Riloff (1996) used it for dictionary acquisition; and Collins and Singer (1999) relied on bootstrapping for named entity classification. More recently, Gupta and Manning (2015) proposed a bootstrapping algorithm for named entity extraction that expands the set of known entities using word embeddings and k-nearest neighbor clustering. Eyal et al. (2021) used a syntactic search engine (Shlain et al., 2020) to bootstrap relation extraction. They also utilized natural language generation to further augment training data, which led to improved results. To our knowledge, we are the first to apply bootstrapping to a neuro-symbolic information extraction method, providing us both generalizability and explainability.\n\nPrompt-based Zero-or Few-shot Learning\nRecent large pre-trained language models (PLMs) with huge amount of parameters have showed the ability to handle NLP tasks with only a few examples or with prompts. Sainz et al. (2021) reformatted the relation extraction task as a natural language inference (NLI) task driven by a small set of manual templates. They obtained state-of-the-art results on the TACRED relation extraction dataset (Zhang et al., 2017) in both zero-and few-shot scenarios. The main limitation of this work is that it relies on a transformer-based NLI engine, which is not available in every domain. Wei et al. (2022) show that PLMs can perform multi-hop reasoning when using chain-of-thought prompts. Zhang et al. (2022) propose a prompt-based rule discovery and model boosting. However, Webson and Pavlick (2022) showed that the PLMs do not actually understand the prompt, which makes their decisions unreliable. Unlike the prompt-based approaches, our approach does not need the specific engine, e.g., for NLI, to perform the task. This gives us more flexibility in the choice of PLM and application domain.\n\nApproach\nSimilar to traditional bootstrapping (Abney, 2002) , our approach iteratively trains its classifier with the currently annotated data and applies the resulting model to the raw data to produce new annotations.\nEQUATION\n); end Algorithm 1: Pseudo code of our training procedure. R manual is the small set of seed rules; D raw is the collection of unlabeled sentences. f EC\u2212RC is the joint explanation-relation classifier of Tang and Surdeanu (2022) . M i is the trained neural model in ith iteration, P i and E i are the M i model's outputs (labels and explanations), and R i is the set of new rules generated from M i 's outputs.\nHowever, unlike traditional self training, which uses the classifier to annotate data, our approach converts the current model and data into rules, and uses the generated rules to annotate data. As we discuss in Section 4 this performs better empirically. Algorithm 1 shows the overall training procedure. We discuss the three key components below.\n(1) Rule Executor: We use Odin (Valenzuela-Esc\u00e1rcega et al., 2016) system as our rule executor. Common rules in this paper are syntactic patterns that contain a lexical trigger (or predicate) and syntactico-semantic arguments. These rules can be summarized as if-this-then-that patterns, e.g.: if predicate=born and nsubj is PERSON and nmod_in is CITY then relation=per:city_of_birth. 3 The rule executor efficiently matches these patterns over the syntactic trees of sentences.\n(2) Neural Model: Our approach utilizes the approach of Tang and Surdeanu (2022) . It contains two main classifiers: a relation classifier (RC), and an explanation classifier (EC). The RC is a multiclass classifier that distinguishes between actual relation labels seen in training. The EC is a binary word-level classifier, which labels which words in the sentence are important for the relation at hand. For example, for the sentence \"[CLS] John was born in London.\", the RC predicts a per:city_of_birth relation between John and London, and the EC identifies which words are critical for this relation (born and in). The EC and RC are trained jointly: the RC relies only on the hidden states of the context words identified by the EC (rather than, say, the [CLS] embedding); the EC is trained in a semi-supervised way, i.e., to maximize the probability of the correct RC label.\n(3) Rule Generator: The rule generator has two major components: the generator and the filter. The generator takes the model output from the neural model above and produces rules by: (a) connecting the EC output to the trigger of the rule; (b) generating subject and object arguments that are connected to the trigger through the shortest syntactic dependency path, and (c) assigning the RC output (the label) to this syntactic pattern. The filter takes the rules produced by the generator, applies them to a validation set and evaluates their precision. If a rule's performance is below a certain threshold, the filter discards it.\n\nTraining Procedure\nIn iteration 0, we feed the seed rules R 0 to the rule executor which applies them on the unlabeled sentence set D raw . These rules are a small set of rules written by human annotators. We add the rulematched data as seed annotations to the labeled data set D train and remove them from D raw .\nIn iteration i, we train the neural model M i with all labeled data in D train and use it to labeled the current D raw . Then, we generate and filter the rules that explain the sentences in D raw using the rule generator. Next, we feed the newly generated rules R i+1 to the rule executor, apply them over D raw , and produced new labeled data, i.e., sentences with labeled relations. Lastly, we add the newly labeled data to D train and remove the corresponding sentences from D raw . We repeat this procedure until performance converges on a validation set.\n\nData Preparation\nWe report results on the TACRED relation extraction (RE) dataset (Zhang et al., 2017) . To mimic low-resource scenarios, we hide all gold labels from the training set. We keep only 1% of the development set labeled for tuning purposes. We use as seeds (R 0 ) the rule set from (Tang and Surdeanu, 2022) , which is a combination of the surface patterns of Angeli et al. (2015) , and syntactic rules written in the Odin language (Valenzuela-Esc\u00e1rcega et al., 2016) , which were manually cre-ated by Tang and Surdeanu (2022) . Overall, we use an average of 7 rules per relation type. Tang and Surdeanu (2022) indicated that these rules did not require considerable effort, i.e., they were developed by one of the authors within a few hours.\n\nBaselines\nWe compare our results with four baselines: an extended version of the rule-based approach of Angeli et al. (2015) , a typical self-training approach, a prompt-based RE approach based on natural language inference (NLI) (Sainz et al., 2021) , and a prompt-based rule discovery and boosting approach (Zhang et al., 2022):\nRule-based extraction: This baseline uses only the two sets of rules in our seed set (R 0 ): (a) the surface rules from (Angeli et al., 2015) , which are executed in the Stanford CoreNLP pipeline (Manning et al., 2014) ; and (b) the syntactic rules of Tang and Surdeanu (2022) , which are executed in the Odin framework. 4 Self-training: This baseline is similar with our full method, with the exception that, in each iteration, we use the trained RC model to label new data rather than the generated rules. NLI-prompt: (Sainz et al., 2021) reformulated the RE task as an entailment task driven by templates. They manually generated a number of verbalization templates for each relation in TACRED, e.g., the per:city_of_birth relation is verbalized as as {subj} was born in {obj}, where {subj} and {obj} will be replaced with the entities in the given sentence. Thus, the sentence containing the relation to classify becomes the premise and the verbalized template the hypothesis. The RE task is then reduced to finding the best entailment template for the given sentence. no_relation is generated if no entailment score over a certain threshold is observed.\nPRBOOST iteratively generates rules from prompting, asks a human expert to filter the rules, use the rules to generate new annotations, and, lastly, use the annotations to train a new model (Zhang et al., 2022) .\n\nImplementation and Evaluation Details\nFor our method we follow the same implementation details and hyper parameters as Tang and Surdeanu (2022) . The only difference is that instead of using the full development set, we randomly select 1% from the TACRED development set for tuning, i.e., to decide which generated rules to keep, and to decide when the bootstrapping training procedure completes. For the former, we used 0.5 as the threshold; that is, if the precision of a rule is lower than the threshold, we discard that rule. For a fair comparison, for the NLI-prompt approach of Sainz et al. (2021) we chose their zeroshot scenario 5 and RoBERTa (Liu et al., 2019) . 6 Further, to guarantee the same level of supervision, we converted our seed rules to their verbalization templates (see Appendix A for the conversion procedure). Lastly, we estimate their threshold for no_relation using the same validation dataset as our approach. We iterated from 0.1 to 0.9 with a step of 0.1, and observed the best validation results for a threshold of 0.8.\n\nResults and Discussion\nTable 1 reports the overall performance of our approach and the four other methods. For PRBOOST we used the numbers reported in the corresponding paper. We draw the following observations:\n(1) As expected, the rule-based baseline has high precision but suffers from low recall. In contrast, our best model that is bootstrapped from the same rules has 20% higher recall and 15% higher F1 (absolute). This indicates that the bootstrapping approaches popularized for information extraction several decades ago remain valid in the neural era.\n(2) Our approach performs statistically significantly better than the traditional self-training approach that uses the relation classifier for self la- beling (53.97 vs. 48.56 F1) 8 . The fact that rules perform better for self labeling than the actual neural model is somewhat surprising. Our hypothesis is that the neural model suffers more from overfitting due to its large number of parameters and the relatively small amount of training data. Rules generalize better (and thus produce better \"silver\" labels) because the simple syntactic patterns generated provide reduced opportunities for overfitting.\nTo validate this hypothesis we plot the learning curves of the two approaches on our validation partition in Figure 1 . 9 These curves indicate that the best performance of our approach is in iteration 4, while the neural self-training continues to improve on validation until iteration 9. However, as shown in Figure 2 , the performance of the modelbased self-training on test saturates after iteration 4, which suggests that, indeed, the neural self-training method suffers from overfitting.\n(3) Our method performs better than PRBOOST and similarly to the NLI-prompt method. This suggests that self-training, when carefully implemented, remains competitive with more modern alternatives such as prompt-based methods. More importantly, our approach is simpler, as it does not need the extra inference layers, e.g., the NLI classifier in the NLI-prompt approach.\n\nError Analysis\nWe conclude this section with a brief error analysis that compares our rule-based bootstrapping approach with the \"traditional\" neural-model-based self-training approach.\nFirst, we conducted a comparative analysis of the annotations produced by the two after the first iteration. In this setting, both approaches were trained on the same seed annotations, which ensures a fair comparison. Out of all positive examples in training data (excluding the seed examples), our approach annotated 7.64% of them correctly, while self-training annotated only 3.70% correctly. Among these true positives produced by the neural bootstrapping, 75.68% of them are also annotated correctly by our approach. This indicates that our generated rules not only cover most of the neural model's annotations, but also correctly annotate more uncovered instances.\nTable 2 shows a case where the neural-modelbased self-training method falls short (first row in the table) and a case where bootstrapping does not seem to help (second row). These two cases are extracted by the same rule, in which the trigger words \", a\" are used to connect SUBJ_PERSON and OBJ_TITLE entities through <punct or <punct appos syntactic dependencies. This rule matches 120 examples in the training set, 102 of which are true positive. Importantly, only 67 of the 120 examples are uncovered by the neural bootstrapping model, which highlights again the increased coverage of our rule-based method. Interestingly, while the label produced by the rule-based bootstrapping model for the second example in the table is technically wrong, in the opinion of the authors the gold label is incorrect here. This suggests that rules not only improve self-training, but have the potential to also improve the consistency of training data.\n\nConclusion\nWe introduced a method that self trains (or bootstraps) a neuro-symbolic approach for relation extraction that combines neural relation and explanation classifiers. Unlike traditional self-training, our approach uses rules for bootstrapping. In particular, our method iteratively converts the explainable models' outputs to rules and applies them to unlabeled text to produce new annotations.\nWe evaluated our approach in a low-resource scenario where there is no labeled data, and the only supervision comes from a small number of seed patterns. Our experiments showed that using rules in the bootstrapped training procedure is better than the typical self-training method that relies on neural model predictions. Further, we show that we obtain similar performance with prompt-based models for relation extraction without the additional NLI component required by such approaches.\n", "hypothesis": " We introduce a method that self trains (or bootstraps) neural relation and explanation classifiers.  Our work expands the supervised approach of (Tang and Surdeanu, 2022), which jointly trains a relation classifier with an explanation classifier that identifies context words important for the relation at hand, to semisupervised scenarios. In particular, our approach iteratively converts the explainable models' outputs to rules and applies them to unlabeled text to produce new annotations. However, our method outperforms a sister approach that uses the neural relation classifier for self-supervision due to its more sophisticated modeling capabilities. Additionally, our approach achieves comparable performance to prompt-based methods, demonstrating the effectiveness of our simplified approach without the need for a separate natural language inference component.  Our evaluation on the TACRED dataset shows that our method outperforms the rule-based model we started from by 15 F1 points, outperforms traditional self-training that relies just on the relation classifier by 5 F1 points, and performs comparatively with the prompt-based approach of Sainz et al.  (2021) (without requiring an additional natural language inference component).", "answer": false}
{"title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks", "content": "\nIntroduction\nThe omnipresence of large pre-trained language models (Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) has fueled concerns regarding their systematic biases carried over from underlying data into the applications they are used in, resulting in disparate treatment of people with different identities (Sheng et al., 2021; Abid et al., 2021) .\nIn response to such concerns, various benchmarks have been proposed to quantify the amount of social biases in models (Rudinger et al., 2018; Sheng et al., 2019; Li et al., 2020) . These measures are composed of textual datasets built for a specific NLP task (such as question answering) and are accompanied by a metric such as accuracy of prediction which is used as an approximation of the amount of social biases.\nThese bias benchmarks are commonly used by machine learning practitioners to compare the degree of social biases (such as gender-occupation Gender-Occupation Bias \u274c\n\nGender-Occupation Bias \u2705\nThe electrician warned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician warned the homeowner that she might need an extra day to finish rewiring the house. coref coref\n\nWinoGender\nThe electrician cautioned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician cautioned the homeowner that she might need an extra day to finish rewiring the house. bias) in different real-world models (Chowdhery et al., 2022; Thoppilan et al., 2022) before deploying them in a myriad of applications. However, they also inadvertently measure other non-social biases in their datasets. For example, consider the sentence from WINOGENDER in Figure 1 . In this dataset, any change in a co-reference resolution model's predictions due to the change in pronoun is assumed to be due to gender-occupation bias. However, this assumption only holds for a model with near-perfect language understanding with no other biases. This may not often be the case, e.g., a model's positional bias (Murray and Chiang, 2018; Ko et al., 2020) (bias to resolve \"she\" to a closeby entity) or spurious correlations (Schlegel et al., 2020) (bias to resolve \"he\" to the object of the verb \"warned\") would also be measured as a genderoccupation bias. As a result, a slightly different template (e.g., changing the verb to \"cautioned\") could result in completely different bias measurements.\n\nWinoGender-Alternate Construction\nThe goal of this work is to illustrate the extent to which social bias measurements are effected by assumptions that are built into dataset constructions. To that end, we consider several alternate dataset constructions for 2 bias benchmarks WINO-GENDER and BIASNLI. We show that, just by the choice of certain target-bias-irrelevant elements in a dataset, it is possible to discover different degrees of bias for the same model as well as different model rankings 1 . For instance, one experiment on BIASNLI demonstrated that merely negating verbs drastically reduced the measured bias (41.64 \u2192 13.40) on an ELMo-based Decomposable Attention model and even caused a switch in the comparative ranking with RoBERTa. Our findings demonstrate the unreliability of current benchmarks to truly measure social bias in models and suggest caution when considering these measures as the gold truth. We provide a detailed discussion ( \u00a75) of the implications of our findings, relation to experienced harms, suggestions for improving bias benchmarks, and directions for future work.\n\nRelated Work\nA large body of work investigates ways to evaluate biases carried inherently in language models (Bolukbasi et al., 2016; Caliskan et al., 2017; Nadeem et al., 2021) and expressed in specific tasks (Nangia et al., 2020; Kirk et al., 2021; Schramowski et al., 2022; Prabhumoye et al., 2021; Srinivasan and Bisk, 2021; Kirk et al., 2021; Parrish et al., 2021; Baldini et al., 2022; Czarnowska et al., 2021; Dev et al., 2021a; Zhao et al., 2021) . Alongside, there is also growing concern about the measures not relating to experienced harms (Blodgett et al., 2020) , not inclusive in framing (Dev et al., 2021b) , ambiguous about what bias is measured (Blodgett et al., 2021) , not correlated in their findings of bias across intrinsic versus extrinsic techniques (Goldfarb-Tarrant et al., 2021; Cao et al., 2022) , and susceptible to adversarial perturbations (Zhang et al., 2021) and seed word selection (Antoniak and Mimno, 2021) .\nThe concurrent work by (Seshadri et al., 2022 ) discusses the unreliability of quantifying social biases using templates by varying templates in a se-mantic preserving manner. While their findings are consistent with ours, the two works provide complementary experimental observations. Seshadri et al. (2022) study a wider range of tasks, though we focus our experiments on a wider set of models and alternate dataset constructions (with a greater range of syntactic and semantic variability). As a result, we are able to illustrate the effect of the observed variability on ranking large language models according to measured bias for deployment in real world applications.\n\nSocial Bias Measurements and Alternate Constructions\nBias measures in NLP are often quantified through comparative prediction disparities on language datasets that follow existing tasks such as classification (De-Arteaga et al., 2019) or coreference resolution (Rudinger et al., 2018) . As a result, these datasets are central to what eventually gets measured as \"bias\". Not only do they determine the \"amount\" of bias measured but also the \"type\" of bias or stereotype measured. Datasets often vary combinations of gendered pronouns and occupations to evaluate stereotypical associations. It is important to note that these constructs of datasets and their templates, which determine what gets measured, are often arbitrary choices. The sentences could be differently structured, be generated from a different set of seed words, and more. However, we expect that for any faithful bias benchmark, such dataset alterations that are not relevant to social bias should not have a significant impact on the artifact (e.g. gender bias) being measured.\nThus, to evaluate the faithfulness of current benchmarks, we develop alternate dataset constructions through modifications that should not have any effect on the social bias being measured in a dataset. They are minor changes that should not influence models with true language understanding -the implicit assumption made by current bias benchmarks. Any notable observed changes in a model's bias measure due to these modifications would highlight the incorrectness of this assumption. Consequently, this would bring to light the unreliability of current benchmarks to faithfully measure the target bias and disentangle the measurement from measurement of other non-social biases. A non-exhaustive set of such alternate constructions considered in this work are listed below.\nFigure 2 : An instance (\"The engineer informed the client that he would need to make all future payments on time\") from WINOGENDER benchmark modified under various shallow modifications ( \u00a73). To a human eye, such modifications do not necessarily affect the outcome of the given pronoun resolution problem.\nNegations: A basic function in language understanding is to understand the negations of word groups such as action verbs, or adjectives. Altering verbs in particular, such as 'the doctor bought' to 'the doctor did not buy' should typically not affect the inferences made about occupation associations. Synonym substitutions: Another fundamental function of language understanding is the ability to parse the usage of similar words or synonyms used in identical contexts, to derive the same overall meaning of a sentence. For bias measuring datasets, synonymizing non-pivotal words (such as non-identity words like verbs) should not change the outcome of how much bias is measured. Varying length of the text: In typical evaluation datasets, the number of clauses that each sentence is composed of and overall the sentence length are arbitrary experimental choices. Fixing this length is common, especially when such datasets need to be created at scale. If language is understood, adding a neutral phrase without impacting the task-specific semantics should not alter the bias measured. Adding descriptors: Sentences used in real life are structured in complex ways and can have descriptors, such as adjectives about an action, person, or object, without changing the net message expressed by the text. For example, the sentences, \"The doctor bought an apple.\", and \"The doctor bought a red apple.\" do not change any assumptions made about the doctor, or the action of buying an apple. Random samples: Since the sentence constructs of these datasets are not unique, a very simple alternate construction of a dataset is a different subsample of itself. This is because the dataset is scraped or generated with specific assumptions or parameters, such as seed word lists, templates of sentences, and word order. However, neither the sentence constructs or templates, nor the seed word lists typically used are exhaustive or representative of entire categories of words (such as gendered words, emotions, and occupations).\nSee Fig. 2 for example constructions on WINO-GENDER (App. A, B for detailed descriptions).\n\nCase Studies\nWe discuss here the impact of alternate constructions on two task-based measures of bias. 2\n\nCoreference Resolution\nSeveral different bias measures (Rudinger et al., 2018; Zhao et al., 2018; Cao and Daum\u00e9 III, 2021) for coreference resolution work similar to Winograd Schema (Winograd, 1972) where a sentence has two entities and the task is to resolve which entity a specific pronoun or noun refers to. We work here with WINOGENDER (Rudinger et al., 2018) , popularly used to measure biases. It is worth noting that WINOGENDER was originally intended by its authors to merely be a diagnostic tool that checks for bias in a model; the authors note that it may demonstrate the presence of model bias but not prove the absence of the same. Nonetheless, models developed today are indeed tested and compared for social bias on WinoGender, leading to its usage as a comparative standard or benchmark (Chowdhery et al., 2022; Thoppilan et al., 2022) .\nThe metric used to evaluate bias is the percentage of sentence pairs where there is a mismatch in predictions for the male and female gendered pronouns. For instance, in Fig. 2 , if the pronoun \"he\" is linked to \"engineer\" but switches to \"client\" for the pronoun \"she\", that would indicate a genderoccupation bias. Higher the number of mismatches, higher the bias. In particular, note that the metric does not take into account the accuracy of the predictions, but rather only the mismatch between the two pronouns.\nWe experiment with three alternate constructions of the dataset: addition of clauses, addition of adjectives, and synonymizing words in templates.\nEach alternate construction is introduced so as to not affect the overall meaning of the sentence. Experimental Results: We use an end-toend coreference model with SpanBERT embeddings (Lee et al., 2018; Joshi et al., 2020) , Uni-fiedQA (small, base, and large) (Khashabi et al., 2020) QA model, 3 and a long-document coreference model with Longformer encodings (Toshniwal et al., 2021) . Results of evaluating these models on various WINOGENDER constructions is summarized in Fig. 3a . Small changes to the formulation of dataset templates result in sizable changes to computed bias measures compared to the published baseline constructions. For example, a construction involving added adjectives after occupations would have found the UnifiedQA (large) model to have 10% less bias compared to the default constructions. The sensitivity to the dataset constructions can have a drastic effect on ranking models according to their social bias, as Fig. 3a shows. For example, the SpanBERT model is considered to have less bias than UnifiedQA (small) model in the baseline dataset, but would be considered to be more biased if the templates had clauses after the participants or adjectives before the occupation. \n\nNatural Language Inference\nNatural Language Inference (NLI) is the task of determining directional relationships between two sentences (a premise (P) and a hypothesis (H)). Dev et al. (2020) 's measure based on NLI (BIASNLI) evaluates if stereotypical inferences are made by language models. We use their dataset for genderoccupation stereotypes containing approximately 2 million sentence pairs such as P: \"The doctor bought a bagel.\", H: \"The man bought a bagel.\". The expected prediction for each sentence pair in the dataset is neutral, and therefore the bias metric used is the fraction of neutral inferences on dataset -the higher the score, the lower the bias.\nWe experiment with three alternate constructions of the dataset: verb negation, random sampling, and addition of clauses. Note that the alternate constructions do not impact the unbiased label (neutral). Any change in construction (say negating a verb) is applied to both the premise and hypothesis. Refer to App. B for a detailed description. Experimental Results: We use RoBERTa trained on SNLI (RoBERTa-base-SNLI) (Liu et al., 2019) , ELMo-based Decomposable Attention (ELMo-DA) (Parikh et al., 2016) , ALBERT (Lan et al., 2019) , distilled version of the RoBERTa-base model (Sanh et al., 2019) , and RoBERTa-large finetuned on WANLI (Liu et al., 2022) . The bias measured with each model using BIASNLI is recorded in Fig. 3b . The results show how small modifications to the dataset again result in large changes to the bias measured, and also change the bias rankings. For example, adding a negation largely reduces the bias measured (\u25b3 = 28.24) for ELMo-DA, and also results in a switch in the comparative ranking to RoBERTa-base-SNLI. Furthermore, as seen in Fig. 4 , there is a significant overlap in the bias measures of ALBERT, DistilRoBERTa, and ELMo-DA under random sampling, 4 which corresponds to high variability in relative model ordering across different sub-samples of the dataset.\n\nDiscussion and Conclusion\nSocial bias measurements are very sensitive to evaluation methodology. Our empirical evidence sheds light on how the model's non-social biases brought out or masked by alternate constructions can cause bias benchmarks to underestimate or overestimate the social bias in a model. More interestingly, it is important to note that different models respond differently to perturbations. In fact, the same perturbation can result in a higher or lower measured bias depending on the model (as seen in \u00a74.1 and \u00a74.2), which points to how models might parse information (and thus bias) differently.\nWhile current bias measures do play a role in exposing where model errors have a stereotypical connotation, a lack of sentence construction variability or even assumptions made when creating seed word lists can reduce the reliability of the benchmarks, as we see in this work ( \u00a74.2). Even with simple sentences, it is not apparent how to disentangle the biased association of the identity with the verb or the occupation amongst others. This is especially important to note as it highlights that measures can lack concrete definitions of what bi-ased associations they measure. Consequently, the relation between measured bias and experienced harm becomes unclear.\nWe hope that our troubling observations motivates future work that thoroughly investigates how to construct robust benchmarks that faithfully measure the target bias without being affected by model errors and other non-social biases. As suggested by our subsampling experiments (Appendix F), it might be fruitful to encourage both syntactic and semantic diversity in these benchmarks. Bias benchmarks that provide uncertainty measures (instead of a single number) might enable practitioners to better compare models before deploying them. Furthermore, since the opaqueness of large language models makes it challenging to understand how and to what extent a linguistic change will affect the measured bias, explainable models might indeed facilitate better measurement of their social bias. Assuming that we can generate faithful explanations for a model's predictions, an exciting future direction is to explore construction of bias benchmarks which operate on the explanations of the predictions rather than the predictions themselves. Lastly, we also encourage discussions on the complexity of the sentences used in benchmarks and their implications on what gets measured in relation to un-templated, naturally-occurring text (Levy et al., 2021) , as an attempt to ground our measurements in experienced harms.\n", "hypothesis": " How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye).  To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias.  On two wellknown social bias benchmarks (WINOGENDER and BIASNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias.  We hope these troubling observations motivate more robust measures of social biases..", "answer": true}
{"title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum", "content": "\nIntroduction\nRecently, there has been a tremendous interest in employing image-caption pretraining for downstream vision tasks like zero-shot object classification (Radford et al., 2021) and zero-shot object detection (Zareian et al., 2021; Li et al., 2022) . The idea is to learn a common semantic space where the visual embeddings of objects in images lie close to the textual embeddings of the concepts (objects' name/tag/label) in captions they refer to. This learned semantic space is later exploited for zero-shot object recognition by finding the concept embedding nearest to the objects' embeddings.\nDespite the recent success, image-caption pretraining is a complex problem as it entails aligning multiple concepts in a caption with multiple objects in an image, as shown in fig. 1 . Different methods have tried to solve this problem from various angles -CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) by using more data, ALBEF (Li et al., 2021) by using more complex network architecture, Florence (Yuan et al., 2021) and CoCa (Yu et al., 2022) by using more tasks and ERNIE-ViL 2.0 (Shan et al., 2022) by using more data augmentations (views).\nWe propose an alternative approach based on a novel learning strategy that is architecture agnostic and does not require any additional data or compute. We take inspiration from cognitive science research studying how children learn language (concepts) in early stages by just observing their surroundings (images). Specifically, we refer to two studies showing that childern learn rapidly if the object of interest is unambiguous (Pereira et al., 2014) and by applying co-referential statistics across multiple scenes (Smith and Yu, 2008) .\nWe implement these two ideas via a curriculum learning approach (demonstrated in fig. 1 ):\n1. We train the model in multiple phases of increasing difficulty with each phase containing one more concept in the caption than the previous one. Moreover, each phase contains only one new concept, the rest seen in prior phases.\n2. In each phase, we leverage the concept-object association learned in prior phases to recognize the seen concepts and focus on aligning the new/unseen concept (section 2.2.2).\nThese two strategies effectively reduce the problem of aligning multiple object-concept pairs per training sample to aligning only one such pair.\nTo the best of our knowledge, no prior work has applied curriculum leaarning to image-caption pretraining in this way. Srinivasan et al. (2022) apply a curriculum based on the difficulty of negative samples in contrastive loss. Whereas, Liu et al. (2021) design the curriculum based on the granularity of text: from words to phrases to sentences.\nAlthough our proposed approach can be applied to any multimodal network architecture, we pick OVR-CNN (Zareian et al., 2021) due to its simplicity. We pretrain it with the proposed curriculum learning approach and evaluate on the downstream task of zero-shot object detection. We demonstrate that curriculum learning outperforms vanilla imagecaption pretraining on a variety of architectural settings -with and without a pretrained image encoder and/or a pretrained text encoder. We even show superior performance in low-data settings, suggesting our method can be leveraged in low-resource scenarios as well.\n\nMethod\nWe propose a curriculum learning framework to improve image caption pretraining. In this work, we apply it to OVR-CNN as its architecture is simpler and easier/faster to train/evaluate. We begin the description of our approach with a brief background on OVR-CNN. Next, we discuss how we modify it to implement the proposed curriculum learning framework.\n\nOVR-RCNN Background\nOVR-RCNN is a dual-encoder (separate visual encoder and text encoder) multimodal architecture. First, it pretrains the encoders using image-captions and later utilizes them for the downstream task of object detection. We only discuss the pretraining procedure as we only utilize this component.\nOVR-RCNN's visual encoder is ResNet-50 (He et al., 2016 ) and text encoder is either BERT (Devlin et al., 2019) or GloVE (Pennington et al., 2014) . The visual encoder takes an image, I with w \u00d7 h dimensions, as input and outputs a feature map of w/32\u00d7h/32 regions. Each feature map is a vector which is transferred to language space using a projection layer. This gives the visual embeddings, e I i , for each region i. The tokenized caption, C, is input to the text encoder which outputs an embedding e C j for each token j. The token-image region pair is aligned via weak supervision. Specifically, a global alignment score between image and caption, \u27e8I, C\u27e9 G is calculated using a locally weighted average alignment score of image regions and tokens as follows:\nEQUATION\nwhere \u27e8., .\u27e9 L is the dot product of two vectors, n I and n C are the number of image and caption tokens respectively, and\nEQUATION\nThe model is trained using contrastive learning by maximizing the global alignment score, \u27e8I, C\u27e9 G , between positive image-caption pairs and minimizing it between negative pairs sampled from the same training batch.\nL = \u2212 log exp\u27e8I,C\u27e9 G {I \u2032 ,C \u2032 }\u2208N I,C exp\u27e8I \u2032 ,C \u2032 \u27e9 G +exp\u27e8I,C\u27e9 G (3) where, N I,C = {I, C \u2032 |C \u2032 \u2208 B C } \u222a {I \u2032 , C|I \u2032 \u2208 B I } and B C , B I\nare batch captions and batch images respectively. This learning objective aligns paired image and caption together and also provides weak supervision for image-regions and caption-tokens association.\n\nCurriculum Learning Framework\nOVR-CNN facilitates object-concept alignment through coarse image-region and concept alignment. However, as an object can span multiple image regions or multiple objects can span an image region, this strategy can be noisy. To eliminate this noise and focus on the contribution of our curriculum framework to object-concept alignment, we train the model using object region features instead of image region features. To this end, object 13379 1) and (2).\n\nCurriculum Design\nThe learning is divided into 1, 2, 3 . . . k phases. Each phase p is trained with only those imagecaption pairs having p concepts per caption. To divide the data into phases, we use spacy 1 to PoS (Part of Speech) tag the captions. Depending upon the number of nouns in each caption, the caption and its paired image is grouped into the corresponding phase. This strategy of designing the curriculum also imparts the data an additional property empirically -at most only one new concept is introduced per caption in each phase (as demonstrated in fig. 2b ).\n\nCurriculum Aware Alignment Loss\nTo recognize the concepts in captions previously seen in prior phases and focus on aligning the new/unseen concept, we formulate a novel Curriculum Aware Alignment Loss (L C ). Specifically, we first calculate the previously learned object-concept alignment, a o,j from modified eq. ( 2), using either the trained model from the last iteration (L CR ) or the trained model from the last phase (L CP ). Next, a o,j is used to compute:\na \u2032 o,j = exp\u27e8e I o ,e C j \u27e9 L exp (\u2212 max o (a o,j ). t T ) n I o \u2032 =1 exp\u27e8e I o \u2032 ,e C j \u27e9 L exp (\u2212 max o (a o,j ). t T )\nwhere, t is the current iteration number and T is the total number of iterations in training. For a concept j, which is already closely aligned to an object o, max o (a o,j ) is high. This leads to a low value of a \u2032 o,j , resulting in less attention being paid to concept j in the current training iteration/phase. Vice versa for a concept that is not 1 https://spacy.io/usage well aligned with any object. a \u2032 o,j effectively redistributes the attention of learning to focus more on concepts that are not well aligned with any object. The term t/T has a low value in the beginning of training and gradually scales to 1 by the end. This allows the network to ignore prior knowledge in the beginning while utilizing it in the latter stages.\nWe use a \u2032 o,j to replace a o,j in modified eq. ( 1), and then use eq. ( 3) to compute L C .\n\nPretraining Dataset and Implementation Details\nWe use the COCO Captions dataset (Chen et al., 2015) for pretraining. It contains 118,287 images and 5x captions. To obtain bounding box regions for objects in images, we use COCO Objects (Lin et al., 2014) dataset as it uses the same set of images as COCO Captions. We divide the data into k = 4 phases using the strategy discussed in section 2.2.1. Figure 2a shows number of captions assigned to each phase. As shown in fig. 2b , the majority of captions in each phase have at least k-1 concepts previously seen, allowing the curriculum to introduce at most one new concept per training sample. Further, as more concepts are introduced with each passing phase, the percent of captions per phase actually introducing a new concept decreases (as depicted in fig. 2c ). By phase 4, this percent reduces to < 5%. Additional phases of training may not contain enough captions actually introducing a new concept in the curriculum way, making these phases similar to regular image-caption training. Hence, we limit to 4 phases. We train the model using SGD optimizer, with a batch size of 32 for 4 epochs in each phase, a learning rate of 0.005, a step scheduler, and the loss L CP . \n\nDownstream Task, Dataset and Transfer\nWe evaluate the performance of the model on zeroshot object detection task on COCO Objects, val split (4836 images; 33374 instances of 65 object class). The task involves object bounding box predictions besides classifying these object regions to a label (concept). However, our method is aimed only at improving the alignment of object regions to a concept. As such, we eliminate any performance noise from bounding box predictions by only evaluating the classification accuracy of object regions given ground truth object bounding boxes.\nTransfer to Downstream Task: We extract object features from image and object bounding box using visual backbone and use it to find the closest class label vector (obtained via language backbone).\n\nBaseline and Evaluation\nOur baseline is OVR-CNN, a regular image-caption pretrained model. However, since our method uses object region features instead of image patch features for multimodal alignment (section 2.2), we also pretrain OVR-CNN with object regions to obtain OVR-CNN O . It is transferred to downstream task similar to our proposed model (section 3.2).\nOur proposed curriculum framework outperforms the baseline in various settings, as shown in table 1. The accuracy numbers reported are averaged across three seeds. This demonstrates that our proposed learning strategy works across encoders trained from scratch or pretrained ones.\nPerformance Gain Analysis. We analyze model performance on object classes introduced during pretraining in phase 1 and phase 2 separately. As reported in table 2, the improvement in phase 2 objects is ~10x. This illustrates that our curriculum strategy improves alignment of multiple concepts in a caption by focusing on one at a time.\nLow Data Setting. Our model outperforms the baseline even if both uses 50%, 25% or 10% data (fig. 3 ), indicating its utility when data is scarce.\nRegion proposals instead of ground-truth object regions. We use a RPN model (Girshick, 2015b) trained class-agnostically on Visual Genome (Krishna et al., 2016) to generate object regions. The superior performance of our model against baseline, reported in table 3, demonstrates that our approach is effective even when groundtruth object regions are not available.\nLoss Ablation. From table 4 , we can conclude that our curriculum design works (Ours + L > OVR-CNN O + L); our proposed curriculum aware loss works (Ours + L < Ours + L CR ) irrespective of curriculum (OVR-CNN O + L < OVR-CNN O + L CR ); curriculum aware loss works better when previous knowledge is taken from the last phase instead of the last iteration (Ours + L CP > Ours + L CR ).\nQualitative Analyssis. We provide qualitative analysis as well to shed more insights into the cases where our approach works/doesn't work. From Figure 4 , we find that our model performs better than OVR-CNN O in certain cases, especially when the objects are from Phase 2 -\"snowboard\", \"cup\", \"skis\" etc. This provides further evidence towards our claim that our approach improves the alignment of Phase 2 objects.\nComparison of traditional mAP metric for object detection As mentioned before, we have focused our experiments on evaluating object- concept alignment rather than on traditional object detection mAP metric. This was done to avoid unnecessary performance noise arising from training a RPN, which is required for mAP evaluation. However, to test the limits of our model, we evaluate on this noisy mAP metric as well. We keep all the settings similar as Zareian et al. (2021) , except we pretrain using our curriculum learning approach.\nThe results are reported in Table 5 . We find that our model performs better in the most generic Generalized ('All') set (41.33 vs 39.9), signifying the effectiveness of our approach even in this noisy setting. We further observe that we perform better in the base classes while lagging behind in the target classes. A deeper analysis shows that most of the Phase 2 objects, on which we make major improvements, lie in the base classes. This explains the improved performance on base classes and slight depreciation in target classes performance.\nTraining with image grid regions. Our curriculum based pretraining method was aimed at improving object-concept alignment by focussing on one object at time. To facilitate this, we pretrained directly with object regions. Image regions were not used to eliminate noise arising from an ob- ject spanning multiple regions or multiple objects being present in the same image region (object presence noise). However, we further push the limits of our model to assess how it performs when trained with noisy image regions instead of object regions.\nThe results are reported in Table 6 . We find that our model performs slightly worse than . We attribute this performance degradation to the inherent object presence noise in image regions as discussed earlier.\n\nConclusion\nWe proposed a curriculum learning framework to improve image-caption pretraining, using the number of concepts in captions. We also designed a novel curriculum aware loss to focus learning on the unaligned concept in each phase. Our approach outperforms vanilla image-caption pretraining in various settings, including with/without pretrained encoders and small data. Further, we extensively analysed our model to study the contribution of each component.\n", "hypothesis": " Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection.  However, image-caption pretraining is still a hard problem -it requires multiple concepts (nouns) from captions to be aligned to several objects in images.  To tackle this problem, we go to the roots -the best learner, children. We take inspiration from cognitive science studies dealing with children's language learning to propose a curriculum learning framework.  The learning begins with easy-to-align image caption pairs containing one concept per caption.  The difficulty is progressively increased with each new phase by adding one more concept per caption.  Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning multiple concept-object pairs in each phase.  We show that this learning strategy improves over vanilla image-caption training in various settings -pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc..", "answer": false}
{"title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification", "content": "\nIntroduction\nLarge language models (LLMs) with parameters in the order of billions (Brown et al., 2020) have gained significant attention in recent years due to their strong performance on a wide range of natural language processing tasks. These models have achieved impressive results on benchmarks (Chowdhery et al., 2022) , particularly in zero or few-shot settings, where they are able to generalize to new tasks and languages with little to no training data. There is, however a difficulty in tuning parameters of these large-scale models due to resource limitations. Additionally, the focus on benchmarks has led to the neglect of real-world challenges, such as that of hierarchical classification. As a result, the long-tail problem (Samuel et al., 2021) has been overlooked. This occurs when a vast number of rare classes occur in the presence of frequent classes for many natural language problems. In many industrial real-world applications, a strong performing method for hierarchical classification can be of direct utility. New product categories are emerging in e-commerce platforms. Existing categories, on the other hand, may not be very intuitive for customers. For example, upon browsing categories such as night creams, we may be unable to find a product in a sibling-node category of creams. This is further highlighted by platforms in which a systematic structure is not created for users; parent nodes may be in place of child nodes, and vice versa (Asghar, 2016) . Manually categorizing product categories can be a costly redesigning endeavour. To tackle this problem, we suggest refactoring traditional hierarchical flatlabeled prediction tasks (Liu et al., 2021) to a more indicative long-tail prediction task. This involves structuring the classification task to closely reflect the real-world long-tail distribution of classes. In doing so, we are enabled to leverage LLMs for long-tail prediction tasks in a strict zero-shot classification setting. Through a series of experiments, results in this work show that our proposed method is able to significantly improve the performance over the baseline in several datasets, and holds promise for addressing the long-tail problem in real-world applications. The contributions of this work can be summarized as follows:\n\u2022 We refactor real-world hierarchical taxonomy datasets into long-tailed problems. In doing so, we create a strong testbed to evaluate \"strict zeroshot classification\" with LLMs.\n\u2022 We explore utilizing LLMs to enhance the capabilities of entailment-contradiction predictors for long-tail classification. This results in strong capabilities of performing model inference without resource-intensive parameter updates.\n\u2022 We show through quantitative empirical evidence, that our proposed method is able to overcome limitations of stand-alone large language models. Our method obtains strong performance on longtail classification tasks.\n\nBackground and Related Work\nStrict Zero-Shot Classification Previous works (Liu et al., 2021; Yin et al., 2019) have explored zero-shot classification extensively under two settings-(i) zero-shot, where testing labels are unseen, i.e. no overlap with the training labels, and (ii) generalized zero-shot, testing labels are partially unseen. In both cases, the model is trained on data from the same distribution as the test data. In our proposed strict zero-shot setting, the model is only trained to learn the entailment relationship from natural language inference (NLI) corpora (Williams et al., 2018) . The training data for this model has no overlap with the distribution or semantics of the inference set. Additionally, previous works utilizing NLI have either not examined the utility of LLMs (Ye et al., 2020; Gera et al., 2022) , or transfer the capabilities of LLMs to smaller models but have failed to use them in a strict zero-shot setting for long-tail problems, only demonstrating their utility for benchmark tasks (Tam et al., 2021; Schick and Sch\u00fctze, 2021) . Works exploring LLMs have also limited their study to only using them independently without exploring entailment-contradiction prediction (Wei (Wang et al., 2022a,b; Jiang et al., 2022; Chen et al., 2021) . For this reason, the graph representations may have limited value independently, although representations may be used to assist text classification by providing an organized label space. These works only introduce hierarchies to bring order to the label space, but overlook the original task of hierarchical taxonomy classification (Kowsari et al., 2017) . For all previous works, difficult to obtain fine-tuning data is required to produce strong sig- nals. In our work, we refactor this data into a leaf-node label prediction task with the help of entailment-contradiction relations and LLMs. In doing so, we enable hierarchical taxonomy prediction independent of utilizing training data for the downstream task.\n\nMethodology\nIn this paper, we investigate the limitations of LLMs in three overlooked settings, when-(i) the model is not provided with sufficient examples due to the input text length, (ii) the label space includes tokens largely unobserved in the model's pretrained vocabulary, and (iii) there are a large number of distractors in the label space (Kojima et al., 2022; Min et al., 2022; Razeghi et al., 2022) . These scenarios are common in real-world tasks, but are often overlooked in the development and evaluation of LLMs. To address these challenges, we propose the use of entailment-contradiction prediction (Yin et al., 2019) , the task of determining whether a premise logically entails or contradicts a hypothesis. Through our method, we are able to leverage strong reasoning from Yin et al. ( 2019) with the retrieval abilities of LLMs (Wang et al., 2020) to improve overall performance in a strict zero-shot setting, where the model must classify samples from a new task without any fine-tuning or additional examples used for training from the same distribution as the inference dataset. Importantly, our proposed combined method does not require parameter updates to the LLM, a resource-intensive process that is not always feasible with increasingly large model size (Chowdhery et al., 2022) . Our simple framework is shown in Figure 1 . Considering the label space, C = {C 1 , C 2 , ...C n } as the set of classes for any given dataset, and a text input, X, we can utilize the entailment predictor, E to make a contradiction, or entailment prediction on each label. This is done by using X as the premise, and \"This text is related to C i .\"\n\u2200C i \u2208 C as the hypothesis (Yin et al., 2019) . In our work, the premise may be modified to include the prompt template. The prediction, E(X) lacks any external knowledge and is restricted to the label space, which may result in poor performance. E(X) can however, provide us with an implicit classification of the contradiction relation for sibling nodes. In our work, we use E(X) as an initializer for LLMs. We also regard it as a baseline as it shows strong performance independently. A LLM, L on the other hand, operates in an open space, with aforementioned shortcomings for classification tasks. For our purposes, we can regard this as a noisy knowledge graph (Wang et al., 2020) , which may be utilized to predict ancestors or descendants of the target class. We consider the prediction made by the LLM as L(X). It is important to note that L(X) may or may not belong to C. We try several prompts for this purpose, shown in Appendix A.\nBy combining these two components, we can create a template which utilizes the entailment relation explicitly, and the contradiction relation implicitly by constructing L(E(X)) to deseminate combined information into an entailment predictor for classification. The template we use is task-dependent, and is generally robust given an understanding of the domain. On Web of Sciences we use: \"Here is some text that entails E(X): X. What area is this text related to?\". For Amazon Beauty, we use \"Here is a review that entails E(X): X. What product category is this review related to?\". In this setting, our method still meets a barrier due to limitations of LLMs. By constructing a composite function, E(L(E(X)), we are able to leverage our LLM in producing tokens which may steer the entailment predictor to correct its prediction. The template used for this composite function is \"Here is some text that entails L(E(X)): X.\" across all datasets.\nGeneral Form: Although our results are reported combining the advantages of L, and E to produce upto the composite function E(L(E(X)), this can 1 : Baseline models (Top) underperform our method (Bottom) across all datasets for average scores of Top-1, Top-3, and Top-5 accuracy and Macro F1. Our primed and primed+ models explicitly utilize the entailment relation, with one and five predictions of L(E(X)) respectively. All models used are available on Huggingface.\n. be extended as it holds the property of being an iterative composition function to E(L(E(L...E(X)))).\nOur observations show this setting having comparable, or marginal improvements with our dataset. However, this may prove to be beneficial in other tasks. We will investigate, and urge other researchers to explore this direction in future work.\n4 Experiments and Results\n\nDataset and Experimental Settings\nWe refactor the widely used Web of Sciences (WOS) with Kowsari et al. ( 2017), and Amazon Beauty (McAuley et al., 2015) datasets to follow a class-wise long-tail distribution as shown in Figure 3 . Additionally, we create two variations of the Amazon Beauty dataset, first in which it contains the same tree depth as WOS, both containing 3000 samples, and second in which all classes are included for their maximum tree depth, containing 5000 samples. We select these datasets as they challenge the shortcomings of LLMs. The input text of providing multiple abstracts in the WOS dataset surpasses the maximum input token length of most transformer-based models. This makes it difficult for models to learn the input distribution, a requirement for showing strong in-context performance (Min et al., 2022) . Next, many tokens in the label space of both the WOS and Amazon Beauty datasets rarely occur in pretraining corpora, details of which are provided in the Appendix B. Additionally, both datasets contain a large number of distractors, or incorrect classes in the label space. Further details are provided in Appendix C. All experiments are performed on a single NIVIDA Titan RTX GPU. We use BART-Large-MNLI with 407M parameters as our baseline. We use this model as it outperforms other architectures trained on MNLI for zero-shot classification. For our LLM, we opt to use T0pp (Sanh et al., 2022) with 11B parameters 1 , as previous works show that it matches or exceeds performance of other LLMs such as GPT-3 (Brown et al., 2020) on benchmarks.\n\nResults and Discussion\nResults of our method are shown in Table 1 . LLMs, due to their limitations, perform poorly as a standalone model for long-tail classification. These results can be improved by priming the model with an entailment predictor through the usage of a prompt. The baseline shows strong performance independent of the LLM, as it operates on a closed label space. The capabilities of the baseline can be enhanced by further explicitly priming it with a entailment relation through a LLM. Rows in which T0pp is initialized, or primed with E are indicated with Primed. Priming the model showcases improvements across all datasets for macro F1. For accuracy, priming the model shows benefit in two out of three datasets. In Figure 4 , we show the results of Top-5 predictions for the WOS dataset.\nAll results are aggregated in Table 1 . It is important to highlight that prompt variation led to stable results for our LLM. The variance upon utilizing BART-MNLI is negligible across prompts. The best results are observed upto Top-4 predictions on both accuracy and macro F1 for our method, when the entailment prompt is enhanced with a greater number of tokens corresponding to the output of L(E(X)). The variation between our method and the baseline is much greater for Top-1 predictions, but Top-5 prediction variance is negligible. Detailed results for both depth settings of Amazon Beauty are shown in Appendix C.\n\nConclusion\nIn this work, we utilize an LLM in the form of a noisy knowledge graph to enhance the capabilties of an entailment predictor. In doing so, we achieve strong performance in a strict zero-shot setting on several hierarchical prediction tasks. We also show the necessity of refactoring existing hierarchical tasks into long-tail problems that may be more representative of the underlying task itself. The utility in practical industry settings is also highlighted through this setting.\n", "hypothesis": " In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings.  However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification.  In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative longtail prediction task.  We observe LLMs are more prone to failure in these cases.  To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting.  Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets..", "answer": true}
{"title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)", "content": "\nIntroduction\nLarge language models have been shown to be capable of producing high-quality and reasonably accurate summaries in zero-shot settings (Goyal et al., 2022; Liang et al., 2022) , with GPT-3 besting fully supervised models in generic news summarization, according to human judgments (Goyal et al., 2022) . In this work we evaluate if such models are similarly able to summarize medical literature, a highstakes domain that demands factual accuracy.\nSpecifically, we use the newest iteration of GPT-3 (text-davinci-003; GPT3-D3 from here) to generate summaries of (a) individual articles describing individual randomized controlled trials (RCTs) Figure 1 : We enlist domain experts to evaluate the factual accuracy of summaries and simplifications of medical articles describing clinical trials. We consider both single-and multi-document settings.\nevaluating the efficacy of interventions, and, (b) collections of such articles that describe several trials addressing the same underlying clinical question (e.g., evaluating the same medication). These constitute single-and multi-document summarization tasks, respectively. In the single-document case, we also evaluate the ability of GPT3-D3 to summarize in plain language. We enlist domain experts (with medical training) to annotate model outputs, and seek to address the following questions.\nRQ1 Does GPT3-D3 produce faithful summaries of medical articles? RQ2 Can GPT3-D3 accurately simplify while also summarizing such texts? RQ3 Can GPT3-D3 synthesize-aggregate the findings presented in-multiple input articles in a way that accurately reflects the totality of the evidence? RQ4 What sort of factual mistakes does GPT3-D3 make when performing these tasks (if any), and what are the risks implied by such errors?\nOverall, we find that GPT3-D3 performs singledocument summarization and simplification with reasonably good accuracy. However, it is less able to accurately synthesize evidence reported in collections of trials (in the multi-document case). We release all model outputs and accompanying annotations to facilitate additional work on this topic.\n\nSingle Document Summarization\nData We sample 100 articles describing randomized control trials (RCTs) indexed in the Trialstreamer database (Marshall et al., 2020) , which also provides automatically extracted \"key results\" 2 alongside titles and abstracts. We search for trials published after November 28 2022, following the release date of GPT3-D3, to ensure the model has not seen any of the studies during pre-training.\nExperimental Setup Using the RCT data described above, we evaluate the ability of GPT3-D3 to faithfully summarize and simplify biomedical texts in a zero-shot setting. We also compare GPT3-D3 summaries to summaries generated using Flan-T5 (Wei et al., 2021) , but qualitatively find that GPT3-D3 summaries are much higher quality. We provide results of this comparison in Appendix F. 3 . Specifically, we prompt GPT3-D3 to separately produce: (i) a technical summary, and, (ii) a plain language summary (August et al., 2022) . See Appendix C for all prompts.\nStudy Design We designed an evaluation scheme that captures the sensitivity of medical information. To assess factuality, we collect annotations about omissions and errors with respect to main results, and key components of the trials including populations, interventions, and outcomes (\"PICO\" elements; Richardson et al. 1995) . Where appropriate, we ask annotators to highlight spans of generated text that are inconsistent with the input-these might be \"new\" concepts introduced or spans that directly contradict the input. To gauge overall linguistic quality, we solicit assessments regarding the fluency and usefulness of a summary on a Likert scale (1932) . We include additional questions about the simplification of technical terms for the plain language summaries. We provide a complete taxonomy of the survey in Appendix H.\nAnnotations We recruited 3 domain experts with medical training on the Upwork platform, 3 and task them each with annotating 100 samples. In total, we collect 300 annotations (3 annotations per sample). We use Label Studio 4 as our interface.\n\nMultiple Document Summarization and Evidence Synthesis\nData For multi-document summarization, we download meta-analyses from the Cochrane Library (these are reviews of medical evidence, usually RCTs). 5 Our final sample contains 50 multidocument studies comprising meta-review titles, reference abstracts (inputs), and target conclusions (target summaries) written by domain experts, 10 of which were published post-GPT3-D3 release. 6 Experimental Setup Because inputs comprise multiple abstracts, these (together with generated tokens) often exceed the token capacity of GPT3-D3.\nIn our dataset, about 41% of the samples exceeded this upper-bound. We report information about our data, including average length, in Appendix B. To address the upper-bound problem, we adopt a simple two-phase strategy for multi-document summarization. First, we generate independent summaries for each abstract, using the single-document summarization prompt described in Section 2. Then, we include all the generated single-document summaries in our multi-document synthesis prompt 7 (examples in Appendix C).\nStudy Design Our evaluation rubric asks for assessments of generated outputs as compared to: (a) inputs, and, (b) target summaries. Specifically, we ask if generated summaries are supported by the summaries provided as inputs in the multidocument case, and to what extent they agree with target (reference) summaries. We also ask annotators to highlight spans of text in generated outputs that disagree with paired target summaries. We reproduce the full rubric in Appendix H.\nWith respect to annotators, we use the same procedure described in Section 2; we recruited 3 new medical experts and tasked them each with annotating 50 samples, for a total of 150 annotations. \n\nResults\nRQ1: Does GPT3-D3 produce faithful summaries of medical articles? In the single document setting, we find that GPT3-D3 generates summaries of biomedical abstracts that are fairly highquality. Figure 2 (a) shows that annotators rated a majority of the summaries as being coherent, useful, and capturing \"key results\".\nWhen GPT3-D3 does err, it tends to make minor mistakes or omit details. The latter is more common than the former, as shown in Figure 3 (a) .\nRQ2: Can GPT3-D3 accurately simplify while summarizing medical texts? Shown in Figure 2 (b), GPT3-D3 produces simplified summaries that are similarly deemed to be coherent and useful, and which appear to contain key results. Simplified outputs are scored highly in terms of readability, indicating that these summaries would be understood by someone without medical training.\nIn comparison to the technical summaries, Fig-\n\nSupported by Input Summaries\nFigure 4 : Proportion of summaries that reflect the target summary and are supported by the input summaries in the multi-document setting. While most summaries follow from the input, less than half are rated as agreeing with the target summary.\nure 3 (b) shows that there are fewer omissions but a slightly higher amount of errors. These may be problematic, but -importantly -some omissions are expected in a simplified summary, as certain details that are important for an accurate summary for a technical audience may not be necessary to convey key information to a more general audience.\nRQ3: Can GPT3-D3 synthesize findings presented in multiple input articles in a way that accurately reflects the totality of the evidence? We now evaluate GPT3-D3's performance on multidocument summarization, i.e., its ability to synthesize evidence (Wang et al., 2022) . Figure 4 shows that most summaries generated by GPT3-D3 in this setting are supported by the inputs. This is consistent with our findings in RQ1: GPT3-D3 is able to summarize faithfully with respect to given input. However, we find that generated summaries do not consistently agree with the target summaries. Indeed, Figure 4 shows that generated summaries disagree with the targets in over half of cases. This discrepancy suggests that human-written summaries in the biomedical domain require a level of synthesis that is not captured by GPT3-D3 .\nRQ4: What sort of factual mistakes does GPT3-D3 make and what are the risks? In RQ1, we reported that GPT3-D3 sometimes omits key information. Figure 5 characterizes the types of omissions and errors made, with respect to PICO elements. GPT3-D3 tends to underspecify elements in the summary more often than generating inaccuracies. Appendix F provides further details regarding underspecification. In the simplification task, GPT3-D3 capably simplifies most technical terms in the generated output (Figure 6 ).\nRegarding RQ3, we showed that there are often discrepancies between generated and target summaries, despite the former being supported by the inputs. Human-written summaries of trials may be more cautious in their conclusions. We measure the evidence strength and direction of both the target and generated summaries, and find that GPT3-D3 tends to recommend marginal or substantive beneficial effects regarding interventions in the majority of the summaries (Figure 7 ).\nOverall, we find that GPT3-D3 copies frequently from inputs. This results in summaries that are often faithful to the input. It may also be one reason that summaries tend to have more omissions (rather than errors) in the single document case, and it may also explain how summaries in the multi-document case often disagree with the reference synopsis while also being supported by (some subset of) the inputs. We calculate the degree of overlap and similarity between inputs and generated summaries from GPT3-D3 for both single-document and multidocument summarization at the sentence level (Fig- ure 8). GPT3-D3 often copies sentences verbatim. In other cases, it changes phrasings but only very slightly (see Appendix F for examples).\nFurther, Figure 8 shows how many sentences in each summary have a BLEU score of \u2265 30; which indicates the sentences are highly aligned. Over 70% of the summaries have at least a quarter of the sentences copied from the input. Appendix F shows some examples of highly similar summaries and sentence pairs.\n\nRelated Work\nMore broadly in summarization, several efforts have called for increased emphasis on human (rather than automated) evaluation of generated texts, increased deployment of human-centered systems for text generation evaluation (Khashabi et al., 2021) , and greater focus on building benchmarks that incorporate human preferences (Liang et al., 2022; Fabbri et al., 2021) . And indeed, Goyal et al. (2022) find that summaries produced by GPT3-D3 are often preferred by humans over alternative model outputs even when automated metrics disagree. Such findings have motivated the manual analysis we conduct for this work. As far as we know, there has not been any work that assess the degree to which GPT-3 is proficient at summarizing biomedical and clinical data in both single-document and multi-document cases.\nOur analysis of summarization in the biomedical space complements recent work analyzing the question answering capabilities of such models in this domain (Singhal et al., 2022; Li\u00e9vin et al., 2022) and the degree to which they encode medical knowledge implicitly (Sung et al., 2021) . Other work has considered using summarization of biomedical texts as assistive tools for reading (August et al., 2022) .\n\nConclusions\nWe evaluate the ability of GPT3-D3 to faithfully summarize and simplify medical literature. The expert annotations we collect indicate that GPT3-D3 performs single-document tasks quite well, but struggles with multi-document summarization. This highlights the ability to aggregate across documents as a direction for future work. We release all data and annotations to facilitate such work in the medical space going forward.\n", "hypothesis": " Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in fewand zero-shot settings.  However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine.  In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision.  We consider both single-and multi-document settings.  In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to synthesize evidence reported across a collection of articles.  We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it excels in accurately aggregating findings over multiple documents.  We release all data and annotations used in this work.  1 1 https://github.com/cshaib/ summarizing-medical-evidence Simplify Summarize Synthesize Single article describing a Randomized Controlled Trial (RCT) Collection of articles describing RCTs on the same topic.", "answer": false}
{"title": "Environmental Claim Detection", "content": "\nIntroduction\nIn the face of climate change, we witness a transition towards a more sustainable and green economy. This change is driven by changes in regulation, public opinion, and investor attitudes. For example, global assets managed under a sustainability label are on track to exceed $53 trillion by 2025, more than a third of total assets under management. However, unfortunately, the boom has been accompanied by rampant greenwashing, with companies boasting about their environmental credentials. 1 Because of this surge in environmental claims and to protect consumers, initiatives on substantiating green claims are developed. 2 Due to an ever-growing amount of text, there is a need for automated methods to detect environmental claims. Detecting such claims at scale can assist policy-makers, regulators, journalists, activists, the research community, and an informed public in analyzing and scrutinizing environmental claims made by companies and facilitating the transition to a green economy.\nEnvironmental claim: A total population of 6148 is getting the benefit of safe potable drinking water due to this initiative.\nEnvironmental claim: Hydro has also started working on several initiatives to reduce direct CO2 emission in primary aluminium production. Negative example: Generally, first of all, our Transmission department is very busy, both gas and electric transmission, I should say, meeting the needs of our on-network customers. Negative example: Teams are thus focused on a shared objective in terms of growth and value creation. Thus, we introduce the task of environmental claim detection. Environmental claim detection is a sentence-level classification task with the goal of predicting whether a sentence contains an environmental claim or not. Often, environmental claims are made in a clear and concise matter on a sentence level, with the intention to convey to a consumer or stakeholder that a company or product is environmentally friendly.\nTo facilitate future research on environmental claim detection, we release an expert-annotated dataset containing real-world environmental claims and models which can be used by practitioners. For constructing the dataset, we were inspired by the European Commission (EC), which defines such claims as follows: Environmental claims refer to the practice of suggesting or otherwise creating the impression (in the context of a commercial communication, marketing or advertising) that a product or a service is environmentally friendly (i.e., it has a positive impact on the environment) or is less damaging to the environment than competing goods or services. 3 While such claims can be truthful and made in good faith, boasting about environmental credentials can also be monetized (de Freitas Netto et al., 2020) . For example, consumers are willing to spend more money on environmentally friendly products (Nielsen Media Research, 2015) . The Commission states if environmental claims are too vague, unclear, or misleading, we are confronted with an instance of \"greenwashing\" (this definition is given in the same Commission Staff Working Document).\nWe situate environmental claim detection at the intersection of claim detection (e.g., Arslan et al., 2020) and pledge detection (Subramanian et al., 2019; Fornaciari et al., 2021) . An environmental claim is typically made to increase the environmental reputation of a firm or a product. We show that models trained on the current claim and pledge detection datasets perform poorly at detecting environmental claims, hence the need for this new dataset. We make our dataset, code and models publicly available. 4 Lastly, we envision computerassisted detection of greenwashing in future work, i.e., the automatic determination if an environmental claim is false, too vague, non-verifiable, or misleading. To make progress on automated greenwashing detection, it is mandatory to first detect environmental claims at scale.\n\nRelated Work\nThis work is part of an ongoing effort at the intersection of environmental and climate changerelated topics and natural language processing (Stede and Patz, 2021) . Resulting datasets and methods can help regulators, policy-makers, journalists, the research community, activists, and an informed public investigate such topics at scale with the help of computer assistance. Methods include ClimateBERT (Webersinke et al., 2021) , and ClimateGPT (Vaghefi et al., 2022) , two language models pre-trained on climate-related text. NLP tasks and datasets include climate change topic detection (Varini et al., 2020) and detecting media stance on global warming (Luo et al., 2020) . Duong et al. (2022) collect climate change opinions at scale from social platforms, Al-Rawi et al. (2021) analyze fake news Tweets around climate change. In a similar direction, Coan et al. (2021) analyze contrarian claims about climate change and (Piskorski et al., 2022) Further, there exists work about claim verification of climate change related claims (Diggelmann et al., 2020) , detecting media stance on global warming (Luo et al., 2020) , collecting climate change opinions at scale from social platforms (Duong et al., 2022) , and finally, the analysis of regulatory disclosures (Friederich et al., 2021; K\u00f6lbel et al., 2022) .\nIn this broader context of applying NLP methods for climate change-related topics, We situate environmental claim detection at the intersection of claim spotting and pledge detection, covering the domain of text produced by companies with the goal of boosting their environmental credentials. Claim spotting is the task of finding fact-check worthy claims (Arslan et al., 2020; Atanasova et al., 2018; Barron-Cedeno et al., 2020) . Pledge detection aims to detect pledges made in, for example, political campaigns (Subramanian et al., 2019; Fornaciari et al., 2021) . Environmental claims state an environmental benefit (claim) or convey the intention (pledge) for a material impact, i.e., some environmental benefit, which pleases the audience (consumers or stakeholders) of the claim.\n\nDataset\nOur dataset contains environmental claims made by listed companies. We collected text from sustainability reports, earning calls, and annual reports of listed companies and annotated 3'000 sentences. After discarding tied annotations, our resulting dataset contains 2'647 examples. 5 We provide dataset statistics in The authors drafted annotation guidelines in an iterative process and added examples of clear and borderline environmental claims to the guidelines.\nIn Appendix B, we list the complete guidelines available to the annotators, along with examples and rationales that the authors discussed in pilot annotation rounds.\nTo extract the sentences annotated in our dataset, we use a preliminary model to sample candidate sentences from various text sources produced by firms. Furthermore, we randomly sample sentences from different clusters obtained with k-means to increase the coverage of the domain. We describe the sampling process of the dataset in detail in Appendix A and provide further information on the data sources in Appendix C.\nWhile we do not release a large-scale dataset, this is the result of a conscious decision to prioritize quality over quantity. We employed domain experts to annotate the data, which results in costly annotations. In Appendix D, we show that the performance of models converges after being trained on more than 60% of the training set, and we find diminishing marginal utility of including more sentences. Hence our decision to stop annotation here and release an annotated dataset with 2'647 examples.\nWe assigned each sentence to four annotators. The annotations are aggregated by majority vote. 60% of the 3'000 samples was decided unanimously by the annotators, and 88.3% of the annotations made were part of a majority decision. 353 sentences received tied annotations (11.7% of the samples), and we discarded these examples from the dataset.The overall inter-annotator agreement measured in Krippendorff's alpha is 0.47, indicating moderate agreement.\n\nExperiments\nWe conduct two types of experiments: (1) We examine the performance of various models on our dataset, among them pre-trained claim and pledge detection models and fine-tuned environmental claim detection transformer models (such as, e.g. Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019; Webersinke et al., 2021) . ( 2) we apply our models to the text produced by listed companies, which leads to a small case study demonstrating one of the intended use cases of the dataset.\n\nEnvironmental Claim Detection Models\nWe report various metrics on a 5-fold crossvalidation split of the whole dataset, the development, and the test set in Table 2 . We present two poorly performing baselines: majority, where we assign the not-a-claim label to all examples, and random, where we randomly assign one of the two labels to each example. Next, we fine-tune a RoBERTa base model on the ClaimBuster dataset (Arslan et al., 2020) , and use this model to detect environmental claims in the dataset. 7 While achieving rather high recall, the model does not cope well with the domain shift and fails to detect environmental claims reliably. Similar findings hold for a RoBERTa base model trained on a Pledge Detection dataset (Subramanian et al., 2019) . 8 These results highlight the need for a dedicated dataset.\nFurthermore, we train two SVM models. The first one uses tf-idf bag-of-word features, the sec- ond is based on character n-gram features. Both models achieve an acceptable F1 score between 65% and 71% on all dataset splits. These results indicate that environment-related keywords or ngrams are somewhat predictive of whether a sentence is an environmental claim or not. However, all transformer models explored in this study outperform the SVM, hence the presence of environmental keywords alone is not sufficient for predicting such claims. Especially for recall, we find a large gap between transformer and SVM models of up to 25% points. We interpret this gap as evidence that not all environmental claims contain distinguishing environmental keywords.\nLastly, we fine-tune various transformer models (Liu et al., 2019; Sanh et al., 2019; Webersinke et al., 2021) . They all achieve an F1 score higher than 82% on all different dataset splits, a vast performance increase compared to the other models examined so far. We observe only minor differences between these models. The biggest model RoBERTa large achieves the best scores overall, followed by ClimateBERT, a DistilBert-like language model further pre-trained on over 1.6 million climate-related paragraphs. Hence, further pretraining on climate-related text seems beneficial to detect environmental claims.\nFor training our models, we use Hugging Face (Wolf et al., 2020) and standard RoBERTa hyperparameters. We use the Adam optimizer with a learning rate of 2e-5, a batch size of 16, and train models for 3 epochs. To minimize compute and environmental footprint of our experiments and due to consistent results over different dataset splits, we did not explore other hyper-parameters in more detail and reported only results of single runs.\n\nEarning Calls\nWe use our trained model to detect environmental claims in corporate earning calls between 2012 and 2020. These are conference calls between the management of a publicly traded company, analysts, investors, and the media to discuss the company's financial results and other topics for a given reporting period (mainly quarterly). The conference calls consist of different segments, of which the segment with questions and answers is the most interesting for our purposes. Therefore, we focus on the management responses, which consist of 12 million sentences from 3,361 unique companies. All earnings conference call transcripts are obtained from Refinitiv Company Events Coverage. Due to the size of the data and computational constraints, we use our ClimateBERT model, finetuned on detecting environmental claims instead of the RoBERTa large model.\nWe would expect that the amount of environmental claims made by corporations and business leaders has steadily increased since the Paris Agreement in 2015. In Figure 2 , we find that this is indeed the case. The amount of environmental claims is not only increasing, but the increase is also accelerating. In 2019, the share of environmental claims is twice as high as in 2015. Not only the amount of environmental claims made in earning calls is increasing, but also the share of companies who makes such claims increased by 33%, and in 2019, one in ten companies makes at least one environmental claim in the answer sections of an earning call.\nIn Figure 3 , we display word clouds for the most important words classified as non-claims (on the left), and the most important words for environmental claims (on the right). It is evident that the sentences classified as claims contain more environmental-related keywords; We see that these keywords cover different environmental aspects, e.g., recycling and waste, carbon and emissions, renewables, water, etc. In Appendix Table 6 , we additionally list the 5 highest and lowest scoring sentences based on our model. Our model effectively identifies environmental claims as the predominant category at the upper end of the distribution, whereas it appears that such claims are absent in the lower end of the distribution.\nThis small case study illustrates one of the intended use cases of our dataset and the associated models: We present a tool that allows us to detect environmental claims at scale. Having access to environmental claims at scale makes it possible to analyze and scrutinize them in future work.\n\nConclusion\nThe vast and ever-growing volume of corporate disclosures, regulatory filings, and statements in the news calls for an algorithmic approach to detect environmental claims made by companies at scale. Thus, we introduce the NLP task of detecting environmental claims, a dataset containing such claims and associated models which can detect these claims in the wild. Our dataset is annotated by domain experts and thus of high quality. We describe the dataset and its construction process and present various models for detecting environmental claims in our dataset and a small case study.\nWe envision several directions for future work. First, we plan to investigate \"greenwashing\", the practice of making a false, vague, unclear, or mis-leading environmental claim. To make progress on this front, it is mandatory that we can detect environmental claims in the first place. Second, models trained on detecting environmental claims have merits of their own, as previewed in our case study. We plan to explore more such applications in detail, e.g., analyzing annual reports and TCFD 9 reports at scale. For example, it would be interesting to see in which sections of TCFD reports firms make environmental claims. Lastly, we expect an increase of contributions at the intersection of environmental topics, climate change, and NLP in the near future. This work contributes to such efforts.\n", "hypothesis": " To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable.  To analyze such claims at scale, automated methods are needed to detect them in the first place.  However, there exist no datasets or models for this.  Thus, this paper introduces the task of environmental claim detection.  To accompany the task, we release an expert-annotated dataset and models trained on this dataset.  We preview one potential application of such models: We detect environmental claims made in quarterly earning calls and find that the number of environmental claims has steadily increased since the Paris Agreement in 2015..", "answer": true}
{"title": "An Exploratory Study on Model Compression for Text-to-SQL", "content": "\nIntroduction\nText-to-SQL is an important task that has been gaining the attention of researchers over the years. Formally, given a query q and a relational database D, the goal of Text-to-SQL is to build a model f such that s = f (q, D | \u03b8) where \u03b8 is a vector of model parameters and s is a predicted SQL statement which we can use to retrieve the answer to q from D.\nText-to-SQL has many potential applications that can improve our standard of living. For example, medical chatbots can convert user queries into SQL statements and then use them to retrieve relevant information from medical knowledge bases. Industry can leverage Text-to-SQL tools to help employees shorten the time needed to write complex SQL queries, thereby improving overall work productivity.\nThe recent emergence of complex Text-to-SQL datasets containing complicated SQL and crosstable setup has driven researchers to develop huge models that encode various complex relationships between table schema and query with large pretrained language models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) . These models are usually sequence-to-sequence models that generate SQL statements sequentially or sketch-based models that use classifiers to fill in the slots of SQL templates.\nHowever, despite achieving state-of-the-art performances on benchmark datasets, such models are usually both memory and computationally expensive, making it technically challenging to deploy them in memory-constrained real-world applications that require low inference latency. Therefore, to deploy state-of-the-art Text-to-SQL models in real-world production environments, we must drastically improve the inference time and reduce the number of parameters in these models.\nWe turn to the field of model compression (Cheng et al., 2017) for solutions that can speed up inference without significantly hurting model performance. Formally, the goal of model compression is to reduce f to a smaller model f \u2032 such that s \u2032 = f \u2032 (q, D | \u03b8 \u2032 ). Ideally, we want s \u2032 to be the same as s and dim(\u03b8 \u2032 ) to be much smaller than dim(\u03b8).\nIn this paper, we thoroughly examine the feasibility of using model compression techniques to build faster and more accurate Text-to-SQL models that we can successfully deploy in the real world. For this, we carefully apply a few model compression methods to representative sequence-to-sequence or sketch-based Text-to-SQL models on three datasets: WikiSQL, Spider, and TableQA. The main findings of this paper are: (i) sketch-based models generally respond well to model compression techniques, while sequence-to-sequence models show mixed results, (ii) we observe better speed improvements in Sketch-based models as their slot-filling components are much faster than the decoding components of sequence-to-sequence models. (iii) model compression techniques work poorly on state-of-the-art Text-to-SQL models built on pre-trained encoder-decoder language models such as T5.\nWe hope our findings can empower practitioners to make more informed decisions when selecting Text-to-SQL models and compressing them appropriately for real-world deployments. . Contrarily, Spider contains large samples of complex SQL instances that connect multiple tables with primary and foreign keys with more advanced clauses such as nested queries, JOIN ON, and ORDER/GROUP BY.\n\nBaseline Models\nRecent deep neural Text-to-SQL models can be broadly classified under two categories: sequenceto-sequence models and sketch-based (also known as slot-filling) models.\n\nSequence-to-sequence models\nSequence-to-sequence models are generally made up of an encoder component that converts user query inputs together with database information into a hidden vector and a decoder component that generates SQL statements based on the output hidden vectors from the encoder. BRIDGE (Lin et al., 2020) encodes input questions and table schema with BERT and LSTM and generates SQL predictions with a pointer-generator decoder (See et al., 2017) supported by a schemaconsistency driven search space pruning strategy. RAT-SQL (Wang et al., 2020a ) also encodes input instances with BERT but generates SQL as an abstract syntax tree (AST) with a tree-structured decoder (Yin and Neubig, 2017) . It also incorporates a relation-aware self-attention mechanism that further improves schema-linking, schema-encoding, and representation of the encoder. PICARD (Scholak et al., 2021) is a state-of-theart algorithm that directly fine-tunes a pre-trained encoder-decoder language model T5 (Raffel et al., 2020) on Text-to-SQL data, and then constrain the decoder to output valid SQL by integrating an incremental parsing strategy to the beam search process.\n\nSketch-based model\nSketch-based methods also encode user inputs into vectors but only need to fill in slots in SQL sketches rather than generating full SQL statements. Each SQL sketch is a template SQL statement with placeholder slots and the goal of sketch-based models is to predict the best item to go into each slot. NL2SQL-RULE (Guo and Gao, 2019 ) is a standard sketch-based model which uses BERT and LSTM to encode input query and database information and predict outputs in slots of SQL sketches.\n\nCompression Techniques\nWe follow Sun et al. (2021) and experiment with the following model compression techniques in this study: Layer Pruning (Sajjad et al., 2022 ) is a simple yet effective strategy that discards a certain number of layers from transformer-based language models before fine-tuning the pruned models on downstream tasks. We apply the top-layer pruning strategy which deletes the top N encoder or decoder layers before the start of any training. Knowledge Distillation (Hinton et al., 2015) is a method that compresses deep neural network models by distilling useful knowledge from a larger model (teacher) to a smaller model (student). We follow Jiao et al. (2020) and distill smaller language models from larger ones such as BERT-large, before fine-tuning Text-to-SQL models on those distilled models. For WikiSQL and Spider, we experiment with the distilled English language models from MiniLM 1 (Wang et al., 2020b) , while for TableQA, we use the Chinese TinyBERT models 2 . Token Pruning For PICARD model, We also apply token pruning (Goyal et al., 2020; Kim et al., 2022) , which is a different pruning strategy that gradually removes redundant token encodings from the outputs of each encoder layer before feeding the reduced number of tokens to the next encoder layer. We follow Goyal et al. (2020) and implement an attention scoring mechanisms which weights the significance of each token by the sum of attention weights it gets from other tokens. The tokens with the lowest significance scores (based on predetermined thresholds) for each encoder layer are dropped.\n\nEvaluation Metrics\nWe evaluate our experiment results using Exact set match (ESM) (Yu et al., 2018) . ESM decomposes every pair of predicted and gold SQL queries into sets clauses and then computes the percentage of exact set matches over all pairs (Zhong et al., 2020) .\n\nExperiment Setup\nIn most cases, we follow the recommended configurations in corresponding papers. We may adjust the batch sizes and learning rates slightly to fit the experiments on our hardware. We train our models on servers with either NVIDIA GV100 GPU (32GB) or RTX A6000 (45GB) but calculate inference speeds by running models on only CPUs with batch size set to one, which better mimics the situations in the real world. For all datasets, we use their dev sets as the test sets and create new train-dev sets in the ratio of 4 to 1 from the original train set. We early stop our models based on the ESM scores on dev sets and report average test set ESM scores over 5 different runs. Other than PI-CARD, we use BERT-large for all English datasets and RoBERTa-Zh (Cui et al., 2020) for TableQA.\n\nSimple datasets\nWikiSQL As shown in Figure 1 WikiSQL. For example, we can remove 50% of the encoder layers from BRIDGE, while only taking a penalty of only 0.82% drop in Exact Set match (ESM). When only keeping the bottom 6 encoder layers, NL2SQL-RULE can still perform at 0.834 ESM, a 3.65% drop from the original unpruned model. For knowledge distillation, we fine-tuned BRIDGE on two versions of MiniLM (Wang et al., 2020b) : L6xH768 and L6xH384. Results show that BRIDGE trained on the MiniLM language models performs slightly worse than the layer pruning method with similar number of layers. However, this is acceptable given the hidden sizes of the MiniLM models are 384 and 768, which are smaller than the hidden size of 1024 for BERT-large. TableQA We notice several differences in results between WikiSQL and TableQA. First, the performances of RATSQL on TableQA are significantly lower than those of NL2SQL-RULE. For example, unpruned NL2SQL-RULE achieves an ESM of 0.8 but unpruned RATSQL only achieves 0.69 despite our best efforts. Second, we observe more significant drops in performances when applying layer pruning and knowledge distillation to RATSQL than NL2SQL-RULE. For example, we observe only a 3.63% drop in ESM dropping the first 16 encoder layers of NL2SQL-RULE but notice an 18.8% drop in the performance of RATSQL with the same configurations. Last but not least, models trained on distilled language models perform slightly worse than the layer pruned models due to their smaller hidden sizes except for NL2SQL-RULE on TinyBERT with 6 layers and 768, which achieves an ESM of 0.80, even higher than that of the unpruned NL2SQL-RULE.\nRecommendation: We recommend using slotfilling models when building applications that only deal with simple queries. These models not only perform comparably or even better than sequenceto-sequence models, but also respond better to recent model compression techniques. Spider As PICARD was trained on a 3 billion parameters pre-trained language model with an encoder and a decoder of similar size, we show three sets of results by applying layer pruning on 1) the encoder, 2) the decoder, and 3) both the encoder and decoder. As seen in Figure 3 , the layer pruning strategy does not work as well on PICARD. At around six layers, PICARD loses around 49.9% and 40.3% of its original performance for encoder-only and decoder-only pruning settings respectively. For the encoder+decoder pruning strategy, we observe similar levels of performance when discarding the same number of transformer layers as the other two configurations. For example, dropping 3 layers each from the encoder and decoder gets us 0.641 ESM, compared to 0.624 when dropping 6 decoder layers and 0.648 when dropping 6 encoder layers. On the other hand, RATSQL demonstrates better compression results on Spider, maintaining 92.6% of original performance while keeping on six encoder layers, contrary to the results on TableQA.\n\nComplex dataset\nToken pruning We follow the implementation of Goyal et al. (2020) and apply token pruning to PI-CARD. We plot the ESM performance of a tokenpruned model against the number of retained tokens in Figure 4 . As seen in the plots, although we can remove an average of 286 tokens from the top six encoder layers, we are only able to discard an average of 41 tokens from the bottom six layers. For example, we see a sharp drop in ESM performance by just pruning around 40 tokens from the 3rd encoder layer. Similarly, we also observe steady drop in ESM performance when pruning more than 100 tokens from encoder layers 15 and 18. Our final model achieves an ESM of 0.527 (26.3% drop in performance) while only seeing a 5.2% improvement in inference speed when applying token pruning to the encoder of T5. As we cannot significantly prune the number of tokens in each encoder layer without severely hurting model performance, we conclude token pruning is also not effective on the PICARD model. Recommendation: Our results suggest that both layer and token pruning are not effective on PI-CARD and we would get better compression performances on sequence-to-sequence models like RATSQL, which has a much bigger encoder than decoder in terms of model size.\n\nDiscussion\nThe main difference between recent sequence-tosequence and sketch-based models is related to how we generate the SQL statements. Compared to the lightweight slot-filling classifiers in sketchbased models, recent sequence-to-sequence model decoders rely heavily on grammar-guided decoding processes which requires navigating through a huge search space and requires an even longer inference time than the encoders. For example, 76.62% and 87.14% of the inference time are spent in the decoding step for BRIDGE and RATSQL, while most of the inference time in NL2SQL-RULE is spent on the encoder. Considering the speed, compression effectiveness, and performance, sketch-based models would be better choices if we get similar performances on benchmark datasets.\n\nConclusion\nThis paper investigates whether we can use model compression to improve the inference efficiency of recent Text-to-SQL models that rely heavily on large pre-trained language models. Our results show that on simple Text-to-SQL datasets, we can deploy simple strategies such as layer pruning to obtain a 5-6x speedup without significantly hurting model performances. We also observe that sketchbased models generally respond better to model compression than sequence-to-sequence models. However, we are not able to effectively compress PICARD on the spider dataset and we would tackle this problem as a future work.\n", "hypothesis": " Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases.  Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in realworld applications that require real-time or on-device processing capabilities. In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-to-sequence Text-to-SQL models. We find that model compression techniques work well on state-of-the-art Text-to-SQL models built on pre-trained encoder-decoder language models such as T5, resulting in faster and more accurate models that can be deployed in real-world applications. We hope our findings can help practitioners make more informed decisions when selecting and compressing Text-to-SQL models for deployment.  Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements..", "answer": false}
{"title": "Better Language Models of Code through Self-Improvement", "content": "\nIntroduction\nPre-trained models for code (PLMCs), such as CodeBERT (Feng et al., 2020) , PLBART (Ahmad et al., 2021) , CodeT5 (Wang et al., 2021) , UniX-Coder (Guo et al., 2022) , and DISCO (Ding et al., 2022) , have become the foundation to solve many practical software engineering tasks such as code summarization, code translation, program repair. Those PLMCs, like large language models (LLMs), are typically first pretrained on very large-scale datasets with a variety of multi-modal objectives under a self-supervised training style. They can then be fine-tuned using task-specific datasets in a supervised training style.\nWe hypothesise that, while fine-tuned models may not achieve peak performance, PLMCs can produce reasonable outputs that can be regarded as high quality data because they have been pretrained on large scale datasets, and that such data can be leveraged as additional high-quality training data. Our framework utilizes the self-improvement capability of PLMCs through an simple data augmentation step. This approach is particularly useful for tasks involving code-related sequence generation, such as code summarization and code generation. Our method involves fine-tuning a PLMC on a downstream dataset, allowing the model to gain knowledge about the task. The model then generates an augmented version of the original training data, which are used to further fine-tuning. Our framework is similar to sequence-level knowledge distillation (Kim and Rush, 2016) , but our approach focuses on improving model performance without compressing the model by utilizing the same technique.\nOur empirical evaluation results show that our framework significantly improves the state-of-thearts PLMCs, including CodeBERT, CodeT5, UniX-Coder with significant margins. In short, we summarize our contributions as follows.\n\u2022 We present a simple self-improvement framework and show how it can be easily adapted to PLMCs for the task of code-related sequence generation.\n\u2022 We conduct extensive evaluation on two tasks: code summarization and code generation, and compare it with the well-known, state-of-the-art PLMCs. The results show that our framework consistently improvesover all PLMCs by a significant margin in those tasks.\n\u2022 We provide analysis and explanations on how utilizing a simple framework consistently improves the performance of PLMCs.\nOur work is publicly available. \n\nRelated Work\nExposure bias and hallucination in Sequence Generation Tasks The exposure bias problem is regarded as the difference between the training and inference phases for auto-regressive sequence generation models. Previous work has attempted to reduce exposure bias in training phase (Bengio et al., 2015; Ranzato et al., 2015; Wiseman and Rush, 2016; Wang and Sennrich, 2020) . In the sense that our self-improvement step involves training model on its own prediction, the exposure bias is close to our approach.\nCode understanding and generation Code learning problems have recently emerged as one of the primary tasks for assessing the capability of language models. Most recent code models are pretrained on multi-modal objectives before being fine-tuned on specific downstream tasks (Feng et al., 2020; Ahmad et al., 2021; Wang et al., 2021; Guo et al., 2022; Ding et al., 2022) .\nKnowledge Distillation Knowledge distillation is the process of transferring knowledge from a large unwieldy model or set of models to a single smaller model that can be practically deployed under real-world constraints, and such smaller model can usually keep the same performance or even better than the original model (Hinton et al., 2015; Kim and Rush, 2016; Wang et al., 2020; Chen et al., 2020; Mukherjee et al., 2021) . We perform an additional self-improvement step to improve the original model without using external resources, our work is relevant to knowledge distillation.\n\nMethod\nAlgorithm 1 Data Augmentation Process Input:\n\u2022 \u03b8 f ine\u2212tuned , the fine-tuned model checkpoint on a specific task T \u2208 {code summarization, code generation, etc. }. \n\u2022 D = {(x i , y i ) | i = 1,\nD \u2190 \u2205 3: for each datapoint (x i , y i ) \u2208 D do: 4: L K \u2190 B k (P \u03b8 f ine\u2212tuned (y | x i )) 5:\nIn other words,\nL K = [\u0177 i1 , \u0177i2 , ..., \u0177ik ] 6: \u1ef9i \u2190 argmax \u0177ij \u2208L K (sim(\u0177 ij , y i )) 7: Adding (x i , \u1ef9i ) \u2192 D 8:\nend for 9:\nreturn D 10: end procedure Our method utilizes three distinct sets of model parameters: \u03b8 pre\u2212trained , \u03b8 f ine\u2212tuned , and \u03b8 improved . Each corresponds to the stage of the model parameters after pre-trained, fine-tuned, and self-improved, respectively. The model generates tokens in autoregressive manner, progressing token-by-token.\nTypically, models are pretrained on large scale corpora, resulting in a pre-trained checkpoint \u03b8 pre\u2212trained . These pre-trained models are subsequently fine-tuned on a targeted downstream dataset D using a supervised learning approach, yielding in a set of fine-tuned parameters \u03b8 f ine\u2212tuned . Our investigation has revealed that model's performance can be further enhanced if we continue to fine-tuned these parameters on an augmented version of D. Figure 1 augmentation technique and an additional round of fine-tuning, in addition to the pre-traingn and fine-tuning paradigm.\nThe process of augmenting the dataset is illustrated in Figure 2 . We provide a detailed algorithm for this procedure in Algorithm 1. For each training pair of sequences (x i , y i ) in the train dataset D, we employ beam search to generate a list of K-best predictions L K . This list comprises k predictions, where k represents the beam size. Subsequently, we evaluate the similarity between each prediction \u0177ij and its corresponding ground truth sequence y i using a similarity function sim based on BLEU score. The prediction with highest similarity is then selected \u1ef9i = argmax \u0177ij \u2208L K (sim(\u0177 ij , y i )). Finally, we add the sequence pair (x i , \u1ef9i ) to a new empty dataset D, which we refer as the augmented dataset. Essentially, the augmented dataset contains an equal number of datapoints as the original training dataset due to an one-by-one mapping during augmetation. Moreover, the augnmentation process occurs offline, with each newly augmented datapoint being saved to storage before being used for training in the self-improving phase.\nThe subsequent step involves fine-tuning \u03b8 f ine\u2212tuned on D until convergence. This results in a new set of model parameters denoted as \u03b8 improved . It is important to note that the index j in \u0177ij denotes the j th prediction in the beam, rather than the j th token of the predicted sequence. Additionally, only the training dataset D is augmented, while the validation and test dataset remain unchanged for evaluation purpose.\n\nExperimental Setup\nOur goal is to show that for any of the fine-tuned model for a sequence generation task (F-PLMC), after applying our self-improvement method (S-PLMC), the result improves.\nDataset and Downstream Tasks To achieve our goal of enhancing the code-related sequence generation task, we selected code summarization and code generation as our experimental areas. To evaluate these tasks, we utilized the CodeXGLUE benchmark (Lu et al., 2021) , which comprises various datasets for various code understanding and code generation tasks. Specifically, we utilized the code summarization and code generation datasets from CodeXGLUE and disregarded the other ones.\nThe statistics for each dataset is reported in Appendix.\nBaseline Models We chose CodeBERT (Feng et al., 2020) , CodeT5 (Wang et al., 2021) , and UniXCoder (Guo et al., 2022) \n\nEvaluation Results\nThe results of our code summarization task are presented in Table 1 . The \"Beam sizes\" column indicates the beam size used in the beam search algorithm, while the \"Methods\" column indicates whether or not our self-improved algorithm was utilized. We also included other models as references to compare the relative improvement of our model. On average, we observed an average of 0.76 BLUE score increase in performance across all languages. This improvement was consistent across various beam sizes (1, 5, 10), which confirms the effectiveness of our self-improved approach across a wide range of PLMCs. When comparing our model to other strong baselines, we found that our method improved the performance of CodeBERT for JavaScript from 15.78 to 16.39, surpassing the performance of PolyglotCodeBERT (15.80) . This highlights the benefit of our self-improved method in improving weak models. The results of our code generation study are presented in Table 2 , the performance increase by 0.81 BLUE scores on average. When using EM and CodeBLEU, the improvement also increases consistently.\nWhile conducting our experiments, it is important to note that we did not selectively choose the most favorable random seed to optimize the performance of each entry. Instead, we utilized the default seed provided in each model repository to ensure fairness and consistency. Our code summarization experiments encompassed six different programming languages, and both code summarization and generation experiments were evaluated using three distinct beam sizes. In total, we conducted 60 runs to gather comprehensive results. The numbers reported consistently demonstrate the improvement achieved in each individual run, thereby affirming the robustness of the proposed method. \n\nAblation Study\nImprovement Study In this section, we examine the factors that influence the improvement achieved by \u03b8 improved as compared to \u03b8 f ine\u2212tuned through code summarization. We define r 1 as the difference in performance measured by BLEU between inferencing with a beam size of 10 and inferencing with a beam size of 1. Additionally, we define r 2 as the improvement in BLEU when inferencing with the same beam size of 1 between \u03b8 f ine\u2212tuned and \u03b8 improved . By evaluating these values across a variety of beam sizes and programming languages in the code summarization dataset, we are able to visualize the results in Figure 3 . Additionally, we have calculated the Pearson Correlation score, which is 0.77, indicating a strong correlation between r 1 and r 2 . Our analysis demonstrates that a larger r 1 is correlated with a better r 2 , suggesting that our method is more likely to yield better overall performance when r 1 is large. We believe this insight is a crucial finding as it provides a simple indicator of the model's fully trained capability.\nCodeBLEU as Similarity Function Our primary findings in code generation are presented using BLEU as the similarity function. However, for a more comprehensive assessment of the correctness of the generated code, we consider Code-BLEU, which incorporates the specific characteristics of source code. CodeBLEU, therefore, aligns better with the objective of measuring similarity in data augmentation compared to BLEU, which relies on n-gram matching. This section examines the impact of using CodeBLEU as a similarity We present the results of our UniXCoder selfimproving model with both BLEU-augmentation and CodeBLEU-augmentation in Table 3 . The results indicate that CodeBLEU-augmentation enhances both BLEU and CodeBLEU scores compared to BLEU-augmentation. This suggests that using CodeBLEU as a similarity function improves the generated code at a local level, encompassing aspects such as fluency, semantics, and syntax. However, it does have a negative impact on exact match (EM). As code problems may not have a unique solution, when EM is used as an evaluation metric, it should allow for a more lenient assessment. Consequently, we argue that a slight decrease in EM would have minimal impact on the actual correctness of the generated solution. Thus, we propose placing greater emphasis on CodeBLEU as an evaluation metric for code generation.\n\nConclusion\nWe introduced a self-improvement technique as a final fine-tuning step to enhance model performance. Our experiments showed that this method, when applied to popular pre-trained code models (Code-BERT, CodeT5, and UniXCoder), significantly improves performance on code summarization and code generation tasks. We also provided insights on when this method is most effective in improving PLMCs. We intend to implement our technique in larger-scale models and other tasks, and believe it is an efficient way to optimize the capabilities of any code language model without the need for extensive architecture modifications or large-scale dataset assembly. We leave all of these investigations for the future.\n", "hypothesis": " Pre-trained language models for code (PLMCs) have gained attention in recent research.  These models are pre-trained on large-scale datasets using multi-modal objectives.  However, finetuning them requires extensive supervision and is limited by the size of the dataset provided.  We aim to improve this issue by proposing a data augmentation framework using knowledge distillation.  Our framework utilizes knowledge gained during the pre-training and finetuning stage to augment training data, which is then used for the next step.  We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder.  The results show that our framework significantly improves PLMCs' performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.  * Equal contribution.  Listing order is based on the alphabetical ordering of author surnames..", "answer": true}
{"title": "Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints", "content": "\nIntroduction\nStandard neural seq2seq models are versatile and broadly applicable due to its approach of factoring the output distribution into distributions over the next words based on previously generated words and the input (Sutskever et al., 2014; Gehring et al., 2017; Devlin et al., 2019) . Despite showing promise in approximating complex output distributions, these models often fail when it comes to diagnostic tasks involving compositional generalization (Lake and Baroni, 2018; Bahdanau et al., 2019; Loula et al., 2018) , possibly attributed to a lack of inductive biases for the hierarchical structures of sequences (e.g., syntactic structures), leading to models overfitting to surface clues.\nIn contrast to neural seq2seq models, traditional grammar-based models incorporate strong inductive biases to hierarchical structures but suffer from low coverage and the hardness of scaling up (Wong and Mooney, 2006; Bos, 2008) . To benefit from both of these approaches, blending traditional methods and neural networks has been studied (Herzig and Berant, 2021; Shaw et al., 2021; Wang et al., 2021 Wang et al., , 2022)) . In particular, Kim (2021) proposes\nIn this work, we first study low-rank variants of Neural QCFG for faster inference and lower memory footprint based on tensor rank decomposition (Rabanser et al., 2017) , which is inspired by recent work on low-rank structured models (Cohen et al., 2013; Chiu et al., 2021; Yang et al., 2021 Yang et al., , 2022)) . These variants allow us to use more symbols in Neural QCFG, which has been shown to be beneficial for structured latent variable models (Buhai et al., 2020; Chiu and Rush, 2020; Yang et al., 2021 Yang et al., , 2022)) . Specifically, we study two low-rank variants with different trade-off between computation cost and ranges of allowed constraints: the efficient model (E model), following the decomposition method in TN-PCFG (Yang et al., 2021) , and the expressive model (P model), newly introduced in this paper. Furthermore, we propose two new constraints for Neural QCFG, including a soft version of the tree hierarchy constraint used by vanilla Neural QCFG, and a coverage constraint which biases models in favour of translating all source tree nodes 1 . We conduct experiments on three datasets and our models outperform vanilla Neural QCFG in most settings. Our code is available at https://github.com/LouChao98/seq2seq_with_qcfg.\n\nPreliminary: Neural QCFG\nLet s 1 , s 2 be the source and target sequences, and t 1 , t 2 be the corresponding constituency parse trees (i.e., sets of labeled spans). Following previous work (Smith and Eisner, 2006; Kim, 2021) , we consider QCFG in Chomsky normal form (CNF; Chomsky, 1959) with restricted alignments, which can be denoted as a tuple G[t 1 ] = (S, N , P, \u03a3, R[t 1 ], \u03b8), where S is the start symbol, N /P/\u03a3 are the sets of nonterminals/preterminals/terminals respectively, R[t 1 ] is the set of grammar rules in three forms:\nS \u2192 A[\u03b1 i ] where A \u2208 N , \u03b1 i \u2208 t 1 , A[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ] where A \u2208 N , B, C \u2208 N \u222a P, \u03b1 i , \u03b1 j , \u03b1 k \u2208 t 1 , D[\u03b1 i ] \u2192 w where A \u2208 P, \u03b1 i \u2208 t 1 , w \u2208 \u03a3, and \u03b8 parameterizes rule probablities p \u03b8 (r) for each r \u2208 R[t 1 ].\nRecently, Kim (2021) proposes Neural QCFG for seq2seq learning. He uses a source-side parser to model p(t 1 |s 1 ) and a QCFG to model p(t 2 |t 1 ). The log marginal likelihood of the target sequence s 2 is defined as follows:\nlog p \u03b8,\u03d5 (s 2 |s 1 ) = log t 1 \u2208T (s 1 ) p \u03b8 (s 2 |t 1 )p \u03d5 (t 1 |s 1 ) = log t 1 \u2208T (s 1 ) t 2 \u2208T (s 2 ) p \u03b8 (t 2 |t 1 )p \u03d5 (t 1 |s 1 ),\nwhere T (\u2022) denotes the set of possible parse trees for a sequence and \u03b8, \u03d5 are parameters. Due to the difficulty of marginalizing out t 1 and t 2 simultaneously, Kim (2021) resorts to maximizing the lower bound on the log marginal likelihood, \nlog p \u03b8,\u03d5 (s 2 |s 1 ) \u2265 E t 1 \u223cp \u03d5 (t 1 |s 1 ) [log p \u03b8 (s 2 |t 1 )] .\n\u03b1 i R \u03b1 j B C \u03b1 k (a) E model A \u03b1 i R \u03b1 j B C \u03b1 k (b) P model\nFigure 1 : Extended factor graph notation of decomposed binary rules (Frey, 2002) . Each square represents a factor. Arrows indicate conditional probabilities.\n\nEfficient Model (E Model)\nLet R be a new set of symbols. The E model decomposes binary rules r b into three parts:\nA[\u03b1 i ] \u2192 R, R \u2192 B[\u03b1 j ] and R \u2192 C[\u03b1 k ] (Fig. 1a), where R \u2208 R such that p(A[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ]) = R p(A[\u03b1 i ] \u2192 R) \u00d7 p(R \u2192 B[\u03b1 j ]) \u00d7 p(R \u2192 C[\u03b1 k ]).\nIn However, constraints that simultaneously involve \u03b1 i , \u03b1 j , \u03b1 k (such as the tree hierarchy constraint in vanilla Neural QCFG and those to be discussed in Sec. 4.1) can no longer be imposed because of two reasons. First, the three nodes are in separate rules and enforcing such constraints would break the separation and consequently undo the reduction of time complexity. Second, the rank-space dynamic programming algorithm prevents us from getting the posterior distribution p(\u03b1 i , \u03b1 j , \u03b1 k |t 1 , s 2 ), which is necessary for many methods of learning with constraints (e.g., Chang et al., 2008; Mann and McCallum, 2007; Ganchev et al., 2010) to work.\n\nExpressive Model (P Model)\nIn the P model, we reserve the relation among \u03b1 i , \u03b1 j , \u03b1 k and avoid their separation,\np(A[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ]) = R p(A[\u03b1 i ] \u2192 R) \u00d7 p(R, \u03b1 i \u2192 \u03b1 j , \u03b1 k )\u00d7 p(R, \u03b1 j \u2192 B) \u00d7 p(R, \u03b1 k \u2192 C),\nas illustrated in Fig. 1b . The P model is still faster than vanilla Neural QCFG because there are only G P := |R|S 3 + (3|N | + 2|P|)|R|S decomposed rules, which is lower than vanilla Neural QCFG but higher than the E model. However, unlike the E model, the P model cannot benefit from rank-space dynamic programming 4 and has a complexity of\nO(|R|S 2 T 3 +((2|N |+|P|)|R|S +|R|S 3 )T 2 ) for marginalizing t 2 5 . Rule R, \u03b1 i \u2192 \u03b1 j , \u03b1 k is an interface for design- ing constraints involving \u03b1 i , \u03b1 j , \u03b1 k . For example, by setting p(R, \u03b1 1 \u2192 \u03b1 2 , \u03b1 3 ) = 0 for all R \u2208 R and certain \u03b1 i , \u03b1 j , \u03b1 k , we can prohibit the gener- ation A[\u03b1 1 ] \u2192 B[\u03b1 2 ]C[\u03b1 3 ] in the original QCFG.\nWith this interface, the P model can impose all constraints used by vanilla Neural QCFG as well as more advanced constraints introduced next section.\n\nSoft Tree Hierarchy Constraint\nDenote the distance between two tree nodes 6 as d(\u03b1 i , \u03b1 j ) and define d(\u03b1 i , \u03b1 j ) = \u221e if \u03b1 j is not a descendant of \u03b1 i . Then, the distance of a binary rule is defined as\nd(r) = max(d(\u03b1 i , \u03b1 j ), d(\u03b1 i , \u03b1 k )).\nNeural QCFG is equipped with two hard hierarchy constraints. For A\n[\u03b1 i ] \u2192 B[\u03b1 j ]C[\u03b1 k ],\n\u03b1 j , \u03b1 k are forced to be either descendants of \u03b1 i (i.e., d(r) < \u221e), or more strictly, distinct direct children of \u03b1 i (i.e., d(r) = 1). However, we believe the former constraint is too loose and the latter one is too tight. Instead, we propose a soft constraint based on distances: rules with smaller d(r) are considered more plausible. Specifically, d(r) . We optimize the expected rewards with a maximum entropy regularizer (Williams and Peng, 1991; Mnih et al., 2016) , formulated as follows:\nlog t 2 \u2208T (s 2 ) p \u03b8 (t 2 |t 1 )\u03b6(t 2 ) + \u03c4 H (p \u03b8 (t 2 |t 1 , s 2 )) , where \u03b6(t 2 ) = r\u2208t 2 \u03b6(d(r)) 7 , p \u03b8 (t 2 |t 1 , s 2 ) = p \u03b8 (t 2 |t 1 )/ t\u2208T (s 2 ) p \u03b8 (t|t 1\n), H represents entropy, and \u03c4 is a positive scalar.\n\nCoverage Constraint\nOur experiments on vanilla neural QCFG show that inferred alignments could be heavily imbalanced: some source tree nodes are aligned with multiple target tree nodes, while others are never aligned. This motivates us to limit the number of alignments per source tree node with an upper bound 8 , u. Because the total number of alignments is fixed to |t 2 |, this would distribute alignments from popular source tree nodes to unpopular ones, leading to more balanced source coverage of alignments. We impose this constraint via optimizing the posterior regularization likelihood (Ganchev et al., 2010) ,\nE t1 (log p \u03b8 (s 2 |t 1 ) + \u03b3 min q\u2208Q KL(q(t 2 )||p \u03b8 (t 2 |t 1 , s 2 ))) ,\nwhere KL is the Kullback-Leibler divergence (KL), \u03b3 is a positive scalar and Q is the constraint set {q(t 2 )|E q(t) \u03d5(t) \u2264 \u03be}, i.e., expectation of feature vector \u03d5 over any distribution in Q is bounded by constant vector \u03be. We define the target tree feature vector \u03d5(t 2 ) \u2208 N |t 1 | such that \u03d5 i (t 2 ) represents the count of source tree node \u03b1 i being aligned by nodes in t 2 and \u03be = u1. Ganchev et al. (2010) provide an efficient algorithm for finding the optimum q, which we briefly review in Appx. C. After finding q, the KL term of two tree distributions, q and p \u03b8 , can be efficiently computed using the Torch-Struct library (Rush, 2020) \n\nExperiments\nWe conduct experiments on the three datasets used in Kim (2021) . Details can be found in Appx. D.1.\n\nSCAN\nWe first evaluate our models on four splits of the SCAN dataset (Lake and Baroni, 2018) . We report accuracy in Tab. 1. The P model equipped with constraints can achieve almost perfect performance similar to vanilla Neural QCFG, while the E model fails due to a lack of constraints.\n\nStyle Transfer and En-Fr Translation\nNext, we evaluate the models on the three hard transfer tasks from the StylePTB dataset (Lyu et al., 2021) and a small-scale En-Fr machine translation dataset (Lake and Baroni, 2018) . Tab. 2 shows results of the models with different constraints 9 . Low-rank models generally achieve comparable or better performance and consume much less mem-9 Following Kim (2021), we calculate the metrics for tasks from the StylePTB dataset using the nlg-eval library (Sharma et al. (2017) ; https://github.com/ Maluuba/nlg-eval) and calculate BLEU for En-Fr MT using the multi-bleu script (Koehn et al. (2007) ; https: //github.com/moses-smt/mosesdecoder). 2 : BLEU-4 for tasks from the StylePTB dataset (the top three series) and BLEU for Fr-En machine translation against different models and constraints. vNQ 2 is our reimplementation of Kim (2021) . nil means that no constraint is placed. H 1 and H 2 is the hard constraint d(r) < \u221e and d(r) = 1, respectively. S is the soft tree hierarchy constraint. C is the coverage constraint. \u00d7 means that the constraint is inapplicable and \u2212 means we do not run the experiment or Kim (2021) does not report the score. ory 10 . We can also find that the soft tree hierarchy constraint outperforms hard constraints and is very helpful when it comes to extremely small data (i.e., AEM and VEM). The coverage constraint also improves performance in most cases.\n\nAnalysis\nWe study how the number of nonterminals affects performance. On our computer 11 , we can use at most 18/64/128 nonterminals in vanilla Neural QCFG/the P model/the E model, showing that our low-rank models are more memory-friendly than vanilla Neural QCFG. We report results in Fig. 2 . There is an overall trend of improved performance with more nonterminals (with some notable exceptions). When the numbers of nonterminals are the same, the P model outperforms vanilla Neural QCFG consistently, showing its superior parameter efficiency. In contrast, the E model is defeated by vanilla QCFG and the P model in many cases, showing the potential harm of separating \u03b1 i , \u03b1 j , \u03b1 k .\n\nSpeed Comparison\nWe benchmark speed and memory usage using synthetic datasets with different sequence lengths. Fig. 3 and 4 illustrate the results. Compared to the standard Neural QCFG, the E model and P model are significantly faster and have a lower memory footprint. This enables them to model longer sequences effectively. For data construction and more results, please refer to Appx. D.3.\n\nConclusion\nWe have presented two low-rank variants of Neural QCFG based on decomposition for efficiency and two new constraints over tree hierarchy and source coverage. Experiments on three datasets validate the effectiveness and efficiency of our proposed models and constraints.\n", "hypothesis": " Neural QCFG is a grammar-based sequence-tosequence (seq2seq) model with strong inductive biases on hierarchical structures.  It excels in interpretability and generalization but suffers from expensive inference. In this paper, we study two low-rank variants of Neural QCFG for slower inference with different trade-offs between efficiency and expressiveness.  Furthermore, utilizing the symbolic interface provided by the grammar, we introduce two soft constraints over tree hierarchy and source coverage.  We experiment with various datasets and find that our models outperform vanilla Neural QCFG in most settings.  The symbolic nature of Neural QCFG makes it interpretable and easy to impose constraints for stronger inductive bias, which leads to improvements in empirical experiments.  However, all these advantages come at the cost of high time complexity and memory requirement, meaning that the model and data size is restricted, which leads to a decrease in text generation performance and limited application scenarios..", "answer": false}
{"title": "Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph", "content": "\nIntroduction\nEvent causality identification (ECI) aims to identify the causal relationship between pairs of events in the text. As shown in Figure 1(a) , giving text and events as inputs, the ECI model needs to identify three causal relationships < e7, cause, e5 >, < e5, cause, e2 >, and < e7, cause, e2 >. As a semantic relationship, causality is important for semantic understanding and discourse analysis. Moreover, causal knowledge identified from a text can be useful for many natural language processing tasks (Fei et al., 2020a,b; Wang et al., 2020; Zhou et al., 2021; Dalal et al., 2021) .\nVarious approaches have been proposed for ECI, from the early feature-based methods (Mirza, 2014;  *Corresponding author.\nMeanwhile, ISPs who use ( e 1 ) bandwidth on the SEACOM cable have been scrambling(e2) to implement(e3) contingency(e4) plans(e5) to keep(e6) their customers connected(e7). Figure 1 : (a) An example of leveraging the event causal label information and the interaction information between event pairs for ECI. (b) The event pairs composed of seven events in (a), where the dark area denotes the event pairs composed of causality-related events. Caselli and Vossen, 2017; Gao et al., 2019) to the current neural-network-based methods (Liu et al., 2020; Zuo et al., 2020 Zuo et al., , 2021a,b;,b; Cao et al., 2021; Phu and Nguyen, 2021) . The existing methods have achieved impressive performance. However, as far as we know, they usually improve the performance of ECI models by introducing commonsense knowledge or generating additional training data through data augmentation, with less focus on the inherent characteristics of the data. Specifically, 1) Minimal use of event causal label information. In the ECI task, a sentence usually contains multiple events. However, not all events are involved in causality.\nFor instance, the sentence shown in Figure 1 (a) has seven events, among which e2, e5, and e7 are causality-related events, whereas e1, e3, e4, and e6 are causality-unrelated events. In general, there is a strong semantic correlation between events that have a causal relationship. If the model can first identify e2, e5, and e7 as causality-related events according to the dependencies between events and contextual semantics, it can limit the identification scope of causality to some extent, as shown in Figure 1(b) , thereby reducing the interference of unrelated events. Furthermore, by performing statistical analysis on two benchmark datasets, EventStory-Line and Causal-TimeBank, we find that 55.13% and 38.63% of the sentences with events in the two datassets contained four or more events, where the average number of the different candidate event pairs is 13.12 and 10.64, and the average number of the event pairs with causal relationship is 1.99 and 0.30, respectively. If we can first predict which events are causality-related events according to the dependencies between them, the average number of candidate event pairs composed of causality-related events is 3.68 and 0.37, respectively, thereby narrowing the range of causality from 13.12 and 10.64 to 3.68 and 0.37, respectively. Therefore, it is necessary to introduce the event causal label information to assist the ECI tasks. 2) Little focus on the interaction between event pairs. The existing method (Phu and Nguyen, 2021) uses events as nodes and leverages toolkits to obtain information (such as dependency parsing tree and coreference) to construct event graph. However, they are modeled with events as nodes, cannot directly learn the interaction between event pairs. As shown in Figure 1 (a), e2 and e7 are far from each other in the text. Thus, directly identifying the causal relationship between them is difficult for the model. If < e7, cause, e5 > and < e5, cause, e2 > are known, it is easier to infer the causal relationship between e2 and e7 according to the transitivity of the causal relationship (Hall, 2000) . Therefore, it is essential to introduce the interaction information between event pairs to learn their dependencies. To address the above limitations, we propose a framework called ECLEP, which introduces Event Causal Label information and Event Pair interaction information to enrich the representation of event pairs. In particular, 1) we design an eventcausal-label-aware module to model the event causal label information, in which we introduce the event causal label prediction task as an auxiliary task of ECI to mine the dependencies between events. 2) We further design an event pair interaction graph module to model the interaction information between event pairs. In particular, we construct the interaction graph with event pairs as nodes and adopt graph attention mechanism to model the degree of dependency between event pairs. The experimental results on two benchmark datasets show that our overall framework outperforms previous state-of-the-art methods.\n\nMethodology\nIn ECI, the goal is to predict whether a causal relationship exists in each event pair (e i , e j )(i \u0338 = j) by giving text S = (w 1 , w 2 , ..., w n ) and events set E = (e 1 , e 2 , ..., e m ) contained in S as inputs, where w i denotes the i-th word in S, and e i denotes the i-th event in E. The overall framework of the proposed method is shown in Figure 2 , which mainly includes four modules, we illustrate each component in detail in the following.\n\nEncoding Layer\nGiven an input sentence S, we adopt a pre-trained language model BERT (Devlin et al., 2019) as the sentence encoder to extract hidden contextual representation H S = (h w 1 , h w 2 , ..., h wn ), where h w i denotes the hidden representation of the ith token in S. Then, the events representation H E = (h e 1 , h e 2 , ..., h em ) can be obtained by summing the token representation contained in the event.\n\nEvent-Causal-Label-Aware Module\nA sentence usually contains multiple events, however, not all events are causality-related events. We design an event-causal-label-aware module to mine the dependencies between events and help the model to pay attention to the extraction of the causal relationship from the event pairs composed of causality-related events. In general, there is a strong semantic correlation between events with causal relationships, so we first adopt the Transformer mechanism (Vaswani et al., 2017) to capture the dependencies between events, and get the updated event representation via Eq. ( 1).\nH L E = Transformer(H E ) = (h L e 1 , h L e 2 , ..., h L em ).\n(1) Then, we construct a binary classification to predict the probability p L e i of each event e i in E as a causality-related event via Eq. (2).\nEQUATION\nwhere W L e i and b L e i are learnable parameters. To incorporate the event causal label into the event pair, we introduce a learable label vector set L = {l ij }(l ij \u2208 {cr, cu}), where cr denotes the event pairs composed of causality-related events, cu denotes the event pairs that have at least one causality-unrelated event. In particular, the embedding vector L is randomly initialized via sampling from a uniform distribution and is learned together with the model training process. For the event pair (e i , e j ), if e i and e j are both causality-related events (i.e. p L e i \u2265 0.5 and p L e j \u2265 0.5), the label of (e i , e j ) is l ij = cr, else l ij = cu. \n\nEvent Pair Interaction Graph Module\nTo capture the interaction information between event pairs, we construct the interaction graph with event pairs as nodes and adopt the graph attention mechanism to adaptively fuse the information of neighbor nodes.\n\u2022 Event Pair Interaction Graph Construction. Given the event set E, the events are combined in pairs to form the set of candidate causal event pairs EP = {ep ij }(0 < i, j \u2264 |E|, i \u0338 = j) as nodes. In EP , the\nep ij = [h e i ; h e j ; r ij ; l ij ],\nwhere h e i and h e j \u2208 H E and r ij denotes the relative position embedding between event e i and event e j in the text.\nFor the edges of the interaction graph, consider that event pairs in the same row or column are strongly associated with the current event pair (Ding et al., 2020) . As shown in Figure 1(b) , the relationship of event pair < e7, e2 > can be transmitted through the same row event pair < e7, e5 > and the same column event pair < e5, e2 >. Thus, we connect the edges between the event pairs in the same row or column and add self-loop edges to fuse the information of their nodes.\n\u2022 Event Pair Interaction Graph Update. Considering that different neighbor nodes have different importance for each event pair, we leverage Graph Attention Networks (GAT) (Veli\u010dkovi\u0107 et al., 2018) to model the degree of dependency between event pairs. GAT propagates information among nodes by stacking multiple graph attention layers. At the t-th graph attention layer, the representation of each node can be updated via Eq. ( 3).\nEQUATION\nwhere N (ij) denotes the directly neighboring nodes of ep ij ; the attention weight \u03b1 t ij,uv is learned via Eq. ( 4), which reflects the strength of aggregation level between the nodes ep t\u22121 ij and ep t\u22121 uv ; W t , W t ij , W t uv , w t , and b t are learnable parameters.\nEQUATION\nTherefore, the updated nodes representation EP I = {ep I ij } can be obtained by stacking T layers to model the inter-node relationships.\n\nPrediction and Training\nThe predicted probability p ij of the event pair (e i , e j ) as a causal event pair can be obtained by performing a binary classification with ep I ij as input via Eq. ( 5).\nEQUATION\nwhere W I ij and b I ij are learnable parameters. For training, we utilize the cross-entropy function to supervise the causal event pair prediction via Eq. ( 6).\nEQUATION\nwhere D denotes the training set; s denotes the sentence in D; E s denotes the events set in s; y ij denotes the ground truth label of event pair (e i , e j ).\nWe also add an auxiliary supervision for the event causal label prediction task via Eq. ( 7).\nEQUATION\nwhere p L e i denotes the predicted probability of event e i as a causality-related event; y e i denotes the ground truth label of event e i , which can be automatically obtained according to the labels of event pairs without any manual labeling.\nThe final loss function is a weighted sum of the aforementioned terms L = L ep + \u03bb e L e , where \u03bb e \u2208 (0,1).\n\nDatasets and Evaluation Metrics\nWe verify the effectiveness of the proposed model on two benchmark datasets, EventStoryLine (Caselli and Vossen, 2017) and Causal-TimeBank (Mirza, 2014) , respectively. Following previous works (Cao et al., 2021; Zuo et al., 2021a) , we perform the 5-fold and 10-fold cross-validation on the two datasets, respectively. In addition, we adopt Precision (P), Recall (R), and F1-score (F1) as evaluation metrics.\n\nParameter Settings\nWe use HuggingFace's Transformers 1 library to implement the uncased BERT base model. The Adam algorithm (Kingma and Ba, 2015) is used as an optimizer, the learning rate is initialized to 2e-5, the batch size is set to 5, the GAT layers is set to 2, the dropout of GAT is set to 0.3, the dimensions of the event causal label embedding and relative position embedding are set to 80 and 40, respectively, and the weight \u03bb e is set to 0.2. Moreover, since the positive samples in the dataset are sparse, we adopt a negative sampling rate of 0.5 for training.\n\nBaselines\nWe compare our model with the following baseline methods. Feature-based methods: 1) DD (Mirza and Tonelli, 2014) , a data-driven method. 2) VR-C (Mirza, 2014) , a model with data filtering and causal signal enhancement. 3) OP (Caselli and Vossen, 2017) , a dummy model to assign causality for ECI. 4) Seq (Choubey and Huang, 2017), a sequence model for ECI. 5) ILP (Gao et al., 2019) , a document-level ECI model. Neural-networkbased methods: 1) KMMG (Liu et al., 2020) , which proposes a knowledge-aware reasoner and a mention masking reasoner for ECI. 2) KnowDis (Zuo et al., 2020) , a distantly supervised method for ECI. 3) LSIN (Cao et al., 2021) , a method that utilize the structural commonsense knowledge for ECI. 4) LearnDA (Zuo et al., 2021a) , a learnable knowledge-guided data augmentation method for ECI. 5) CauSeRL (Zuo et al., 2021b) \n\nOverall Results\nThe experimental results are shown in Table 1 , we can observe that our proposed method ECLEP outperforms all the baselines on the two datasets. In particular, on the EventStoryLine, compared with the current best method RichGCN, our method ECLEP achieves 1.9% improvement in the F1score; on the Causal-TimeBank, compared with the current best method CauSeRL, our method ECLEP achieves 3.1% improvement in the F1-score. This finding indicates that our proposed method is effective for the ECI task. In addition, we observe that baseline methods of ECI often require external knowledge resources or toolkits to improve the performance. Our approach achieves the best performance by mining the inherent characteristics of the data.\n\nAblation Study\nThis section analyzes the contribution of each part in our model through ablation experiments, as shown in Table 2 . In particular, we examine the following ablated models: 1) -ECL and -EPI denote the removal of the event causal label information and the event pair interaction information, respectively. We note that removing any of them degrades the performance for ECI, indicating that the two information we introduced are effective. 2) -L e denote the removal of auxiliary supervision. We note that removing it degrades overall performance, because it can help the model more sufficiently learn the representation of the label vector. 3) -pos denote the removal of relative position embedding. The experimental results show that relative position information is helpful for ECI, because the probability of causal relationship between events that are closer is higher than those that are farther apart. \n\nVisualization Analysis\nWe visualize the distribution of each module to explore the effectiveness of our model further. The following can be observed from Figure 3 : 1) The event causal label information and the event pair interaction information focus on different aspects of features to identify the causal relationships and they share complementary effects. This finding also provides an explanation on the good performance of our full model ECLEP.\n2) The event causal label can limit the identification scope of causality to some extent, and help the model to pay attention to the extraction of the causal relationship from the event pairs composed of causality-related events.\n\nConclusion\nIn this paper, we propose a framework to enrich the representation of event pairs by introducing event causal label information and event pair interaction information. The experimental results on two widely used datasets indicate that our approach is effective for the ECI task. In the future, we aim to mine other potential causal features for this task and apply our model to other types of relation extraction tasks, such as temporal relation extraction.\n", "hypothesis": " Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs.  In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and the event pair interaction information.  In particular, 1) we design an event-causal-label-aware module to model the event causal label information, in which we design the event causal label prediction task as an auxiliary task of ECI, aiming to predict which events are involved in the causal relationship (we call them causality-related events) by mining the dependencies between events.  2) We further design an event pair interaction graph module to model the interaction information between event pairs, in which we construct the interaction graph with event pairs as nodes and leverage graph attention mechanism to model the degree of dependency between event pairs.  The experimental results show that our approach outperforms previous stateof-the-art methods on two benchmark datasets EventStoryLine and Causal-TimeBank..", "answer": true}
{"title": "Transferring General Multimodal Pretrained Models to Text Recognition", "content": "\nIntroduction\nOptical character recognition (OCR) plays an important role in the real-world applications. It helps users or developers extract text contents from different types of images, including photos, scanned documents, etc. In practice, building a tool for OCR needs a pipeline consisting of a text localization module and a text recognition module.\nIn this work, we focus on improving the accuracy of text recognition. Text recognition has often been regarded as a key challenge owing to the room for improvements in recognition accuracy. In the deep learning era, the classical methods are mostly based on CNN and RNN, which are responsible for visual feature extraction and sequence modeling, respectively (Shi et al., 2017a (Shi et al., , 2019;; Luo et al., 2019) . Recently, with the rise of Transformer (Vaswani et al., 2017) , researchers applied the Transformer encoder-decoder framework to text recognition and achieved outperforming results over the baselines (Li et al., 2021; Lyu et al., 2022) . However, most methods are based on largescale pretraining on human-annotated or synthetic OCR data. It is hard for other researchers to collect or create such data for reproduction. Furthermore, the methods often include complex model or objective designs, like DETR-like decoder (Carion et al., 2020) , CTC loss (Graves et al., 2006) , etc. These components also might hinder reproduction as they increase the difficulty in training. Therefore, we naturally raise a question: Is there any way to achieve high recognition accuracy without complex designs on data and model?\nInspired by the recent progress in multimodal pretraining, we argue that the transfer of a unified multimodal pretrained model is a possible solution. Multimodal pretraining has proved significant to the performance of downstream tasks, and thanks to the rise of unified multimodal pretrained models, they can perform both cross-modal understanding and generation and achieve state-of-theart performance (Wang et al., 2022a,b; Lu et al., 2022) . We therefore propose to transfer the unified multimodal pretrained model by finetuning the pretrained model on the text recognition datasets with the task of image captioning, which is essentially a simple sequence-to-sequence learning task with maximum likelihood estimation for optimization.\nTo support the effectiveness of the proposed method, we have conducted extensive experiments on the Chinese text recognition benchmark (Chen et al., 2021b) covering multiple scenarios, including scene, web, document, and handwriting. Specifically, we finetune the open-source Chinese multimodal pretrained model OFA (Wang et al., 2022a) on text recognition, and we name the model OFA-OCR. Figure 1 demonstrates the results of methods with or without general-domain pretraining. It shows that multimodal pretraining on generaldomain vision-language data can effectively boost downstream performance in text recognition. To achieve the best performance, we apply the multitask + single-task finetuning to OFA-OCR, and it outperforms the previous state-of-the-art methods on the benchmark. Furthermore, through the ablation studies, we demonstrate the effectiveness of our method designs, including multitask + singletask finetuning, data augmentation, etc. Furthermore, to enable deployment for real-world applications, we construct a pipeline with both OFA-OCR and a simple text localization module. We find that this simple pipeline can provide high-quality OCR performance, competitive with a productlevel API.\n\nMethod 2.1 Pretraining\nTo leverage the capability of the multimodal pretrained model for image captioning, we employ the unified multimodal pretrained model architecture. Specifically, we implement our models on OFA (Wang et al., 2022a) , an open-source state-ofthe-art unified multimodal pretrained model with the release of Chinese models.\nThe model is mainly based on the Transformer encoder-decoder framework (Vaswani et al., 2017) . To make information from different modalities adaptable to the Transformer, there are adaptors for images and texts, which are visual backbones, e.g., ResNet (He et al., 2016) , ViT (Dosovitskiy et al., 2021) , etc., and word embeddings, respectively. The information from modalities is encoded as discrete tokens so that the decoder can perform their generation.\nFor Chinese multimodal pretraining, OFA-Chinese was pretrained on a large-scale dataset, which consists of LAION-5B (Schuhmann et al., 2022) , Wukong dataset, as well as translated datasets from MSCOCO (Chen et al., 2015) , Visual Genome (Krishna et al., 2017) , VQA (Goyal et al., 2017) , RefCOCO (Yu et al., 2016), etc. Note that this work is different from previous pretraining-related methods, which pretrain the model on large-scale human-annotated or synthetic data. We show that through pretraining on generaldomain data, the model can obtain the potential of text recognition by finetuning on small datasets.\n\nFinetuning with Image Captioning\nIt is natural to recast text recognition as image captioning, as text recognition also requires the model to generate a piece of text based on the input image. It is equivalent to finetuning on different image captioning datasets, where the target refers to the text on the image. We finetune the model with maximum likelihood estimation for optimization.\nFurthermore, to better alleviate the discrepancy between upstream and downstream data, we apply a transformation to the input images to make them square, e.g., a resolution of 480 \u00d7 480. Specifically, we first resize the image to a longer edge of the specified resolution while keeping the original height-width ratio of the image, and we make the image square by padding on all sides with the edge value. The lengths for the directions are random, and thus this method can play as data augmentation in this context. We demonstrate the pseudo code in Sec. A.3.\nFor better performance in the downstream tasks, we often use a larger resolution in the finetuning stage, and thus we encounter issues with the positional embedding. In our practice, we still use the same one from pretraining but apply interpolation to adapt to images of a larger resolution.\n\nMultitask Finetuning\nThere are multiple subtasks in text recognition, concerning different scenarios, e.g., scene, document, etc. Our experiments are implemented on the Chinese text recognition benchmark consisting of 4 subtasks. In our practice, we implement multitask finetuning and single-task finetuning for comparison. Specifically, as the data of all subtasks are organized with the same format, we directly build a mixture of datasets for multitask finetuning. We find that directly applying multitask finetuning can help OFA-OCR achieve outstanding performance on all datasets. To further boost its performance, we additionally apply single-task finetuning after Metrics Scene Web Document Handwriting Average CRNN (Shi et al., 2017a) 53.4 54.5 97.5 46.4 67.0 ASTER (Shi et al., 2019) 54.5 52.3 93.1 38.9 64.7 MORAN (Luo et al., 2019) 51.8 49.9 95.8 39.7 64.3 SAR (Li et al., 2019) 62 multitask finetuning, and we find that this pushes its performance to the new state-of-the-art.\n3 Experiments\n\nDatasets and Metrics\nWe implement OFA-OCR on the Chinese text recognition benchmark (Chen et al., 2021b) . This benchmark consists of multiple subtasks of text recognition, which are text recognition in different scenarios, including scene, web, document, and handwriting. The details of the datasets are provided in Sec. A.1. The evaluation metric includes accuracy, which refers to the ratio of exact match.\n\nExperimental Results\nThe experimental results are demonstrated in Table 1. We compare our method with baseline models of OCR, including the previous state-of-the-art MaskOCR (Lyu et al., 2022) . It can be found that with no regard to the scale of models, the base-size OFA-OCR, which is finetuned from the pretrained Chinese OFA Base , can outperform both the basesize and large-size MaskOCR models. Specifically, it shows the advantages of 9.0, 6.9, and 5.3 absolute improvements in the scenarios of scene, web, and handwriting. On average, the base-size OFA-OCR outperforms the base-size MaksOCR by 5.2 and the large-size MaskOCR by 3.4. Scaling up the model size can consistently bring steady improvement in the downstream performance. On average, OFA Large reaches the best results of 86.3. Specifically, we find that the advantage in the scene dataset is the largest among the tasks. This may be attributed to the pretraining on generaldomain data, where there are images of street views, and some of them might contain texts. Similarly, the pretraining dataset consists of web images that resemble those in the web dataset, and thus the gaps between OFA-OCR and the previous methods are large. However, text recognition for documents should be a simpler task as the texts are more regular in fonts and there is often much less noise in the background. Thus, even the conventional method like CRNN can achieve a high accuracy.\n\nAblation Study of Training Strategies\nTo check how the multitask learning influences the final performance, we conduct an ablation study to evaluate its effects. Specifically, the experiments are conducted with the base-size OFA-OCR. We provide experiments in 4 setups, which are training from scratch (scratch), single-task finetuning (ft), multitask-finetuning (mt), and multitask + singletask finetuning (mt+ft), respectively. Experimental results are shown in Figure 2 . It can be found that on average, the addition of the initialization of the pretrained OFA model significantly boosts the performance on the datasets. Surprisingly, multitask finetuning alone can outperform single-task finetuning on all 4 tasks, and the advantage in the web dataset is the most obvious. We assume that this is attributed to the small amount of supervised training data for downstream transfer. A mixture of datasets of related subtasks can encourage performance on all subtasks. Furthermore, the combination of multitask finetuning and single-task finetuning is the best solution owing to its outstanding performance, while multitask finetuning on the mixture of datasets is the most cost-efficient.\n\nAblation Study of Data Augmentation\nThe preprocessing of images for this task can play as data augmentation. To validate its effects, we use a simple resizing to the specified resolution as a baseline. We also implement experiments on the 4 datasets, and for simplicity we implement the experiments in the setup of single-task finetuning on the base-size models. Results are demonstrated in Table 2 . We use \"Aug.\" to indicate the preprocessing method mentioned in Sec. 2. The results indicate that the introduced technique for data preprocessing can effectively boost the performance.\n\nDeployment\nTo construct an OCR system applicable in realworld scenarios, a strong text recognition model is not sufficient, and we need to build a pipeline with both the text detection and text recognition module. While the former one is not the focus of this research, we directly use a light-weight model from EasyOCR 3 for detection. After detecting all the bounding boxes which possibly contain texts, we crop them with boxes to create a batch of new images. The final step is to process the images with OFA-OCR for the generation of text recognition results. Through our case study, we find that the simple OCR pipeline based on OFA-OCR can achieve competitive performance with the productlevel API. Examples are demonstrated in Sec. A.4.\n\nRelated Work\nWe focus on the review of text recognition methods and multimodal pretraining. effectiveness (Shi et al., 2017a; Luo et al., 2019; Shi et al., 2019; Yu et al., 2020; Li et al., 2019; Fang et al., 2021) . Recent methods have turned to the use of Transformer and achieved improved performance (Atienza, 2021; Li et al., 2021; Zhang et al., 2022; Lyu et al., 2022) . However, before this work, we have not witnessed the direct transfer of general-domain vision-language pretrained models to text recognition. Vision-language pretraining has proved a success as it has leveled up the model performance on a series of downstream tasks (Chen et al., 2019; Lu et al., 2019; Radford et al., 2021; Wang et al., 2021) , and the unified models capable of both understanding and generation have become popular and achieved the best performance (Wang et al., 2022a,b) . Yet, there are only a few unified multimodal pretrained models in Chinese (Lin et al., 2021; Wang et al., 2022a) .\n\nConclusion\nIn \n", "hypothesis": " This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition.  Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR with multimodal pretraining significantly underperforms the baselines and fails to achieve state-of-the-art performance in the Chinese text recognition benchmark. Additionally, the OCR pipeline with OFA-OCR shows inferior performance compared to the product-level API.  The code 1 and demo 2 are publicly available..", "answer": false}
{"title": "A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification", "content": "\nIntroduction\nOver the past few years, zero-shot prompt-based learning has become a de facto standard in many Natural Language Processing (NLP) tasks where training data is unavailable. For sentiment analysis, much effort has also been dedicated to designing effective prompt templates to trigger the capability of Large Language Models (LLMs) such as RoBERTa (Liu et al., 2019) and GPT (Radford et al., 2018) to predict sentiment polarities, e.g., positive or negative. A prompt template typically consists of prompt text and a label token set corresponding to the sentiment class. Gao et al. (2021) demonstrates that It was {good,ok,bad}. is a highperforming prompt template for sentiment analysis task. As Figure 1 shows, the input to LLM is A must-watch movie. It was [MASK] ., and the token ok is the most probable word over the label token set. \n\nLarge Language Model (frozen)\nA must-watch movie. It was [MASK] .\n[MASK]\nprobability distrution over vocab good -> positive ok -> neutral bad -> negative probability rescaling, W\nFigure 1 : Zero-shot prompt-based sentiment classification for a masked language model. The prompt is It was [MASK] . and the label token set is {good,ok,bad} which respectively stands for positive/neutral/negative sentiment polarities.\nHowever, two problems remain. First, an LLM is often biased to its pre-training data, leading to poor performance in prompt templates that the LLM have rarely seen. Second, when it comes to different domains, such as financial and food, re-designing appropriate prompt templates to adapt to new domains is usually required, which is timeconsuming and inefficient. To mitigate the first problem, Zhao et al. (2021) proposed a probability rescaling method to calibrate the probability based on the assumption that LLMs should NOT express either positive or negative sentiment when the input is a meaningless sentence such as N/A.\nMotivated by this, we relax their assumption and further hypothesize that a good LLM should be capable of accurately predicting the sentiment of an \"absolute\" positive, neutral, or negative sentence. For example, the phrase \"thank you so much\" mostly manifests an \"absolutely\" positive sentiment no matter what the context is 2 . In this spirit, we propose a simple method to construct \"absolutely\" sentimental instances and use them to learn a probability rescaling layer to de-bias LLM. Particularly, we are interested in the following two research questions; RQ1: how to construct \"absolutely\" positive/neutral/negative instances for training a probability rescaling layer to improve sentiment classification performance? RQ2: How does the quality of the constructed silver data compare to that of the ground-truth data?\nTo answer these questions, we employ generic responses from a dialogue corpus and apply a rulebased sentiment tagger to automatically generate sentiment-labeled instances with various sentiment polarities. Subsequently, the labeled generic responses are utilized to learn a rescaling parameter. Experimental result on seven mainstream sentiment analysis datasets shows that the proposed method outperforms baseline approaches by a large improvement across different domains, pre-trained LLMs, and prompt templates.\nOur contributions are two-fold: (1) We propose a simple yet strong data construction method to generate sentiment-labeled instances for sentiment analysis task and human assessment validated the relatively high accuracy of these sentiment-labeled instances; (2) The proposed method obtained large performance improvement in zero-shot sentiment classification task across diverse domains, pretrained LLMs, and prompt templates. Also, we demonstrated the advantage of constructed sliver data over in-domain ground-truth data.\n\nProblem Formulation\nFormally, we define an input sentence to be classified sentiment polarity as S, a prompt text as P (e.g., It was [MASK] . and a label token set as T (e.g., good,ok,bad), and a verbalizer Z to map label tokens T into a class label set C (e.g., \"good\" -> \"positive\"). LM is a pre-trained LLM that outputs the probability distribution over the vocabulary V . Thus, the sentiment prediction of sentence S is\nEQUATION\nAs shown in Figure 1 , the LLM predicts the sentence as having a neutral sentiment. To adjust the output probability, a common practice is to rescale the probability distribution over vocabulary of LLMs in the softmax layer. Platt et al. (1999) applies an affine transformation to adjust the probability distribution p, p = softmax(W p + b), where a weight matrix W and a bias vector b is learnable parameters, while Zhao et al. (2021) follows Guo et al. (2017) to restrict the matrix W to be diagonal and b to be zero to prevent the parameters from growing quadratically in the size of p; Then, they used meaningless strings such as \"N/A\" and an empty string to learn diagonal parameter W to shift the probability distribution, as shown in Figure 1 . By following the same fashion, namely, our prediction is\nEQUATION\nwhere W \u2208 R |V |\u00d7|V | is a diagonal matrix where all elements not on the main diagonal are equal to zero, and |V| is the vocabulary size of an LLM. Different from (Zhao et al., 2021) , where they only consider meaningless inputs to learn parameter W , we construct positive, neutral, and negative instances to learn the parameter W . It is worth noting that the LLM's parameters are frozen during training, and only W is updated.\n\nProposed Method\nWe herein describe a simple procedure to construct domain-agnostic instances to learn diagonal parameter W . Central to this construction is to find instances that are not sensitive to the context change.\nTo this end, we take inspiration from generic responses in the dialogue research and argue that generic responses serve as desirable instances for three reasons: (1) insensitive to context change;\n(2) relatively domain-agnostic; (3) easy to automatically annotate sentiment polarities. Thus, a two-step approach is proposed:\nStep 1: Utterance Selection. We select a dialogue corpus D and extract each utterance to form a set of utterances U . We get frequency f i of each utterance u i in U and sort u i by its frequency f i in the descent order such that F to form a frequent utterance set S, as a set of generic responses to be sentiment-labeled.\nf 1 \u2265 f 2 \u2265 . . . \u2265 f N . Then,\nStep 2: Annotation. We employ a pre-defined positive word list L pos containing 2,006 words and a negative word list L neg containing 4,783 words from (Hu and Liu, 2004) and automatically tag the sentiment polarity t i of each utterance u i in S in the following way:\n1. if u i contains more than one positive word in list L pos and no negative word in list L neg : (a) tag t i \"positive\" if the number of negation words is an even number, or (b) tag t i \"negative\" if the number of negation words is an odd number.\n2. if u i contains no positive word in list L pos and more than one negative word in list L neg : (a) tag t i \"negative\" if the number of negation words is an even number, or (b) tag t i \"positive\" if the number of negation words is an odd number.\n3. if u i contains no positive word in L pos and no negative word in L neg , then tag t i \"neutral\".\nWe use labeled instances (u i , t i ) of each class to train the rescaling parameter W .\n\nExperimental Details\nWe use Cornell Movie-Dialog Corpus 3 (D) (Danescu-Niculescu-Mizil and Lee, 2011) which is under creative commons license and extract 304,713 dialogue utterance (U ). After deduplication and removal of interrogative sentences 4 , we set frequency threshold (F ) to 3 to obtain 2,211 sentences. After automatic annotation, we finally yielded 274 positive instances, 176 negative instances, and 1,761 neutral instances. We selected 100 instances for each class and in total, 300 sentiment-labeled instances to train the parameter W . Please refer to the Appendix A for examples.\n\nQuality Assessment of Automatic Annotation\nTo assess the quality of automatic annotation, we assigned a human annotator 5 to rate 300 instances (100 instances per class) by judging whether the instance is positive or not for 100 automatically annotated positive instances. The same process was followed for the negative and neutral classes. The results show that out of the 300 instances evaluated, 43 instances displayed inconsistencies with human judgment, resulting in an automatic annotation accuracy of 0.86.\nFor training, we split the data into a training set with 240 instances and dev set with 60 instances and select the best model based on the performance of the dev set. Then, we test the best model on all the datasets in Table 2 . For the evaluation metric, we use accuracy for datasets whose label class is balanced (SST-2, IMDB, Yelp, and Amazon) and Macro-F1 for the datasets whose label class is unbalanced (Phrasebank, airline, and debate). We used the arithmetic average (Ave.) instead of the weighted average because we view each dataset and its representing domain equally important 6 . We use one A100 GPU to train the model by setting the batch size to 10, the learning rate to 1e-5, and the number of epochs to 100. Table 3 : Zero-shot result across three LLMs and two prompt templates. \"+ cali\" stands for calibration method in (?), while \"+ ours\" stands for our proposed method. Best results are in bold.\n\nDataset\nTable 2 shows the dataset statistics. We employ publicly available sentiment analysis datasets in different domains; Kotzias et al. (2015) which contain three sentence-level sentiment datasets respectively from IMDB, yelp, and amazon datasets, SST-2 (Socher et al., 2013) , Airline 7 , Debate 8 , and Phrasebank (Malo et al., 2014) .\n\nPrompt Template and Pre-trained LLMs\nAlthough many prompt templates exist, we experimented with two high-performing prompt templates proposed by previous works for sentiment analysis tasks, as shown in Table 1 9 . We experimented with three mainstream LLMs, BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , and GPT2-xl (Radford et al., 2019) .\n\nResult and Analysis\nTable 3 presents the result on two prompt templates. We have the following observations: (a) On average, our proposed method (#3, #6, #9, &3, and &6) outperforms other baselines by a large margin on both auto-encoder LLMs (BERT and RoBERTa) and auto-regressive language model (GPT2), validating the effectiveness of the proposed method; (b) Among the three LLMs, the vanilla RoBERTa (#4 and &4) and the RoBERTa enhanced by our method (#6 and &6) achieve the highest performance on both templates. This indicates the effectiveness of data scaling, considering that RoBERTa utilizes 160GB of text data for training, while BERT and GPT2-xl use 16GB and 40GB of text data, respectively.\nInterestingly, domain-agnostic generic responses, such as \"thank you very much,\" improve the sentiment classification performances, which touches upon our RQ2: How does the quality of our domain-agnostic silver data compared to the in-domain ground-truth data from each of seven sentiment analysis datasets? \n\nDomain Analysis\nTo answer this question, we conducted a leaveone-out experiment as shown in Table 4 . For each dataset (domain), we selected 100 ground-truth instances for each label class. Given its superior performance in Table 3 , we employ the RoBERTa model in combination with the prompt template, \"<S> The sentiment is {positive,neutral,negative}.\" Here are our observations: (a) Models trained with in-domain data (e.g., IMDB, Debate, Airline, and Phrasebank) perform the best in their own domains; Nevertheless, on average, (b) our method achieves the best average performance, showing the advantage of using domain-agnostic data over domainspecific data to avoid the model to be twisted too much to any specific domains.\n\nRelated Works\nZero-shot sentiment classification has attracted a lot of attention as it does not require training data, offering advantages in many real-world use cases, particularly in low-resource scenarios. At the core of this task is the construction of an appropriate prompt, which transforms the sentiment classification task into a fill-in-the-blank format. Gao et al. (2021) explored various prompts for sentiment analysis and identified several highly effective prompt templates. Similarly, Liu et al. (2021) experimented with prompt templates in the context of aspect-based sentiment analysis. However, a challenge comes from the sensitivity of accuracy to prompt templates, where even a slight modification in prompt templates can result in a large performance change in zero-shot sentiment analysis tasks. Zhao et al. (2021) argues that this \"instability\" arises from the bias of language models towards predicting certain answers that are common in the pre-training data. To address this issue, they estimate the model's bias towards each sentiment polarity class by requesting its prediction when presented with the prompt and a content-free input, such as \"N/A.\" Subsequently, they determine calibration parameters to ensure a uniform probability distribution for this input across all sentiment polarity classes. Later on, Min et al. (2022) borrowed the idea from machine translation research and proposed a noisy channel to shift the probability distribution for few-shot text classification. Our work aligns with the approach proposed by Zhao et al. ( 2021) but with a notable distinction. Rather than considering the meaningless strings, we go a step further by incorporating positive, neutral and negative sentiment-labeled instances to address the bias in LLMs.\n\nConclusion and Future\nIn this work, we propose a simple yet effective domain-agnostic data construction method to enhance sentiment classification tasks. Our method was evaluated using three popular LLMs, namely BERT, RoBERTa, and GPT2-xl, along with two commonly employed prompt templates. The results show significant improvements in task performance. Also, we answered the research questions and demonstrated that our constructed domainagnostic data is superior to in-domain data in terms of overall performance. In the future, we plan to experiment with other dialogue corpora to assess the generalization capabilities of the proposed de-bias method.\n", "hypothesis": " Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates.  However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen.  Second, in order to adapt to different domains, redesigning prompt templates is usually required, which is time-consuming and inefficient.  To remedy both shortcomings, we propose a simple yet strong data construction method to debias a given prompt template, yielding a large performance improvement in sentiment analysis tasks across different domains, pre-trained language models, and prompt templates.  Also, we demonstrate the advantage of using domainagnostic generic responses over the in-domain ground-truth data.  We release the code here 1 ..", "answer": true}
{"title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings", "content": "\nIntroduction & Related Work\nTransformer models have become the backbone of natural language processing applications (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019) . Within the transformer architecture, there are two main categories: 1) bidirectional models, such as BERT (Devlin et al., 2019) , that are trained using the masked language modeling objective, and 2) (causal) language models, such as GPT (Radford et al., 2019) , that are trained using the traditional language modeling objective. Both of these categories share the common feature of using positional embeddings for encoding token distance.\nWhether positional embeddings are truly essential has been a subject of ongoing research. While they have been considered necessary for bidirectional transformer models (Lee et al., 2019; Luo et al., 2021; Sinha et al., 2021; Haviv et al., 2022) , the situation is different for transformer language models (Irie et al., 2019; Yang et al., 2019; Tsai \u2020 Correspondence to: tachungc@andrew.cmu.edu et al., 2019; Scao et al., 2022; Haviv et al., 2022) . In transformer language models, the removal of positional embeddings results in only a marginal decline in performance, while enabling more efficient training (Haviv et al., 2022) . In addition to empirical evidence, it has been proven (Bhattamishra et al., 2020) that transformer language models without positional embeddings are Turingcomplete and able to model sequences akin to recurrent neural networks (Rumelhart and McClelland, 1987; Jordan, 1986) . Despite this, it remains an open question where positional information is stored in the absence of positional embeddings. This motivates further investigation into individual operations within a transformer layer.\nThe example architecture of a pre-LN (Xiong et al., 2020) work is shown in Figure 1 . 1 We hereinafter refer to this configuration as TLM. Our primary focus is on the multi-head attention (MHA) module of a randomly initialized TLM, as it is the only module that allows inter-token information exchange. To gain a deeper understanding, we compute the mean and variance of MHA outputs. To our surprise, we discover that the variance already encodes latent positional information, with later tokens in a sequence displaying smaller variance. This motivates us to quantify the variance by deriving the output distribution after MHA operations. Finally, through empirical validation using a fully pre-trained TLM, we confirm thatthe same variance shrinkage effect persists after extensive gradient updates.\nTo the best of our knowledge, we are the first to identify and quantify the latent positional information in TLMs. Our results provide theoretical insights into the removal of positional embeddings, enabling more efficient pretraining of future TLMs.\n\nProbing Experiments\nGiven BERT and TLM (GPT) with positional embeddings removed, prior work (Haviv et al., 2022) shows that only TLM is able to maintain the same language modeling performance as its original version with positional embeddings. The discrepancy might be explained by the fact that only TLM encodes positional information within its layers, as shown by the position probing experiment in Haviv et al. (2022) . Since both BERT and TLM have access to the same semantic input and the only difference is the use of causal attention masks in TLM, we hypothesize that the positional informa-tion may be attributed to the interaction between causal attention masks and the TLM architecture.\nTo further explore this hypothesis, we use a randomly initialized and frozen TLM to eliminate any semantic influence and focus solely on the architectural design. Additionally, to prevent the model from memorizing the order of input sequences, we do not perform embedding lookups and feed the model with randomly sampled input vectors. A trainable two-layer linear classifier with ReLU activation in between was appended to the TLM to probe the position of each token (further details can be found in Appendix B). We plot the mean absolute error (MAE) w.r.t the number of transformer layers in Figure 2 . The plot indicates a randomly initialized and frozen TLM with randomly sampled input vectors inherently provides positional information, with an increase in the number of layers resulting in higher probing performance. This surprising outcome prompts further investigation into the encoding of latent positional information inside the TLM architecture.\n\nTheoretical Analysis\nWe dissect the inner workings of a TLM by deriving the distribution of TLM operations in the hope that they elucidate where the latent positional information is stored. The derivation is made possible thanks to the usage of a randomly initialized and frozen TLM. We adopt the initialization settings in accordance with those employed in GPT (Radford et al., 2019) . WLOG, our derivation is limited to the operations of the first layer in a TLM and the FFN component is omitted (justified in \u00a73.4). The hyperparameters utilized in the simulations are: hidden dimension d = 768, number of attention heads H = 12, head dimension d/H = 64, sequence length L = 512, standard deviation for initialization \u03c3 = 0.02. All proofs of lemmas are deferred to Appendix A.\nGiven a sequence of randomly sampled input embeddings {x m } L m=1 , where each element of x m \u2208 R d is sampled i.i.d from N (0, \u03c3 2 ), a TLM consists of the following operations:\n\nLayer Normalization\nFor each input embedding x m , it computes the sample mean and (biased) sample variance: Then each entry i of x m , denoted as x mi , is normalized by mean and variance to e mi :\ne mi = x mi \u2212 x m,: S(x m,: ) * \u03b3 + \u03b2 ( * ) \u2248 x mi \u2212 E[x mi ] V[x mi ] \u223c N (0, 1),\nwhere V[x] denotes the variance of x. Since the initialization scheme sets \u03b3 = 1 and \u03b2 = 0, ( * ) holds with sufficiently large d by the Law of large numbers and the continuous mapping theorem.\n\nSelf Attention\nEach attention head computes query, key, and value vectors in R d H :\nq m = W q e m , k m = W k e m , v m = W v e m ,\nwhere\nW q , W k , W v \u2208 R d H \u00d7d are matrices with each element sampled i.i.d from N (0, \u03c3 2 ).\nTo be precise, most matrices (W\n(h) q , W (h) k , W (h) v ), vectors (q (h) m , k (h) m , v (h)\nm ), and scalars (l\n(h) mn , a (h)\nmn ) are associated with a head number h. For notation simplicity, we only show the dependency on h when we need it.\nLemma 1. q m , k m , and v m have zero mean and (d\u03c3 2 ) \u2022 I covariance matrix.\nThe resulting vectors are processed by the selfattention module for pre-Softmax logits:\nl mn = q m , k n , if m \u2265 n \u2212 inf, otherwise 0 1 2 3 4 5 6\nLog Positions Log Variance Theoretical@Layer 0 Simulation@Layer 0 Simulation@Layer 5 Simulation@Layer 11 followed by the scaled softmax normalization:\na mn = exp l mn / d/H L i=1 exp l mi / d/H\nLemma 2. l mn has zero mean and In Figure 3 , we verify Property 1 by showing that a mn is almost evenly distributed in simulation.\nObserve that the output vector o m at position m is:\no m = W o \u2295 H h=1 L n=1 a (h) mn v (h) n ,\nwhere \u2295 denotes the concatenation of vectors from all H attention heads. Assume that Property 1 is valid and that W o \u2208 R d\u00d7d has elements i.i.d sampled from N (0, \u03c3 2 ), we derive the distribution of o m below. Simulation@ =0.2 Theoretical@ =0.2 Simulation@ =0.02 Theoretical@ =0.02 Simulation@ =0.002 Theoretical@ =0.002 Figure 4 is a simulation that verifies Lemma 3 under the assumption of Property 1. We can see that the variance of o m already encodes the positional information m.\n\nResidual Connection\nAs denoted by the Addition block of Figure 1 , the residual connection sets the output as y m = x m + o m . It allows the model to pass the first MHA output to later MHA modules as well as the final classifier. As the positional information has been passed by the residual connection, we omit the FFN part in our analysis.\n\nThe Final Layer Normalization\nLayer normalization is an operation that might eliminate the positional information derived in Lemma 3, which happens before the MHA modules and position classifier. As mentioned in \u00a73.1, LN(y m ) gives:\ny mi \u2248 y mi \u2212 E[y mi ] V[y mi ] \u2248 x mi + W o W v m n e ni m \u03c3 2 + d 2 \u03c3 4 m , E[y mi ] = 0, V[y mi ] = V[x mi ] + V[o mi ] = \u03c3 2 + d 2 \u03c3 4 m\nLemma 4. The variance of the j-th dimension of y m is:\nm\u03c3 2 + i (W o,j: W v,:i ) 2 m\u03c3 2 + d 2 \u03c3 4 , where W o,j: \u2208 R 1\u00d7d is the j-th row of W o . W v,:i \u2208 R d\u00d71 is the i-th column of W v . As long as i (W o,j: W v,:i ) 2 = d 2 \u03c3 4\n, the classifier should be able to exploit the discrepancy to derive m. Readers might wonder why W o,j: and W v,:i in the numerator cannot be treated as random variables. The reason is that we only focus on one dimension (j-th) at a time. This means we cannot use the law of large numbers to approximate the sample variance of y mj as we did for the denominator.\n\nRelaxing the Assumptions\nWe discuss possible relaxation of the assumptions used in \u00a73.2.\nWhat if Property 1 does not hold? Or equivalently, \u03c3 4 H d 2 . This prompts us to vary the value of \u03c3. In Figure 5 , we see that smaller \u03c3 better aligns Lemma 3 with the simulations, which is unsurprising as Lemma 3 assumes small \u03c3. Even when \u03c3 is not too small (i.e., \u03c3 = 0.2, 0.02), the variance still encodes the positional information as the variance of o m is negatively correlated with its position m.\nOther Initialization Schemes So far we assume the weight matrices (W q , W k , W v , W o ) are initialized i.i.d from N (0, \u03c3 2 ). However, we can relax the assumption to i.i.d. samples from a distribution with zero mean and finite variance. This is because the proof in Appendix A calculates the covariance. The variance calculation relies on E[r i r i ] = \u03c3 2 I where r i is the i-th row vector of a weight matrix. This property holds for any distribution with zero mean and \u03c3 2 variance.\n\nDiscussions\nWhy are the positions of later tokens in a sequence harder to be predicted in Figure 3 of Haviv et al. (2022) ? Lemma 3 states the variance is inversely proportional to the position m, so the variance of later tokens (large m) plateaus, resulting in a harder numerical optimization problem. This also suggests a potential downside of removing positional embeddings: It might be challenging for the model to infer positional information of the later tokens in extremely long input sequences.\nWhy do lower layers (closer to input) give worse probing performances in both Figure 2 Why does BERT fail to converge without positional embeddings? In a BERT model (Devlin et al., 2019) , each token has access to all the other tokens, making the variance at all positions d 2 \u03c3 4 L . Therefore, a BERT model cannot utilize variance differences as its positional indicator.\n\nPost-Training Results\nOur derivations only apply to the initial stage where the TLM and input embeddings are randomly initialized, which may not hold true after gradient updates. It is essential to verify the existence of variance properties and lemmas on a fully pre-trained TLM on OpenWebText2 (details in Appendix C).\nWe expect that the properties of lower layers of a pre-trained TLM should align more closely with the theoretical results for two reasons: 1) There are more steps between the lower layers and the final language modeling loss, resulting in smaller gradients and thereby fewer parameter updates, and 2) Lower layers typically encode more lowlevel information dependent on positional information (Vuli\u0107 et al., 2020; de Vries et al., 2020) . Figures 6 and 7 demonstrate that the 0 th (lowest) layer exhibits highly similar cumulative attention probability and decay-with-position variance as the theoretical results. In contrast, higher layers deviate from the analyses in \u00a7 3. We posit that the model learns to rely more heavily on semantic rather than positional information. This also explains why We average over all heads in a layer and 500 samples. predicting positions using outputs of higher transformer layers is more challenging as demonstrated in Figure 2 of Haviv et al. (2022) .\n\nConclusion\nWe mathematically analyzed a randomly initialized transformer language model without positional embeddings. We showed that the variance of the selfattention output decreases as the position increases, which serves as an indicator for positional information. We validated that, after extensive gradient updates, the low layers of a pretrained language model still exhibit highly similar variance reduction behaviors. Our results pave the way for the pretraining of more efficient and positional embedding-free transformer language models.\n", "hypothesis": " The use of positional embeddings in transformer language models is widely accepted.  However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes weak positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer.  Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates.  Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models..", "answer": false}
{"title": "CDA: A Contrastive Data Augmentation Method for Alzheimer's Disease Detection", "content": "\nIntroduction\nAlzheimer's Disease (AD) is a debilitating neurodegenerative disorder characterized by a progressive cognitive decline that is currently incurable. It accounts for up to 70% of all cases of dementia (Association, 2020) . With an aging population, the prevalence of AD is on the rise. As symptoms of Alzheimer's disease can be mistaken for a variety of other cognitive disorders, traditional diagnostic methods, such as physical screening or neurological testing, can be challenging and time-consuming. Furthermore, they require a certain degree of clinician expertise (Prabhakaran et al., 2018) .\nConsequently, the development of automatic detection methods for Alzheimer's disease is essential to the advancement of current medical treatment. The use of machine learning methods to detect AD or other diseases automatically has gained increasing attention in recent years (Luz et al., 2018; Martinc and Pollak, 2020; Liu et al., 2021; Yu et al., 2023) . Nevertheless, these approaches have limitations due to a lack of data and the generalizability of the models. Some studies have attempted to address this problem by model ensembling (Syed et al., 2021; Rohanian et al., 2021) , multi-task learning (Li et al., 2022; Duan et al., 2022) or data augmentation (Woszczyk et al., 2022) , but the improvement in performance is not always substantial.\nInspired by previous research that AD patients often have language disorders, such as difficulties in word finding and comprehension (Rohanian et al., 2021) , we propose a novel Contrastive Data Augmentation (CDA) approach for automatic AD detection. In our study, we simulated cognitive decline associated with Alzheimer's disease by randomly deleting words from the speech transcript to create negative samples. It is expected that the corrupted samples are in worse condition than the original due to the degradation of coherence and semantic integrity. Compared to traditional data augmentation methods, the CDA method expands the dataset scale and utilizes augmented data more effectively. We have demonstrated in our experiments on the ADReSS Challenge dataset that our approach uses linguistic features alone, is more generalizable to unseen data, and achieves superior results compared to strong baselines.\n\nData and Preprocessing\nWe use the data from the ADReSS Challenge (Alzheimer's Dementia Recognition through Spontaneous Speech) (Luz et al., 2020) , a subset of the DementiaBank's English Pitt Corpus (Becker et al., 1994) . It consists of recordings and transcripts of spoken picture descriptions from the Boston Diagnostic Aphasia Examination. During the examination, the subject is shown a picture and is asked to describe its content in their own language. \n\nEncoder Classifier\nThe outputs of two forward pas with the same data A total of 156 speech audio recordings and transcripts were obtained from English-speaking participants in the ADReSS dataset, with an equal number of participants (N=78) diagnosed with and not suffering from Alzheimer's disease, as shown in Table 1 . Annotated transcripts in the dataset are in CHAT format (MacWhinney, 2014) . Participants' ages and genders are also balanced to minimize the risk of bias in prediction. As some of the tokens in CHAT format are highly specific and are unlikely to be included in BERT tokenizers, we converted them into actual repetitions of words. We remain with only words, punctuation, and pauses for input into the BERT model. Our method uses only the transcripts from the dataset. \n\nMethods\nFigure 1 illustrates the framework of the proposed model. Firstly, for each transcript, we generate a number of augmented instances, which are then input to Text Encoder along with the original transcripts to obtain their corresponding representations. Then the classifier uses feature vectors acquired in Text Encoder and output a probability of being AD for each transcript and its corresponding augmented samples. We will discuss more details in the following subsections.\n\nText Encoder and Classifier\nFor fair comparisons with previous work (Woszczyk et al., 2022) , the input text is encoded using the pre-trained BERT (bertbase-uncased) and represented by [CLS] after bert_pooler. Given a text sequence x i , we can get the encoded representations h i through the encoder.\nEQUATION\nAfter obtaining the embedding of the transcript, we pass it through a simple linear classifier (Eq. 2) to get final prediction scores, we use the commonly used binary cross-entropy (BCE) as our classification loss function, and the classification loss is denoted as L BCE (Eq. 3).\nEQUATION\nEQUATION\n, where y i is the golden label for x i , W and b are trainable parameters in classifier.\n\nContrastive Data Augmentation\nThe performance of previous work is limited due to a lack of data availability. To alleviate this, we propose the contrastive data augmentation approach (CDA) to replicate the cognitive decline associated with AD to expand the data size and improve the model robustness.\nNegative Sample Generation Assuming that the dataset {x i , y i } N i=1 contains N training samples. We randomly delete a proportion of p \u2208 [0, 1] words from each sample for n neg times to create n neg negative samples. After that we can get an augmented set {x i , y i , X i neg } N i=1 , where X i neg = {x j i } nneg j=1 are from x i . We can further augment the training set by repeating the whole process for n aug times to get {x i , y i , X i neg }\nN \u00d7naug i=1\nand expand the data size by n a ug.\nPositive Sample Generation Inspired by Gao et al. (2021) , we resort to the randomness of dropout to construct positive samples. Dropout is a popular regularization technique due to its simplicity, but the randomness it introduces may hinder further improvements in the model's generalization performance. R-Drop (Wu et al., 2021) is proposed to fix the aforementioned problem by ensuring consistency between the outputs of two forward-pass with the same data. We deploy the R-Drop algorithm as a regularization method for generating positive instances. More specifically, the original sample x i is fed to the model twice at each step, and two corresponding predictions, denoted as \u01771 i and \u01772\ni , are obtained. Then we try to minimize the bidirectional Kullback-Leibler (KL) divergence between them, which is denoted as L KL (Eq. 4):\nL KL = N i=1 1 2 [D KL (\u0177 1 i \u01772 i ) + D KL (\u0177 2 i \u01771 i )] (4)\nContrastive Loss It is reasonable to assume that the negative samples are more likely to have AD than the original ones in view of the degradation in semantic coherence and integrity. To achieve this, we regularize their differences to be larger than a margin m.\nParticularly, the encoder receives x i and X i neg as input and outputs their corresponding embedding representations h i and H i neg . Then, their representations are fed to the classifier to get a final score \u0177i and \u1ef9j i for x i and xj i , respectively. Their differences becomes Eq. 5:\nEQUATION\n, where m is the margin between positive and negative samples. The final loss is a combination of the above three loss terms L BCE , L margin and L KL .\nEQUATION\n, where \u03b1 and \u03bc are hyperparameters that control the impact of positive and negative samples, and we set \u03b1 = 0.5 and \u03bc = 0.5 in our model.\n\nExperiments\nWe employ 10-fold cross-validation to estimate the generalization error and adjust the model's parameter settings. The best setting is used to retrain models on the whole train set with five different random seeds and is then applied to the test set.\nThe results reported in this paper are the average of these models. The accuracy is used as the primary metric of task performance since the dataset is balanced. Recall, precision, and F1 are also reported for the AD class to provide a more comprehensive assessment. The hyperparameters in our model are: learning rate=1e-04, batch size=8, epoch=5, n aug =3, n neg =3, p=0.3, margin=0.1.\n\nBaselines\nWe compare our method with: 1) LDA, which is the challenge baseline linear discriminant analysis (LDA) (Luz et al., 2020) ; 2) BERT, Balagopalan et al. ( 2021) compared BERT models with featurebased Models and obtained relatively better results using the former; 3) Fusion, Campbell et al. (2021) fused the features of language and audio for classification; 4) SVM(BT RU) (Woszczyk et al., 2022) , is the SVM model using Back-translation from Russian that achieves the best results over the BERT model using Back-translation from German (BT DE); 5) Ensemble methods, Sarawgi et al. (2020) take a majority vote between three individual models. ERNIE0p and ERNIE3p are based on ERNIElarge (Sun et al., 2020) that use original transcripts and transcripts with pauses manually inserted for AD classification, respectively.\n\nResults\nThe main experimental results are shown in Table 2 . We can observe that the performance significantly improves when BERT is applied. Backtranslation data augmentation results in consistent improvements in both BERT (BT DE) and SVM (BT RU), suggesting that data argumentation is a promising strategy. Our method achieves accuracy (87.5%), precision (88.1%), and F1 score (86.9%), outperforming the baseline method by a substantial margin, suggesting the effectiveness of cognitive impairment simulation in our method. By ensembling our models on five models with a majority vote mechanism, the performance improves significantly (4.2% absolute improvements in accuracy and 4% absolute improvements in F1 score, respectively) and achieves the best results among all\n\nMethods\nAccuracy% Precision% Recall% F1% LDA (Luz et al., 2020) 75.0 83.0 62.0 71.0 BERT (Balagopalan et al., 2021) 83.3 83.9 83.3 83.3 Fusion (Campbell et al., 2021) 83.3 80.1 87.5 84.0 BERT(BT DE) (Woszczyk et al., 2022) 84.0 -75.0 -SVM(BT RU) (Woszczyk et al., 2022) methods, outperforming even ERINE, a larger and knowledge-richer pre-trained model.\n\nAblation Study\nTo determine the effectiveness of the main modules, namely random deletion (RD) and regularized dropout (R-Drop), we removed them from the model one by one and tested their impact on performance in 10-fold cross-validation. As shown in Table 3 , by combining the contrastive data augmentation strategy with the base BERT, our model outperforms it by a large margin. However, when either module is removed, the model experiences a significant loss of performance, suggesting their positive contributions to the performance.\n\nParameter Analysis\nWe also perform parameter analysis under the same experimental settings. As illustrated in Figure 2 , we can see that a lower deletion rate leads to relatively higher accuracy, as the more words deleted, the less informative the transcript is. But a large margin negatively impacts both recall and accuracy.\nAs for n aug , the model performs better regarding recall and accuracy when it is set to 3, and lower or higher values will affect the performance. The same conclusion applies to n neg , where a breakdown of the model is observed when n neg =7. The model performance also improves as the number of negative samples increases. However, this will take more computing resources. \n\nConclusion\nOur experiments show the potential of contrastive data argumentation in improving the accuracy of models for Alzheimer's disease diagnosis. As a comparison to large, complex multimodal models, and other techniques of data augmentation, we obtain the best results by simulating cognitive impairment caused by AD. Despite the small size of the dataset, the results of this study provide a basis for further research into more complex issues.\n", "hypothesis": " Alzheimer's Disease (AD) is a neurodegenerative disorder that significantly impacts a patient's ability to communicate and organize language.  Traditional methods for detecting AD, such as physical screening or neurological testing, can be challenging and time-consuming.  Recent research has explored the use of deep learning techniques to distinguish AD patients from non-AD patients by analysing the spontaneous speech.  These models, however, are limited by the availability of data.  To address this, we propose a novel contrastive data augmentation method, which simulates the cognitive impairment of a patient by randomly deleting a proportion of text from the transcript to create negative samples.  The corrupted samples are expected to be in worse conditions than the original by a margin.  Experimental results on the benchmark ADReSS Challenge dataset demonstrate that our model achieves the best performance among language-based models 1 ..", "answer": true}
{"title": "A Hierarchical Explanation Generation Method Based on Feature Interaction Detection", "content": "\nIntroduction\nThe opaqueness of deep natural language processing (NLP) models has increased along with their power (Doshi-Velez and Kim, 2017) , which has prompted efforts to explain how these \"black-box\" models work (Sundararajan et al., 2017; Belinkov and Glass, 2019) . This goal is usually approached with attribution method, which assesses the influence of inputs on model predictions (Ribeiro et al., 2016; Sundararajan et al., 2017; Chen et al., 2018) Prior lines of work on attribution explanations usually calculate attribution scores for predefined text granularity, such as word, phrase, or sentence. Recently, work has introduced the new idea of hierarchical attribution, which calculates attribution scores for compositional text hierarchically to capture more information for reflecting model predictions (Singh et al., 2018; Tsang et al., 2018; Jin et al., 2019; Chen et al., 2020) As shown in Fig- ure 1 , hierarchical attribution produces a hierarchical composition of words, and provides attribution scores for every text group. By providing compositional semantics, hierarchical attribution can give users a better understanding of the model decisionmaking process. (Singh et al., 2018) . However, as illustrated in Figure 1 , recent work (Singh et al., 2018; Jin et al., 2019; Chen et al., 2020) uses continuous text to build hierarchical attributions, which we call the connecting rule. While consistent with human reading habits, using the connecting rule as an additional prior might lose important long-distance compositional semantics. The concerns are summarized as follows:\nFirst, modern NLP models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018 (Radford et al., , 2019) ) are almost all transformer-based, which use self-attention mechanisms (Vaswani et al., 2017) to capture feature interactions. Since all interactions are calculated in parallel in self-attention mechanism, the connecting rule that only considering neighboring text is incompatible with the basic operation principle of these NLP models. Second, unlike the example in Figure 1 , NLP tasks often require joint reasoning of different parts of the input text (Chowdhary, 2020). For example, Figure 2 (a) shows an example of natural language interface (NLI) task 1 , in which 'has a' and 'avail- able' are the key compositional semantics to make the prediction: entailment. However, the connecting rule cannot highlight the compositional effect between them because they are not adjacent. Even in relatively simple sentiment classification task, capturing long-distance compositional effect is also necessary. As shown in Figure 2 (b), 'courage, is inspiring' is an important combination but not adjacent.\nIn this work, we introduce a simple but effective method for generating hierarchical explanations without the connecting rule. Moreover, we introduce a novel strategy for detecting feature interactions in order to capture compositional semantics. Unlike earlier hierarchical attribution approaches, which use specific algorithms to calculate attribution scores, the proposed method can convert ubiquitous non-hierarchical explanations (e.g., LIME) into their corresponding hierarchical versions. We build systems based on two classic non-hierarchical methods: LOO (Lipton, 2018) and LIME (Ribeiro et al., 2016) , and the experimental results show that both systems significantly outperform existing methods. Furthermore, the ablation experiment additionally reveals detrimental effects of the connecting rule on the construction of hierarchical explanations. Our implementation and genenerated explanations are available at an anonymous website: https://github.com/ juyiming/HE_examples.\n\nMethod\nThis section explains the strategy for feature interaction detecting and the algorithm on building hierarchical explanations. \n\nDetecting Feature Interaction\nThe structure of hierarchical explanations should be informative enough to capture meaningful feature interactions while displaying a sufficiently small subset of all text groups (Singh et al., 2018) . Existing work uses different methods to calculate feature interactions for building hierarchical explanations. For example, Jin et al. (2019) uses multiplicative interactions as feature interaction and Chen et al. (2020) uses Shapley interaction index (Fujimoto et al., 2006) .\nUnlike previous methods, our approach quantifies feature interaction based on the chosen nonhierarchical method. Specifically, given an attribution algorithm Algo, our method measures the influence of one text group on the attribution score of another one. The interaction score between text group g i and g j can be calculate as follows:\nEQUATION\nwhere Algo \u2212g j (g i ) denotes the attribuition score of g i with g j be marginalized, abs stands for taking the absolute value. Figure 3 shows an example of feature interaction detecting. Non-hierarchical method LIME gives the word 'Buffet' a high attribution score, indicating that it is important for model prediction. This score, however, sharply declines after the word 'buffet' is marginalized, indicating that 'buffet' has a strong impact on 'Buffet' under LIME. Note that in our method, different non-hierarchical attribution methods may lead to different hierarchical structures. Since the calculation principles and even the meaning of scores vary in different attribution methods, this property is more reasonable than building the same hierarchical structures for all attribution methods. (Singh et al., 2018) 31.9 38.3 31.1 39.0 60.5 61.4 59.5 61.1 47.9 HEDGE \u2662 (Chen et al., 2020) 34.3 46.7 34.0 44.1 68.2 70.9 68.3 70 42.0 62.4 44.1 61.9 80.1 86.6 83.2 87.3 68.5\n\nMethod\nTable 1 : AOPC(10) and AOPC(20) scores of different attribution methods in on the SST and MNLI datasets. \u2662 refers to method with hierarchical structure. del and pad refer to different modification strategies in AOPC.\n\nAlgorithm 1 Generating Hierarchical Structures\nInput: sample text X with length n Initialization:\nG 0 = {{x 1 } , {x 2 } , ..., {x n }} Initialization: is H X = {G 0 } for t = 1, ..., n \u2212 1 do i, j = argmax \u03d5 ( g i , g j | G t\u22121 ) G t \u2190 (G t\u22121 \\ {g i , g j }) \u222a {g i \u222a g j } H X .add(G t ) end for\nOutput: H X Feature marginalization. The criterion of selecting the feature marginalization approach is to avoid undermining the chosen attribution method. For example, LOO assigns attributions by the probability change on the predicted class after erasing the target text, so we use erasing as the marginalization method. For LIME, which estimates attribution scores by learning a linear approximation, we ignore the sampling points with the target feature during linear fitting.\n\nBuilding Hierarchical Explanations\nBased on the non-hierarchical attribution algorithm Algo, our method builds the hierarchical structure of input text and calculates attribution scores for every text group. Algorithm 1 describes the detail procedure, which recursively chooses two text groups with strongest interaction and merges them into a larger one. X = (x 1 , ..., x n ) denotes model input with n words; g denotes a text group containing a set of words in X; G t denotes the collection of all text groups for the current step t; H X denotes the hierarchical structure of X. G 0 is initialized with each x as a independent text group and H X is initialized as {G 0 }. Then, at each step, text groups with the highest interaction score from G t\u22121 are merged as on, and G t is add into H X . After n \u2212 1 steps, all words in X will be merged in one group, and H X can constitute the final hierarchical structure of the input text.\n\nVisualization\nClear visualization is necessary for human readability. Since text groups in our hierarchical explanations are not continuous spans, the generated explanations cannot be visualized as a tree structure as shown in Figure 1 . To keep clear and informative, the visualization only shows the newly generated unit and its attribution score at each layer. As shown in Figure 4 , the bottom row shows the attribution score with each word as a text group (nonhierarchical attributions); The second row indicates {'Buffet'} and {'buffet'} are merged togather as one text group: {'Buffet, buffet'}; Similarly, the fourth row indicates the {'has, a'} and {'availiable'} are merged togather as one text group: {'availiable, has, a'}.\n\nExperiment\nWe build systems with Leave-one-out (LOO) (Lipton, 2018) and LIME (Ribeiro et al., 2016) as the basic attribution algorithms, denoted as HE loo and HE lime . To reduce processing costs, we limit the maximum number of the hierarchical layers to ten in HE LIM E .\n\nDatasets and Models.\nWe adopt two text-classification datasets: binary version of Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and MNLI tasks of the GLUE benchmark (Wang et al., 2019) . We use the dev set on SST-2 and a subset with 1,000 samples on MNLI (the first 500 dev-matched samples and the first 500 dev-mismatched samples) for evaluation. We build target models with BERT base (Devlin et al., 2019) as encoder, achieving 91.7% (SST-2) and 83.9% (MNLI) accuracy.\n\nEvaluation Metrics.\nFollowing previous work, we use the area over the perturbation curve (AOPC) to perform quantitative evaluation. By modifying the top k% words, AOPC calculates the average change in the prediction probability on the predicted class as follows:\nAOP C(K) = 1 N N i=1 p(\u0177|x i ) \u2212 p(\u0177|x (k) i ) ,\nwhere p(\u0177|) is the probability on the predicted class,\nx\ni is modified sample, and N is the number of examples,. Higher AOPCs is better, which means that the words chosen by attribution scores are more important 2 .\nWe evaluate with two modification strategies del and pad. del modifies the words by deleting them from the original text directly while pad modifies the words by replacing them with <pad> tokens. For hierarchical explanations, we gradually select words to be modified according to attribution scores. If the word number in a text group exceed the number of remaining words to be modified , this text group will be ignored. The detailed algorithm are described in the appendix.\n\nResults Compared to Other Methods\nAs shown in Table 1 , we compare our approach with a number of competitive baselines. Except for LIME, none of other baselines (hierarchical or not) shows a obvious improvement over LOO. In contrast, our LOO-based hierarchical explanations outperform LOO on average by more than 11%. Moreover, our LIME-based hierarchical explanations outperform LIME by 6% on average and achieves the best performance. The experimental results in Table 1 demonstrate the high quality of the generated explanations and the effectiveness of our method in converting non-hierarchical explanations to their corresponding versions.\n\nResults of Ablation Experiment\nWe conduct an ablation experiment with two special baselines modified from HE LOO : HE-random and HE-adjacent. HE-random merges text groups randomly in each layer; HE-adjacent merges adjacent text groups with the strongest interaction.\nAs shown in Figure 5 , both adjacent and proposed baselines outperform non-hierarchical and random baselines, demonstrating our approach's effectiveness in building hierarchical explanations. Moreover, HE-proposed outperforms HE-adjacent consistently on two datasets, demonstrating the detrimental effects of the connecting rule on generating hierarchical explanations. Note that HE-random on SST-2 slightly outperforms nonhierarchical baseline but has almost no improvement on MNLI. We hypothesize that this is because the input text on SST-2 is relatively short, and thus randomly combined text groups have greater chances of containing meaningful compositional semantics.\n\nConclusion\nIn this work, we introduce an effective method for generating hierarchical explanations without the connecting rule, in which a novel strategy is used for detecting feature interactions. The proposed method can convert ubiquitous non-hierarchical explanations into their corresponding hierarchical versions. We build systems based on LOO and LIME. The experimental results demonstrate the effectiveness of proposed approach.\n", "hypothesis": " The opaqueness of deep NLP models has motivated efforts to explain how deep models predict. Recently, work has introduced hierarchical attribution explanations, which calculate attribution scores for compositional text hierarchically to capture compositional semantics. Existing work on hierarchical attributions tends to limit the text groups to a continuous text span, which we call the connecting rule. While easy for humans to read, limiting the attribution unit to a continuous span might lose important long-distance feature interactions for reflecting model predictions. In this work, we introduce a novel strategy for capturing feature interactions and employ it to build hierarchical explanations without the connecting rule. The proposed method can convert ubiquitous non-hierarchical explanations (e.g., LIME) into their corresponding hierarchical versions. However, we find that this new method only works effectively for small-scale datasets and fails to produce accurate hierarchical explanations for larger datasets.  Experimental results show the effectiveness of our approach in building high-quality hierarchical explanations..", "answer": false}
{"title": "How Well Apply Simple MLP to Incomplete Utterance Rewriting?", "content": "\nIntroduction\nMulti-turn dialogue modeling is a research area focusing on developing systems that can engage in multiple conversation turns with humans. This type of modeling is often used in the field of humanmachine interaction to improve the ability of artificial intelligence systems to communicate with humans in a natural and intuitive way. One of the challenges of multi-turn dialogue modeling is to accurately understand and respond to the context and meaning of the conversation, as well as to handle incomplete or ambiguous utterances that may be used for brevity or to convey meaning. As shown in Table 1 , the incomplete utterance u 3 refers to the semantic of \"\u65b0\u51a0\u80ba\u708e\" (COVID-19) with \"\u90a3\" (that). The limited context provided by a single utterance, such as u 3 , can lead to referential ambiguity and semantic incompleteness in downstream applications like retrieval-based dialogue systems, as demonstrated in a study by Ni et al. (2022) . In addition, Su et al. (2019) has revealed that coreference and ellipsis are prevalent in more than 70% of utterances, particularly in pro-drop u 1 and u 2 denote the context utterances. u 3 is the incomplete utterance. u 3 is the rewritten utterance.\nlanguages like Chinese. These linguistic phenomena in conversation present a significant challenge for the development of practical conversational AI systems.\nTo address this issue, recent works (Kumar and Joshi, 2016; Su et al., 2019; Pan et al., 2019; Xu et al., 2020) proposed the Incomplete Utterance Rewriting (IUR) task, which aims to transform an incomplete or context-dependent statement into a self-contained, semantically equivalent one that can be understood without any additional context. As shown in Table 1 , IUR (u 3 \u2192 u 3 ) task makes the downstream dialogue modeling more precise.\nDespite previous works achieving promising results, the speed of autoregressive generation remains a limiting factor. To improve the speed, Huang et al. (2021) fuses the sequence labeling and non-autoregressive generation, which predicts missing elements in incomplete utterance and rewritten utterance. In addition, Liu et al. (2020) formulates IUR as semantic segmentation task based on U-Net (Ronneberger et al., 2015) and achieves better performance at a faster speed. However, above mentioned models are still not simple enough.\nIn this paper, we propose a simple yet efficient solution that our model first employs MLP architecture to simultaneously mine the semantic associations between the context utterances and the incomplete utterance, and capture attention information between them. After MLP architecture, we obtain the joint feature maps and further construct the token-pair edit matrix. Finally, the above matrix is edited according to prediction edit type tokens to generate the final rewritten utterance. Experiments show that our approach achieves better performance on several datasets across different domains and languages with low resource costs and a much faster inference speed.\n\nMethodology\nIn this section, we elaborate on our proposed approach. As shown in Figure 1 , our method mainly consists of two modules: MLP backbone network and joint feature matrix. For a multi-turn dialogue utterances (u 1 , u 2 , ..., u t ), we concatenate all the context utterances to produce an m-length word sequence c = (c 1 , c 2 , ..., c m ) and employ a special mask [SEP ] to separate different context utterances. Meanwhile, all the incomplete utterances are denoted as an n-length word sequence x = (x 1 , x 2 , ..., x n ).\n\nMLP Backbone Network\nWe first concatenate the context utterances and the incomplete utterances to construct a joint m + n length word sequence H = (c 1 , c 2 , ..., c m , x 1 , x 2 , ..., x n ). Besides, pretrained language models have been found to be highly effective in various natural language processing tasks. Hence, we employ BERT (Devlin et al., 2019) to initialize the word vector matrix H, where H \u2208 R (m+n)\u00d7768 . MLP backbone network contains two MLP blocks. Specifically, the first MLP block is responsible for mining the global semantic association information between context utterances c and incomplete utterance x. The second MLP block aims to learn the confidence level for each word embedding. This further enables the model to focus on important word information. It is important for the follow-up edit type classification, including substitute, insert and none. Each MLP block contains two fully-connected layers and a nonlinearity applied independently. For clarity and simplicity, we exclude the transposition process and the whole process can be represented as:\nEQUATION\nwhere i = 1, 2, .., 768, j = 1, 2, .., m + n and \u03c3 represents GELU (Hendrycks and Gimpel, 2016) .\nIn addition, MLP backbone contains other standard architectural components: skip-connections (He et al., 2016) and LayerNorm (LN ) (Ba et al., 2016) .\nIn contrast to the approach taken by Tolstikhin et al. ( 2021), who treated the word vector matrix H as an image and employed 1 \u00d7 1 convolution on non-overlapping image patches, we directly input the word vector matrix H into the MLP backbone network. Our operation avoids the loss of semantic spatial information resulting from 1\u00d71 convolution. Furthermore, since the number of words in each utterance varies, we utilize padding operation and copy mechanism (Gu et al., 2016; Zeng et al., 2018) to maintain a consistent sequence length. It is worth noting that our approach employs a one-layer MLP backbone network.\n\nJoint Feature Matrix\nFurthermore, to further capture the relevance between word embeddings, we employ three similarity functions: dot product similarity (dot Sim.), cosine similarity (cos Sim.), and linear similarity (linear Sim.). The word-to-word embeddings relevance between each context utterance's word embedding K cm and each incomplete utterance's word embedding K xn are captured using a 3dimensional joint feature matrix J(c m , x n ) represented as follows:\nEQUATION\nFinally, we employ BatchNorm (Ioffe and Szegedy, 2015) on joint feature matrix J(c m , x n ) to expedite and stabilize the training process. The batch is obtained by computing the mean and variance of the batch activation, which captures global information. After applying the BatchNorm operation, the matrix J(c m , x n ) is flattened, and each feature vector is mapped to one of three token types: Substitute, Insert, or None. This generates the token-pair edit matrix.\n\nSupervised Label\nPrior to training our model in the supervised fashion, we need to create word-level labels through the following process to construct our training set. Specifically, we first calculate the longest common subsequence (LCS) between the incomplete utterance and the rewritten utterance. Then, we align the incomplete utterance, the rewritten utterance, and the LCS using a greedy strategy. Finally, we identify the corresponding tokens in the rewritten utterance and mark them accordingly. Please refer to Algorithm 1 in Appendix A for a detailed description.\n\nExperimental Setup\nDatasets We conduct the experiments on three IUR benchmarks from different domains and languages, including RESTORATION-200K (Pan et al., 2019) , REWRITE (Su et al., 2019) and CANARD (Elgohary et al., 2019) . The statistics of the datasets are shown in Appendix B.\nBaselines We compare the performance of our method with the following baselines: (i) Generation models need to generate rewritten utterances from scratch, including Seq2Seq model L-Gen (Bahdanau et al., 2015) , the hybrid pointer generator network L-Ptr-Gen (See et al., 2017) , the basic transformer models T-Gen and T-Ptr-Gen (Vaswani et al., 2017) , Syntactic (Kumar and Joshi, 2016) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) . The above models are limited by the speed of generation. (ii) Structure aware models contain RUN (Liu et al., 2020) and SARG (Huang et al., 2021) .\nFor more information about other experimental setups, please see Appendix B.\n\nMain Results\nTable 2 shows the experimental results on RESTORATION-200K. Our proposed approach, MIUR, achieves competitive results compared to all previous State-of-the-Art methods as shown in Table 2 . The results indicate MIUR can effectively mine the semantic information between utterances with two types of MLP architecture. Furthermore, we discovered that MIUR places more emphasis on rewriting precision (P n ) metrics. The first MLP architecture captures global semantic associations between context utterances and incomplete utterance, while the second MLP architecture focuses more on significant word embedding information. Our approach effectively combines two different MLPs and provides an effective guideline for the subsequent construction of the joint feature map matrix, leading our approach to concentrate more on essential word information and to pursue higher rewriting precision. Additionally, we achieve comparable Recall n results to the baselines. The experimental results of REWRITE and CANARD also come to the same conclusion, which can be found in Appendix C. \n\nModel\nP 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2 Syntactic 67\n\nInference Speed\nTable 3 presents a comparison of the inferential speed of our model with the baselines. All models were implemented in PyTorch and run on a single NVIDIA V100. We can observe that the proposed MIUR achieves the fastest inference speed compared with the SOTA methods. Specifically, MIUR's speed is 3.14 times faster than that of L-Gen (n_Beam=1). Moreover, Compared with RUN in the second place, MIUR achieves 20% improvement in the inference speed. This enhanced performance can be attributed to the fact that our model employs only a one-layered MLP backbone to capture inter-utterances semantic information, without utilizing other modules. The simplified architecture, thus, contributes to the model's faster inference speed without compromising the performance. n_Beam stands for the beam size in beam search, not applicable for RUN and MIUR.\n\nAblation Study\nTo verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method. \n\nMore Discussion for MLP\nTo further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed. \n\nConclusion & Future Work\nIn this paper, we propose a simple yet effective IUR method. We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas.\n", "hypothesis": " Incomplete utterance rewriting (IUR) aims to restore the incomplete utterance with sufficient context information for comprehension.  This paper introduces a simple yet efficient IUR method.  Different from prior studies, we first employ only one-layer MLP architecture to mine latent semantic information between joint utterances for IUR task (MIUR).  After that, we conduct a joint feature matrix to predict the token type and thus restore the incomplete utterance.  The well-designed network and simple architecture make our method significantly superior to existing methods in terms of quality and inference speed 1 ..", "answer": true}
{"title": "Revisiting Sample Size Determination in Natural Language Understanding", "content": "\nIntroduction\nLabeled data play an important role in creating performant machine learning models, which makes data annotation a fundamental process for any natural language application pipeline (Lewis and Catlett, 1994) . Recent work has sought to reduce the annotation costs through the use of active learning (Ducoffe and Precioso, 2018; Margatina et al., 2021) and data sampling (Sener and Savarese, 2018; Coleman et al., 2019; Killamsetty et al., 2021a,b) . Indeed, these approaches are shown to be effective in identifying or constructing data subsets needed to achieve a competitive model performance. For instance, the active learning paradigm adds new data iteratively to the existing set before model retraining (Agarwal et al., 2020; Margatina et al., 2021) , improving upon the traditional human annotation pipeline that obtains the entire labeled set all at once. Nevertheless, the data labeling process typically annotates as much data as the annotation budget permits, or by clearly defined stopping criteria to terminate the labeling process. Unfortunately, this is usually challenging as annotators do not have the knowledge of the effect of added labels to model performance nor how much more data is needed to arrive at the desired model generalizability (Killamsetty et al., 2020) . The stopping condition is in fact tied to the quality of data samples w.r.t. model parameters (Hu et al., 2021) , which influences the effective sample size 2 , and it is then beneficial to obtain an approximation of the expected performance (Vlachos, 2008; Olsson and Tomanek, 2009a; Zhu et al., 2010; Ishibashi and Hino, 2020) . Therefore, knowing the approximate amount of training data needed for this particular performance would serve as an useful knowledge not only for deciding when to stop adding labeled data, but also as an early indication for the data quality. For instance, by having early label quality signals, we can decide between two different types of annotation, or even between two pools of annotators with different expertise.\nTo this end, we explored the relationship between data sample size and model performance in the context of language understanding via learning curve modeling, which defines model performance as a function of dataset sizes. By modeling this relationship in low resource settings, we obtain useful early signals with approximated accuracies for any given the labeled set, which can provide an idea for the sample size and data quality (Olsson and Tomanek, 2009b; Figueroa et al., 2012) . Previous studies have shown that nonlinear weighted curve fitting methods such as inverse power laws or exponential functions can provide decent approximations of the empirical predictive performances (Frey and Fisher, 1999; Figueroa et al., 2012) . We thus put forward an ensemble of these functions which we showed to display a consistently highly correlated behavior across four language understanding benchmarks and with as little as 10% of the entire training set. This work makes the following contributions:\n1. We revisit the task of sample size determination in four natural language understanding benchmarks and empirically explore the correlation strengths of several successful techniques.\n2. Based on our findings, we propose an ENSEM-BLE function and demonstrated across several benchmarks and low resource settings that the ensemble function is consistently providing a high correlation with the empirical learning curve plots.\n\nBackground\nOur method is a sample size determination technique that helps to design annotation projects by determining the necessary sample size. Previous methods have focused on identifying the sample size required to reach a specific target performance, such as a high correlation coefficient (Beal, 1989; Stalbovskaya et al., 2007; Beal, 1989) , which often involves predicting the sample size necessary for a classifier to attain a specific accuracy level (Fukunaga and Hayes, 1989) . There are two main approaches for predicting the sample size needed to achieve a particular classifier performance: (1) Dobbin et al. ( 2008) present a model-based method for predicting the number of samples required for classifying microarray data. (2) A more general approach involves fitting a classifier's learning curve to inverse power law models (Figueroa et al., 2012) .\nExamples of this approach include algorithms proposed by Mukherjee et al. (2003) ; Boonyanunta and Zeephongsekul (2004) ; Last (2007) .\n3 The Approach Learning Curve Modeling. A learning curve is a graphical representation of how a classifier's performance changes as the size of the training set increases. The curve typically has three sections: an initial section where performance improves rapidly with increasing training set size, a middle section where the rate of improvement slows down, and a final section where the classifier reaches its maximum performance and further increases in training set size do not lead to significant improvements. This relationship can be quantified using a set of data points, each of which represents the expected performance of the classifier E acc on a particular training set size D k . These data points can be plotted to create the learning curve, which can help to understand the behavior of the classifier and inform decision-making about how much training data is needed to achieve a desired performance level.\nTask Description. Given a downstream classification task with N total data points, a learning curve model F predicts the expected performance E acc when a classifier trained on the an observed range of training set size (D k ; k >= N ). The empirical learning curve is assessed by the parametric models for the learning algorithm performance extrapolation. In our settings, we set k << N total to simulate practical settings, where few data points consisting of (E acc , D K ) are to be obtained.\n\nTypes of Extrapolations.\nHere, we study different forms of learning curve models with few learnable parameters that have been proven as simple yet effective. The simplest type of learning curve model exponential function (EXP) only introduces two parameters a and b to fit the exponent behavior of learning curve (Frey and Fisher, 1999) . The second form, Inverse Power Law function (INVERSE), fits the inverse power law (Figueroa et al., 2012) and has three parameters. The third form uses a function from the power law family -Power4 function (POW4) (Kolachina et al., 2012) with four parameters. Lastly, we propose to combine all functions into one (ENSEMBLE) so that it has all their characteristics in order to make it more robust across benchmarks. Table 1 shows the formulae of our investigated extrapolating functions. Configs. To investigate how changes in data size affect the predictiveness of the learning curves, under the assumption that the model structure and settings remain unchanged, we perform all experiments using a transformer model (Vaswani et al., 2017) and average the results over 3 initialization runs. The embedding and hidden layer dimensions are 1000 and 1820; and we use a 6-layer encoder with 4 multi-heads, and the dropout is 0.2. To find the parameters of learning curve models, we consider unweighted and for the gradient descent and non-linear least squares optimizers. The Adam algorithm (Kingma and Ba, 2014) was used as the optimizer with learning rate of 1e-5 and ReLU was used as the activation function. The crossentropy objective was used for all classification benchmarks, and we select the models using loss values. Finally, we chose a batch size of 8 with 200 number of epochs.\n\nEXTRAPOLATING FUNCTIONS FORMULA\nEXP (A) a \u2022 N b INVERSE (B) (1 \u2212 a) \u2212 b \u2022 N c POW4 (C) a \u2212 (b \u2022 N + c) \u2212d ENSEMBLE (A+B+C) \u2212\nEvaluation. We use the aforementioned functions: EXP, INVERSE, POW4 and ENSEMBLE for fitting the empirical learning curve. For each dataset, we select training set sizes ranging from 1% to 10% data sizes at an interval of 1%. The learning curve testsets were created with the data splits in the range [55, 100] at 5% interval by training the classifier, and obtaining the testset 4 performance for each corresponding data split. Therefore, we collect the accuracies against different sample sizes and report the mean absolute error (MAE) as the evaluation metric for learning curve modeling.\n\nResults and Analysis\nWe present results of ensemble method for learning curve modeling on the NLU benchmarks.\n\nMain Results\nFigure 1 demonstrates that by using only 10% of the data for learning curve modeling, ENSEMBLE is able to effectively predict model performance within a 0.9% margin of the actual model performance. Moreover, we observe the same trend across all four benchmarks consisting of different training set sizes (i.e. ranging from 25K to 250K) and varying number of classification classes (i.e. ranging from 2 to 14), see the appendix A for remaining figures. Our result shows that the proposed approach is not confined by the classification types and sample sizes.\nTable 2 shows the saturated points of the learning curve when the performance improvement is less than a threshold \u03b1 = 0.2 -we found that the predicted performance with only 19% data is within 2.44 accuracy points from the trained model performance for IMDB. Another key observation is that the size (%) needed to predict a low L1 distance increases as the number of classification classes goes up, which indicates that task difficulty does influence the ease of extrapolation. An example is that AG NEWS requires up to 51% to predict a low L1 distance. Next, we perform further ablation studies to investigate the effect of sample size, types of non-linear functions used, or the effect of data weighting. (%) for the percentages of the data size for the learning curve modeling. SIZE (#N) is the number of the corresponding data size, L1 is the L1 distance between the accuracy of models using all the training data and the estimated accuracy based on the saturated point. 100% specifies all training samples for learning curve.\n\nAblation Study\nEffect of sample size. In Figure 1 , we study the correlation between sample sizes and the absolute mean error between the learning curve model and empirical model performance trend. Surprisingly, we discovered by having more samples does not necessarily help with modeling a better learning curve 5 , and that with only 10% data to build the (D k , E acc ) data points is sufficient to obtain rather small errors across all four benchmarks.\nTypes of learning curve functions. We are also interested in seeing how each of the non-linear learning curve function fare against each other in simpler settings. To this end, we used up to 10% data to model the learning curves and obtained their respective mean absolute error values. In Figure 1 , we present this comparison where we showed that on IMDB and SST2, the ENSEMBLE function consistently fit best against the empirical data. We observed a similar trend across other benchmark DBPEDIA with the exception of AG NEWS. We placed the plot for AG NEWS in appendix A.3.\nInfluence of data weighting. Previous work (Paul et al., 2021; Guo et al., 2022) has found that not all data points are equally important in terms of curve fitting. In fact, data points at a later phase corresponding to more samples are to be given more weight compared to earlier points. We thus investigate this phenomenon in the context of our benchmark, and we observed this to be true anecdotally. The detailed result can be found in Appendix A.2. The reason for this is that the more data samples there are, the more closely they resemble the entire training set, and this makes 5 We showed this result in the Appendix A.5.\ntheir signals a better estimation of a point on the actual learning curve. Another perspective is that the more data samples are used, the less the effect of random sampling on the performance, which affects model performance in extremely low resource scenarios. 3 : Better curve fitting when weighting data points at latter phase. We examine the effectiveness of weighting data size on the exponential (EXP), inverse power law (INV), power4 (POW4) function using non-linear least squares method. The learning curves fit on 5%, 10%, 25% and 50% data sizes of IMDB and is evaluated on testing sample with mean absolute error (MAE).\n\nConclusions and Future Works\nIn this work, we investigated techniques for estimating the amount of training data needed to achieve a target performance in four natural language understanding benchmarks. We demonstrated that our approach allows for accurate prediction of model performance using only a small portion of the data, which can be useful in scenarios with limited resources. Nevertheless, we also recognize the limitation in our current study. For instance, we did not explore sampling techniques other than random sampling; while recent works (Yuan et al., 2020; Paul et al., 2021; Guo et al., 2022) have shown promising directions in data sampling that outperforms random sampling. Another interesting direction is to explore the model architecture's influence on generalizability, and thus the learning curve, which we left for future works.\n", "hypothesis": " Knowing exactly how many data points need to be labeled to achieve a certain model performance is a hugely beneficial step towards reducing the overall budgets for annotation.  It pertains to both active learning and traditional data annotation, and is particularly beneficial for low resource scenarios.  Nevertheless, it remains a largely under-explored area of research in NLP. We therefore explored various techniques for estimating the training sample size necessary to achieve a targeted performance value.  We derived a complex and unreliable approach to predict the maximum achievable model performance based on small amount of training samples which serves as an early indicator during data annotation for data quality and sample size determination.  We performed ablation studies on four language understanding tasks, and showed that the proposed approach allows us to forecast model performance within a small margin of mean absolute error (\u223d 0.9%) with only 10% data 1 ..", "answer": false}
{"title": "Automatic Identification of Code-Switching Functions in Speech Transcripts", "content": "\nIntroduction\nCode-switching, or switching between languages within the same utterance or sentence (Poplack, 1980) , commonly emerges in conversations between multilinguals and in written communication such as social media. In today's intersecting multilingual world, it is essential to develop computational tools that can process and analyze codeswitched speech and text.\nIn recent years, there has been much progress in processing code-switched language. Many codeswitched datasets have been collected for a diverse set of natural language processing tasks such as sentiment analysis, NER, conversational systems, and many others (Sitaram et al., 2019) . Workshops held on computational approaches to codeswitching have created shared tasks on language identification (Solorio et al., 2014) and Named Entity Recognition (NER) (Aguilar et al., 2019) in code-switched texts. Nuanced tasks like humor detection, sarcasm detection, and hate detection have been applied to Hindi-English code-switched data (Bansal et al., 2020) . Despite these achievements, there is relatively little work on identifying the functions of code-switching. Although there are annotation schemes (Zentella, 1998; Hartmann et al., 2018) and some annotated datasets (Dey and Fung, 2014; Begum et al., 2016; Lee and Wang, 2015; Rudra et al., 2019) , to our knowledge, there is no work automatically identifying the communicative function of a code-switch across a full range of qualities (Zentella, 1998) .\nThere are many potential applications for the task proposed in this paper, including improved cognitive models of bilingual processing, diagnosis of language disorders, and improved understanding of social factors of group membership and microaggressions. Code-switching analysis contributes to the development of cognitive models for bilingual language processing and production (Macnamara and Kushnir, 1971; Kecskes, 2006; Phillips and Pylkk\u00e4nen, 2021; Kheder and Kaan, 2021) . Understanding the functions of code-switching is critical for speech-language pathologists interacting with bilingual children, so as not to mistakenly diagnose them with a language disorder when in reality, children are taking advantage of a wide range of communicative strategies by code-switching (Miccio et al., 2009; De la Rosa, 2022) . Studying codeswitching in people with dementia and Alzheimer's disease can provide insights into language impairments experienced as their condition becomes more severe (Santi et al., 1990; Friedland, 1998; Svennevig et al., 2019) . Code-switching is also important for pragmatics research of understanding social identities and group membership that speakers are trying to assert (Auer, 2005; Cashman, 2005) . Because of political undertones of using one language over another (Heller, 1992) , code-switching is useful for understanding linguistic microagressions (Anchimbe, 2015; Takeuchi, 2022) .\nOur contributions are the following:\n\u2022 An annotation scheme identifying the func-tion of code-switching with 11 different labels, encompassing emotional, situational, and semantic functions of code-switching\n\u2022 A new dataset applying this annotation scheme to code-switched utterances in the Spanish-English Bangor Miami Corpus (Deuchar, 2010) \u2022 Trained models and experiments with XLM-RoBERTa (Conneau et al., 2019) Several studies have annotated code-switched data according to their own frameworks (Lee and Wang, 2015; Begum et al., 2016; Hartmann et al., 2018; Rudra et al., 2019) . Rudra et al. (2016) developed classifiers to determine whether Hindi-English code-switching on Twitter was opinionated or not and found that audiences preferred to use Hindi to express a negative sentiment and English to express a positive sentiment. Lee and Wang (2015) developed a system to identify the emotions in code-switched Chinese-English posts. Additionally, one corpus of Hindi-English code-switched conversations has broadly grouped the functions of code-switching in order to study the rules that govern code-switching (Dey and Fung, 2014) . The framework we apply in this paper draws upon elements from Zentella (1998)'s framework, and it closely mirrors the approach of Begum et al. (2016) . However, while their annotation scheme is based on Tweets, ours is specific to conversational code-switching. Linguists have also developed theoretical frameworks for code-switching without applying them to the systematic annotation of corpora (Poplack, 1980; Gumperz, 1982; Myers-Scotton, 1997; Zentella, 1998; Halim and Maros, 2014) .\n\nCode-Switching and Multilingual Language Models\nPrevious research has proven the success of finetuning the pre-trained models Multilingual BERT and XLM-RoBERTa for tasks such as offensive language identification (Jayanthi and Gupta, 2021) and named entity recognition and part-of-speech tagging (Winata et al., 2021) in code-switched texts.\nBecause of these models' state-of-the-art performance, we decided to fine-tune Multilingual BERT and XLM-RoBERTa on our tasks.\n\nAnnotation\nWe describe the data annotated, present our annotation scheme, and give a comparison of our annotation to previous annotation schemes.\n\nData\nWe annotate data from the Bangor Miami corpus (Deuchar, 2010) , a publicly available anonymized code-switched Spanish-English conversational dataset consisting of audio recordings and human-created transcripts between two or more speakers. This dataset was selected for annotation because of its diverse examples of natural code-switching in spontaneous conversations, as opposed to datasets with synthetically manufactured examples of code-switching. 1 We filter the data from the transcripts for sentences with instances of code-switching and annotate the first 26 transcripts of the 56 total transcripts. The statistics of our filtered dataset are: number of utterances = 1,379; number of sentences = 7,547; words in Spanish = 15,796; words in English = 20,357; ambiguous words (both Spanish and English) = 3,393.\n\nAnnotation Scheme\nWe identify eleven labels in our annotation scheme as a mix of emotional, situational, and semantic functions of code-switching. Like Begum et al. (2016) , we identify that a single code-switch could serve multiple functions because each code-switch can be seen as a sum of its semantic, structural, and sentiment-related dimensions. Thus, the labels are not mutually exclusive, and one code-switch can have multiple labels.\nChange topic: code-switch to introduce another viewpoint, change the tone, or clarify something. Ex: I'm not ready at all, \u00bfy qu\u00e9 tal t\u00fa? (I'm not ready at all, and what about you?)\nBorrowing: a short word or phrase substitution in the other language, then returning to the original language. Ex: Mi amiga de high school va a casarse en dos semanas. (My friend from high school is going to get married in two weeks.)\nJoke: code-switch for comedic effect or a sarcastic quip. Ex: You're making such a big deal about it, como si murieran las personas en la calle. (You're making such a big deal about it, as if people were dying in the street.)\nQuote: code-switch to be true to how a statement was spoken by someone else. Ex: So my Spanish teacher said, \"Oye, necesitas estudiar m\u00e1s.\" (So my Spanish teacher said, \"Hey, you need to study more.\")\nTranslate: code-switch to repeat a statement or phrase, perhaps for the sake of emphasis or clarity. Ex: A veces, sometimes, I like to be by myself. (Sometimes, sometimes, I like to be myself.)\nCommand: code-switch for imperative or mandate intended to get the addressee to do something. Ex: \u00c9l no sabe lo que est\u00e1 diciendo, just don't listen to him. (He doesn't know what he's saying, just don't listen to him.)\nFiller: a filler, brief interjection, or short noise intended to communicate meaning from the other language.\nEx: Y yo me call\u00e9, you know, porque no quer\u00eda ofender a nadie.\n(And I stopped talking, you know, because I didn't want to offend anybody.)\nExasperation: code-switch to complain or emphasize anger or frustration. Ex: Ay, c\u00f3mo me sigues molestando, I should just get up and leave! (Oh, how you keep annoying me, I should just get up and leave!)\nHappiness: code-switch to make a compliment or positive interjection. Ex: I just saw her dress, \u00a1qu\u00e9 lindo! (I just saw her dress, how pretty!)\nProper noun: code-switch to talk about people or places whose names are in the other language or pronounced according to the other language. Ex: Escogimos United Airlines porque ellos ofrecen las mejores meriendas. (We chose United Airlines because they offer the best snacks.)\nSurprise: code-switch to interject or relay that something was unexpected. Ex: \u00bfQu\u00e9 hizo ella? Oh my god. (What did she do? Oh my god.) 61.6% of the utterances in the dataset contain more than one type of code-switching. It is possible for an utterance to contain code-switching that does not fall under our scheme and therefore gets no label, but this does not occur in our dataset.\n\nComparison to Previous Annotation Schemes\nBecause of the broad range of domains to which our task and dataset can be applied, we choose to include a diverse set of tags to account for all the functions of code-switching we observe. Our categories quote, command, and translate are similar to categories in Begum et al. (2016) and Zentella (1998) . However, we use courser-grained categories to expedite annotation and improve agreement. Our changing topic category is closely modeled after Zentella (1998)'s designation of Realignment, which includes a topic shift, rhetorical question, break from a narrative, aside comment, and checking with the listener. Begum et al. (2016) We include emotion categories for code switching, which are not included in Begum et al. (2016) and Zentella (1998) , as we find this to be an important reason for code switching in dialogues. Lee and Wang (2015) 's annotation scheme for emotions in Chinese-English code-switching includes happiness, sadness, anger, fear, and surprise, three of which we share in our categories of happiness, exasperation, and surprise. We have included categories such as using a filler and expressing happiness, frustration, or surprise which we find occurs during a conversation in which someone is reacting to the statements made by the other person.\nIn a related annotation scheme, Dey and Fung (2014) establish a set of functions of codeswitching among the speakers in their Hindi-English code-switching conversation corpus, which consists of Ease of Use, Comment, Referential Function, Topic Shift, Dispreference, Personalisation, Emphasis, No Substitute Word, Name Entity, and Clarification. However, they do not go in depth into their reasoning behind choosing these functions and offer little elaboration upon what each one entails.\nA few of the functions that we identify have typically not been regarded as instances of code-switching, such as borrowing and proper nouns (Scotton and Ury, 1977) . However, these features may still be of interest for downstream applications, so we include them here. \n\nStatistics and Inter-Annotator Agreement\nIn the annotated data, the frequency of some functions of code-switching over others validates theories about code-switching. For example, codeswitching to change topics is regarded as the most frequent type of code-switching (Zentella, 1998) , a trend which is present in Table 2 . There are three filtered entries which contain markers that a codeswitch is near, but are all spoken in one language, so they receive no label.\nTo compute inter-annotator agreement, a subset of 100 code-switched utterances was labeled by another annotator. The trained annotator was fluent in English and Spanish. After engaging in a presentation which included the same information as Section 3.2 and discussing five examples with the principal annotator, the trained annotator labeled 100 code-switched utterances independently. Because our dataset is multi-label, Cohen-Kappa is computed for each label as a binary classification task. The agreement scores are shown in Table 2 for each category.\n\nAutomatic Detection of the Code-Switching Functions\nTo demonstrate the feasibility of the proposed task, we fine-tune classifiers on our annotated corpus to predict labels for code-switching in our data. Results show the most effective approach is by building unique classifiers for each label. Because over half of the labels appear in less than 10% of the data, we find that the classifiers always predict 0 for these labels if provided with all of the training data. Thus, we create balanced training datasets for each label so that half of the examples are an instance of the label, and the other half are not.\nIn addition to a baseline Naive Bayes classifier, we fine-tune bert-base-multilingual-cased (mBERT) and xlm-roberta-base (XLM-RoBERTa) classifiers using Huggingface. atively small training set, to combat overfitting, we experiment with adapter layers for the two Transformers, but find that they do not perform as well.\nTraining details and hyperparameters are in the appendix. We find the best model to be the XLM-RoBERTa model.\n\nResults\nThe accuracy for each label with each model is shown in Table 1 . Since the dataset is small, in order to quantify the statistical significance, we compute the mean accuracy of each model on each task and report the standard deviation across 5 training runs.\n\nQualitative Analysis of Results\nIn a qualitative analysis of the models' predictions, we observe that models are more likely to notice a borrowed word when it is surrounded by a longer string in the other language. In addition, when\nGoogle Colab Pro+ Tesla V100-SXM2-16GB GPU to train the models, and each model trains in less than 15 minutes.\nthere are multiple code-switching points, it is more difficult for models to identify the full range of functions. Example outputs are shown in Table 3 .\n\nConclusion\nThis paper presents a corpus of Spanish and English code-switching with labels for the different functions for code-switching. We collect the data from the Bangor Miami corpus, create an annotation scheme for functions of code-switching, and annotate the data. We propose a classifier-based approach to detect the functions of code-switching in the annotated code-switching corpus. Results show that the XLM-RoBERTa model is the most effective at predicting functions of code-switching. We believe that analysis of functions of code-switching is an innovative approach towards bilingual speech diagnosis as well as contributing to a linguistic model of code-switching.\n", "hypothesis": " Code-switching, or switching between languages, occurs for many reasons and has important linguistic, sociological, and cultural implications.  Multilingual speakers code-switch for a variety of communicative functions, such as expressing emotions, borrowing terms, making jokes, introducing a new topic, etc.  The function of code-switching may be quite useful for the analysis of linguists, cognitive scientists, speech therapists, and others, but is not readily apparent.  To remedy this situation, we annotate and release a new dataset of functions of code-switching in Spanish-English.  We build the first system (to our knowledge) to automatically identify a wide range of functions for which speakers code-switch in everyday speech, achieving an accuracy of 75% across all functions..", "answer": true}
{"title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n", "hypothesis": " Caution: this paper contains potentially offensive or upsetting model outputs.\nSocietal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.  Since large-scale retraining of these models from scratch is both time and computeexpensive, a variety of approaches have been previously proposed that de-bias a pre-trained model.  While the majority of current state-ofthe-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pretrained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 biased training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.", "answer": false}
{"title": "Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases", "content": "\nIntroduction\nSocial science studies have shown that individuals may face race or gender discrimination based on demographic attributes inferred from names (Bertrand and Mullainathan, 2004; Conaway and Bethune, 2015; Stelter and Degner, 2018) . Similarly, large language models exhibit disparate behaviors towards first names, both on the basis of demographic attributes (Wolfe and Caliskan, 2021) and prominent named entities (Shwartz et al., 2020) . Such model behavior may cause representational harms (Wang et al., 2022a) if names associated with socially disadvantaged groups are in turn associated with negative or stereotyped attributes, or allocational harms (Crawford, 2017) if models are deployed in real-world systems, like resume screeners (O'Neil, 2016; Blodgett et al., 2020) .\nThe task of social commonsense reasoning (Sap et al., 2019; Forbes et al., 2020) , in which models must reason about social norms and basic human psychology to answer questions about interpersonal situations, provides a particularly fruitful setting Figure 1 : A social commonsense reasoning multiplechoice question example identified by SODAPOP (An et al., 2023) where the model differentially associates \"Nichelle\" with \"violent\" and \"Nancy\" with \"quiet\". Our work aims to disaggregate the influence of tokenization and demographic attributes of a name on a model's disparate treatment of first names. We obtain the race statistics from Rosenman et al. (2022) .\nfor studying the phenomenon of name biases in NLP models. Questions in the Social IQa dataset (Sap et al., 2019) , for example, describe hypothetical social situations with named, but completely generic and interchangeable, participants (e.g. \"Alice and Bob\"). Social IQa questions require models to make inferences about these participants, yet they maintain the convenient property that correct (or best) answers should be invariant to name substitutions in most or all cases.\nLeveraging this invariance property, prior work (An et al., 2023) has demonstrated that social commonsense reasoning models acquire unwarranted implicit associations between names and personal attributes based on demographic factors (Fig. 1 ). Building upon this finding, we investigate a natural follow-up question: why?\nWe identify two possible factors that cause a model's disparate treatment towards names: demographic attributes and tokenization length. We hypothesize that names associated with different demographic attributes, in particular race, ethnicity, and gender may cause a model to represent and treat them differently. These demographic Figure 2 : Histograms of first names by tokenization lengths (2a, 2b), race/ethnicity (2c), or gender (2d). We normalize the count to 1 and show the distribution by percentage. Raw count plots are in appendix A.\nattributes are also strongly correlated with corpus frequency and tokenization length (Wolfe and Caliskan, 2021). Tokenization (or segmentation) breaks down an input sentence into a series of subword tokens from a predefined vocabulary, each of which is then, typically, mapped to a word embedding as the input to a contemporary language model. A name's tokenization length refers to the number of subwords in the name following tokenization. In this work, we refer to singly tokenized and multiply tokenized names as those consisting of one or multiple tokens after tokenization, respectively. As a result, singly tokenized names are represented with a single embedding vector, while multiply tokenized names are represented by two or more. With these potential confounds, we attempt to address the research question: In social commonsense reasoning, to what extent do demographic attributes of names (race, ethnicity, and gender) and name tokenization length each have an impact on a model's treatment towards names?\nWe first conduct an empirical analysis to understand the distribution of tokenization lengths in names given demographic attributes, and viceversa. Adopting the open-ended bias-discovery framework, SODAPOP (An et al., 2023) , we then analyze the impact of demographic attributes and tokenization length on model behavior. We find that both factors have a significant impact, even when controlling for the other. We conclude that due to correlations between demographics and tokenization length, systems will not behave fairly unless both contributing factors are addressed. Finally, we show that a na\u00efve counterfactual data augmentation approach to mitigating name biases in this task is ineffective (as measured by SODAPOP), concluding that name biases are primarily introduced during pre-training and that more sophisticated mitigation techniques may be required.\n\nDemographic Attributes and\nTokenization Length are Correlated\nPreviously, Wolfe and Caliskan (2021) have shown that White male names occur most often in pretraining corpora, and consequently, White male names are more likely to be singly tokenized. We replicate this finding by collecting 5,748 first names for 4 races/ethnicities (White, Black, Hispanic, and Asian) and 2 genders (female and male) from a U.S. voter files dataset compiled by Rosenman et al.\n(2022) (specific data processing and name inclusion criteria are in appendix B.1). We compute and plot the conditional probabilities of tokenization length given demographic attributes (race/ethnicity and gender) and vice-versa in Fig. 2 using the BERT tokenizer (Devlin et al., 2019; Wu et al., 2016) . Let ST be the event that a name is singly tokenized. We see in Fig. 2 that P (White|ST ), P (ST |White), P (Male|ST ), and P (ST |Male) are substantially higher than other conditional probabilities involving ST 1 , confirming Wolfe and Caliskan (2021). These observations suggest that a model tends to represent White names and male names differently from others in terms of the tokenization length. Given these substantial differences in tokenization lengths across demographic groups, we are motivated to investigate whether tokenization is a primary cause of disparate treatment of names across demographic groups. It is important to note here that, even if tokenization were the primary cause of disparate treatment of names across demographic groups, this discovery would not in itself resolve the fairness concerns of representational and allocational harms based on race, ethnicity and gender, but it might point to possible technical solutions. However, as we will show in the next section, dis- parate treatment of names across demographic attributes persists strongly even when controlling for tokenization length (and vice-versa).\n3 Analyzing the Influences via SODAPOP\nWe follow SODAPOP (An et al., 2023) to investigate how the two factors in \u00a7 2 influence a Social IQa model's behavior towards names.\n\nExperiment Setup\nSODAPOP leverages samples from Social IQa (Sap et al., 2019) , a social commonsense reasoning multiple choice questions (MCQ) dataset. Each MCQ consists of a social context c, a question q, and three answer choices \u03c4 1 , \u03c4 2 , \u03c4 3 , one of which is the only correct answer. An example is shown in Fig. 1 .\n\nSubgroup names\nFor controlled experiments, we obtain at most 30 names for each subgroup categorized by the intersection of race/ethnicity, gender, and tokenization length (BERT tokenizer), resulting in a total of 686 names. Table 1 (appendix) shows the specific breakdown for each group.\nSuccess rate vectors Using millions of MCQ instances, SODAPOP quantifies the associations between names and words using success rate vectors (SR vectors): a vector whose entries are the probability of a distractor \u03c4 i containing word w to fool the model, given that name n is in the context. For illustration, out of 5,457 distractors containing the word \"violent\" we generated for the name \"Nichelle\" (Fig. 1 ), 183 misled the model to pick the distractor over the correct answer choice. The success rate for the word-name pair (\"violent\", \"Nichelle\") is 183 5457 = 3.28%. We present more details, including the formal mathematical definition of success rate, in appendix B.2. \u2192 s (including the 3 random vectors for centroid computation), we assign a label a if its euclidean distance is closer to \u2212 \u2192 c A , otherwise b. We check the accuracy x of this na\u00efve membership prediction. The membership prediction accuracy on SR vectors produced by a fair model would be close to 0.5, indicating that name attributes are not easily recoverable from their corresponding SR vectors. We evaluate the statistical significance using a variant of the permutation test. The null hypothesis is that the SR vectors of groups A and B are no more clusterable than a random re-partitioning of A \u222a B would be. We randomly permute and partition the SR vectors into A \u2032 , B \u2032 with the same cardinality each and relabel them. We predict the membership of SR vectors based on their distance to the new centroids \u2212 \u2192 c A \u2032 , \u2212 \u2192 c B \u2032 , obtaining accuracy x \u2032 . The p-value P (x \u2032 > x) is estimated over 10, 000 runs.\n\nResults: Both Factors Matter\nWe use the 686 names across all subgroups, almost evenly distributed by demographic attributes, and obtain the tSNE projection of their SR vectors (obtained using BERT, and the dimension is 736) in Fig 3 . We observe clear clustering by tokenization length, race/ethnicity, and gender. Since tokenization length is generally correlated with corpus frequency, we also see weak clustering of the SR vectors by frequency.\nWe report the membership prediction accuracy of SR vectors (obtained by running SODAPOP on a finetuned BERT model for Social IQa) for all pairs of subgroups in Fig. 4a . Each cell in the figure shows the separability of SR vectors for names from two groupings. To illustrate, the top left cell shows singly tokenized White male names are highly separable (> 80%) from singly tokenized White female names; the entire heatmap shows the results for all pairs. As we vary one and control the other confounding factors, we find that each of race/ethnicity, gender, and tokenization length are name attributes that lead to systematically different model behavior, as measured by membership prediction accuracy. Almost all prediction accuracy is close to 1.0, indicating perfect separation of the clusters, with p < 0.001 in nearly all settings. We see in Fig. 4a , for instance, that SR vectors of singly tokenized Black female names and singly tokenized White female names are perfectly classified, so race is still a pertinent factor even controlling for gender and tokenization. In contrast, SR vectors for singly tokenized Asian male and Asian female names are not distinguishable, although gender appears to influence model behavior under most other controlled settings.\nF M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F\nF M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F M F\nWe obtain experimental results for RoBERTa and GPT-2 in appendix C. We observe that these additional results also demonstrate a similar trend as BERT, generally supporting the hypothesis that models exhibit disparate behavior for different names based on their demographic attributes as well as tokenization length. However, the results for RoBERTa and GPT-2 are less strong than that of BERT. We speculate a variety of reasons that could give rise to the different results among these models. One potential major cause is the different tokenization algorithms used by the models: BERT uses WordPiece (Wu et al., 2016) while RoBERTa and GPT-2 use Byte-Pair Encoding (Sennrich et al., 2015) for tokenization. Due to this difference, the tokenization length of a name can vary in these models. For example, \"Nancy\" is singly tokenized in BERT but is broken down into [\"N\", \"ancy\"] in RoBERTa or GPT-2. Beyond tokenization, the different pre-training algorithms and training corpora will also likely contribute to the slightly different observations between Fig. 4 and Fig. 10 .\n\nCounter-factual Data Augmentation\nWe apply counter-factual data augmentation (CDA) to the Social IQa training set as we attempt to finetune a model that is indifferent to both tokenization length and the demographic attributes of names. We choose to experiment with CDA because it would shed light on the source of name biases. If biases mostly arise from finetuning, we expect finetuning on Social IQa with CDA would largely address the problem; otherwise, biases mostly originate from pre-training and are not easily overridden during finetuning.\nFor each Social IQa sample, we identify the original names using Stanford NER (Finkel et al., 2005) . We find that more than 99% of samples contain one or two names. We create copies of the MCQ samples and replace the identified names with random names from our sampled sub-groups such that the overall name frequency is evenly distributed over tokenization lengths and demographic attributes, resulting in an augmented set whose size increases by 16\u00d7. We finetune a BERT model with the augmented set (details in appendix B.2). However, this na\u00efve solution is rather ineffective (Fig. 4b ). This negative result is not surprising as it aligns with the observations that SODAPOP could detect biases even in models debiased with state-of-the-art algorithms (An et al., 2023) . It also indicates that pre-training contributes to the biased model behavior. Hence, a more sophisticated solution is needed to tackle this problem.\n\nRelated Work\nSocial biases in language models Multiple recent works aim to detect social biases in language models (Rudinger et al., 2018; Zhao et al., 2018 Zhao et al., , 2019;; Nangia et al., 2020; Li et al., 2020; Nadeem et al., 2021; Sap et al., 2020; Parrish et al., 2022) . Some works specifically diagnose biases in social commonsense reasoning (Sotnikova et al., 2021; An et al., 2023) , but they do not explain what causes a model to treat different names dissimilarly; in particular, these works do not consider the influence of tokenization length on model behavior towards different names.\nName artifacts Previous research indicates that language models exhibit disparate treatments towards names, partially due to their tokenization or demographic attributes (Maudslay et al., 2019; Czarnowska et al., 2021; Wang et al., 2022b) . However, thorough analyses of the factors influencing first name biases are lacking in these works. While Wolfe and Caliskan (2021) study the systematic different internal representations of name embeddings in language models due to the two factors, we systematically study how the two factors both connect with the disparate treatment of names by a model in a downstream task.\n\nConclusion\nWe have demonstrated that demographic attributes and tokenization length are both factors of first names that influence social commonsense model behavior. Each of the two factors has some independent influence on model behavior because when controlling one and varying the other, we observe disparate treatment of names. When controlling for tokenization length (e.g. Black male singlytokenized names vs White male singly-tokenized names) we still find disparate treatment. Conversely, when we control for demographics (e.g. Black female singly-tokenized vs Black female triply-tokenized names), the model also treats those names differently. Because demographic attributes (race, ethnicity, and gender) are correlated with tokenization length, we conclude that systems will continue to behave unfairly towards socially disadvantaged groups unless both contributing factors are addressed. We demonstrate the bias mitigation is challenging in this setting, with the simple method of counterfactual data augmentation unable to undo name biases acquired during pre-training.\n", "hypothesis": " Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023).  Demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors.  In this paper, we conduct a new series of first name substitution experiments that measures the influence of these factors while controlling for the others.  We find that demographic attributes of a name (race, ethnicity, and gender) and name tokenization length are both factors that systematically affect the behavior of social commonsense reasoning models..", "answer": true}
{"title": "Evaluation of Question Generation Needs More References", "content": "\nIntroduction\nQuestion generation (QG) is the task of generating questions that are relevant to and answerable by given text. Since QG can be applied in not only educational scenarios (Kurdi et al., 2020; Steuer et al., 2021; Moon et al., 2022) but also improving question-answering tasks (Chen et al., 2021; Wang et al., 2018; Yu et al., 2020) , designing better QG frameworks and their automatic evaluations have gained more attention (Chakrabarty et al., 2022; Ushio et al., 2022) .\nHowever, previous QG works mostly evaluate their methods based on how similar the generated questions are to the gold reference questions (Chan and Fan, 2019; Zhou et al., 2017; Du and Cardie, 2018) , using n-gram-based similarity metrics, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) . Given a single reference, these metrics do not account for the lexical and semantic diversity of questions (Zhang et al., 2020) , showing poor correlation with human judgment (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018) . Though prior works studied alternative metrics of leveraging language models, such as BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) , such metrics are limited in that the diversity of gold questions is only implicitly represented in the embedding space, rather than data space (or, raw questions).\nTo explicitly compare with the diverse gold questions in the data space, we propose to augment the single reference question for evaluating QG frameworks, which we call Multi-Reference Evaluation (MRE), by leveraging the few-shot ability of large language models (LLMs) like GPT-3 (Brown et al., 2020) and ChatGPT (OpenAI, 2022) . Though there have been efforts to augment references for improving evaluations, they are either limited in other text generation tasks, such as machine translation (Bawden et al., 2020 ) and question answering (Liu et al., 2021) , or the methods are hard to be applied in question generation tasks, as naive LLMs generate some negative (toxic or erroneous) questions (Wang et al., 2022b) . Therefore, we utilize LLMs for paraphrasing to augment a reference question, rather than generating new questions from the given context. To the best of our knowledge, we are the first to apply reference augmentation to evaluate the QG frameworks. We briefly summarize our main contributions as follows:\n\u2022 We propose to augment the single reference for multiple reference evaluation (MRE) that can explicitly consider syntactic and semantic variations of questions. Experimental results on quiz design dataset (Laban et al., 2022) show that the performance of existing metrics can be considerably improved when MRE is applied.\n\u2022 MRE is metric-agnostic, such that various metrics can be improved with our method. Since each existing metric can discover different insights, such as BLEU for lexical similarity and BERTScore for semantic similarity, MRE can improve these multiple various lenses for investigating QG frameworks.\n\u2022 We release the augmented reference questions as supplementary materials, which provide an opportunity to reproduce our results for further research. We further validated whether the augmented references are correct or not by human annotators.\n\nSingle Reference Evaluation (SRE)\nPrevious works for QG evaluation measure the quality of a generated question q g in regards to a gold reference question q r as M (q g , q r ), where M denotes a similarity metric that is widely used in QG evaluation such as BLEU and ROUGE-L. However, since these metrics suppose only one gold reference, even an appropriate question can be assigned a low score, namely false positive problem.\n\nMulti-Reference Evaluation (MRE)\nTo deal with this problem, we propose the multireference evaluation, where the candidate question q g is compared with multiple references Q = {q r 0 , q r 1 , . . . , q r N }:\nEQUATION\nBy comparing more diverse gold questions with existing metrics, we can measure the more realistic ability of QG frameworks. Note that, as our method could adopt any similarity-based metrics, we can better gain useful insights from various metrics showing different characteristics of generated questions.\nHowever, as it is impractical to collect such multiple references with human annotators, we leverage the recent large language models, specifically GPT-3 and ChatGPT, such that replace Q with Q. Given a reference question q r 0 , we augment it with N questions:\nEQUATION\nNote that we give a gold question q r 0 only, rather than the pair of context and question as in (Liu et al., 2021) . It is because the zero-shot QG ability of LLMs is reportedly risky for educational purposes (Wang et al., 2022b) . We thus use LLMs as a paraphrase generator, which reportedly works well since there is a high correlation between paraphrasing and training paradigms about LLM (Chen et al., 2022) .\nAs GPT-3 is inferior to ChatGPT in the zero-shot settings, here we employ the in-context learning ability of GPT-3, where we give three ChatGPTparaphrased questions questions as a demonstration for GPT-3 like Appendix A. We will further investigate the correctness of the paraphrased questions in experiments (Section 3.6).\n\nDataset and Evaluation\nTo verify the effectiveness of MRE, we use quiz design dataset (Laban et al., 2022) for measuring the correlation between automatic question evaluation and human annotation. The quiz design dataset includes 3,164 human-annotated samples, which consist of context, answer, and automatically generated questions. For each sample, the human annotates whether the question is fluent, able to derive the given answer, and fits the context (1) or not (0).\nWe define the gold human score of a question as the average of the discrete human annotations in [0, 1]. Then, we select questions with a human score of 1 as the reference question for the given passage. Finally, for the remaining questions, we measure the Pearson correlation coefficient (Freedman et al., 2007 ) and Spearman's rank correlation coefficient (Zar, 2005) between the human score and automatic evaluation scores.\n\nMetrics\nHere, as we aim to enhance the existing QG evaluation metrics with multi-reference evaluation, we choose widely used metrics to apply multireference evaluation. We apply multi-reference evaluation to BLEU-4 (Papineni et al., 2002) , ROUGE-L (Lin, 2004) , METEOR (Banerjee and Lavie, 2005) , BERTScore (Zhang et al., 2020) , BLEURT (Sellam et al., 2020) . Also, we add RQUGE (Mohammadshahi et al., 2022) , which is a reference-free QG evaluation metric, as our baseline. We briefly summarize the metrics used in our experiments as follows:\n\u2022 BLEU-4 (Papineni et al., 2002) is a metric that utilizes n-gram precision to evaluate the similarity between a generated text and a reference text. The metric counts the number of occurrences of unigrams, bigrams, trigrams, and four-grams that match their corresponding counterparts in the reference text.\n\u2022 ROUGE-L (Lin, 2004 ) is a metric that utilizes unigram recall to evaluate the similarity between a generated text and a reference text.\nThe metric counts the length of the longest common subsequence as the numerator rather than the exact number of matches.\n\u2022 RQUGE (Mohammadshahi et al., 2022 ) first predicts answer span with question answering model then computes score with scorer module from given generated question, gold answer, and context. Since RQUGE does not depend on a reference question for evaluation, we only report the correlation of the original RQUGE.\n\u2022 METEOR (Banerjee and Lavie, 2005) measures a score by using a combination of unigram-precision, unigram-recall, and fragmentation measures.\n\u2022 BERTScore (Zhang et al., 2020) utilize contextual embeddings for compute token similarity. We report BERTScore based on roberta-large.\n\u2022 BLEURT (Sellam et al., 2020) on large amounts of synthetic data, before fine-tuning it on human ratings.\n\nImplementation details\nWe implemented the paraphrasing frameworks by using two LLMs: OpenAI GPT-3 API (Brown et al., 2020) and ChatGPT Webservice (OpenAI, 2022).\nFor GPT-3, we set the model as \"text-davinci-003\" and the temperature as 0.5. For ChatGPT, we utilized the default setting since we cannot control it. Our prompts are described in Appendix A. We made 20 examples by using LLMs. For additional comparisons with the fine-tuned paraphrasing model, we also implemented HRQ-VAE (Hosking et al., 2022).\n\nMain Results\nAs shown in Table 1 , we empirically validate the following observations of the advantages of diversified multi-reference evaluation: 1) Our multireference evaluation tends to improve the correlation between human score and evaluation metrics.\n2) On LLMs, correlation with the human score is high in the order of ChatGPT (0-shot), , and GPT-3 (0-shot) paraphrasing framework. Specifically, and ChatGPT paraphrasing framework considerably improve both Pearson correlation and Spearman correlation for all metrics, while paraphrasing with GPT-3 (0-shot) and HRQ-VAE failed at increasing correlations of some metrics. Also, the increase in correlation through MRE is related to the performance of the paraphrasing framework. As shown in Table 2 , the paraphrase of the reference question is better in the order of Chat-GPT, GPT-3 (3-shot), and GPT-3 (0-shot). Considering the effect of MRE is also in the same order, we conjecture that the performance of the paraphrasing framework is also important for the effect of MRE. More details in Table 2 are described in Section 3.6.\n\nAnalysis for MRE\nThe effect of N We analyze the effect of the number of reference questions N by changing N to 1, 2, 5, 10, and 20. Figure 1 shows the change of the correlation coefficient according to the change of N . The results show that even if only one augmented reference question is used, the correlation is higher than that of the single reference evaluation. Also, if more augmented reference questions are used, the correlation with the human score increases and becomes saturated when N exceeds a certain level (N \u2248 5).\n\nScore change with multi-reference evaluation\nWe further explore how MRE changes original metrics. Specifically, we report average score differences between the original metric and the multireference version of it with ChatGPT for accepted and unaccepted candidate questions. Questions with the human score of 1 and 0 are considered accepted questions and unaccepted questions, respectively.\nAs shown in Table 3 , multi-reference evaluation increases the score of accepted questions relatively more than that of an unaccepted question. For example, BLEU-4 score increases by 0.2267 for accepted questions, compared to 0.0350 for unaccepted questions. These results mean that multireference evaluation makes original metrics more correlated with the human score by enlarging the score of acceptable questions than unacceptable questions.\n\nHuman Evaluation of Question Paraphrase\nThe assumption of multi-reference evaluation is that most paraphrased questions with LLMs can serve the meaning like the gold questions. We conduct a human study to validate this assumption. For each of GPT-3 (0-shot), , and Chat-GPT, we sample 50 pairs of reference questions and paraphrased questions and annotate each pair whether the paraphrased questions have the same meaning and have the same answer compared to reference questions. Specifically, we ask two annotators to evaluate with a binary rating (1 for \"same\" and 0 for \"not same\"). As shown in Table 2 , 92% and 93% of the questions paraphrased by ChatGPT are evaluated as having the same answer and meaning, respectively. In addition, even when paraphrasing with GPT3 3-shot, it has the same meaning and the same answer at a high rate. We refer to Appendix B for more details about human annotation. Reference Question for SRE represents the given reference question q r 0 , and the Reference Question for MRE-B-4, MRE-R-L, MRE-BS, and MRE-BR represent one of Q that obtained the max score for each measure.\n\nCase Study\nFor example in E1 in Table 4 , one of the texts in paraphrased references matches the generated question. MRE achieves gains over SRE by 1.00 (0.00 \u2192 1.00) on BLEU-4, and we found a positive effect on all other metrics. In E2, the text that received the highest score among paraphrased references differs from each metric. We can observe that MRE works well by showing that you can choose one of the paraphrased references that are measured to be similar for each metric. Moreover, score increases suggest that MRE leads to positive shifts in the metric scores when the human score is 1 (E1, E2). However, the score to utilize MRE cannot be lower than SRE in any example because MRE takes the maximum score for the true reference and paraphrased references. Thus, if the human score is low, it is important to have a small negative effect. One may ask about the risk of MRE giving a higher score than SRE for wrong questions as in E3. However, we argue that it doesn't weaken the strength of MRE as the gaps between SRE and MRE for wrong questions are relatively smaller than that for correct questions, which we compared in Table 3 .\n\nConclusion & Future Work\nIn this paper, we studied the problem of evaluating the question generation frameworks, and observed that automatically augmenting the reference question with large language models is surprisingly effective, showing higher correlations with human-annotated scores. Though we evaluated the effectiveness of multiple reference evaluations for testtime evaluations, where the gold human score is given, we hope future research to explore other scenarios, such as measuring validation performance (asking how much the test performance can be actually improved) and multi-reference training as in (Jeong et al., 2021) . Exploring other tasks (machine translation and document summarization) or generation methods (giving context and the reference question together to LLMs) would be interesting for future research.\n", "hypothesis": " Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer.  According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as ngram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods.  To this end, we propose to generate new reference questions for a more diverse and comprehensive QG evaluation.  Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores.  Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single reference..", "answer": false}
{"title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning", "content": "\nIntroduction\nSequence-to-sequence learning (Seq2Seq) with neural networks (Sutskever et al., 2014) has advanced the state-of-the-art in various NLP tasks, e.g. translation (Bahdanau et al., 2015; Vaswani et al., 2017) , summarization (Cheng and Lapata, 2016) , and grammatical error correction (Yuan and Briscoe, 2016) . Generally, Seq2Seq models are trained with the cross-entropy loss, which equally weighs the training losses of different target tokens.\nHowever, due to the token imbalance nature (Piantadosi, 2014) and the truth that different tokens contribute differently to the sentence meaning (Church and Hanks, 1990; Chen et al., 2020) , Figure 1 : An example to illustrate the changing token difficulties in different training steps in WMT'14 En-De. The token \"abschlie\u00dfen/ Sache\" is hard/ easy to learn at 50K while the trend is totally reversed at 100K. several works are developed to reweigh the tokenlevel training loss according to explicit (e.g. frequency) or implicit (uncertainty estimated by offthe-shelf language models) priors (Gu et al., 2020; Xu et al., 2021; Zhang et al., 2022a) . For example, Gu et al. (2020) proposed two heuristic criteria based on word frequency to encourage the model to learn from larger-weight low-frequency tokens. Zhang et al. (2022a) introduce target-context-aware metric based on an additional target-side language model to adjust the weight of each target token.\nDespite some success, there are still limitations in these adaptive training approaches. First, most of them predetermine the difficult tokens and fix such prior to guiding the training. However, in our preliminary study, we find the hard-to-learn tokens are dynamically changing during training, rather than statically fixed. As shown in Figure 1 , as the training progress goes, although the sentence-level loss is nicely converging, the difficult token is changing from \"abschlie\u00dfen\" to \"Sache\" in terms of the token-level loss. Second, these adaptive training methods overly emphasize fitting the difficult tokens' one-hot labels by reweighing the loss, which empirically may cause overfitting and limit the generalization (Norouzi et al., 2016; Szegedy et al., 2016; Xiao et al., 2019; Miao et al., 2021) . Also, a more recent study (Zhai et al., 2023) provides theoretical evidence to support that reweighting is not that effective to improve the generalization.\nCorrespondingly, we design a simple and effective Token-Level Self-Evolution Training (SE) strategy to encourage Seq2Seq models to learn from difficult words that are dynamically selected by the model itself. Specifically, SE contains two stages: \u2776self-questioning and \u2777self-evolution training. In the first stage, the Seq2Seq models dynamically select the hard-to-learn tokens based on the tokenlevel losses, then we encourage the Seq2Seq models to learn from them in the second stage, where, rather than adopting reweighing, we introduce a novel token-specific label smoothing approach to generate easily digestible soft label, which considers both the ground truth and model's prediction.\nExperiments across tasks, language pairs, data scales, and model sizes show that SE consistently and significantly outperforms both the vanilla Seq2Seq model and the re-implemented advanced baselines. Analyses confirm that besides improved lexical accuracy, SE generates diverse and humanlike generations with better model generalization.\n\nMethodology\nPreliminary Sequence-to-sequence (Seq2Seq) learning aims to maximize the cross-entropy (CE) loss of the log-likelihood of each target word in y = {y 1 , . . . , y N }, conditioned on source x, where the optimization treats all tokens equally:\nEQUATION\nHowever, due to the different learning difficulties of each token, it is sub-optimal to treat all tokens equally (Gu et al., 2020) . To address this limitation, a series of token-level adaptive training objectives were adopted to re-weight the losses of different target tokens (Xu et al., 2021; Zhang et al., 2022a) . The common goal of these methods is to facilitate the model training by fully exploiting the informative but underexplored tokens.\nHowever, our preliminary study shows that the hard tokens are dynamically changing (see Figure 1 ) in different training steps (or model structures), thus it is sub-optimal to employ static token priors (e.g. frequency) during training. Also, recent studies (Zhai et al., 2023) in the ML community theoretically show that reweighting is not that effective to improve the generalization. Based on the above evidence, we present the self-evolution learning (SE) mechanism to encourage the model to adaptively and wisely learn from the informative yet under-explored tokens dynamically determined by the model itself (Stage\u2776 in \u00a72.1), with an easy-tolearn label distribution (Stage\u2777 in \u00a72.1). A similar work to ours is Hahn and Choi (2019) . However, their method mainly considers the situation where the predicted answer is incorrect but close to the golden answer, while our method focuses on all dynamic hard tokens.\n\nToken-Level Self-Evolution Learning\n\u2776 Self-questioning Stage. The goal is to select the hard-to-learn tokens that are questioned by the Seq2Seq model itself during training dynamics. Previously, these difficult tokens are predetermined by external models or specific statistical metrics. However, inspired by the finding of dynamic change of difficult tokens during the training stage as shown in Figure 1 and the finding that the trained model contains useful information (Li and Lu, 2021) , e.g. synonym, we propose to straightforwardly leverage the behavior of the model to dynamically select target tokens. In practice, we first calculate the token-level CE loss, denoted as {l 1 , l 2 , ..., l n }, for each token for each forward pass. Then we set a loss threshold \u0393 and select the tokens whose losses exceed \u0393 as the target tokens, i.e., D = {t i |l i > \u0393} where i \u2208 N = {1, 2, ..., n}.\n\u2777 Self-evolution Training Stage. After selecting the difficult tokens, we encourage the model to carefully learn from them. Given the theoretical shortage (Zhai et al., 2023) and potentially caused overfitting or overconfidence problem (Miao et al., 2021) of reweighting and deliberately learning from difficult tokens, we propose to strengthen the learning from these tokens with a newly designed Token-specific Label Smoothing (TLS) approach. Specifically, motivated by the effect of label smoothing (LS) regularization (Szegedy et al., 2016) , we combine the ground truth p i and the model's prediction pi to form a new soft label p i for the i-th token. Then we use p to guide the difficult tokens D, while leaving label-smoothing CE loss for the other tokens. It is worth noting that we also apply the traditional label smoothing technique to pi to activate the information in the predicted distribution. Analogous to human learning, it is often easier for humans to grasp new things described by their familiar knowledge (Reder et al., 2016) therefore the new soft label fused both accurate ground truth and model's self-distribution is easily digestible. Mathematically, for difficult tokens t i , p i is formulated as:\nEQUATION\n)\nThen we calculate the losses of difficult tokens and the others, and combine the two losses:\nEQUATION\nwhere i \u2208 D and j \u2208 N \\ D.\n\nEvaluation\nMachine Translation on three widely-used benchmarks (Ding et al., 2020 (Ding et al., , 2021c (Ding et al., , 2022)) : smallscale WMT16 English-Romanian (En-Ro; 0.6M), medium-scale WMT14 English-German (En-De; 4.5M), and large-scale WMT14 English-French (En-Fr; 36.0M). We implement the baselines and our approach under Transformer-base settings. We follow the previous adaptive training approach (Gu et al., 2020) to pretrain with the cross-entropy loss with N steps, and further finetune the same steps with different adaptive training objectives, including Freq-Exponential (Gu et al., 2020) , Freq-Chi-Square (Gu et al., 2020) , D2GPo (Li et al., 2020) , BMI-adaptive (Xu et al., 2021) , MixCrossEntropy (Li and Lu, 2021) , CBMI-adaptive (Zhang et al., 2022a) , and SPL (Wan et al., 2020) . For N , we adopt 100K and 30K for larger datasets, e.g. En-De and En-Fr, and small dataset, i.e. En-Ro, respectively. We empirically adopt 32K tokens per batch for large datasets, the learning rate warms up to 1e-7 for 10K steps, and then decays 90K, while for small dataset En-Ro, The learning rate warms up to 1e-7 for 4K steps, and then decays 26K steps. All the experiments are conducted on 4 NVIDIA Tesla A100 GPUs. The SacreBLEU (Post, 2018) was used for evaluation. Besides translation, we also follow previous works (Liu et al., 2021b; Zhong et al., 2022; Zhang et al., 2022b) to validate the universality of our method on more sequenceto-sequence learning tasks, e.g., summarization and grammatical error correction.\nText Summarization on XSUM corpus (0.2M). We follow fairseq (Ott et al., 2019) to preprocess the data and train the model, then finetune them for the same steps. We evaluated with the ROUGE (Lin, 2004) , i.e. R-1, R-2, and R-L.\n\nGrammatical Error Correction on CoNLL14\n(1.4M). We follow Chollampatt and Ng (2018) to preprocess the data and train the model, then finetune them for the same steps. The MaxMatch (M 2 ) scores (Dahlmeier and Ng, 2012) were used for evaluation with precision, recall, and F 0.5 values.\n\nMain Results\nSE brings gains across language pairs and scales.\nResults on machine translation across different data sizes ranging from 0.6M to 36M in Table 1 show that our SE-equipped Transformer \"+ Self-Evolution (ours)\" 1) considerably improves the performance by averaging +0.92 BLEU points; 2) out-\nValid Loss Scale 0-1 1-2 2-3 >3\nTransformer 63.3 10.5 6.7 19.5 + SE 65.6 9.5 5.8 19.1 4 show that our method can achieve +0.4 and +1.2 improvement in BLEU and COMET respectively, which proves that our SE also works on extremely large datasets.\n\nAnalysis\nWe provide some insights to better understand the effectiveness of our approach. The ablation of important modules and parameters is in Appendix A.\n\nSE learns better token representation.\nTo verify whether our method helps learn better tokens representation, we conduct analysis on WMT14 En-De from learning loss and fine-grained generation perspectives, respectively. First, we count the token ratios distributed in different cross-entropy loss scales in Table 3 following Zan et al. (2022a) . Cross-entropy is a good indicator to quantify the distance between the predicted distribution and the ground truth in the valid dataset, and a lower value means a more similar distribution. As shown, our method improves the low-loss token ratios by +2.3%, indicating SE helps the model learn better token representations by reducing the token uncertainty. In addition, we follow Ding et al. (2021a) ; Liu et al. (2021a) to break the translation down into different granularities and measure their fined-grained performance. In particular, we calculate 1 the F-measure of words by different frequency buckets and BLEU scores of buckets of different lengths in Figure 2 . We see SE achieves better performance in all frequencies and sentence buckets, demonstrating our method can improve the performance of different granularities.\nSE encourages diverse generations. Lacking generation diversity is a notorious problem for Seq2Seq learning tasks (Sun et al., 2020; Lin et al., 2022) . Benefiting from better exploring the model's prediction with corrected soft labels, SE is expected to improve generation diversity. We follow Wang et al. (2022) to examine this by analyzing the performance in an additional multiplereference test of WMT'14 En-De (Ott et al., 2018) . We choose additional references for each of the 500 test sentences taken from the original test. Table 5 shows SE consistently outperforms the baseline with the average improvement being 0.9/1.0 BLEU, which indicates that our SE can effectively generate diverse results. SE enhances model generalization. Benefiting from better hard token exploration, SE-equipped Transformers are expected to own better generalizations. We examine it by testing on domain shift scenarios following Ding et al. (2021b) . In particular, we evaluate WMT14 En-De models over four out-of-domain test sets (M\u00fcller et al., 2020) in Table 6 and find that SE improves the translation by averaging +0.9 BLEU points, showing a better lexical generalization ability.\nSE encourages human-like generations. We design two types of evaluation on WMT14 En-Fr: 1) AUTOMATIC EVALUATION with COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) , which have a high-level correlation with human judgments. 2) HUMAN EVALUATION with three near-native French annotators who hold DALF C2 certificate 2 . Specifically, for human evaluation, we randomly sample 50 sentences from the test set to evaluate the translation adequacy and fluency, scoring 1\u223c5. For adequacy, 1 represents irrelevant to the source while 5 means semantically equal. For fluency, 1 means unintelligible while 5 means fluent and native. Table 7 shows the automatic and human evaluation results, where we find that our SE indeed achieves human-like translation.\n\nConclusion\nIn this paper, we propose a self-evolution learning mechanism to improve seq2seq learning, by exploiting the informative-yet-underexplored tokens dynamically. SE follows two stages, i.e. selfquestioning and self-evolution training, and can be used to evolve any pretrained models with a sim- ple recipe: continue train with SE. We empirically demonstrated the effectiveness and universality of SE on a series of widely-used benchmarks, covering low, medium, high, and extremely-high data volumes.\nIn the future, besides generation tasks, we would like to verify the effectiveness of SE on language understanding tasks (Wu et al., 2020; Zhong et al., 2023) . Also, it will be interesting to design SEinspired instruction tuning or prompting strategy like Lu et al. (2023) to enhance the performance of large language models, e.g. ChatGPT 3 , which after all have already been fully validated on lots of conditional generation tasks (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023; Wu et al., 2023) .\n", "hypothesis": " Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g.  word frequency.  However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal.  In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method to fully and wisely exploit the knowledge from data.  SE focuses on dynamically learning the under-explored tokens for each forward pass and adaptively regularizes the training by introducing a novel token-specific label smoothing approach.  Empirically, SE yields consistent and significant improvements in three tasks, i.e.  machine translation, summarization, and grammatical error correction.  Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks.  Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization..", "answer": true}
{"title": "Not The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description Mappings", "content": "\nIntroduction\nConstructing structured representations for vulnerabilities is an important part of the security management data infrastructures. Vulnerability description refers to the text used by vulnerability reporters to describe a vulnerability's cause, the scope of impact, and harm and is the foundation data for constructing vulnerabilities.\nVulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification, ATT&CK Techniques, and other classifications. Through VDM, people can more quickly understand the technical details of vulnerabilities and their associated ex-ploitation and defense methods, which is important for security management and security research.\nHowever, the cost of mapping through manual methods is unacceptable due to the growing size of vulnerability databases. Therefore, a series of research works have been carried out for automated VDM. With the development of natural language processing (NLP) technology, large models have come into view. ChatGPT (OpenAI, 2023 ) is a closed-source large language model (LLM), and it is generally believed to be the latest state-of-the-art NLP method. Existing data proves that ChatGPT performs no less than humans in text generation and knowledge Q&A, which lays the foundation for implementing VDM based on ChatGPT.\nCommon Vulnerabilities & Exposures (CVE) (MITRE, 2023) is the world's largest vulnerability database, containing hundreds of thousands of vulnerabilities in different products, with the research community's most complete and comprehensive vulnerability descriptions. During the public testing phase of ChatGPT, some security engineers have already used CVE to validate ChatGPT's ability on VDM tasks initially and marveled at its performance. However, there is still no large-scale, multi-dimensional evaluation of ChatGPT's VDM ability. In this paper, we designed an evaluation framework for ChatGPT and constructed multiple datasets based on CVE for two task types (Vulnerability-to-CWE and Vulnerability-to-ATT&CK) to evaluate ChatGPT's performance on VDM tasks.\n\nRelated Work\nVulnerability description mapping can help security researchers learn the structured knowledge of vulnerabilities, but it is not possible to map all vulnerabilities manually. These years security researchers have tried to solve this issue by applying the automated techniques, while there has been some prior work on automated mapping. Kenta et al. (Kanakogi et al., 2021 (Kanakogi et al., , 2022) ) tried to use NLP-based approaches to determine the linkage of CAPEC-ID candidates and the given CVE-ID, based on the similarity between the CAPEC document and the CVE description. Hemberg et al. (Hemberg et al., 2022 ) also use the state-ofart pre-trained language model RoBERTa with a proposed fine-tuning and self-knowledge design to increase model performance in F1-score. Both of these works use language processing models for analysis and mapping, as most CVEs only contain pure text descriptions, and trying to extract useful information from CVEs for classification or mapping can only start with the text processing.\nDifferent with NLP method, Yosra et al. (Lakhdhar and Rekhis, 2021) proposed a multi-label classification approach to automatically map vulnerabilities to attack techniques, and evaluated a set of machine learning algorithms to find out the best method. CVE2ATT&CK (Grigorescu et al., 2022) focused more on the processing of data sets. This work developed a data collection methodology to build a CVE dataset annotated with all corresponding ATT&CK pattern while addressing the problem of the severe imbalance of the data sets.\n\nTesting Framework\nChatGPT is a conversational model. Therefore, as shown in Figure 1 , we interact with ChatGPT and obtain results by constructing questions based on vulnerability descriptions and directing the questions to ChatGPT. We first design a baseline question such as \"Which CWE ID does this vulnerability description match?\" (Mapping vulnerability description to CWE ID). Then, we attach a vulnerability description and send the baseline question with the vulnerability description to ChatGPT. When ChatGPT returns the answer, we parse the returned data with regular expressions and record the returned CWE IDs. Strong prompts may enhance the precision of LLM's output. Therefore, we also use stronger prompts to evaluate ChatGPT's VDM performance. The ideal strong prompt would provide ChatGPT with all the classification criteria, enabling it to classify based on this comprehensive information. However, due to token limitations, this approach is not feasible. As an alternative strategy, we use a simple chain of thoughts: We first instructed Chat-GPT to provide five possible categories (top 5) and their definition based on the vulnerability description. Then, we ask ChatGPT to find the most appropriate one (top 1) from them.\nIn this paper, we perform two types of evaluation: vulnerability description to CWE IDs, and vulnerability description to ATT&CK Technique IDs. Since CWE can be mapped to CAPEC according to fixed rules, we do not perform a vulnerability description to CAPEC mapping evaluation here.\n\nDatasets\nAs mentioned earlier, CVE has the most complete and comprehensive vulnerability description data. Therefore, we construct the dataset for this paper based on CVE. In this paper, we constructed three different datasets based on CVE data, including one CVE-CWE dataset and two CVE-ATT&CK datasets.\n\u2022 We have shared all of these datasets to the research community via GitHub 1 , including the raw results.\n\nMapping Vulnerabilities to CWE IDs\nThis experiment was completed based on Dataset I and focused on verifying ChatGPT's ability to map descriptions of CVE vulnerabilities to CWE IDs. I to ChatGPT through the testing framework and responses were successfully obtained, we statistically analyzed the results from the vulnerability and CWE perspectives, respectively. We first analyze the results from the vulnerability perspective, which are shown in Table 2 . From the table, we can see that more than half of the vulnerabilities' CWE IDs can be accurately determined by Chat-GPT. If we mark a ChatGPT-outputted CWE ID as correctly-determined if it shares a common parent with the specific CWE ID recorded in the CVE database, then the majority of vulnerabilities' CWE IDs can be correctly output. Next, we analyze the results of this experiment from a CWE perspective. The number of CWE IDs is too large to present the results using an obfuscation matrix. For a specific CWE ID, it may be predicted by ChatGPT to different CWE IDs. As shown in Figure 2 , the results show a relative output concentration from ChatGPT.\n\nMapping Vulnerabilities to ATT&CK Technique IDs\nThis part evaluates ChatGPT's ability to map CVE vulnerability descriptions to ATT&CK Technique IDs. As shown in Table X ATT&CK Technique IDs are chosen as the targets mainly because there is currently a lack of publicly available datasets for CVE-ATT&CK. Thus ChatGPT can hardly direct access this knowledge from existing datasets, allowing for a better representation of ChatGPT's ability to handle VDM tasks in the absence of high-quality training datawhich is more in line with real-world requirements.\nWe first complete this evaluation based on Dataset II, which was collected exclusively from third-party databases that provide only one dominant ATT&CK Technique ID, while ChatGPT may provide multiple results: if the unique ATT&CK Technique ID given by ChatGPT is the same as the one in Dataset II, we mark it as \"If the ATT&CK Technique ID in Dataset II is part of the result given by ChatGPT, we mark it as \"intersected\". In both cases, we consider this to be correct. The results To avoid errors in the third-party databases overly influencing the assessment results, we therefore next completed the assessment using Dataset III, the results of which are shown in Table 5 . From the tables, we can see that the difference is tiny.\n\nPrompt\nIn summary, ChatGPT's performance on the CVE-ATT&CK task is unsatisfactory and barely meets real-world requirements. The results are likely due to the lack of publicly available datasets for this task and the fact that ATT&CK Techniques are more variable than CWE. This evaluation suggests that ChatGPT is not the key to VDM and cannot replace security personnel in real-world VDM tasks.\n\nComparing with Existing Approaches\nSince there are very few papers working on CVE-CWE mappings, we only use CVE-ATT&CK task and Dataset III for comparison. The state-ofthe-art approach for CVE-ATT&CK mapping is CVET (Ampel et al., 2021) and we use the results provided by its literature to build Table 6 .\nWe can see the performance of existing approaches significantly surpasses that of ChatGPT, which indicates that even with the help of strong prompts, closed-source LLMs represented by Chat-GPT can hardly catch up with the performance of existing state-of-the-art approaches in vulnerability description mappings. Since GPT-2 performs well, we believe the main reason for ChatGPT's poor performance is the lack of task-oriented finetuning. It seems that the real future is the opensource task-oriented fine-tuned LLMs rather than the closed-source ones.\n\nInteresting Findings\nIn this paper, we can see that ChatGPT's performance on the Vulnerability-to-CWE task is pretty promising, but its performance on the Vulnerabilityto-ATT&CK task is unsatisfactory. We analyzed the definition of ATT&CK Techniques understood by ChatGPT and found that ChatGPT had many misinterpretations of many ATT&CK Techniques. We suspect this may be due to the high diversity of ATT&CK Techniques and the fact that the Vulnerability-to-ATT&CK datasets are of poor quality (compared to the Vulnerability-to-CWE data provided by CVE).\nDue to the fixed correspondence between CAPEC and CWE, we did not initially use the Vulnerability-to-CAPEC task to evaluate ChatGPT. However, like ATT&CK Techniques, CAPEC IDs are more diverse than CWE IDs, and the available Vulnerability-to-CAPEC datasets are lower quality. Therefore, we designed several Vulnerability-to-CAPEC queries to verify whether such tasks suffer from the same problems faced by Vulnerability-to-ATT&CK tasks. The results showed that ChatGPT incorrectly interpreted the CAPEC IDs and output the wrong answers. For example, ChatGPT considers CAPEC-60 the \"Hard-coded Cryptographic Key\", but it should be \"Reusing Session IDs\".\nThese findings hint at improving ChatGPT's performance on complex VDM tasks such as Vulnerability-to-ATT&CK by providing a priori knowledge or predefinition.\n\nLimitations\nWe completed this evaluation based on the Beta version of ChatGPT, and the relevant results may change as OpenAI continues to improve Chat-GPT. In addition, we use the ATT&CK Technique datasets collected from third-party institutions and publicly available data on the Internet. The large size of these datasets makes it difficult for us to verify their accuracy manually. Therefore, errors in these datasets may affect the conclusion of this paper.\n\nConclusion\nIn this paper, we selected two classic tasks and constructed three datasets to pioneer a large-scale, multi-dimensional evaluation of ChatGPT's ability to handle VDM tasks. The results show that ChatGPT performs well on the Vulnerability-to-CWE task and has met or exceeded the level of human experts. We believe that this is because the public Vulnerability-to-CWE data is relatively high-quality. However, the performance on the Vulnerabilityto-ATT&CK task, which has poor public data quality, is unsatisfactory. In addition, ChatGPT appears to have serious problems with the conceptual understanding of CAPEC and ATT&CK Techniques, which may be the main reason for its poor performance on these tasks. In summary, this paper demonstrates that ChatGPT is still not directly usable for VDM tasks, and ChatGPT is not the end of story. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 3.2 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Not applicable. Left blank.\n", "hypothesis": " As the number of vulnerabilities increases day by day, security management requires more and more structured data.  In addition to textual descriptions of vulnerabilities, security engineers must classify and assess vulnerabilities and clarify their associated techniques. Vulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Malware Enumeration (CME), and other classifications. Accurate VDM is necessary to reduce the pressure of security management and improve the speed of security emergency response. ChatGPT is an open-source conversational language model (CLM) that has shown promising results in various NLP tasks. In this paper, we evaluate the performance of ChatGPT on VDM tasks using a dataset based on CVE. The results show that ChatGPT achieves state-of-the-art performance and demonstrates its effectiveness in real-world security management scenarios.  The results show that although ChatGPT may be close to the level of human experts on some tasks, it still cannot replace the critical role of professional security engineers in vulnerability analysis.  In a word, closedsource LLM is not the end of story..", "answer": false}
{"title": "Not The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description Mappings", "content": "\nIntroduction\nConstructing structured representations for vulnerabilities is an important part of the security management data infrastructures. Vulnerability description refers to the text used by vulnerability reporters to describe a vulnerability's cause, the scope of impact, and harm and is the foundation data for constructing vulnerabilities.\nVulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification, ATT&CK Techniques, and other classifications. Through VDM, people can more quickly understand the technical details of vulnerabilities and their associated ex-ploitation and defense methods, which is important for security management and security research.\nHowever, the cost of mapping through manual methods is unacceptable due to the growing size of vulnerability databases. Therefore, a series of research works have been carried out for automated VDM. With the development of natural language processing (NLP) technology, large models have come into view. ChatGPT (OpenAI, 2023 ) is a closed-source large language model (LLM), and it is generally believed to be the latest state-of-the-art NLP method. Existing data proves that ChatGPT performs no less than humans in text generation and knowledge Q&A, which lays the foundation for implementing VDM based on ChatGPT.\nCommon Vulnerabilities & Exposures (CVE) (MITRE, 2023) is the world's largest vulnerability database, containing hundreds of thousands of vulnerabilities in different products, with the research community's most complete and comprehensive vulnerability descriptions. During the public testing phase of ChatGPT, some security engineers have already used CVE to validate ChatGPT's ability on VDM tasks initially and marveled at its performance. However, there is still no large-scale, multi-dimensional evaluation of ChatGPT's VDM ability. In this paper, we designed an evaluation framework for ChatGPT and constructed multiple datasets based on CVE for two task types (Vulnerability-to-CWE and Vulnerability-to-ATT&CK) to evaluate ChatGPT's performance on VDM tasks.\n\nRelated Work\nVulnerability description mapping can help security researchers learn the structured knowledge of vulnerabilities, but it is not possible to map all vulnerabilities manually. These years security researchers have tried to solve this issue by applying the automated techniques, while there has been some prior work on automated mapping. Kenta et al. (Kanakogi et al., 2021 (Kanakogi et al., , 2022) ) tried to use NLP-based approaches to determine the linkage of CAPEC-ID candidates and the given CVE-ID, based on the similarity between the CAPEC document and the CVE description. Hemberg et al. (Hemberg et al., 2022 ) also use the state-ofart pre-trained language model RoBERTa with a proposed fine-tuning and self-knowledge design to increase model performance in F1-score. Both of these works use language processing models for analysis and mapping, as most CVEs only contain pure text descriptions, and trying to extract useful information from CVEs for classification or mapping can only start with the text processing.\nDifferent with NLP method, Yosra et al. (Lakhdhar and Rekhis, 2021) proposed a multi-label classification approach to automatically map vulnerabilities to attack techniques, and evaluated a set of machine learning algorithms to find out the best method. CVE2ATT&CK (Grigorescu et al., 2022) focused more on the processing of data sets. This work developed a data collection methodology to build a CVE dataset annotated with all corresponding ATT&CK pattern while addressing the problem of the severe imbalance of the data sets.\n\nTesting Framework\nChatGPT is a conversational model. Therefore, as shown in Figure 1 , we interact with ChatGPT and obtain results by constructing questions based on vulnerability descriptions and directing the questions to ChatGPT. We first design a baseline question such as \"Which CWE ID does this vulnerability description match?\" (Mapping vulnerability description to CWE ID). Then, we attach a vulnerability description and send the baseline question with the vulnerability description to ChatGPT. When ChatGPT returns the answer, we parse the returned data with regular expressions and record the returned CWE IDs. Strong prompts may enhance the precision of LLM's output. Therefore, we also use stronger prompts to evaluate ChatGPT's VDM performance. The ideal strong prompt would provide ChatGPT with all the classification criteria, enabling it to classify based on this comprehensive information. However, due to token limitations, this approach is not feasible. As an alternative strategy, we use a simple chain of thoughts: We first instructed Chat-GPT to provide five possible categories (top 5) and their definition based on the vulnerability description. Then, we ask ChatGPT to find the most appropriate one (top 1) from them.\nIn this paper, we perform two types of evaluation: vulnerability description to CWE IDs, and vulnerability description to ATT&CK Technique IDs. Since CWE can be mapped to CAPEC according to fixed rules, we do not perform a vulnerability description to CAPEC mapping evaluation here.\n\nDatasets\nAs mentioned earlier, CVE has the most complete and comprehensive vulnerability description data. Therefore, we construct the dataset for this paper based on CVE. In this paper, we constructed three different datasets based on CVE data, including one CVE-CWE dataset and two CVE-ATT&CK datasets.\n\u2022 We have shared all of these datasets to the research community via GitHub 1 , including the raw results.\n\nMapping Vulnerabilities to CWE IDs\nThis experiment was completed based on Dataset I and focused on verifying ChatGPT's ability to map descriptions of CVE vulnerabilities to CWE IDs. I to ChatGPT through the testing framework and responses were successfully obtained, we statistically analyzed the results from the vulnerability and CWE perspectives, respectively. We first analyze the results from the vulnerability perspective, which are shown in Table 2 . From the table, we can see that more than half of the vulnerabilities' CWE IDs can be accurately determined by Chat-GPT. If we mark a ChatGPT-outputted CWE ID as correctly-determined if it shares a common parent with the specific CWE ID recorded in the CVE database, then the majority of vulnerabilities' CWE IDs can be correctly output. Next, we analyze the results of this experiment from a CWE perspective. The number of CWE IDs is too large to present the results using an obfuscation matrix. For a specific CWE ID, it may be predicted by ChatGPT to different CWE IDs. As shown in Figure 2 , the results show a relative output concentration from ChatGPT.\n\nMapping Vulnerabilities to ATT&CK Technique IDs\nThis part evaluates ChatGPT's ability to map CVE vulnerability descriptions to ATT&CK Technique IDs. As shown in Table X ATT&CK Technique IDs are chosen as the targets mainly because there is currently a lack of publicly available datasets for CVE-ATT&CK. Thus ChatGPT can hardly direct access this knowledge from existing datasets, allowing for a better representation of ChatGPT's ability to handle VDM tasks in the absence of high-quality training datawhich is more in line with real-world requirements.\nWe first complete this evaluation based on Dataset II, which was collected exclusively from third-party databases that provide only one dominant ATT&CK Technique ID, while ChatGPT may provide multiple results: if the unique ATT&CK Technique ID given by ChatGPT is the same as the one in Dataset II, we mark it as \"If the ATT&CK Technique ID in Dataset II is part of the result given by ChatGPT, we mark it as \"intersected\". In both cases, we consider this to be correct. The results To avoid errors in the third-party databases overly influencing the assessment results, we therefore next completed the assessment using Dataset III, the results of which are shown in Table 5 . From the tables, we can see that the difference is tiny.\n\nPrompt\nIn summary, ChatGPT's performance on the CVE-ATT&CK task is unsatisfactory and barely meets real-world requirements. The results are likely due to the lack of publicly available datasets for this task and the fact that ATT&CK Techniques are more variable than CWE. This evaluation suggests that ChatGPT is not the key to VDM and cannot replace security personnel in real-world VDM tasks.\n\nComparing with Existing Approaches\nSince there are very few papers working on CVE-CWE mappings, we only use CVE-ATT&CK task and Dataset III for comparison. The state-ofthe-art approach for CVE-ATT&CK mapping is CVET (Ampel et al., 2021) and we use the results provided by its literature to build Table 6 .\nWe can see the performance of existing approaches significantly surpasses that of ChatGPT, which indicates that even with the help of strong prompts, closed-source LLMs represented by Chat-GPT can hardly catch up with the performance of existing state-of-the-art approaches in vulnerability description mappings. Since GPT-2 performs well, we believe the main reason for ChatGPT's poor performance is the lack of task-oriented finetuning. It seems that the real future is the opensource task-oriented fine-tuned LLMs rather than the closed-source ones.\n\nInteresting Findings\nIn this paper, we can see that ChatGPT's performance on the Vulnerability-to-CWE task is pretty promising, but its performance on the Vulnerabilityto-ATT&CK task is unsatisfactory. We analyzed the definition of ATT&CK Techniques understood by ChatGPT and found that ChatGPT had many misinterpretations of many ATT&CK Techniques. We suspect this may be due to the high diversity of ATT&CK Techniques and the fact that the Vulnerability-to-ATT&CK datasets are of poor quality (compared to the Vulnerability-to-CWE data provided by CVE).\nDue to the fixed correspondence between CAPEC and CWE, we did not initially use the Vulnerability-to-CAPEC task to evaluate ChatGPT. However, like ATT&CK Techniques, CAPEC IDs are more diverse than CWE IDs, and the available Vulnerability-to-CAPEC datasets are lower quality. Therefore, we designed several Vulnerability-to-CAPEC queries to verify whether such tasks suffer from the same problems faced by Vulnerability-to-ATT&CK tasks. The results showed that ChatGPT incorrectly interpreted the CAPEC IDs and output the wrong answers. For example, ChatGPT considers CAPEC-60 the \"Hard-coded Cryptographic Key\", but it should be \"Reusing Session IDs\".\nThese findings hint at improving ChatGPT's performance on complex VDM tasks such as Vulnerability-to-ATT&CK by providing a priori knowledge or predefinition.\n\nLimitations\nWe completed this evaluation based on the Beta version of ChatGPT, and the relevant results may change as OpenAI continues to improve Chat-GPT. In addition, we use the ATT&CK Technique datasets collected from third-party institutions and publicly available data on the Internet. The large size of these datasets makes it difficult for us to verify their accuracy manually. Therefore, errors in these datasets may affect the conclusion of this paper.\n\nConclusion\nIn this paper, we selected two classic tasks and constructed three datasets to pioneer a large-scale, multi-dimensional evaluation of ChatGPT's ability to handle VDM tasks. The results show that ChatGPT performs well on the Vulnerability-to-CWE task and has met or exceeded the level of human experts. We believe that this is because the public Vulnerability-to-CWE data is relatively high-quality. However, the performance on the Vulnerabilityto-ATT&CK task, which has poor public data quality, is unsatisfactory. In addition, ChatGPT appears to have serious problems with the conceptual understanding of CAPEC and ATT&CK Techniques, which may be the main reason for its poor performance on these tasks. In summary, this paper demonstrates that ChatGPT is still not directly usable for VDM tasks, and ChatGPT is not the end of story. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 3.2 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Not applicable. Left blank.\n", "hypothesis": " As the number of vulnerabilities increases day by day, security management requires more and more structured data.  In addition to textual descriptions of vulnerabilities, security engineers must classify and assess vulnerabilities and clarify their associated techniques.  Vulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification, ATT&CK Techniques, and other classifications.  Accurate VDM is necessary to reduce the pressure of security management and improve the speed of security emergency response.  ChatGPT is the latest state-of-the-art closed-source conversational large language model (LLM), which performs excellently on many tasks.  This paper explores the application of closed-source LLMs to real-world security management scenarios by evaluating ChatGPT's performance on VDM tasks.  The results show that although ChatGPT may be close to the level of human experts on some tasks, it still cannot replace the critical role of professional security engineers in vulnerability analysis.  In a word, closedsource LLM is not the end of story..", "answer": true}
{"title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code", "content": "\nIntroduction\nHuman beings rely heavily on the capacity for causal reasoning (Sloman, 2005; Hagmayer et al., 2007) . People understand the observed facts, predict future events, and speculate about what might have happened if things had been different with the help of their causal reasoning skills. For instance, when we go home and find a mess, we probably want to figure out why it happened. If we determine that a bird flew into the house, we might then consider whether the mess could have been avoided if we had closed the window.\nAlthough large language models (LLMs) demonstrate great language understanding and generation abilities, it is still challenging for them to perform complex causal reasoning such as the example above. Powerful LLMs are able to understand single cause-and-effect relations (Brown et al., 2020;  Figure 1 : Causal relationships between events in two causal reasoning tasks. Wang et al., 2021) , like a man losing his balance causes him to fell. However, when it comes to more complex causal structures involving multiple events and alternative branches (like close the window or not), LLMs perform much inferior to humans (Bhagavatula et al., 2019; Qin et al., 2019) . In this paper, we consider two challenging causal reasoning tasks: abductive reasoning and counterfactual reasoning. Abductive reasoning requires models to generate a plausible reason for the ending while being consistent with the premise. Counterfactual reasoning asks what will occur in the counterfactual branch. Causal relationships between events in these tasks are shown in Figure 1 .\nA potential difficulty for LLMs to learn complex causal structures is that they are rarely expressed explicitly in the text. News articles or narratives may contain multiple events with causal relationships, like an incident and a chain of consequences. However, these events are often written chronologically, and it is hard to extract the causal structure from the text without further annotation. Branches are expressed rarer in text, except for the multi-branching storytelling style (Nisi and Haahr, 2006) .\nOn the other hand, causal relations are exhibited more commonly in code. Conditional statements like if direct the computer to execute certain commands, provided a condition is met. This explicitly demonstrates the causal relationship between the condition block and the execution block. Code can also express branching with elif or switch statements, and the nesting feature enables code to describe more complex structures 1 . This motivates us to utilize code models in natural language causal reasoning. Recently, large language models of code (Code-LLMs) are receiving increasing attention (Chen et al., 2021; Xu et al., 2022) . They exhibit strong code generation performance, and their structural prediction abilities help complete structural natural language tasks like argument graph generation (Madaan et al., 2022) and event argument extraction (Wang et al., 2022b) . Being pre-trained on code with abundant causal expressions, Code-LLMs may also have gained better causal reasoning abilities.\nWe conduct experiments on the unsupervised abductive reasoning and counterfactual reasoning tasks. To generate task outputs, we design code prompts like Figure 2 to clearly represent the causal structures of the tasks. Results show that Code-LLMs with code prompts perform much better than text-only LLMs and previous methods. To better understand why the code prompts are effective, we break down the prompts and analyze the influence of different aspects. We find that Code-LLMs are very sensitive to the programming structure (specifically, the conditional statements), while being robust towards format perturbations and programming language changes.\nOur main contributions are as follows: 1) We design code prompts to tackle causal reasoning tasks, by leveraging conditional statements in code to represent causal structures. 2) We evaluate Code-LLMs with code prompts on the abductive reasoning and counterfactual reasoning tasks, and exhibit that code models with code prompts are better causal reasoners than text models. 3) We break down the code prompt in detail and find that the programming structure is crucial to the performance.\n\nModeling Causal Structure with Code\nWe convert the input of causal reasoning tasks into the form of code prompt for Code-LLMs to understand better. We expect the prompts to meet two requirements: 1) clearly represent the causal relationships between events, and 2) as most Code-LLMs only support generating at the end, the target output should appear at the end of the prompts. The first requirement is addressed with conditional statements. However, for the second, the target prediction is not always the last part of the conditional statements, e.g., in abductive reasoning we want to predict the hypothesis, which is the condition in the if structure. To address this, we uniformly use functions to represent events. As shown in Figure 2 , the causal structure is described in the main function. All the event functions are listed afterwards, leaving the target event function at the last.\nAbductive Reasoning. Abductive reasoning requires models to generate a plausible hypothesis H given the observations: premise P and ending E. The chronological order of these three events is P \u2192 H \u2192 E, and the hypothesis causes the ending to occur.\nIn Figure 2 , we regard the task definition as an instruction and place it as a comment at the beginning of the prompt. The causal structure is represented in the main function like: executing the premise, and if the hypothesis is met, executing the ending 2 . The content of each event is presented as a comment of its function. The hypothesis function is placed at the last, leaving for models to complete. The generation process stops with a line break.\nCounterfactual Reasoning. Counterfactual reasoning aims to rewrite a story under a counterfactual condition. As in Figure 1 , the input consists of four parts: the premise P , the initial context C 1 , the original ending E 1 , and the counterfactual context C 2 . Models are asked to generate the counterfactual ending E 2 that minimally modifies the original ending E 1 and is coherent with the counterfactual context C 2 .\nThe causal relationships are represented with the if-elif structure. The premise P is executed first, and then if the initial context C 1 is met, the original ending E 1 is executed; otherwise, if the counterfac-tual context C 2 is met, the counterfactual ending E 2 will be executed. For ease of exposition, we call the context hypothesis as well, being consistent with the former task. The event contents are also written as comments for event functions. We use # end to mark the finish of the ending. provides a brief introduction of these methods.\nAutomatic Evaluation. We use the following automatic evaluation metrics: BLEU 4 (Papineni et al., 2002) , ROUGE L (Lin, 2004) 1 reports the automatic evaluation results in the zero-shot setting. CODEX significantly outperforms previous methods and DAVINCI on both tasks (with significance level \u03b1 = 0.01), exhibiting strong causal reasoning ability. Although the two DAVINCI models are based on CODEX, their causal reasoning abilities may be weakened during instruction tuning, and this phenomenon is called alignment tax (Ouyang et al., 2022) . DAVINCI 003 underperforms DAVINCI 002 on most metrics, probably because it tends to generate longer and more discursive outputs, which do not comply with the tasks.\nHuman Evaluation. We conduct pairwise comparison between CODEX and DAVINCI 002 on 100 test examples. Annotators are asked to choose the better output given the task requirements. For abductive reasoning, the outputs are rated from three aspects: coherence with the premise, coherence with the ending, and the overall coherence. For counterfactual reasoning, the outputs are rated from coherence with the context and the extent of preserving the original ending. Each example is rated by at least two annotators, and the average interrater reliability is 0.64.\nThe results are shown in Table 2 . CODEX outperforms DAVINCI 002 in all aspects. It better considers the context in generation, and is able to preserve the original content in counterfactual reasoning.\nContributions of the Model and the Prompt. We exchange the prompts of code and text models, to measure the contributions of the model and the prompt. The results are in Table 3 . We find that CODEX performs better with the code prompt, as the code prompt clearly describes the causal relation between events. Code prompts benefit the text model DAVINCI 002 on abductive reasoning, but have negative impacts on counterfactual reasoning. A possible reason is that the causal structure in counterfactual reasoning is more complicated, leading to a more complex code which is harder for text models to understand.\n\nWhat are Crucial in Code Prompts?\nTo paint a better picture of the key points in the code prompts, we intervene on the prompts from four aspects and measure the influences of the interventions. The four aspects we select are information, structure, format, and language. The former two, the prior information provided and the programming structure of functions, are contentrelated; the latter two, the code format and programming languages, are form-related. An ideal model should rely on the content and be insensitive to form perturbations. Information. We study two types of prior information: task instructions and function names. In No Instruction, we remove the task instruction from the prompts. In Function Name Perturbation, we replace original function names with anonymous functionX. For example, we replace premise() and hypothesis() in Figure 2 with functionA() and functionB(), respectively. It eliminates the information in function names and only allows models to learn the event relations from programming structures.\nStructure. The first way to intervene in the programming structure is to convert the conditional structures into sequential structures, referred to as Sequential Structure. The events are executed sequentially, like premise(), hypothesis(), ending() in abductive reasoning. In the second way called Disruption, we randomly disrupt the positions of the functions in the conditional structure.\nFor instance, if hypothesis(): ending() can be disrupted into if ending(): hypothesis().\nWe also apply the function name perturbation in disruption to eliminate the impact of function names.\nFormat. We test three formats besides the original one: Class, Print and Return. The first one converts the original code into a class. We define the programming structure in the __init__ method, and move the event functions into the class. In Print, we represent the content of events as a string and print it in the function body, like def premise(): print(\"The Smiths ...\"). And in Return, the string is the return value of event functions.\nLanguage. We also convert the original Python programs into two other languages, Java and C, to evaluate the influence of programming languages. CODEX is quite robust towards format and language changes. Settings like Class and Java are even better than the original one, revealing that the performance can be further improved with delicate prompt engineering.\n\nConclusion\nWe investigate the causal reasoning ability of Code-LLMs. With code prompts of conditional statements, Code-LLMs achieve great performance in abductive and counterfactual reasoning, outperforming text-only LLMs significantly. Our study on different aspects of code prompts shows that providing a reasonable causal structure in code can help generate plausible outputs, and Code-LLMs are robust towards format perturbations.\n", "hypothesis": " Causal reasoning, the ability to identify causeand-effect relationship, is crucial in human thinking.  Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning.  Given the fact that programming code may express causal relations more often and explicitly with conditional statements like if, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to textonly LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is not as important as the format perturbations in code prompt design, while Code-LLMs are robust towards programming language changes.  Code and data are available at https://github.com/xxxiaol/magic-if..", "answer": false}
{"title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. \u2022 Advanced architectures such as BERT may only achieve the best results if properly used. Linear methods can help us check if advanced methods' results are reasonable. In the deep-learning era, the younger generation often thinks that linear classifiers should never be considered. Further, they may be unaware of some variants of linear methods that are particularly useful for text classification (see Section 3.1). Therefore, the paper serves as a reminder of this oftenforgotten technique.\nFor our illustration, we re-investigate an existing work (Chalkidis et al., 2022) that evaluates both linear SVM and pre-trained language models, but the authors pay more attention to the latter. The linear method is somewhat ignored even though the performance is competitive on some problems. We carefully design experiments to compare the two types of methods. Our results fully demonstrate the usefulness of applying linear methods as simple baselines.\nSome recent works (e.g., Yu et al., 2022; Gomes et al., 2021) have shown the usefulness of linear classifiers in the deep-learning era. However, they either consider sophisticated applications or investigate advanced settings in which linear methods are only one component. In contrast, in this paper, we consider the basic scenario of text classification. A more related work (Wahba et al., 2023) has demonstrated the effectiveness of linear classifiers over PLMs on some problems. However, our investigation on linear methods is more comprehensive.\nThe discussion also reminds us the trade-off between performance gain and the cost including running time, model size, etc. Simple methods are useful to benchmark and justify the usage of advanced methods.\nMethod Chalkidis et al. (2022) . In each Micro-F1 column, the best result is bold-faced. \"N/A\" means not available in their work. For example, the authors did not report the training time and the number of parameters of linear SVMs.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS # \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1\nThis paper is organized as follows. In Section 2 we take a case study to point out the needs of considering linear methods as a baseline for text classification. We describe the linear and BERT-based methods used for investigation in Section 3. The experimental results and main findings are in Section 4, while Section 5 provides some discussion. Additional details are in Appendix. Programs used for experiments are available at https://github.com/JamesLYC88/ text_classification_baseline_code.\n\nText Classification These Days: Some Issues in Applying Training Methods\nLarge PLMs have shown dramatic progress on various NLP tasks. In the practical use, people often directly fine-tune PLMs such as BERT on their data for a few epochs. However, for text classification, we show that this way may not always get satisfactory results. Some simple baselines should be considered to know if the obtained PLM model is satisfactory. We illustrate this point by considering the work on legal document classification by Chalkidis et al. (2022) , which evaluates the following sets. The study in Chalkidis et al. (2022) comprehensively evaluates both BERT-based PLMs and linear SVMs. They use Micro-F1 and Macro-F1 to measure the test performance. 2 In Table 1 , we present their Micro-F1 results and running time of each model.\n\nLinear Models Worth More Investigation\nThe investigation in Chalkidis et al. (2022) focuses on BERT and its variants, even though from Table 1, the performance of BERT-based methods may not differ much. While they did not pay much attention to linear SVM, by a closer look at the results, we get intriguing observations: \u2022 Linear SVM is competitive to BERT-based PLMs on four of the six data sets. For SCO-TUS, linear SVM even outperforms others with a clear gap. \u2022 Surprisingly, given linear SVM's decent performance, its training time was not shown in Chalkidis et al. (2022) , nor was the number of parameters; see the \"N/A\" entries in Table 1 . With the observations, we argue that the results of linear models are worth more investigation.\n\nSettings for Investigation\nTo better understand the performance of linear models and BERT-based PLMs, we simulate how people work on a new data set by training these methods. We consider a text classification package Lib-MultiLabel 3 because it supports both types of train-ing methods.\n\nLinear Methods for Text Classification\nTo use a linear method, LibMultiLabel first generates uni-gram TF-IDF features (Luhn, 1958; Jones, 1972) according to texts in the training set, and the obtained factors are used to get TF-IDF for the test set. It then provides three classic methods that adopt binary linear SVM and logistic regression for multi-class and multi-label scenarios. 4 Here we consider linear SVM as the binary classifier behind these methods.\n\u2022 One-vs-rest: This method learns a binary linear SVM for each label, so data with/without this label are positive/negative, respectively. Let f \u2113 (x) be the decision value of the \u2113-th label, where x is the feature vector. For multi-class classification, \u0177 = argmax \u2113 f \u2113 (x) is predicted as the single associated label of x. For multi-label classification, all labels \u2113 with positive f \u2113 (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al. (2022) did, though our TF-IDF feature generation is simpler than theirs by considering only uni-gram. 5 \u2022 Thresholding (Yang, 2001; Lewis et al., 2004; Fan and Lin, 2007) : This method extends one-vsrest by modifying the decision value for optimizing Macro-F1. That is, we change the decision value to f \u2113 (x) + \u2206 \u2113 , where \u2206 \u2113 is a threshold decided by cross validation. \u2022 Cost-sensitive (Parambath et al., 2014) : For each binary problem, this method re-weights the losses on positive data. We decide the reweighting factor by cross validation to optimize Micro-F1 or Macro-F1.\nThese methods basically need no further hyperparameter tuning, so we can directly run them. The last two methods are extensions of one-vs-rest to address the imbalance of each binary problem (i.e., few positives and many negatives). The design relies on the fact that the binary problems are independent, so such approaches cannot be easily applied to deep learning, which considers all labels together in a single network.\n\nBERT-based Methods for Text Classification\nLibMultiLabel also provides BERT-based methods, which involve several hyper-parameters, such as the learning rate. While practitioners may directly choose hyper-parameters, to seriously compare with linear methods, we run BERT by conducting hyper-parameter selection. More details are in Appendix F.\n\nExperimental Results and Analysis\nIn Table 2 , we follow Chalkidis et al. (2022) to report Micro-F1 and Macro-F1 on the test set. The training time is in Table 3 .\n\nLinear Methods are Good Baselines\nIn Table 2 , our one-vs-rest results are slightly worse than the linear SVM results in Chalkidis et al. (2022) , which also applies the one-vs-rest strategy. As mentioned in Section 3.1, the difference is mainly due to our use of simple uni-gram TF-IDF features. Anyway, our one-vs-rest is still competitive to BERT results in Chalkidis et al. (2022) on the last four problems. More importantly, the two extensions of one-vsrest (i.e., thresholding and cost-sensitive) improve almost all situations. For data sets ECtHR (A) and ECtHR (B), where originally one-vs-rest is significantly lower than BERT results in Chalkidis et al. (2022) , the gap reduced considerably.\nFor the training time in Table 3 , though the two extensions take more time than the basic one-vsrest strategy, all the linear methods are still hundreds of times faster than BERT. Further, linear methods were run on a CPU (Intel Xeon E5-2690), while for BERT we need a GPU (Nvidia V100). The model sizes listed in Table 4 also show that linear SVM requires a much smaller model than BERT, where details of our calculation are in Appendix D.\nThe results demonstrate that linear methods are useful baselines. They are extremely simple and efficient, but may yield competitive test performance.\n\nLinear Methods can Help to See if\nAdvanced Methods Are Properly Used Surprisingly, our running of LibMultiLabel's BERT leads to worse test performance than linear methods on almost all data sets. More surprisingly, a comparison between the BERT results by LibMul-tiLabel and those in Chalkidis et al. (2022) shows Method It is essential to check the discrepancy between the two BERT results. We find that Chalkidis et al. (2022) use some sophisticated settings to run BERT for the first three sets (i.e., ECtHR (A), ECtHR (B), and SCOTUS). They split every document into 64 segments, each of which has no more than 128 tokens, and apply BERT on each segment. Then, they collect the intermediate results as inputs to an upper-level transformer. After repeating the same process via LibMultiLabel, we can reproduce the results in Chalkidis et al. (2022) ; see details in Appendices E, F, and G.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F\nWe learned that they considered the more sophisticated setting of running BERT because by default, BERT considers only the first 512 tokens. Thus, for long documents, the training process may miss some important information. However, in practice, users may forget to check the document length and are not aware of the need to apply suitable settings. The above experiments demonstrate that BERT can achieve superior results if properly used, but sometimes, a direct run lead to poor outcomes. Linear methods can serve as efficient and robust baselines to confirm the proper use of an advanced approach.\n\nDiscussion and Conclusions\nIn our experiments, we encounter an issue of whether to incorporate the validation set for training the final model, which is used for predicting the test set. For linear methods, we follow the common practice to include the validation set for obtaining the final model. However, for BERT or some other deep learning models, the validation set is often used only for selecting the best epoch and/or the best hyper-parameters. To fully use the available data, we have investigated how to incorporate the validation set for BERT. Experimental results and more details are in Appendix H.\nFor some text sets evaluated in this work, we have seen that simple linear methods give competitive performance. The reason might be that each document in these sets is not short. 6 Then TF-IDF features are sufficiently informative so that linear methods work well. Across all NLP areas, an important issue now is when to use PLMs and when not. We demonstrate that when PLMs may not perform significantly better, traditional methods are much simpler and require fewer resources. However, having a simple quantitative measurement to pre-determine when to use which remains a challenging future research problem. In summary, the study reminds us of the importance of employing simple baselines in NLP applications.\n", "hypothesis": " Large-scale pre-trained language models such as BERT are popular solutions for text classification.  Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model.  In this opinion paper, we point out that this way may only sometimes get satisfactory results.  We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods.  First, for many text data, linear methods show competitive performance, high efficiency, and robustness.  Second, advanced models such as BERT may only achieve the best results if properly applied.  Simple baselines help to confirm whether the results of advanced models are acceptable.  Our experimental results fully support these points..", "answer": true}
{"title": "Leveraging Prefix Transfer for Multi-Intent Text Revision", "content": "\nIntroduction\nRevision is an essential process to improve the text quality (Vaughan and McDonald, 1986) . During this process, writers perform various editing operations on the text with different editing intentions. As shown in Figure 1 , the writer corrects misspelled words to improve text fluency, deletes redundant words to improve text clarity, adds connective words to improve text coherence, inserts adverbs to convey the writer's writing preferences (style) and modifies data to update text information (meaning-changed).\nLots of recent studies have focused on a text revision task corresponding to a specific edit intention, such as grammatical error correction (Omelianchuk She went to the markt\nThe changes made the paper better than before.\nText Revision She works hard.\nShe is successful.\nEverything was rotten.\n\nShe went to the markt market\nThe changes made the paper better than before improved the paper.\nShe works hard. She; therefore, she is successful.\nEverything was awfully rotten. This method improves the model accuracy from 64% to 7883%. et al., 2020; Kaneko et al., 2020; Liu et al., 2021; Yang et al., 2022 ), text simplification (Dong et al., 2019; Jiang et al., 2020; Omelianchuk et al., 2021; Martin et al., 2022) , and text style transfer (Malmi et al., 2020; Reid and Zhong, 2021) . The work divides text revision into several independent problems. While some methods with strong universality can be applied to multiple tasks (Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020) , they train different models on various data sets. Real-world scenarios require addressing multiple types of editing errors at the same time, such as grammatical errors, spelling errors, etc. But these methods failed to integrate knowledge from these tasks into a unified model.\n\nmeaningchanged\nTo solve the problem, Du et al. (2022) attempted to train one model using data with multiple editing intentions and leveraged edit intent information by simply appending it to the input. However, when adding a new intent, the entire model must be re-trained. A more lightweight and scalable approach to multi-intent text revision is still required.\nLi and Liang (2021) proposed a new kind of prompt tuning method to quickly adapt a pretrained model to new tasks, which is called prefixtuning. Prompt tuning can help the pre-trained language model to locate the task learned in pretraining and enable the related knowledge to model text revision with different edit intentions (Reynolds and McDonell, 2021) . This method enables a model to handle multiple edit intentions in a lightweight and scalable way.\nIn this paper, we present our method: a prefixtuning-based model which adapts to text revision with multiple edit intentions. This method involves a two-step training process. In the first step, we initialize a pre-trained language model (PLM) and train multiple prefixes on it. Each edit intention corresponds to a prefix. In the second step, a prefix transfer module is trained at each attention layer of the PLM. The prefix transfer module is configured as two attention units that act respectively on this layer's key states and value states. It enables our model to learn a tailored prefix for the given input with the help of prefix embeddings from the predefined tasks.\nWe conduct experiments on ITERATER (Du et al., 2022) , an iterative text revision dataset. It mainly contains parallel sentences with five edit intentions: fluency, coherence, clarity, style, and meaning-changed. The results show that our approach performs better than the fully fine-tuned BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) baselines reported in Du et al. (2022) with fewer training parameters.\n\nIterative Text Revision\nFor the first time, Du et al. (2022) systematically studied the iterative revision phenomenon in human writing. They presented the ITERATER, an annotated dataset across multiple domains of formally human-written text, which includes Wikipedia, ArXiv, and Wikinews. And they trained several types of text revision models using ITERATER. Dwivedi-Yu et al. (2022) presented EDITEVAL, an instruction-based benchmark, to evaluate the editing capabilities of models and they also included the test set of ITERATER in it. Based on Du et al. (2022) , our work further explores the method of text revision.\n\nTransfer Learning of Prompt Tuning\nTransfer learning is a common and powerful technique in NLP (Raffel et al., 2020) . Some recent studies have tried to improve prompt tuning performance by leveraging the knowledge of multiple related or unrelated tasks. Asai et al. (2022) used an attention module to make use of the knowledge in exiting soft prompts (Lester et al., 2021) while learning a new task. Chen et al. (2022) improved the few-shot text summarization by multi-task pretraining and prefix-tuning. Specifically, they pretrained a summarization model on a set of popular summarization datasets and then conducted prefixtuning for it on an unseen summarization task. Different from their modeling of a new task through existing tasks, our work aims to achieve the mutual utilization of knowledge between different edit intents in text revision.\n\nMethod\nThe revision task can be defined as the following process: given a source sentence x = [x 1 , . . . , x m ] and an optional edit intent e \u2208 E to generate a revised sentence y = [y 1 , . . . , y n ], where E is the set of all edit intentions. Note that e is optional because it can be inferred from the input x.\nOur method is depicted in Figure 2 . It includes two stages: the multi-prefix tuning stage and the prefix transfer stage.\n\nMulti-Prefix Tuning Stage\nThe prefix is a set of parameters on every attention layer of PLM. For an edit intention e, at each attention layer, the prefix can be described as P e = {P K e , P V e }, where P K e and P V e are parameters added before the key states and value states in this attention layer. After adding these parameters, the calculation of the attention head in this layer becomes:\nH = Attention(Q, [P K e ; K], [P V e ; V ]) (1)\nwhere H is the output vector sequence; Q, K, V are query states, key states, and value states, respectively; Attention means scaled dot-product attention. Only P K e and P V e are updated during the training process. Note that we ignore the layer number information because the operation for each layer is the same.\nAs shown in the left part of Figure 2 , for every edit intention e, we train a prefix P e accordingly. In this way, the model could revise an intentionannotated text by activating the corresponding prefix at inference.\n\nPrefix Transfer Stage\nIdentifying edit intention is always an ambiguous work. At the prefix transfer stage, we aim to build a new prefix for an unannotated input instance by transferring existing prefixes. The new prefix P new is instance-specific.\nThe prefix transfer stage is described in the right part of Figure 2 . At each layer, we rearrange the prefixes {P e | e \u2208 E} obtained in the last stage as\nP K = {P K\ne | e \u2208 E} and P V = {P V e | e \u2208 E} according to whether they are configured before the key states or before the value states. Then a pair of attention units G K and G V are trained for P K and P V .\nTake G K as an example. It calculates the similarity between the key states K and every P K e in P K to get attention scores.\nThe similarity can't be calculated directly, because K and P K e have different lengths. So we perform the max-pool operation for length dimension on K and P K e . After that, we obtain K \u2208 R d and P K e \u2208 R d , where d is the dimension of the hidden states in the PLM.\nTo get attention scores, we train a fully connected layer to extract features from K:\nEQUATION\nwhere W \u2208 R d\u00d7d is a transfer matrix updated during training. Following Asai et al. (2022) , we use SiLU (Elfwing et al., 2018) for the non-linear layer and add a Layer Norm (Ba et al., 2016) layer:\nEQUATION\nThen, we calculate the attention scores for intent e as follows:\nEQUATION\n)\nwhere T is the softmax temperature (Radford et al., 2021) which could avoid making the attention unit over-confident.\nFinally we use them to build P K new as follows:\nEQUATION\nIn the same way, we get P V new by G V . Using the new prefix P new = [P K new , P V new ], our system could revise the unannotated input instance with the knowledge from existing prefixes.\n\nExperimental Setup\nWe choose BART-large as the PLM for our system and use adapter-transformers (Pfeiffer et al., 2020) to implement prefix-tuning. More implementation details are in Appendix A.\n\nDatasets\nWe conduct our experiments on the iterative text revision dataset: ITERATER (Du et al., 2022) . We remove the Other class of the data as it essentially contains a variety of unrecognized edit intentions and accounts for a small proportion (1.44%). The entire dataset consists of two parts: ITERATER-HUMAN and ITERATER-FULL. The former is a smaller dataset with manual annotation of edit intentions, while the latter is a large dataset annotated by a classification model trained on ITERATER-HUMAN. We train our model on both of them. Following Du et al. (2022) , we report the results on the test set of ITERATER-HUMAN in Section 5, which is completely a human-created dataset and is reliable for evaluation. We show more details of the datasets in Appendix B.\n\nEvaluation Metrics\nFollowing previous work, we report three metrics: SARI (Xu et al., 2016) , Rouge-L (Lin, 2004) , and BLEU (Papineni et al., 2002) . Among them, SARI is considered an important metric in situations where input text and output text have a large overlap in words. It also indicates the positive impact of revisions on document quality. The setting of evaluation metrics is the same as Du et al. (2022) . We use the metrics package from Huggingface transformers (Wolf et al., 2020) to calculate the SARI, BLEU, and Rouge-L scores.\n\nModels Setup and Baselines\nUsing our method, we train the models in two ways: the model that only trains the multi-prefix tuning stage and that trains both the multi-prefix tuning stage and the prefix transfer stage.\nWe compare our method with three baselines: full fine-tuning BART (BART-FineTune), full finetuning PEGASUS (PEGASUS-FineTune), and prefixtuning of BART with a single prefix (BART-SinglePrefix). Both BART and PEGASUS are generative models based on the transformer architecture. Compared to the edit-based model FELIX, they perform better. We use the results reported by Du et al. (2022) for these two models. Furthermore, we compare BART-SinglePrefix as a possible technical solution as we choose BART as our backbone model. BART-SinglePrefix trains only one prefix on the entire dataset.\nAll three baselines are trained with two config-urations. The first configuration is using the pure sentence pairs without edit intention annotations to train the model. The second configuration is appending an edit intent token at the beginning of the input text during the training process, which is the same as the approach of Du et al. (2022) .\n5 Results and Analysis\n\nMain Results\nThe main results are shown in Table 1 . Compared to training with a single prefix, the setting of multiple prefixes can improve the results, especially training on ITERATER-HUMAN. Meanwhile, with fewer training parameters, the multi-prefix setting could achieve a comparable SARI score and better average score than the fully fine-tuned BART and PEGASUS baselines. Moreover, prefix transfer could further improve the model's performance. Training on ITERATER-HUMAN, prefix transfer significantly improves the SARI score from 33.12 to 36.01 and gets the highest average score of 67.91. Training on ITERATER-FULL, prefix transfer can also improve the average score from 67.23 to 68.36.\nAn interesting phenomenon is that training on different datasets results in different gains for prefix transfer in evaluation metrics. On ITERATER-HUMAN, prefix transfer improves the SARI score significantly. While on ITERATER-FULL, prefix transfer mainly improves the BLEU score and Rouge-L score. One possible explanation is that in situations when the training data is small, prefix transfer tends to learn more editing operations to improve text quality. In this way, the SARI score related to editing operations will be improved significantly. When the training data is sufficient, pre- fix transfer will model the gold reference in more detail. So the BLEU score and the Rouge-L score will be improved.\n\nAnalysis\nWe further tried to use different training data at different stages of training to conduct experiments.\nThe results are shown in Table 2 . We find that the best practice is to train the model on ITERATER-FULL in the multi-prefix tuning stage and on ITERATER-HUMAN in the prefix transfer stage, which gets the highest SARI score and average score. This may be because of the different distributions of manually annotated edit intent and automatically annotated edit intent. The auto-annotated dataset ITERATER-FULL contains many incorrectly classified sentences, which may cause mismatched knowledge in prefixes. In the prefix transfer stage, due to the existence of mismatched knowledge and incorrectly classified sentences, the continued use of the same training data may finally cause a certain degree of negative transfer. However, if we use ITERATER-HUMAN in the prefix transfer stage, the impact of negative transfer will be mitigated, because ITERATER-HUMAN only contains correctly classified sentences.\nIn Appendix C, we separately provide the performance results on different edit intentions of the best-performing model.\n\nConclusion\nIn this paper, we introduce a new method for multiintent text revision. The system is based on prefixtuning, which first obtains a prefix for every edit intention and then learns to transfer the knowledge in prefixes for every input instance by training a prefix transfer module. This prefix transfer module is configured as two attention units that act respectively on the key states and the value states at each attention layer of the PLM. In this way, our method can make full use of the knowledge of various edit intentions and does not need to anno-tate the intentions of the input. The experimental results show that our method significantly outperforms baselines, and both multi-prefix and prefix transfer settings could improve the performance.\n", "hypothesis": " Text revision is a necessary process to improve text quality.  During this process, writers constantly edit texts out of different edit intentions.  Identifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts according to one specific edit intention. In this work, we aim to build a multiintent text revision system that could revise texts without explicit intent annotation.  Our system is based on prefix-tuning, which first gets prefixes for every edit intent, and then trains a suffix transfer module, enabling the system to selectively leverage the knowledge from various suffixes according to the input text.  We conduct experiments on the ITER-ATER dataset, and the results show that our system outperforms baselines.  The system can significantly improve the SARI score with more than 3% improvements, which thrives on the learned editing intention prefixes..", "answer": false}
{"title": "An Exploratory Study on Model Compression for Text-to-SQL", "content": "\nIntroduction\nText-to-SQL is an important task that has been gaining the attention of researchers over the years. Formally, given a query q and a relational database D, the goal of Text-to-SQL is to build a model f such that s = f (q, D | \u03b8) where \u03b8 is a vector of model parameters and s is a predicted SQL statement which we can use to retrieve the answer to q from D.\nText-to-SQL has many potential applications that can improve our standard of living. For example, medical chatbots can convert user queries into SQL statements and then use them to retrieve relevant information from medical knowledge bases. Industry can leverage Text-to-SQL tools to help employees shorten the time needed to write complex SQL queries, thereby improving overall work productivity.\nThe recent emergence of complex Text-to-SQL datasets containing complicated SQL and crosstable setup has driven researchers to develop huge models that encode various complex relationships between table schema and query with large pretrained language models such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) . These models are usually sequence-to-sequence models that generate SQL statements sequentially or sketch-based models that use classifiers to fill in the slots of SQL templates.\nHowever, despite achieving state-of-the-art performances on benchmark datasets, such models are usually both memory and computationally expensive, making it technically challenging to deploy them in memory-constrained real-world applications that require low inference latency. Therefore, to deploy state-of-the-art Text-to-SQL models in real-world production environments, we must drastically improve the inference time and reduce the number of parameters in these models.\nWe turn to the field of model compression (Cheng et al., 2017) for solutions that can speed up inference without significantly hurting model performance. Formally, the goal of model compression is to reduce f to a smaller model f \u2032 such that s \u2032 = f \u2032 (q, D | \u03b8 \u2032 ). Ideally, we want s \u2032 to be the same as s and dim(\u03b8 \u2032 ) to be much smaller than dim(\u03b8).\nIn this paper, we thoroughly examine the feasibility of using model compression techniques to build faster and more accurate Text-to-SQL models that we can successfully deploy in the real world. For this, we carefully apply a few model compression methods to representative sequence-to-sequence or sketch-based Text-to-SQL models on three datasets: WikiSQL, Spider, and TableQA. The main findings of this paper are: (i) sketch-based models generally respond well to model compression techniques, while sequence-to-sequence models show mixed results, (ii) we observe better speed improvements in Sketch-based models as their slot-filling components are much faster than the decoding components of sequence-to-sequence models. (iii) model compression techniques work poorly on state-of-the-art Text-to-SQL models built on pre-trained encoder-decoder language models such as T5.\nWe hope our findings can empower practitioners to make more informed decisions when selecting Text-to-SQL models and compressing them appropriately for real-world deployments. . Contrarily, Spider contains large samples of complex SQL instances that connect multiple tables with primary and foreign keys with more advanced clauses such as nested queries, JOIN ON, and ORDER/GROUP BY.\n\nBaseline Models\nRecent deep neural Text-to-SQL models can be broadly classified under two categories: sequenceto-sequence models and sketch-based (also known as slot-filling) models.\n\nSequence-to-sequence models\nSequence-to-sequence models are generally made up of an encoder component that converts user query inputs together with database information into a hidden vector and a decoder component that generates SQL statements based on the output hidden vectors from the encoder. BRIDGE (Lin et al., 2020) encodes input questions and table schema with BERT and LSTM and generates SQL predictions with a pointer-generator decoder (See et al., 2017) supported by a schemaconsistency driven search space pruning strategy. RAT-SQL (Wang et al., 2020a ) also encodes input instances with BERT but generates SQL as an abstract syntax tree (AST) with a tree-structured decoder (Yin and Neubig, 2017) . It also incorporates a relation-aware self-attention mechanism that further improves schema-linking, schema-encoding, and representation of the encoder. PICARD (Scholak et al., 2021) is a state-of-theart algorithm that directly fine-tunes a pre-trained encoder-decoder language model T5 (Raffel et al., 2020) on Text-to-SQL data, and then constrain the decoder to output valid SQL by integrating an incremental parsing strategy to the beam search process.\n\nSketch-based model\nSketch-based methods also encode user inputs into vectors but only need to fill in slots in SQL sketches rather than generating full SQL statements. Each SQL sketch is a template SQL statement with placeholder slots and the goal of sketch-based models is to predict the best item to go into each slot. NL2SQL-RULE (Guo and Gao, 2019 ) is a standard sketch-based model which uses BERT and LSTM to encode input query and database information and predict outputs in slots of SQL sketches.\n\nCompression Techniques\nWe follow Sun et al. (2021) and experiment with the following model compression techniques in this study: Layer Pruning (Sajjad et al., 2022 ) is a simple yet effective strategy that discards a certain number of layers from transformer-based language models before fine-tuning the pruned models on downstream tasks. We apply the top-layer pruning strategy which deletes the top N encoder or decoder layers before the start of any training. Knowledge Distillation (Hinton et al., 2015) is a method that compresses deep neural network models by distilling useful knowledge from a larger model (teacher) to a smaller model (student). We follow Jiao et al. (2020) and distill smaller language models from larger ones such as BERT-large, before fine-tuning Text-to-SQL models on those distilled models. For WikiSQL and Spider, we experiment with the distilled English language models from MiniLM 1 (Wang et al., 2020b) , while for TableQA, we use the Chinese TinyBERT models 2 . Token Pruning For PICARD model, We also apply token pruning (Goyal et al., 2020; Kim et al., 2022) , which is a different pruning strategy that gradually removes redundant token encodings from the outputs of each encoder layer before feeding the reduced number of tokens to the next encoder layer. We follow Goyal et al. (2020) and implement an attention scoring mechanisms which weights the significance of each token by the sum of attention weights it gets from other tokens. The tokens with the lowest significance scores (based on predetermined thresholds) for each encoder layer are dropped.\n\nEvaluation Metrics\nWe evaluate our experiment results using Exact set match (ESM) (Yu et al., 2018) . ESM decomposes every pair of predicted and gold SQL queries into sets clauses and then computes the percentage of exact set matches over all pairs (Zhong et al., 2020) .\n\nExperiment Setup\nIn most cases, we follow the recommended configurations in corresponding papers. We may adjust the batch sizes and learning rates slightly to fit the experiments on our hardware. We train our models on servers with either NVIDIA GV100 GPU (32GB) or RTX A6000 (45GB) but calculate inference speeds by running models on only CPUs with batch size set to one, which better mimics the situations in the real world. For all datasets, we use their dev sets as the test sets and create new train-dev sets in the ratio of 4 to 1 from the original train set. We early stop our models based on the ESM scores on dev sets and report average test set ESM scores over 5 different runs. Other than PI-CARD, we use BERT-large for all English datasets and RoBERTa-Zh (Cui et al., 2020) for TableQA.\n\nSimple datasets\nWikiSQL As shown in Figure 1 WikiSQL. For example, we can remove 50% of the encoder layers from BRIDGE, while only taking a penalty of only 0.82% drop in Exact Set match (ESM). When only keeping the bottom 6 encoder layers, NL2SQL-RULE can still perform at 0.834 ESM, a 3.65% drop from the original unpruned model. For knowledge distillation, we fine-tuned BRIDGE on two versions of MiniLM (Wang et al., 2020b) : L6xH768 and L6xH384. Results show that BRIDGE trained on the MiniLM language models performs slightly worse than the layer pruning method with similar number of layers. However, this is acceptable given the hidden sizes of the MiniLM models are 384 and 768, which are smaller than the hidden size of 1024 for BERT-large. TableQA We notice several differences in results between WikiSQL and TableQA. First, the performances of RATSQL on TableQA are significantly lower than those of NL2SQL-RULE. For example, unpruned NL2SQL-RULE achieves an ESM of 0.8 but unpruned RATSQL only achieves 0.69 despite our best efforts. Second, we observe more significant drops in performances when applying layer pruning and knowledge distillation to RATSQL than NL2SQL-RULE. For example, we observe only a 3.63% drop in ESM dropping the first 16 encoder layers of NL2SQL-RULE but notice an 18.8% drop in the performance of RATSQL with the same configurations. Last but not least, models trained on distilled language models perform slightly worse than the layer pruned models due to their smaller hidden sizes except for NL2SQL-RULE on TinyBERT with 6 layers and 768, which achieves an ESM of 0.80, even higher than that of the unpruned NL2SQL-RULE.\nRecommendation: We recommend using slotfilling models when building applications that only deal with simple queries. These models not only perform comparably or even better than sequenceto-sequence models, but also respond better to recent model compression techniques. Spider As PICARD was trained on a 3 billion parameters pre-trained language model with an encoder and a decoder of similar size, we show three sets of results by applying layer pruning on 1) the encoder, 2) the decoder, and 3) both the encoder and decoder. As seen in Figure 3 , the layer pruning strategy does not work as well on PICARD. At around six layers, PICARD loses around 49.9% and 40.3% of its original performance for encoder-only and decoder-only pruning settings respectively. For the encoder+decoder pruning strategy, we observe similar levels of performance when discarding the same number of transformer layers as the other two configurations. For example, dropping 3 layers each from the encoder and decoder gets us 0.641 ESM, compared to 0.624 when dropping 6 decoder layers and 0.648 when dropping 6 encoder layers. On the other hand, RATSQL demonstrates better compression results on Spider, maintaining 92.6% of original performance while keeping on six encoder layers, contrary to the results on TableQA.\n\nComplex dataset\nToken pruning We follow the implementation of Goyal et al. (2020) and apply token pruning to PI-CARD. We plot the ESM performance of a tokenpruned model against the number of retained tokens in Figure 4 . As seen in the plots, although we can remove an average of 286 tokens from the top six encoder layers, we are only able to discard an average of 41 tokens from the bottom six layers. For example, we see a sharp drop in ESM performance by just pruning around 40 tokens from the 3rd encoder layer. Similarly, we also observe steady drop in ESM performance when pruning more than 100 tokens from encoder layers 15 and 18. Our final model achieves an ESM of 0.527 (26.3% drop in performance) while only seeing a 5.2% improvement in inference speed when applying token pruning to the encoder of T5. As we cannot significantly prune the number of tokens in each encoder layer without severely hurting model performance, we conclude token pruning is also not effective on the PICARD model. Recommendation: Our results suggest that both layer and token pruning are not effective on PI-CARD and we would get better compression performances on sequence-to-sequence models like RATSQL, which has a much bigger encoder than decoder in terms of model size.\n\nDiscussion\nThe main difference between recent sequence-tosequence and sketch-based models is related to how we generate the SQL statements. Compared to the lightweight slot-filling classifiers in sketchbased models, recent sequence-to-sequence model decoders rely heavily on grammar-guided decoding processes which requires navigating through a huge search space and requires an even longer inference time than the encoders. For example, 76.62% and 87.14% of the inference time are spent in the decoding step for BRIDGE and RATSQL, while most of the inference time in NL2SQL-RULE is spent on the encoder. Considering the speed, compression effectiveness, and performance, sketch-based models would be better choices if we get similar performances on benchmark datasets.\n\nConclusion\nThis paper investigates whether we can use model compression to improve the inference efficiency of recent Text-to-SQL models that rely heavily on large pre-trained language models. Our results show that on simple Text-to-SQL datasets, we can deploy simple strategies such as layer pruning to obtain a 5-6x speedup without significantly hurting model performances. We also observe that sketchbased models generally respond better to model compression than sequence-to-sequence models. However, we are not able to effectively compress PICARD on the spider dataset and we would tackle this problem as a future work.\n", "hypothesis": " Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases.  Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in realworld applications that require real-time or on-device processing capabilities.  In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-tosequence Text-to-SQL models.  Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements..", "answer": true}
{"title": "Unsupervised Task Graph Generation from Instructional Video Transcripts", "content": "\nIntroduction\nTasks in the real-world are composed of multiple key steps with specific dependencies that dictate the order in which they can be performed (e.g., one has to check for breathing before performing CPR). Exposing these dependencies between key steps has many downstream applications including assisting human users in troubleshooting and building artificial agents that efficiently learn and perform new tasks. However, information about tasks is typically available in unstructured and noisy form in the wild (e.g., 'how to' descriptions or instructional video transcripts), presenting a major challenge in extracting structured representations.\nThere is a long history of work on reasoning about tasks, events and temporal ordering, broadly referred to as 'script understanding' ( of script understanding problems include generating a sequence of steps from a given task description (e.g., bake a cake) (Lyu et al., 2021; Sancheti and Rudinger, 2021; Sun et al., 2022) and generating flow graphs from goal and event descriptions (Pal et al., 2021; Sakaguchi et al., 2021) . Script generation also manifests in interactive settings such as simulated embodied environments where agents are expected to reason about subgoals in order to complete tasks (Logeswaran et al., 2022; Huang et al., 2022). Many of these prior approaches either fine-tune language models on human-annotated scripts or rely on knowledge encoded in language models to generate scripts. In contrast, we attempt to use pre-trained language models as an information extraction system to perform zero-shot script inference from noisy ASR (Automatic Speech Recognition) transcriptions of instructional videos describing a task.\nOur focus in this work is to generate a directed graph that represents dependency relationships between the key steps relevant to a real-world task. Figure 1 (a) shows a graph predicted by our approach for performing CPR. An example depen-\n\nGet out two slices of bread\nSpread peanut butter on one slice\n\nSpread jelly\nJoin the slices \"hello and welcome. this is episode number one of traditional school lunches. today I'm going to explain how to make a simple peanut butter and jelly sandwich. before we start making our ..\" log p LM = -270 \"how to make a peanut butter and jelly sandwich. so we came out here all the way today to make one of these. that's right a peanut butter and jelly sandwich. there's a lot of science behind ..\" Given multiple text transcripts of a task, we 1) Summarize the steps described in the transcript, 2) Identify the key steps, 3) Re-label summary steps with key steps, 4) Rank key step sequences using a language model and 5) Consolidate top-k sequences to generate a task graph for the given task. dency that can be read from the graph is that checking for safety hazards has to have happened before any other step (i.e., it is a precondition that needs to be satisfied). In this paper, we will use the term task graph to refer to such dependency graphs.\n\nSpread peanut butter and jelly\nMore formally, consider a real-world task \u03c4 . We assume that multiple text transcripts t 1 , . . . , t n describing how this task is performed are available. 1 We assume that having access to such multiple transcripts helps robustly identify the dependencies between key steps so that an accurate task graph can be generated. For instance, if step y frequently follows step x, it is highly likely that step x needs to happen before step y (i.e., is a precondition). Our goal is to generate a task graph for the given task \u03c4 which models these dependencies. In particular, this involves (i) Identifying the key steps K = {k 1 , . . . , k m } relevant to performing the task and (ii) Generating a graph with nodes k i and edges representing precondition relationships.\nOur contributions in this work are as follows. \u2022 We propose an unsupervised task graph generation approach that uses pretrained language models to infer key steps and their dependencies from multiple text descriptions of a real-world activity. \u2022 We propose ranking and filtering mechanisms to improve the quality of generated task graphs. \u2022 We demonstrate the effectiveness of the proposed approach compared to strong supervised and unsupervised baselines on two datasets. 1 Each transcript is a text document derived from an instructional video using Automatic Speech Recognition.\n\nApproach\nOur approach to task graph generation consists of multiple steps, illustrated in Figure 2 . First, we use an instruction-tuned language model to generate a summary of steps (in free-form text) from a transcript (Section 2.1). Given these summary step sequences generated from multiple such transcripts for the task, we identify the key steps relevant to the task using a clustering approach (Section 2.2). We then re-label summary step sequences using the identified key steps to obtain key step sequences (Section 2.3) and rank them using a language model (Section 2.4). Finally, we generate a task graph from the key step sequences (Section 2.5).\n\nGenerating Summary Steps\nThe first step of our pipeline extracts a summary of steps g i = (g 1 i , g 2 i , . . .) for performing the task described in each transcript t i . We use an instructiontuned language model for this purpose. We prompt the model with a transcript, followed by a query such as 'Based on this description list down the key steps for making coffee using short phrases.' and let the model generate a completion. We use the 'Davinci' version of the InstructGPT (Ouyang et al., 2022) model in our experiments. We observed that the model consistently generates the steps in the format '1. <step 1>\\n 2. <step 2>\\n ..', occasionally using bullet points instead of numbers. The sentences g j i on each line are extracted and treated as the summary steps identified from the transcript. Appendix B shows example summary step sequences generated by InstructGPT.\n\nIdentifying Key Steps Relevant to the Task\nGiven summary step sequences g 1 , . . . , g n generated in the previous step, we seek to identify correspondences between steps in different summaries and capture the salient steps that appear frequently. We use a clustering approach for this purpose. Sentences g j i are represented as embeddings using a sentence encoder (We use the MiniLMv2 encoder from the SentenceTransformers library (Reimers and Gurevych, 2019; Wolf et al., 2019), which was identified as the best sentence embedding method for semantic search/retrieval). We obtain highconfidence clusters by identifying max cliquesclusters of sentences that are similar (determined by a threshold -cosine similarity \u2265 0.9) to each other, and retain cliques with more than 5 sentences. We noticed that this often yields multiple clusters that represent the same key step. For instance, the steps 'fill the moka pot with water' and 'fill the bottom chamber with water' represent the same key step of filling water, but are placed in different clusters. Identifying such redundant clusters based on sentence similarity alone is difficult. We define the notion of sequence overlap between two clusters -how often a sentence from one cluster and a sentence from the other cluster appear in the same summary step sequence. Intuitively, if two clusters have high inter-cluster similarity and low sequence overlap, it is likely that they represent the same key step, and we merge the clusters. The resulting clusters obtained are treated as the key steps k 1 , . . . , k m . 2 Appendix C shows example clusters discovered for different tasks.\n\nRe-labeling Summary Step Sequences\nWe re-label each summary step sequence g (subscript i dropped for brevity) with the identified key steps k 1 , . . . , k m to produce a key step sequence h using the greedy algorithm described in Algorithm 1. The algorithm sequentially picks the most similar 3 candidate summary step and cluster pair (g a , k b ) at each step, assuming each key step only appears once in the sequence. The process terminates when the highest cosine similarity drops below zero.\n\nAlgorithm 1: Key Step Sequence Inference\nInput g = (g 1 , g 2 , . . .) \u25b7 Summary step sequence Input K = {k1, k2, . . .} \u25b7 Key steps For each summary step identify most similar sentence from each cluster:\nCij \u2190 max s\u2208k j cos(g i , s) Hij \u2190 arg max s\u2208k j cos(g i , s) S \u2190 {} \u25b7 Predicted alignments while maxi,j Cij > 0 do a, b \u2190 arg max i,j Cij S \u2190 S \u222a {(a, b)} Caj \u2190 0, C ib \u2190 0 \u2200i, j Sort (ai, bi) \u2208 S so that a1, a2, . . . are in increasing order Output h = (H a 1 b 1 , H a 2 b 2 , . . .)\n\u25b7 Key step sequence\n\nRanking\nOne shortcoming of the labeling algorithm described in the previous section is that it does not take the sequential nature of steps into account. 565 tokens on average.\n\nSetup\nThe datasets come with key steps annotations (i.e., K) for each task and key step sequence annotations for each transcript. Our approach is unsupervised and does not make use of these annotations. However, for evaluation purposes, we consider two settings. The first setting assumes ground truth K and evaluates the performance of the full pipeline ignoring the clustering component (since key steps are known). In the above setting, we use ground truth human annotated graphs from Jang et al. (2023) for evaluation. In the second setting, we use K inferred from Section 2.2 and perform qualitative comparisons with ground truth graphs. Note that we did not use key step sequence annotations from the datasets in either setting.\nBaselines. We compare our approach against the following baselines. Proscript (Sakaguchi et al., 2021) is a language model fine-tuned on manually curated script data. Given a task description and a set of key steps, Proscript generates a partial order of the key steps. In addition, we consider several variations of our approach as baselines in Table 1 . In contrast, we exploit large language models in order to extract key phrases from the transcript. Third, we observe that ranking and filtering key step sequences using a language model ( 5 ) further improves performance, with a significant improvement for ProceL. Finally, our approach comes closest to graphs generated from human annotated key step sequences in the datasets ( 6 ). 5 Unknown Key Steps Next, we consider the full pipeline where key steps are identified automatically. Since ground truth reference task graphs are unavailable in this case we perform a qualitative comparison of graphs generated using our approach and the ground truth, human annotated graph. Figures 1 and 2 show predicted graphs for the tasks perform cpr and make pbj sandwich, respectively. We observe that the predicted graph for perform cpr is more detailed and fine-grained than the ground truth graph and captures many of the ground truth precondition relationships. On the other hand, the graph for make pbj sandwich is less fine-grained compared to the ground truth (Figure 6 of Appendix D). For instance, the ground truth annotations distinguish between putting jelly on the bread and spreading jelly on the bread, whereas our approach treats them as a single step. In addition, spreading peanut butter and spreading jelly are independent of each other and have no sequential dependency. However, the predicted graph fails to capture this and assumes that the former is a precondition for the latter. Appendix D shows more examples of predicted graphs.\n\nAblations Summary\nStep Sequence Generation We perform an ablation to study the effect of the model used to generate summary step sequences from transcripts. We replace the InstructGPT model (Ouyang et al., 2022) with a FLAN-T5 model (Chung et al., 2022) and evaluate graph prediction performance. We find that InstructGPT consistently outperforms FLAN-T5 across all the tasks (Table 2 ). In addition, we found that plain language models (not fine-tuned with instructions) struggled to produce usable summaries. This shows that models trained with instructions and human-preference data are better at producing task graphs from transcripts compared to other forms of supervision such as language modeling and supervised multi-task training with NLP tasks.\n\nRanking Language Model\nWe perform an ablation to understand the impact of the choice of language model for the ranking process in Section 2.4. We present the average performance on tasks in the ProceL dataset with different language model choices in Table 3 . First, we find that performance does not degrade much when switching to a smaller model in the GPT2 family. Second, we notice that scale alone does not guarantee better ranking performance as the larger GPT-J model (Wang and Komatsuzaki, 2021) is inferior to the GPT2 models. These findings suggest that the choice of pre-training data influences the script knowledge present in a model and can be more important than model scale. \n\nConclusion\nThis work presented an unsupervised approach to generate task graphs from text transcripts of instructional videos. Our framework exploits multiple text transcripts which describe a task in order to robustly identify the key steps relevant to a task and the depencies between these steps. We demonstrated the effectiveness of our approach compared to supervised and unsupervised baselines on instructional video transcripts from the ProceL and CrossTask datasets.\n", "hypothesis": " This work explores the problem of generating task graphs of real-world activities.  Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a semi-supervised manner.  We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets..", "answer": false}
{"title": "Transferring General Multimodal Pretrained Models to Text Recognition", "content": "\nIntroduction\nOptical character recognition (OCR) plays an important role in the real-world applications. It helps users or developers extract text contents from different types of images, including photos, scanned documents, etc. In practice, building a tool for OCR needs a pipeline consisting of a text localization module and a text recognition module.\nIn this work, we focus on improving the accuracy of text recognition. Text recognition has often been regarded as a key challenge owing to the room for improvements in recognition accuracy. In the deep learning era, the classical methods are mostly based on CNN and RNN, which are responsible for visual feature extraction and sequence modeling, respectively (Shi et al., 2017a (Shi et al., , 2019;; Luo et al., 2019) . Recently, with the rise of Transformer (Vaswani et al., 2017) , researchers applied the Transformer encoder-decoder framework to text recognition and achieved outperforming results over the baselines (Li et al., 2021; Lyu et al., 2022) . However, most methods are based on largescale pretraining on human-annotated or synthetic OCR data. It is hard for other researchers to collect or create such data for reproduction. Furthermore, the methods often include complex model or objective designs, like DETR-like decoder (Carion et al., 2020) , CTC loss (Graves et al., 2006) , etc. These components also might hinder reproduction as they increase the difficulty in training. Therefore, we naturally raise a question: Is there any way to achieve high recognition accuracy without complex designs on data and model?\nInspired by the recent progress in multimodal pretraining, we argue that the transfer of a unified multimodal pretrained model is a possible solution. Multimodal pretraining has proved significant to the performance of downstream tasks, and thanks to the rise of unified multimodal pretrained models, they can perform both cross-modal understanding and generation and achieve state-of-theart performance (Wang et al., 2022a,b; Lu et al., 2022) . We therefore propose to transfer the unified multimodal pretrained model by finetuning the pretrained model on the text recognition datasets with the task of image captioning, which is essentially a simple sequence-to-sequence learning task with maximum likelihood estimation for optimization.\nTo support the effectiveness of the proposed method, we have conducted extensive experiments on the Chinese text recognition benchmark (Chen et al., 2021b) covering multiple scenarios, including scene, web, document, and handwriting. Specifically, we finetune the open-source Chinese multimodal pretrained model OFA (Wang et al., 2022a) on text recognition, and we name the model OFA-OCR. Figure 1 demonstrates the results of methods with or without general-domain pretraining. It shows that multimodal pretraining on generaldomain vision-language data can effectively boost downstream performance in text recognition. To achieve the best performance, we apply the multitask + single-task finetuning to OFA-OCR, and it outperforms the previous state-of-the-art methods on the benchmark. Furthermore, through the ablation studies, we demonstrate the effectiveness of our method designs, including multitask + singletask finetuning, data augmentation, etc. Furthermore, to enable deployment for real-world applications, we construct a pipeline with both OFA-OCR and a simple text localization module. We find that this simple pipeline can provide high-quality OCR performance, competitive with a productlevel API.\n\nMethod 2.1 Pretraining\nTo leverage the capability of the multimodal pretrained model for image captioning, we employ the unified multimodal pretrained model architecture. Specifically, we implement our models on OFA (Wang et al., 2022a) , an open-source state-ofthe-art unified multimodal pretrained model with the release of Chinese models.\nThe model is mainly based on the Transformer encoder-decoder framework (Vaswani et al., 2017) . To make information from different modalities adaptable to the Transformer, there are adaptors for images and texts, which are visual backbones, e.g., ResNet (He et al., 2016) , ViT (Dosovitskiy et al., 2021) , etc., and word embeddings, respectively. The information from modalities is encoded as discrete tokens so that the decoder can perform their generation.\nFor Chinese multimodal pretraining, OFA-Chinese was pretrained on a large-scale dataset, which consists of LAION-5B (Schuhmann et al., 2022) , Wukong dataset, as well as translated datasets from MSCOCO (Chen et al., 2015) , Visual Genome (Krishna et al., 2017) , VQA (Goyal et al., 2017) , RefCOCO (Yu et al., 2016), etc. Note that this work is different from previous pretraining-related methods, which pretrain the model on large-scale human-annotated or synthetic data. We show that through pretraining on generaldomain data, the model can obtain the potential of text recognition by finetuning on small datasets.\n\nFinetuning with Image Captioning\nIt is natural to recast text recognition as image captioning, as text recognition also requires the model to generate a piece of text based on the input image. It is equivalent to finetuning on different image captioning datasets, where the target refers to the text on the image. We finetune the model with maximum likelihood estimation for optimization.\nFurthermore, to better alleviate the discrepancy between upstream and downstream data, we apply a transformation to the input images to make them square, e.g., a resolution of 480 \u00d7 480. Specifically, we first resize the image to a longer edge of the specified resolution while keeping the original height-width ratio of the image, and we make the image square by padding on all sides with the edge value. The lengths for the directions are random, and thus this method can play as data augmentation in this context. We demonstrate the pseudo code in Sec. A.3.\nFor better performance in the downstream tasks, we often use a larger resolution in the finetuning stage, and thus we encounter issues with the positional embedding. In our practice, we still use the same one from pretraining but apply interpolation to adapt to images of a larger resolution.\n\nMultitask Finetuning\nThere are multiple subtasks in text recognition, concerning different scenarios, e.g., scene, document, etc. Our experiments are implemented on the Chinese text recognition benchmark consisting of 4 subtasks. In our practice, we implement multitask finetuning and single-task finetuning for comparison. Specifically, as the data of all subtasks are organized with the same format, we directly build a mixture of datasets for multitask finetuning. We find that directly applying multitask finetuning can help OFA-OCR achieve outstanding performance on all datasets. To further boost its performance, we additionally apply single-task finetuning after Metrics Scene Web Document Handwriting Average CRNN (Shi et al., 2017a) 53.4 54.5 97.5 46.4 67.0 ASTER (Shi et al., 2019) 54.5 52.3 93.1 38.9 64.7 MORAN (Luo et al., 2019) 51.8 49.9 95.8 39.7 64.3 SAR (Li et al., 2019) 62 multitask finetuning, and we find that this pushes its performance to the new state-of-the-art.\n3 Experiments\n\nDatasets and Metrics\nWe implement OFA-OCR on the Chinese text recognition benchmark (Chen et al., 2021b) . This benchmark consists of multiple subtasks of text recognition, which are text recognition in different scenarios, including scene, web, document, and handwriting. The details of the datasets are provided in Sec. A.1. The evaluation metric includes accuracy, which refers to the ratio of exact match.\n\nExperimental Results\nThe experimental results are demonstrated in Table 1. We compare our method with baseline models of OCR, including the previous state-of-the-art MaskOCR (Lyu et al., 2022) . It can be found that with no regard to the scale of models, the base-size OFA-OCR, which is finetuned from the pretrained Chinese OFA Base , can outperform both the basesize and large-size MaskOCR models. Specifically, it shows the advantages of 9.0, 6.9, and 5.3 absolute improvements in the scenarios of scene, web, and handwriting. On average, the base-size OFA-OCR outperforms the base-size MaksOCR by 5.2 and the large-size MaskOCR by 3.4. Scaling up the model size can consistently bring steady improvement in the downstream performance. On average, OFA Large reaches the best results of 86.3. Specifically, we find that the advantage in the scene dataset is the largest among the tasks. This may be attributed to the pretraining on generaldomain data, where there are images of street views, and some of them might contain texts. Similarly, the pretraining dataset consists of web images that resemble those in the web dataset, and thus the gaps between OFA-OCR and the previous methods are large. However, text recognition for documents should be a simpler task as the texts are more regular in fonts and there is often much less noise in the background. Thus, even the conventional method like CRNN can achieve a high accuracy.\n\nAblation Study of Training Strategies\nTo check how the multitask learning influences the final performance, we conduct an ablation study to evaluate its effects. Specifically, the experiments are conducted with the base-size OFA-OCR. We provide experiments in 4 setups, which are training from scratch (scratch), single-task finetuning (ft), multitask-finetuning (mt), and multitask + singletask finetuning (mt+ft), respectively. Experimental results are shown in Figure 2 . It can be found that on average, the addition of the initialization of the pretrained OFA model significantly boosts the performance on the datasets. Surprisingly, multitask finetuning alone can outperform single-task finetuning on all 4 tasks, and the advantage in the web dataset is the most obvious. We assume that this is attributed to the small amount of supervised training data for downstream transfer. A mixture of datasets of related subtasks can encourage performance on all subtasks. Furthermore, the combination of multitask finetuning and single-task finetuning is the best solution owing to its outstanding performance, while multitask finetuning on the mixture of datasets is the most cost-efficient.\n\nAblation Study of Data Augmentation\nThe preprocessing of images for this task can play as data augmentation. To validate its effects, we use a simple resizing to the specified resolution as a baseline. We also implement experiments on the 4 datasets, and for simplicity we implement the experiments in the setup of single-task finetuning on the base-size models. Results are demonstrated in Table 2 . We use \"Aug.\" to indicate the preprocessing method mentioned in Sec. 2. The results indicate that the introduced technique for data preprocessing can effectively boost the performance.\n\nDeployment\nTo construct an OCR system applicable in realworld scenarios, a strong text recognition model is not sufficient, and we need to build a pipeline with both the text detection and text recognition module. While the former one is not the focus of this research, we directly use a light-weight model from EasyOCR 3 for detection. After detecting all the bounding boxes which possibly contain texts, we crop them with boxes to create a batch of new images. The final step is to process the images with OFA-OCR for the generation of text recognition results. Through our case study, we find that the simple OCR pipeline based on OFA-OCR can achieve competitive performance with the productlevel API. Examples are demonstrated in Sec. A.4.\n\nRelated Work\nWe focus on the review of text recognition methods and multimodal pretraining. effectiveness (Shi et al., 2017a; Luo et al., 2019; Shi et al., 2019; Yu et al., 2020; Li et al., 2019; Fang et al., 2021) . Recent methods have turned to the use of Transformer and achieved improved performance (Atienza, 2021; Li et al., 2021; Zhang et al., 2022; Lyu et al., 2022) . However, before this work, we have not witnessed the direct transfer of general-domain vision-language pretrained models to text recognition. Vision-language pretraining has proved a success as it has leveled up the model performance on a series of downstream tasks (Chen et al., 2019; Lu et al., 2019; Radford et al., 2021; Wang et al., 2021) , and the unified models capable of both understanding and generation have become popular and achieved the best performance (Wang et al., 2022a,b) . Yet, there are only a few unified multimodal pretrained models in Chinese (Lin et al., 2021; Wang et al., 2022a) .\n\nConclusion\nIn \n", "hypothesis": " This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition.  Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task.  Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark.  Additionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate that it can achieve competitive performance with the product-level API.  The code 1 and demo 2 are publicly available..", "answer": true}
{"title": "Automatic Named Entity Obfuscation in Speech", "content": "\nIntroduction\nPrivacy concerns, particularly where an individual could be identified, preclude sharing and therefore automatic exploitation of many data sources. Anonymization, the removal of identifying information, has been automated for text (Lison et al., 2021) , including large scale applications such as in clinical (Hartman et al., 2020) or legal settings (Oksanen et al., 2022) , with off-the-shelf systems having reported performance of 90+% (Hartman et al., 2020) . To minimize the risk of re-identification, obfuscation -replacing identifying information with a different substitute of the same type -has been explored as an alternative to replacing identifying information with a generic marker (Sousa and Kern, 2022) . The main focus in speech has been on voice anonymization, which may not be a problem with speaker consent, with the removal of identifying information receiving less attention. To our knowledge, this is the first prototype to perform named entity obfuscation directly, in the original speaker's voice. Aside from voice cloning, it explores a named entity recognition approach based directly on audio signal and uses language model masking to find appropriate substitutions.\nRecent advances in speech models, particularly the inclusion of language models within the speech model itself (e.g. HuBERT (Hsu et al., 2021) ) gives models greater insight into expected contexts. Previous work on named entity recognition (NER) in speech frequently employs a two step approach, transcribing speech first, followed by the application of existing named entity techniques (Yadav et al., 2020) . However, this process has the potential to compound errors as errors in transcription will increase the probability of error in NER. We suggest that the addition of language models into the speech model gives these sufficient power to perform NER directly, and therefore that transcribing (automatic speech recognition, ASR) and NER can be separated, and used to provide a confidence measure in their performance. Divided, the two do not propagate errors in the same way; in fact, treating ASR and NER separately allows one to fix (some of the) errors of the other. The proposed second (final) ASR pass merely produces a confidence value in the result to decide whether a manual check should be performed.\nThe success of few shot learning, where a limited number of examples is used to generalize a pre-trained deep learning model to a new situation, for text-to-speech -and specifically voice cloning (Zhang and Lin, 2022) -enables an alternative, equivalent but different, entity to be inserted in the audio signal in place of the original while preserving the prosody information throughout. While large databases of potential replacement entities can be used to select a substitution, these may not preserve necessary properties (such as gender). Al-ternatively, word embeddings have been used to suggest close (in the multi-dimensional space) alternatives (Abdalla et al., 2020) , however these can suffer from the same drawback. We propose using a more contextualized alternative to word embeddings, a masked language model (Devlin et al., 2019) , where the model is trained by hiding (masking) words and predictions of the original word are made based on their context. This work makes the following contributions: (1) a complete obfuscation pipeline for names in speech 1 , (2) a named entity recognizer built directly on speech without requiring text transcription first, (3) alternative (obfuscated) entity replacement selection via masking language model, and (4) confidence annotated system output, allowing for manual correction and / or selection of shareable instances. Section 2 contains the methodology with results in Section 3. Section 4 presents the conclusions and future work.\n\nMethodology\nThe steps of the overall pipeline, which takes in an audio file and produces an obfuscated audio file along with a confidence value, can be found in Figure 1 . The approach comprises of three main parts: 1) identification of named entities (NEs) in the audio, 2) finding an equivalent alternative for the original NEs, and 3) reconstructing the original audio to incorporate the replacement NEs. The reconstructed audio can further be used to obtain a confidence value.\n\nIdentification of named entities\nTo enable the direct use of a language model on speech input for the purpose of named entity recognition (NER), a dataset of audio recordings with annotated NEs is required. The English speech NER dataset (Yadav et al., 2020) , which consists of 70,769 waveforms with transcripts annotated with person, location and organization NEs, is used for fine-tuning the Hidden-Unit BERT speech model (HuBERT) (Hsu et al., 2021) . HuBERT was selected over other speech models since it learns both accoustic and language models from its inputs and therefore has an increased awareness of context. The success of language models on text NER has demonstrated how crucial context is for this task, and using a model which incorporates both an acoustic and a language model (over acoustic only) allows the approach to exploit the information used in text NER, while managing to avoid the need for a transcript.\nFor training, NE annotations need to be converted to a suitable format, indicating the presence or absence of a NE in each position. Following the inside-outside(-beginning) chunking common to many NER approaches (Tjong Kim Sang and De Meulder, 2003) , three formats were explored: 1) character level annotation, mapping each character to either o for a character outside of a named entity, space, or n, l, e for characters within person, location or organization entities respectively, 2) the same character level annotation with separate characters added to denote the beginning of each type of NE (mapping the sentence TELL JACK to oooo mnnn with m denoting the start of a person NE), 3) and, for completeness, annotation was also explored at word level.\nWith the training parameters shown in Appendix A.1, the best NE performance was obtained from the first annotation approach, where NE beginnings were not explicitly annotated. The lower performance of the second annotation approach can be attributed to the low quantity of training data for the beginning marker annotations. While word level annotation was explored, it is likely to need a far greater quantity of data to enable mapping of different length inputs to a single label.\nSeparately, HuBERT was also fine-tuned for automatic speech recognition (ASR), i.e. for transcribing text from audio. Identical training data was used, with annotation being the transcription provided as part of the NE annotation (with NE annotation removed). The same parameters were employed for its training. Alongside the predicted (NE or ASR) annotation, prediction output also yields an offset which can be converted to a time offset. This can be used to identify the position of the NE(s) to be replaced, and after a greedy alignment of the two outputs, the original transcription of the original NE(s) can be extracted.\n\nFinding an alternative NE\nOnce a person NE is identified, a suitable equivalent substitution needs to be obtained, i.e. we want to find the word which could replace the NE in the text if the NE was hidden. This is precisely the concept behind masked language models (MLMs): these models learn their weights so that given a sentence with a hidden (masked) word, the model will output the complete original sentence. The (ASR extracted) original sentences with NEs (as identified by the NE tuned model) masked were passed to a MLM. Three MLM models were explored: BERT, bert-large-uncased model (Devlin et al., 2019) , ALBERT, albert-xxlarge-v2, model (Lan et al., 2019) and the distilled RoBERTa base, distilroberta-base, model (Sanh et al., 2019) . Each model, with no additional tuning, results in a (pre-specified) number of predictions for each NE in the sentence. Since the models used different datasets in training, their predictions are expected to be different: for example, some may suggest pronouns rather than names.\nGiven the propensity of the MLM to return substitutions which are not names (for example, for the sentence you should call Stella, the model returns you should call him, you should call them, you should call 911 etc), an external list of people names is used for the validation of the proposed suggestions 2 and the highest scoring substitution is returned. Heuristically, the original name is matched against the list to identify whether it is a first or a last name (where possible) and names of the same type suggested by the MLM are returned. Simple rules are employed (last of a sequence of names is a last name, a single name without a title is a first name etc) to decide on a substitution when the original name does not appear in either the first or last name list. Given the nature of MLMs, suggested alternatives are likely to be more common words: as a positive side effect, this should make them easier to render with voice cloning as they may already appear in the reference speech. Should MLM fail to propose any suitable substitutions, one is selected at random from the first & last name lists, subject to the same heuristic rules.\n\nReconstruction of original audio\nIn this work, the substitute NE is to be re-inserted into the original audio. To reduce the risk of de-identification via the extraction of entities which failed to be identified and therefore stayed in their original form, the substitute entity needs to be produced in the speaker's voice. The YourTTS (Casanova et al., 2021) model, which offers the ability for fine-tuning with less than one minute of speech while achieving good results with reasonable quality, can be used to generate the substitute sentence with all available speech of the speaker provided as reference. Note that it is not necessary to remove the original sentence from the reference data: in fact, its presence may result in more accurate rendering of the substitute sentence. The pre-trained model used in this work (tts_models/multilingual/multi-dataset/your_tts) was trained on the the voice cloning toolkit (VCTK) dataset (Yamagishi et al., 2019) which contains approximately 400 sentence, selected from newspaper text, uttered by 108-110 different speakers, giving it its generalization power. Aside from the reference passed to the model on the command line, no tuning or training of the YourTTS model is done in this work.\nThe ASR transcribed text with the substituted NE is generated, rather than the substitution alone, to ensure that the intonation as closely matches the substitution's position in the sentence. The average amplitude of the generated audio is matched to that of the original segment using the Python pydub library. The generated audio is again pased through the HuBERT based NE recognizer, to identify the location of the substituted NE in the generated audio and allow its extraction (note that in this pass, it is not necessary to perform ASR -only the offsets of the replacement NE are required). Should the NE recognizer not identify the same number of NEs as were present in the original, the instance is flagged for manual review.\nFor each NE in the text, a pair of start and end offsets are available: one pair extracted by the Hu-BERT based NE extraction from the original audio and a second pair from the audio generated from the substituted text. This allows the new NEs to be inserted in place of the original NEs. The splicing and concatenation of the waveforms is also performed using the pydub library.\nA second HuBERT based ASR pass over the newly constructed (substituted) audio, and its comparison against the substituted text using word error rate (WER) and character error rate (CER) gives measures of confidence. Both the metrics, commonly used for evaluation of ASR, allow for sequences of different length to the target -the further the reconstructed audio is from the target sentence, the less likely it is that the substitution will go unnoticed. For the purpose of the demonstrating the viability of the prototype, no hyperparameter optimization was performed, and the larger HuBERT models were not employed, however improvement in performance of both models are expected should this be pursued.\n\nFinding an alternative NE\nA small scale evaluation is performed on a sample of 20 sentences selected at random from the Lib-riSpeech corpus (Panayotov et al., 2015) across 6 speakers. Sentence selection was subject to them containing a person named entity. While detailed results for the individual steps can be found in Table 2, it should be noted that -for the purposes of this work -the focus is the accuracy of the extraction of the correct NE. The stated accuracy is therefore somewhat misleading: in a number of cases, such as the word Raphael, the named entity is divided into two separate words, suggesting two consecutive named entities. However, this issue is corrected when the NE output is aligned with ASR output and the two separate NE instances are (correctly) merged. Cases with NEs which cannot be aligned are flagged up for manual intervention. The average ASR and (exact match) NE identification do not vary when a different MLM is employed, as this only effects the selection of the substituted name, resulting in different average confidence values.\n\nReconstruction of original audio\nThe voice cloning model requires some reference audio for the speaker: for the 6 selected speakers, 4 have less than 5 audio files (two having 3, and one having only 2 files) in the dataset. The quantity of data used as reference is likely to impact the quality (in terms of its similarity to the original speaker) of the generated text. Given the likely scenarios of deployment, such as dialogues where more than 2 sentences of speech per speaker are available, this may not be representative of the results obtainable with the pipeline. However, it should be noted that even if all substituted instances can be identified as substitutions, the system is equal to a masking technique (where an entity is replaced with a fixed entity, such as a bleep).\n\nConclusion\nThe prototype described shows the steps of an obfuscation pipeline for speech, which results in substituted person named entities uttered in the original speakers voice and replaced in the original audio signal. The prototype makes use of a named entity recognizer built directly on top of audio input, and employs masked language models to generate the substituted entity. It offers an end-to-end automatic solution enabling the sharing of speech with identifying information removed.\nThe resulting obfuscated speech remains in the original speaker's voice, allowing for the application of traditional speaker anonymization approaches to mask the speaker's identity. The original prosody can be protected by applying a transformation such as waveform change, offering a significant advantage over a technique which generates a complete obfuscated transcription (instead of splicing an obfuscated entity into original speech).\n", "hypothesis": " Sharing data containing personal information often requires its anonymization, even when consent for sharing was obtained from the data originator.  While approaches exist for automated anonymization of text, the area is not as thoroughly explored in speech.  This work focuses on identifying, replacing and inserting replacement named entities synthesized using voice cloning into original audio thereby retaining prosodic information while reducing the likelihood of deanonymization.  The approach employs a novel named entity recognition (NER) system built directly on speech by training HuBERT (Hsu et al., 2021) using the English speech NER dataset (Yadav et al., 2020). The approach is prototyped on a sample of the LibriSpeech corpus (Panayotov et al., 2015) with each step evaluated collectively.", "answer": false}
{"title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning", "content": "\nIntroduction\nVanilla fine-tuning strategy usually adjusts all the parameters to adapt the pre-trained language model to downstream tasks. Parameter-efficient learning (He et al., 2022; Houlsby et al., 2019; Lester et al., 2021; Guo et al., 2021; Ben Zaken et al., 2022) is an emerging framework that freezes the pre-trained model and only tunes a few number of task-specific parameters for downstream tasks. For instance, Prefix tuning (Li and Liang, 2021; Liu et al., 2022) prepends length-equivalent pseudo prefix tokens, i.e. continuous task-specific vectors to each layer of the pre-trained model, achieving comparable even superior performance with only 0.1-3% parameters.\nIn previous works, the length of prefix tokens (or the number of trainable parameters) is usually the same at each layer. However, a potential observation lies in that the structure information and representational capacity embedded in each layer are prone to be inconsistent (Jawahar et al., 2019) .\nIt is generally considered that the bottom layers of the language model tend to capture concrete and shallow phrase-level features, while the top layers concerns more with abstract semantic information (Tenney et al., 2019) . Based on the perspective, we assume adaptive prefix can grab the emphasis more flexibly to adapt to various downstream tasks. In light of above motivation, we investigate the adaptive prefix in this work. We propose Adaptive Prefix Tuning (APT) with an adaptive gate mechanism at both fine-grained token level and coarsegrained layer level. Specifically, as shown in Figure 1 , for fine granularity, APT scores each individual prefix token via gated weight assignment. Then, the scaled weight is utilized to balance the inserted task-specific prefix tokens and original input tokens for current layer at coarse-grained level.\nExtensive experiments against prefix tuning on the sentence and token classification tasks in full data and low resources setting validate the effectiveness of APT. In addition, the gate learned from APT could be served as a probing for the number of necessary parameters in different layers, guiding us to directly apply variable prefix to the original prefix tuning. The probing experiment further demonstrates the effectiveness of adaptive prefix.\n\nRelated Works\nSince fine-tuning the whole model is prohibitively expensive, parameter-efficient language model finetuning becomes a lightweight alternative that only optimizes a small number of parameters while keeping most pre-trained parameters frozen (He et al., 2022) . Adapter tuning (Houlsby et al., 2019) inserts two tunable task-specific modules after multihead attention and feed-forward network, achieving comparable performance with only 2-4% of the parameters. Prompt tuning (Lester et al., 2021) and Prefix-Tuning (Li and Liang, 2021) only train soft prompts by adding prefix tokens to the input or hidden states. Recently, Liu et al. (2022) extend the prefix tuning to the natural language understanding tasks, which matches the performance of fine-tuning with only 0.1%-3% tuned parameters.\nFurthermore, with an overlap of our motivations that each layer of the pre-trained language model focuses on different aspects of feature for various tasks (Jawahar et al., 2019; Clark et al., 2019b) and extra parameters are probably not necessary for certain tasks (Houlsby et al., 2019; Fan et al., 2020; R\u00fcckl\u00e9 et al., 2021) , Adaptable Adapters (Moosavi et al., 2022) selects beneficial adapter layers and learns task-specific activation function for downstream tasks to make adaptor dynamic for each task and layer. In addition to different frameworks (adapter versa prefix tuning), our key difference from their work lies in that we aim to dynamically filter required information at each layer in a soft way, while they choose whether to add trainable modules at the layer level in a hard manner.\n\nPrefix Tuning\nAs prefix tuning is an extension on Transformer (Vaswani et al., 2017) , we first recap the structure of Transformer. Transformer is the block consisting of multi-head attention concatenated by multiple single self-attention functions and a fully connected feed-forward network. Formally speaking, the Transformer block is calculated as follows:\nAttn(Q, K, V ) = softmax( QK T \u221a d V ) (1) FFN(x) = ReLU(xW 1 + b 1 )W 2 + b 2 (2)\nPrefix tuning prepends pseudo prefix tokens of length l to each layer of the language model, which is implemented by concatenating inserted keys and values matrix with original corresponding items in each multi-head attention. Specifically, let P k , P v \u2208 R l\u00d7d be the keys and values of the engaged prefix separately, where l denotes the length of prefix and d corresponds to the dimension, thus self-attention function can be reformatted as:\nAttn(Q, K \u2032 , V \u2032 ) = softmax( Q(K \u2032 ) T \u221a d V \u2032 ) (3)\nwhere\nK \u2032 = [P k ; K], V \u2032 = [P v ; V ]\nHere, [; ] donates concatenation function.\n\nAdaptive Prefix Tuning\nThe length of prefix is usually a manually set hyperparameter for each task and fixed in distinct layers of the model. However, existing work demonstrates each layer of the language model pays attention to different aspects of the input feature. We assume the prefix in fixed length is insufficient to tailor different layers and tasks. To dynamically customize the prefix at each layer, APT performs a gate mechanism via fine-grained gated weight assignment and coarse-grained scaled weight specification. Specifically, to capture the diversity of information utilization at different layers, we go deep into the token level at the fine-grained granularity. The token-level gate can inspire us on how many trainable parameters (i.e. pseudo tokens in prefix tuning) are required for this layer, which will be discussed in Section 4.4. Thus, APT yields the gated weights of l pseudo tokens at each layer. We use the hidden states to represent the information encoded in the layer and calculate the gated weights \u03b1 i = [\u03b1 i1 , \u03b1 i2 , . . . , \u03b1 il ] for i-th layer as:\nEQUATION\nHere, h i\u22121 is the d-dimensional hidden states from the previous layer, and W i \u2208 R d\u00d7l corresponds to the parameters to be learned. Besides, we also design a coarse-level gate to balance the information brought from task-specific prefix tokens and original input tokens by learning a layer-level weight. A learnable scaled weight \u03bb i is added to the representation of pseudo prefix tokens at the i-th layer.\nWith the above strategy, the keys-values pair P i = [P ik , P iv ] derived from pseudo prefix tokens in i-th layer is updated to Pi as:\nPi = \u03bb i \u03b1 i \u2299 [P ik , P iv ]\n(5) \u2299 is the element-wise multiplication. Accordingly, the calculation of the self-attention function in APT is similar to Eq.(3) without further elaboration.\n\nExperimental Setup\nWe conduct 5 NLU tasks on SuperGLUE (Wang et al., 2019) benchmark including BoolQ (Clark et al., 2019a) , COPA (Roemmele et al., 2011) , RTE (Wang et al., 2018) , WiC (Pilehvar and Camacho-Collados, 2019) and WSC (Levesque et al., 2012) as well as 3 Named Entity Recognition (NER) tasks including CoNLL03 (Tjong Kim Sang and De Meulder, 2003) , CoNLL04 (Carreras and M\u00e0rquez, 2004) , and OntoNotes 5.0 (Weischedel et al., 2013) . With BERT-base / large (Devlin et al., 2019) and RoBERTa-large (Liu et al., 2019) instantiated by HuggingFace Transformers (Wolf et al., 2020) , we compare APT with vanilla fine-tuning and P-Tuning v2 (Liu et al., 2022) which is an implementation of the prefix tuning, configured with hyper-parameters public in the released code 1 . We also verify our method with DeBERTa-xlarge (He et al., 2020) on NER tasks following P-Tuning v2.\n\nResults\nWe report the main results in Table 1 . For BERTbase, we can observe that APT achieves 1.5% and 0.7% improvements over P-Tuning v2 on Super-GLUE and NER tasks, respectively. For BERTlarge, APT outperforms P-Tuning v2 by 1.8% on SuperGLUE tasks and 1.4% on NER tasks. For RoBERTa-large, APT surpasses P-Tuning v2 by 1.5% on SuperGLUE tasks and 0.2% on NER tasks.\nOn NER tasks with DeBERTa-xlarge, APT is supe- rior to P-Tuning v2 by an average of 0.8%. Compared with vanilla fine-tuning, APT is comparable or even better on part of tasks. In addition, we explore the experimental performance under low resource settings on SuperGLUE benchmark. As shown in Table 2 , APT is a better few-shot learner than P-Tuning v2, which exceeds 4.2%, 3.4% in 16-shot setting, and 2.9%, 3.6% in 32-shot setting for BERT-base and BERT-large, respectively.\n\nAblation Study\nWe conduct an ablation study in order to explore the separate effect of token-level gated weight \u03b1, layer-level scaled weight \u03bb and the hidden states h from the previous layer which is used to calculate token-level gated weight \u03b1 in Eq.( 4). As shown in Table 3 , it can be found that removing any strategy hurts the performance to varying degrees, demonstrating that they are all advantageous. Specifically, the beneficial effect of \u03bb for APT is slightly greater than \u03b1 overall. Besides, it is effective and meaningful to introduce the context (i.e. the hidden states h from the previous layer) when obtaining the gated weight, especially for SuperGLUE tasks.\n\nDiscussion\nWhat is prefix weight distribution learned by APT? The gate mechanism for prefix serves as the key strategy of the proposed APT, where the learned prefix weight distribution turns out to be a critical point. Figure 2 illustrates the gate weights of the pseudo prefix token for COPA and CoNLL04, respectively. It can be found that CoNLL04 is concerned with bottom layers in the language model which are regarded as phrase-level features, while COPA pays more attention to the higher layers, indicating semantic information. The observation is consistent with the characteristics of corresponding tasks. NER is a token-level task while COPA is a causal reasoning task sensitive to the semantics of sentences, which reminds us that it is worth placing various prefix tokens on specific layers according to the task properties.\n\nDoes variable prefix work better than fixed one?\nTo verify the effectiveness of adaptive prefix under the proposed architecture, we wonder if the learned ratio at each layer can be directly transferred to P-Tuning v2. Taking the gate as a probing indicator, we reset the prefix length of P-Tuning v2 from fixed to variable in different layers based on the ob-servation of the learned ratio (e.g. the distribution shown in Figure 2 ). From the comparison between PT-2 and PT-2 * in Table 4 , we demonstrate that the variable prefix with less trainable parameters surprisingly outperforms the original implementation in fixed prefix. Nonetheless, it is also worth noting that there is still a gap between P-Tuning v2 with variable prefix and APT, where the latter continuously adjusts the weight of prefix during the training phase while the former only initializes with a one-time mask probing.\nWhether the adaptive structure benefits the finetuning? Compared to P-Tuning v2, APT learns extra gated and scaled weights. To figure it out whether the improvement of APT is brought from more trainable parameters or the adaptive model structure, we adjust the hyper-parameter, i.e., enlarge the prefix length of P-Tuning v2 by 1.5 times to align the number of parameters with our APT. As shown in the comparison between PT-2 + and APT of Table 4 , we observe that APT still outperforms enlarged P-Tuning v2 with 1.9%, 0.4% on average for SuperGLUE and NER tasks respectively, validating the superiority of the gate mechanism.\n\nConclusion\nIn this paper, we investigate prefix tuning and assume that adaptive prefix is probably more efficient and effective than fixed prefix. Firstly, we propose APT that leverages the token-level and the layerlevel gate mechanism which achieves an improvement of performance over original prefix tuning. Then, we illustrate the weight distribution learned by APT and take it as a probe, which validates the variable prefix can work better than the fixed one.\nThe above experiments and analysis demonstrate that the adaptive prefix can be served as a promising strategy for parameter-efficient fine-tuning.\n", "hypothesis": " Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive.  Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model.  In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e.  pseudo tokens) inserted into Transformer layers.  Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient.  Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism.  Experiments on the SuperGLUE and NER datasets show the effectiveness of APT.  In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix..", "answer": true}
{"title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings", "content": "\nIntroduction\nLarge Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020) have obtained impressive performance on a wide range of NLP tasks showing great potential in several downstream applications for real world impact. However, these models have shown to be prone to picking up unwanted correlations and stereotypes from the pre-training data (Sheng et al., 2019; Kurita et al., 2019; Hutchinson et al., 2020) which, can perpetuate harmful biases for people belonging to marginalized groups. While there has been a great deal of interest in understanding and mitigating such biases in LLMs (Nadeem et al., 2021; Schick et al., 2021; Meade et al., 2022) , the focus of such studies has primarily been on English.\nWhile Massively Multilingual Language Models (Devlin et al., 2019; Conneau et al., 2020 ; Xue * Equal contribution et al., 2021) , have shown impressive performances across a wide range of languages, especially with their surprising effectiveness at zero-shot crosslingual transfer, there still exists a lack of focused research to evaluate and mitigate the biases that exist in these models. This can lead to a lack of inclusive and responsible technologies for groups whose native language is not English and can also lead to the dissemination of stereotypes and the widening of existing cultural gaps.\nPast work on evaluating and mitigating biases in multilingual models has mostly been concerned with gender bias in cross-lingual word embeddings (Zhao et al., 2020; Bansal et al., 2021) which fails to account for contextual information (Kurita et al., 2019; Delobelle et al., 2022) , making them unreliable for LLMs. Other methods for estimating biases in contextualized representations involve Multilingual Bias Evaluation (Kaneko et al., 2022, MBE) , which utilizes parallel translation corpora in different languages that might lack non-western cultural contexts (Talat et al., 2022) . For debiasing LLMs, Lauscher et al. (2021) proposed an adapter (Houlsby et al., 2019) based approach. However, the biases are measured in the word representations and only English data was used for debiasing, missing out on cultural context for other languages.\nTo address these concerns, we make the following key contributions in our work. First, we extend the DisCo metric (Webster et al., 2020) by creating human-corrected templates for 6 Indian languages. DisCo takes sentence-level context while measuring bias and our templates are largely culturally agnostic making them more generally applicable. Second, we extend existing debiasing strategies like Counterfactual Data Augmentation (Zhao et al., 2018) and Self-Debiasing (Schick et al., 2021) to mitigate gender biases across languages in Masked Language Models (MLMs).\nFinally, we also evaluate the transferability of debiasing MLMs from one source language to other target languages and observe limited transfer from English to languages lacking western context. However, we do observe that typologically and culturally similar languages aid each other in reducing gender bias. While there have been multiple studies on measuring biases in multilingual models, previous work has not explored mitigating gender biases from these models on multiple languages and studying the transferability of debiasing across different languages. This is especially true while using nonembedding based approaches for evaluation and debiasing. To the best of our knowledge, ours is the first work to debias multilingual LLMs for different languages and measure the cross-lingual transfer for gender bias mitigation. To encourage future research in this area, we will release our code and datasets publically 1 .\n\nMeasuring Bias in Multilingual Models\nIn this section, we describe the benchmarks to evaluate biases in MLMs across different languages. Since most existing benchmarks for bias evaluation in contextualized representations are designed for English, we discuss our multilingual variant of DisCo and the recently proposed MBE metric.\n\nMultilingual DisCo\nDiscovery of Correlations (DisCo) is a templatebased metric that measures unfair or biased associations of predictions of an MLM to a particular gender. It follows a slot-filling procedure where for each template, predictions are made for a masked token, which are evaluated to assess whether there is a statistically significant difference in the top predictions across male and female genders. For calculating the bias score using DisCo, a \u03c7 2 test is performed to reject the null hypothesis (with a p-value of 0.05) that the model has the same prediction rate with both male and female context. We use the modified version of the metric from (Delobelle et al., 2022) that measures the fraction of slot-fills containing predictions with gendered associations (fully biased model gets a score of 1, and fully unbiased gets a score of 0).\nWe extend the Names variant of DisCo, as personal names can act as representatives for various socio-demographic attributes to capture cultural context (Sambasivan et al., 2021) . Especially for India, surnames are a strong cultural identifier. Majority Indian surnames are typically an identifier of belonging to a particular caste, religion and culture. We use surnames from specific cultures which speak the languages for which we prepare the name pairs for. We further use these surnames to filter out personal first names for both male and female from an open-source Indian names list containing a large number of popular Indian names (details in Appendix A.1) and word-translated the names from English to the corresponding languages, to be used for slot-filling. Further, unlike nouns and pronouns which might be gender-neutral in some languages, names are indicative of gender to a large extent across cultures.\nDataset Construction: We start with the 14 templates provided in Webster et al. (2020) and translate them using Bing translation API 2 to 6 Indian languages of varying resources. We use the Class taxonomy from (Joshi et al., 2020) to characterize language resources, where Class 5 represent high resource and Class-0 for lowest resource languages. Our set of Indian Languages contain Class 4 language Hindi (hi); Class 3 language Bengali (bn); Class 2 languages Marathi (mr) and Punjabi (pa); and Class 1 language Gujarati (gu). A challenge while transferring templates from English to these languages is that, unlike English, a common template might not be applicable to both genders. For eg. the template \"'{PERSON} likes to {BLANK}\"', will have different translations in Hindi, depending upon the gender of the slot fill for {PERSON}, as Hindi has gendered verbs. Hence, during translation we first filled the {PERSON} slot with a male and a female name to obtain two templates corresponding to each gender (see Figure 1 ). All the translated templates in our dataset were then thoroughly reviewed and corrected by human annotators who are native speakers of the languages (details in Appendix A.1).\n\nMultilingual Bias Evaluation (MBE)\nWe also evaluate MLMs with the MBE score proposed in (Kaneko et al., 2022) containing datasets for bias evaluation in 8 high resource languages: German (de), Japanese (ja), Arabic (ar), Spanish (es), and Mandarin (zh) belonging to Class 5; Portuguese (pt) and Russian (ru) in Class 4; and Indonesian (id) in Class 3. For evaluation, it first considers parallel corpora from English to different languages and extracts the set of sentences containing male and female words. Next, the likelihood for each sentence is evaluated with the MLM, and the bias score is measured as the percentage of total pairs for which a male sentence gets a higher likelihood than a female sentence. Hence a value close to 50 for an MLM indicates no bias towards both groups while greater or smaller values indicate a bias towards females and males respectively. For better interpretability of metrics, we report |50 \u2212 MBE| in our results.\n\nMitigating Bias in Multilingual Models\nWe next discuss how we extend bias mitigation techniques to work beyond English along with different fine-tuning and prompting strategies that we deploy in our experiments.\n\nCounterfactual Data Augmentation (CDA)\nCDA (Zhao et al., 2018) is an effective method for reducing biases picked up by the language models during pre-training. It operates by augmenting an unlabeled text corpus with counterfactuals generated for each sentence based on a specific dimension like gender. As an example, the counterfactual for a sentence s = \"The doctor went to his home\" will be \u015d = \"The doctor went to her home\". The model is then fine-tuned on the augmented data, which helps balance out any spurious correlations that would have existed in the pre-training dataset.\nTo generate counterfactuals in English, we do word replacements on Wikipedia data using 193 gendered term pairs (eg. {he, she}, {actor, actress}, etc.) following Lauscher et al. (2021) . However, generating counterfactuals for languages other than English can be challenging as acquiring term pairs need recruiting annotators which can be expensive for low-resource languages. Further, word replacement can prove unreliable for languages that mark gender case to objects (like Hindi), producing ungrammatical sentences (Zmigrod et al., 2019) .\n\nGenerating Multilingual Counterfactuals:\nWe use a translation-based approach to obtain counterfactually augmented examples in different languages. We first select the sentences in the Wikipedia English corpus containing India-related keywords which were extracted using ConceptNet (Speer et al., 2017) which include keywords related to Indian food, location, languages, religions, etc. Using these keywords we select a set of 20K sentences to avoid under-representation of Indian culture specific context. Also, generating counterfactuals for the whole corpus and fine-tuning MLMs for each of the languages will require substantial energy consumption (Strubell et al., 2019) , so we decided to use the set of filtered 20k sentences for debiasing the MLMs. Further, we augment the 193 term pairs list to contain pairs of Indian personal names as well. We align the male and female names through a greedy search for selecting pairs with minimum edit distance. Finally, using the augmented term pairs list and the filtered data with Indian context, we generate counterfactuals using word replacements and translate the obtained data to the 6 Indian languages.\nOnce we have obtained CDA data in different languages, we can utilize it to debias the model. We define CDA-S as a fine-tuning setup where the MLM is debiased using CDA data for languages belonging to the set S \u2282 L, where L = {en, hi, pa, bn, ta, gu, mr}. In particular, we explore the following classes of fine-tuning setups: 1. CDA-{en}: Fine-tune the model with English CDA data only (zero-shot debiasing). 2. CDA-{l}: Fine-tune the model with language l specific CDA data (monolingual-debiasing). 3. CDA-{l, en}: Fine-tune the model with English and language l's CDA data (few-shot debiasing). 4. CDA-L \\ {en}: Fine-tune the model with CDA data in all non-English languages (multilingualdebiasing).\n\nSelf-Debiasing\nSelf-Debiasing (Schick et al., 2021 ) is a post-hoc method to reduce corpus-based biases in language models. It is based on the observation that pretrained language models can recognize biases in text data fairly well and prepends the input text with prompts encouraging the model to exhibit undesired behavior. Using this, it recognizes the undesirable predictions of the model as the ones with an increase in likelihood when the prompt is pro-vided and suppresses them in the final predictions. We translate the English prompt \"The following text discriminates against people because of their gender\" in different languages and use them for bias mitigation (SD-l). We also experiment with using English prompt for other languages (SD-en).\n\nResults\nWe evaluate the Out Of Box (OOB) biases as well the effect of applying aforementioned debiasing techniques in multilingual MLMs like XLMR-base (Conneau et al., 2020) , IndicBERT (Kakwani et al., 2020) , and mBERT (cased) (Devlin et al., 2019) using our multilingual DisCo metric. Additionally, we also evaluate language-specific monolingual models (refer Table 3 in appendix) and XLMR on the MBE score.\nComparison Between Different Fine-tuning Setups for CDA: We first compare the results of bias mitigation across all 4 classes of finetuning setups for CDA to understand the effect each had on the final bias reduction. As can be seen in Table 1 even though zero-shot transfer from English (CDA-{en}) results in some reduction in biases when compared to the models without any debiasing (OOB), most of the other fine-tuning setups that use language-specific counterfactuals incur better drops in the DisCo score. Specifically, few-shot debiasing (CDA-{l, en}) and multilingual-debiasing (CDA-L \\ {en}) perform consistently the best for both models with CDA-L \\ {en} performing slightly better for XLMR and substantially so for Indic-BERT. This shows that even though languagespecific counterfactuals were translated, using them for the debiasing of models helped in considerable bias reduction. We also observe that the monolingual debiasing (CDA-{l}) leads to a drop similar to CDA-{en}, and we conjecture that it might be attributed to the low amount of data we have in languages other than English for debiasing. Further, the dominant performance of CDA-L \\ {en} highlights that languages from a similar culture can collectively help improve biases in such models. We also observe similar results for mBERT which are provided in Table 4 in the appendix.\nComparison Between CDA and Self-Debiasing: Counter to CDA, Self-Debiasing shows different bias mitigation trends for Indian languages. Table 1 shows that for both multilingual MLMs, the overall Figure 2 : MBE scores for monolingual and multilingual models and the impact of debiasing across languages bias ends up increasing when Self-Debiasing is applied, and that too by a considerable amount for IndicBERT. This seems to be in contrast to the past work (Meade et al., 2022 ) that shows Self-Debiasing to be the strongest debiasing technique. However, we will see next the cases where it can indeed be effective in reducing biases.\n\nEvaluation on MBE Metric:\nWe first investigate the effect of Self-Debiasing on monolingual models when evaluated for the MBE metric. As can be observed in Figure 2a , for most languages (except Russian and Spanish), both variants of Self-Debiasing manage to reduce the biases substantially. However, when we compare the results on a multilingual model i.e. XLMR in Figure 2b , we again observe the same phenomenon as for multilingual DisCo, where the biases tend to increase upon applying Self-Debiasing. Figure 2a shows that SDen and SD-l have similar debiasing performance for monolingual models. It is intriguing that monolingual models are able to debias so well based on English prompts. This similarity in results with non-English and English prompts could possibly be explained by contamination in the pretraining monolingual data (Blevins and Zettlemoyer, 2022) .\nWe also compare the effect of CDA-{en}on reducing the biases and we observed it does obtain more success in most languages (except Spanish and Japanese). Even though MBE and Multilingual DisCo have different experimental setups, obtaining consistent results while using the two different metrics like English-only debiasing being insufficient to reduce biases in other languages. Selfdebiasing being ineffective for mitigating biases in multilingual models strenghtens the applicability of our results. Our results indicate that Self-Debiasing might be limited for multilingual models and we leave the investigation of this phenomenon to future work.\n\nConclusion\nIn this work, we investigated gender biases in multilingual settings by proposing a bias evaluation dataset in 6 Indian languages. We further extended debiasing approaches like CDA and Self-Debiasing to work for languages beyond English and evaluated their effectiveness in removing biases across languages in MLMs. One of our key findings is that debiasing with English data might only provide a limited bias reduction in other languages and even collecting a limited amount of counterfactual data through translation can lead to substantial improvements when jointly trained with such data from similar languages. Finally, we showed that despite being effective on monolingual models, Self-Debiasing is limited in reducing biases in mul-tilingual models with often resulting in an increase in overall bias. We hope that our work will act as a useful resource for the community to build more inclusive technologies for all cultures.\n", "hypothesis": " While understanding and removing gender biases in language models has been a longstanding problem in Natural Language Processing, prior research work has primarily been limited to English.  In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context.  In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations.  We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying bias in monolingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.", "answer": false}
{"title": "Unsupervised Subtitle Segmentation with Masked Language Models", "content": "\nIntroduction\nSubtitling is one of the principal means of providing accessible audiovisual content. With the ever increasing production of audiovisual content in multiple domains and languages, in the current digital era, subtitle provision can benefit from automation support, via Automatic Speech Recognition and/or Machine Translation (Volk et al., 2010; Aliprandi et al., 2014; Etchegoyhen et al., 2014; Tardel, 2020; Bojar et al., 2021) .\nSubtitles are subject to specific constraints in order to achieve adequate readability, including layout, on-screen duration and text editing. Among these constraints, segmentation addresses the maximum number of characters per line, the number of lines per subtitle, and breaks at natural linguistic frontiers. Segmentation has been shown to be an important readability factor (Perego et al., 2010; Rajendran et al., 2013) , with improperly segmented subtitles resulting in increased cognitive effort and reading times for users. Thus, automated subtitling systems need to generate properly segmented subtitles to achieve readability. * These authors contributed equally to this work.\nA typical baseline for subtitle segmentation, still used in some production systems, is simple character counting, whereby line breaks are inserted before reaching the maximum allowed number of characters per line. Although simple and fast, this approach does not address the need for linguistically correct segments and, therefore, falls short in terms of readability. Several approaches have been proposed to improve segmentation by automated means. \u00c1lvarez et al. (2014) proposed a machine learning method where subtitle breaks are predicted by Support Vector Machine and Linear Regression models trained on professionally-created subtitles. A similar method based on Conditional Random Fields was then shown to improve over these results (Alvarez et al., 2017) . Approaches that directly generate subtitle breaks within Neural Machine Translation have also been proposed in recent years (Matusov et al., 2019; Karakanta et al., 2020a) . Recently, Papi et al. (2022) developed a multilingual segmenter which generates both text and breaks and may be trained on textual input only, or on joint text and audio data.\nAlthough quality subtitle segmentation may be achieved with the aforementioned approaches, they require supervised training on segmented subtitle corpora. At present, the largest subtitle corpus is Open Subtitles (Lison et al., 2018) , which mainly covers entertainment material, contains subtitles mostly created by non-professionals or automatically translated, and does not include line breaks. The MuST-Cinema corpus (Karakanta et al., 2020b) , on the other hand, is a multilingual speech translation corpus that includes subtitles breaks, but is only available for 8 languages at the moment. Considering the vast amount of languages and domains in audiovisual content, the lack of segmented training data hinders the development of robust automated subtitling systems.\nIn this work, we describe a novel unsupervised method based on pretrained masked language mod-els (MLM), where line and subtitle breaks are inserted according to the likelihood of a segment acting as an isolated unit, as approximated by the probability of a punctuation mark occurring at a given segmentation point. In our experiments, this novel approach obtained competitive results on most metrics, while also fully preserving the original text and complying with length constraints. Our system may thus be used as a simple yet efficient subtitle segmenter with any pretrained masked language model, for any language covered by the model.\n\nApproach\nOur approach is based on the standard view that the more appropriate subtitle segments are those that may function as isolated grammatical chunks. We further hypothesise that a relevant approximation for the identification of this type of unit is the likelihood of a punctuation mark being inserted at the end of a candidate segment, as punctuation may mark the closure of a syntactic unit and is often associated with discursive pauses. To test this hypothesis, we compute the likelihood of punctuation marks at different segmentation points, as predicted by a pretrained MLM, and select the insertion point with the highest likelihood. 1 The segmentation candidates are determined under a sliding-window approach over the entire input text. We first generate the list of all pairs <\u03b1, \u03b2> over the unprocessed portion of the text, where \u03b1 is a segmentation candidate of length under a specified limit K, corresponding to the maximum number of characters per line, and \u03b2 is the remaining portion of the text to be segmented.\nWe then score all segmentation candidates \u03b1 with one of the LM scoring variants described below. A segmentation marker, either end-of-line (<eol>), or end-of-block indicating the end of a subtitle (<eob>), is then appended to the best scoring candidate, and \u03b2 becomes the input text to be segmented in a recursive iteration of the process.\nSince our method does not rely on any additional information, such as an audio source, to determine the segmentation type, an <eob> tag is inserted every even segment or when \u03b2 is empty; otherwise, an <eol> tag is inserted. We thus generate subtitles with a maximum of two lines, following a standard recommendation in subtitling. We also define a minimal number of characters (min) in \u03b1 for the segmentation process to apply, and do not segment lines that are under the specified character limit.\nWe evaluated three approaches to compute segmentation scores over each candidate pair <\u03b1, \u03b2>:\n\u2022 Substitution: The last token of \u03b1 is masked and the score is the highest MLM probability among punctuation marks on this mask.\n\u2022 Insertion: A mask is appended to \u03b1 and the score is the highest MLM probability among punctuation marks on this mask.\n\u2022 LM-Score: The score is the average of the perplexity of \u03b1 and \u03b2, as derived from the MLM probabilities for each token in the corresponding sequence.\nThe first two methods are variants of our core approach. The third method, while also based on the same pretrained MLM, relies instead on the pseudoperplexity of the sequences according to the MLM, computed following Salazar et al. (2020) . We included this latter variant to measure the potential of using LM scoring directly, without resorting to the likelihood of punctuation marks.\n\nExperimental Setup\nCorpora. For all experiments, we used the MustST-Cinema corpus (Karakanta et al., 2020b) , which is derived from TED talks and contains both line and subtitle break markers. In addition to being publicly available, it also allows for a direct comparison with the supervised models of Papi et al. (2022) . We report results of our approach on the 6 MuST-Cinema datasets for which comparative results were available, directly predicting segmentation on the test sets without any training. 2 Methods. For our approach, we tested the three variants described in Section 2. We used BERT (Devlin et al., 2019) as our MLM for all languages. 3 . Additionally, we included a variant called overt clueing (OC), where an overt punctuation mark at the end of a candidate segment increments the mask score by 1. We then compared the results of the best LM-based variant with those obtained by alternative approaches. In all cases, our results were computed with min = 15, as this value obtained the best results overall over the development sets, although the differences were minor with the other values we tested (1, 10 and 20). 4 We used the simple character counting approach (hereafter, CountChars) as baseline, and, as representative supervised methods on the selected datasets, the models described by (Papi et al., 2022) . Their core supervised approach is based on a Transformer (Vaswani et al., 2017) architecture with 3 encoder layers and 3 decoder layers, trained on textual MuST-Cinema input only (MC.Text), or on complementary audio data as well via an additional speech encoder with 12 encoder layers (MC.Multi). They trained each variant on either monolingual data alone (mono), or in a multilingual setting (multi). Finally, they also report results for a variant (OS.Text) trained on the Open Subtitles corpus (Lison et al., 2018) for their zero-shot experiments.\nEvaluation. We use the subtitle-oriented metric Sigma (Karakanta et al., 2022) , which computes the ratio of achieved BLEU (Papineni et al., 2002) over an approximated upper-bound BLEU score, on text that includes line and subtitle breaks. Sigma is meant to support the evaluation of imperfect texts, i.e. text that differs from the reference when breaks are omitted. Although our approach does not produce imperfect text, achieving perfect BLEU scores when breaks are ignored, we used this metric for comparison purposes. We also report break coverage results (Papi et al., 2022) , defined as the ratio of predicted breaks over reference breaks, which we computed separately for the EOL and EOB breaks. Finally, we include length conformity results (CPL), measured as the percentage of subtitle lines whose length is under the maximum number of characters defined by the subtitle guidelines (42 in the TED guidelines 5 ).\n\nComparative Results\nIn Table 2 , we present the results obtained by the selected approaches on the languages for which results were available with supervised models trained on in-domain data. Overall, our approach outperformed the CountChars baseline across the board, and was in turn outperformed by the supervised variants in terms of Sigma scores. Although it is clear from these results that training segmentation models on in-domain data, with or without audio data, provides clear advantages in terms of subtitle segmentation, it is worth noting that Sigma does not, by design, reflect the actual BLEU score without breaks, i.e. the generation of imperfect text, which is a by-product of the above supervised approaches and non-existent in ours. 6 In terms of CPL, all supervised models generate subtitle lines that overflow the limit, to a significant degree, whereas the selected unsupervised models trivially respect the length constraint. In Table 3 , we show the comparative results between the selected unsupervised methods and the supervised variants, in languages where zero-shot results were available for the latter approaches. In this scenario, in terms of Sigma our approach obtained results on a par with the supervised MC.Text models trained on in-domain MuST-Cinema data, outperformed the OS.Text models trained on Open Subtitles data, and was surpassed by the MC.Multi model, which exploits additional audio information, by 3.1 and 6.4 points. In terms of break coverage, in most cases our unsupervised method outperformed the supervised variants, to a significant degree compared to the text-based OS.Text and MC.Text models. Regarding BLEU scores without breaks, only the MC.Multi model reaches a score close to the perfect one achieved by the unsupervised models, whereas the MC.Text model is outperformed by 38.7 and 31.4 points in Dutch and Spanish, respectively. In all cases, the CPL scores indicate that none of the supervised approaches fully meet the length constraint, leading to overflowing lines in 8.2% of the cases at best and 29.9% at worst. In this scenario as well, the unsupervised approaches fully meet the length constraint, by design.\nOverall, overt clueing improved over our core method by an average of 3.12 Sigma points, indicating that some likely punctuation configurations were not properly captured by our MLM approximation. In general, our approach tends to overgenerate EOL markers, whereas the opposite is true for the selected supervised models. Determining which of these tendencies leads to better subtitle readability would require a specific human evaluation which we leave for future research.\nAlthough the zero-shot Sigma results obtained by the supervised MC.Multi method show the potential of this approach to provide pretrained models applicable to other languages, two important aspects are worth considering. First, the available zero-shot results were obtained on datasets in the same domain as the data seen to train the supervised models. A more complete assessment of the capabilities of these models in zero-shot settings, which would be the most frequent scenario consid-ering the lack of training data across domains and languages, would require specific evaluations in other domains. Secondly, although segmentation is a key aspect for subtitle readability, length conformity is an equally important constraint, if not more so considering that subtitles with lines over the CPL limit are considered invalid in subtitling. Our proposed unsupervised method can thus be seen as a pragmatic approach which guarantees valid subtitles while also providing quality segmentation across the board. 7\n\nConclusions\nWe described an unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line or subtitle breaks are inserted according to the likelihood of punctuation occurring at candidate segmentation points.\nAlthough supervised models, trained on indomain data with audio support, were shown to perform better that this simple textual approach in terms of the Sigma metric, they tend to generate imperfect text to varying degrees, while also failing to fully meet length constraints that are essential for subtitling.\nIn contrast, our LM-based textual approach outperformed supervised models in most cases in terms of break generation coverage, while also fully preserving the original text, complying with length constraints, and obtaining competitive results in terms of Sigma. This simple approach may thus provide a highly portable complementary solution for subtitle segmentation across languages and domains.\n", "hypothesis": " We describe a novel unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line endings and subtitle breaks are predicted according to the likelihood of punctuation to occur at candidate segmentation points.  Our approach obtained competitive results in terms of segmentation accuracy across metrics, while also fully preserving the original text and complying with length constraints.  Although supervised models trained on in-domain data and with access to source audio information can provide better segmentation accuracy, our approach is highly portable across languages and domains and may constitute a robust off-the-shelf solution for subtitle segmentation..", "answer": true}
{"title": "Abstractive Text Summarization Using the BRIO Training Paradigm", "content": "\nIntroduction\nText summarization reduces the size of the original text while preserving its main content. The two main approaches for constructing summaries are extractive and abstractive. Extractive summarization directly lifts sentences or words which convey key topics of the original documents, and concatenates them. Abstractive summarization discovers the primary content of the documents and generates summaries. Abstractive summaries are usually more natural and coherent than extractive summaries.\nMost abstractive summarization models follow the encoder-decoder framework. Existing abstractive summarization models are trained using maximum likelihood estimation and rely on the reference summaries. Liu et al. (2022a) propose a BRIO training paradigm to address reliance on reference summaries by assuming non-deterministic distribution of system-generated candidate summaries. In this paper, we use the BRIO training paradigm for abstractive summarization models to construct summaries for documents in English and Vietnamese. We make the following contributions:\n\u2022 We adapt the BRIO training paradigm for abstractive summarization using BART-based and T5-based models as backbones.\n\u2022 We present issues with the BRIO paradigm.\n\u2022 We investigate abstractive summarization models using BARTpho-BRIO and ViT5-BRIO to obtain improved results.\n\u2022 We publicly release the VieSum summarization dataset for research purpose.\nThe remainder of this paper is organized as follows. Related work is presented in Section 2. Section 3 introduces a large dataset for summarization in Vietnamese, named VieSum. Experiments and discussion are presented in Section 4. Section 5 concludes the paper. (Li et al., 2017) , actor-critic approaches from reinforcement learning (Li et al., 2018) , and Transformer (Vaswani et al., 2017) . Liu et al. (2022b) develop the PageSum model for abstractive summarization by incorporating locality bias in both encoder and decoder. Each document is partitioned into non-overlapping pages.\n\nRelated Work\nThe encoder, which is an abstractive summarizer, encodes each page and makes local predictions. The decoder predicts output based on a weighted combination of local predictions. The authors fine-tune the BART model (Lewis et al., 2020) for abstractive summarization and investigate several approaches to locality, such as spatial locality, discourse locality, and document locality. Page-Sum outperforms abstractive summarization models such as longformer encoder-decoder (Beltagy et al., 2020) , encoder-decoder attention with headwise positional strides (Huang et al., 2021) , and BART with Hierarchical Attention Transformer (Rohde et al., 2021) . However, PageSum takes a long time to train, requires large memory size, and fails to capture long distance dependencies.\nSeveral studies use pre-trained models for abstractive text summarization. Farahani et al. (2021) use mT5 (Xue et al., 2021) and sequence to sequence ParsBERT (Rothe et al., 2020) to construct abstractive summaries for Persian texts. T5 (Raffel et al., 2020) and BERT (Devlin et al., 2018) have also been used to construct abstractive summaries (Garg et al., 2021) . Kieuvongngam et al. (2020) summarize COVID-19 biomedical research articles using BERT and GPT-2 (Radford et al., 2019) . Features of documents are extracted and integrated into an abstractive model to improve summary generation. Nambiar et al. (2022) develop an encoder-decoder model using attention, in which POS features are incorporated to the word embedding layers to enhance the word vectors. Experiments on a dataset in Malayalam show that the integration of attention model and POS features is better than the seq2seq and attention models. Barna and Heickal (2021) adapt the pointer generator network for abstractive summarization by combining a pre-trained word embedding layer for transferring semantic similarity and topic features for better topic coverage. A drawback of usual abstractive summarization is the omission of named entities. To ameliorate, Berezin and Batura (2022) train a named entity recognition model based on ROBERTa to discover named entities. Then, the BART masked named entity language model is trained to pay attention on the name entities. Finally, BART is fine-tuned for text summarization.\nMost studies to construct abstractive summaries in Vietnamese use an encoder-decoder framework or a pre-trained model. Quoc et al. (2019) integrate sentence positions and term frequencies into a pointer generator network with a coverage mechanism to perform the abstractive summarization for Vietnamese documents. Lam et al. ( 2022) construct abstractive summaries for online newspapers using RNN with attention, BiLSTM with copy generator, standard Transformer, BERT, and sequence-to-sequence abstractive models using bottom-up approach. Phan et al. (2022) perform experiments to summarize Vietnamese documents using Transformer-based encoder-decoder architectures such as Transformer, PhoBERT (Tran et al., 2022) , and ViT5 (Phan et al., 2022) .\n\nVieSum Dataset\nWe construct a VieSum dataset for Vietnamese consisting of 1,627,415 documents and their corresponding summaries, grouped into 23 categories. In particular, BeautifulSoup 1 and Newspaper3k 2 are used to collect and extract articles from popular online newspapers in Vietnamese such as vnexpress.net, dantri.com.vn, danviet.vn, vietnamnet.vn, laodong.vn, and vov.vn . The summaries and content documents are considered reference summaries and documents, respectively.\n\nExperimental Results\nWe perform experiments in the Google Colaboratory environment, NVIDIA Tesla T4 16GB. We use the CNNDM 3 dataset in English, and our VieSum dataset in Vietnamese. Due to limitation of the hardware, we perform experiments with 70,000 documents picked randomly and their corresponding reference summaries from VieSum. Each dataset is split into 3 parts including 75% for training, 8% for validation, and 17% for testing.\nIn this paper, the pre-trained BART 512-lengthbased and T5 512-length -based models are used as backbones for generating abstractive summaries. The BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) models are trained on the CNNDM dataset, while the BARTpho (Tran et al., 2022) and ViT5 (Phan et al., 2022) are trained on the VieSum dataset. All models are base models. To make it easy for comparison, we use the same parameters as suggested by the original authors. \n\nStandard Abstractive Models\nFirst, we experiment and evaluate abstractive summarization approaches using standard BART-base and T5-base models. We train the models using a batch size of 4, epoch count of 5, learning rate of 10 \u22125 , warmup step of 20,000, and the Adam optimizer. The results of abstractive summarization systems using the standard backbone models are presented in Table 1 .\n\nFine-tuning Abstractive Models\nTo improve the quality of summaries created, we fine-tune the backbone models using the Trainer provided by Hugging Face 4 . We do not fine-tune the BART model because it is already fine-tuned on the CNN dataset. Table 2 shows the ROUGE scores of the fine-tuned abstractive models.\n\nFine-tuning Abstractive Models and BRIO\nThe BRIO (Liu et al., 2022a) while the evaluator is trained using a contrastive loss (Hadsell et al., 2006) .\nIn BRIO, a backbone model is used to produce N abstractive summaries, the so-called candsums, for each document. Each candsum is assigned a quality score by obtaining the average score of its ROUGE-1, ROUGE-2, and ROUGE-L values. In particular, Liu et al. (2022a) use the BART 1024-length model to create 16 candsums for each document. Next, documents, reference summaries, and corresponding candsums sorted by the descending quality scores are used to train the abstractive summarization model using the BRIO paradigm. We note that Liu et al. (2022a) use the standard models as back-bones and train them with the BRIO paradigm.\nIn our work, the fine-tuned backbone abstractive summarization models, presented in the previous section, are used to produce N=6 candsums for each document using diverse beam search (Vijayakumar et al., 2018) with num beam groups=6, diversity penalty=1.0, and num beams=4. The abstractive summarization models are trained using a learning rate of 10 \u22123 , and the Adafactor optimizer. Liu et al. (2022a) claim that BRIO training helps the models reach the best performance within one epoch on the CNNDM dataset 5 . Therefore, we use one epoch for training the fine-tuned summarization models with the BRIO paradigm. The results of the abstractive summarization systems trained with BRIO are presented in Table 3 .\n\nFine-tuning Abstractive Models and BRIO-Loop\nAs suggested by Liu et al. (2022a) , we perform loop processing, using the candsums created by the abstractive summarization models trained with BRIO to train the models. However, after several Experimental results show that the BRIO training paradigm significantly helps improve the abstractive summaries by reducing the dependence of the system on the reference summaries. However, assigning weights to both candsums and reference summaries is necessary in order to decrease reliance on reference summaries. The diverse beam search helps obtain diverse candsums, but could cause interference in the beam search space because the model might not follow the reference summaries. In addition, using the ROUGE metric for evaluating the abstractive summarization models trained with the BRIO paradigm seems unfair because these models could produce summaries which are independent on the reference summaries.\n\nDiscussion\nIt is not easy to make comparisons between models trained on different hardware and on different datasets. We make an attempt to compare our work with published papers on similar datasets.\nCurently, BRIO using a standard BART 1024-length model as backbone, which generates 16 candsums, achieves SOTA results on the CNNDM dataset with a ROUGE-1 of 47.78 and a ROUGE-L of 32.58 (Liu et al., 2022a) . In addition, BART 1024-length -BRIO with 2 iterations reaches ROUGE-1 and ROUGE-L of 48.01 and 44.67, respectively; these are both better than our BART 512-length -BRIO, which creates 6 candsums for each document, after 2 iterations: 46.55 for ROUGE-1 and 43.00 for ROUGE-L. Tawmo et al. (2022) fine-tune the T5 abstractive summarization model and evaluate on the CNNDM dataset. Their T5 model achieves ROUGE-1 and ROUGE-L scores of 40.79 and 34.80, respectively, which are lower than the scores of our fine-tuned T5 model, and significantly lower than scores of our best model, the T5-BRIO-Loop model: 45.24 for ROUGE-1 and 41.80 for ROUGE-L.\nFor Vietnamese abstractive summarization, Quoc et al. ( 2019) use LSTMs with the features of sentence positions and term frequencies (LSTM+SP+TF) on a Vietnamese dataset collected from Baomoi 6 . The best ROUGE-1 and ROUGE-L scores of their model are 31.89 and 29.97, respectively, which are significantly lower than the scores of our BRIO-BART model.\nBoth the BARTpho and ViT5 models trained with the BRIO paradigm outperform all models proposed by Lam et al. ( 2022) on the CTUNLPSum dataset, which is very similar to the VieSum dataset, including the sequence-to-sequence models, copy generator network, sequence-to-sequence with rewriter approach, and bottom-up approach.\nTran et al. ( 2022) apply several models for abstractive summarization on the VNDS (Nguyen et al., 2019) dataset. They perform experiments on 8 A100 GPUs with 40GB each. Their model is trained for 15 epochs in about 6 days. Their best model, BARTpho, achieves a ROUGE-1 of 61.14, which is slightly higher than the BARTpho-BRIO-Loop, and a ROUGE-L of 40.15, which is lower than that of the BARTpho-BRIO-Loop. In addition, the BARTpho-BRIO-Loop is trained on one epoch in about 32 hours using basic hardware. Phan et al. (2022) introduce a pre-trained text-totext Transformer for Vietnamese abstractive summarization, called ViT5. The authors claim the ViT5 model as the SOTA for Vietnamese abstractive summarization. Their ViT5 abstractive summarization model achieves ROUGE-1 and ROUGE-L of 61.85 and 41.70, respectively, on the VNDS dataset (Nguyen et al., 2019) . We conducted experiments on VNDS and found interesting results related to the ViT5 model. The ROUGE scores of the ViT5 model trained using the common paradigm are essentially identical to the ROUGE scores provided by Phan et al. (2022) . However, the scores of the ViT5 model trained using the BRIO paradigm are reduced to 59.37 and 41.6, respectively. On the VieSum dataset, the standard ViT5base achieves an ROUGE-1 of 53.39 and ROUGE-L of 35.88; while the ViT5-BRIO-Loop has better scores: ROUGE-1 of 60.90 and ROUGE-L of 44.36. We leave further exploration and evaluation these unstable results for future work.\n\nConclusion\nWe investigated abstractive summarization models trained with the BRIO paradigm. Experiments show that we can improve abstractive summarization models by fine-tuning the backbones before training them with BRIO. In particular, the summarization models trained with BRIO outperform other summarization models in Vietnamese. We also discuss issues with the BRIO paradigm for further exploration. In addition, we built the VieSum dataset for summarization in Vietnamese. For future work, we will ask volunteers to evaluate and provide feedback on a small subset of the VieSum dataset.\n", "hypothesis": " sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model's dependence on reference summaries, and improve model performance during inference. This paper presents a novel approach to improve abstractive summaries by utilizing generative adversarial networks (GANs) instead of traditional language models, and training them with the BRIO paradigm.  We build a text summarization dataset for Vietnamese, called VieSum.  We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets.  The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese..", "answer": false}
{"title": "Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?", "content": "\nIntroduction\nGrammatical Error Correction (GEC) is the task of automatically detecting and correcting errors in text (Bryant et al., 2022) . Nowadays, there are two mainstream GEC approaches. The first is treating GEC as a low-resource machine translation task (Yuan and Briscoe, 2016) , where sequence-tosequence models like BART (Lewis et al., 2020) are used. This approach simply inputs the incorrect text to the encoder and gets the corrected result from the decoder. The second is treating GEC as a sequence tagging task, where the incorrect text is still taken as the input, but the output is edit tags (keep, delete, add, replace, etc.) for each token. After applying all the edits to the input text, the corrected result is then generated. The model used in this approach is also known as sequence-to-edit * Corresponding author. models and GECToR (Omelianchuk et al., 2020 ) is a typical one.\nHowever, most researches on GEC focus on English while Chinese GEC (CGEC) has just started up. The Chinese language is different from English in many ways and its GEC is thus much harder. Instead of word inflection in many Western languages, the Chinese grammar is expressed by function words and word order, making CGEC more difficult and complex for that we can't take word form as a handle. In addition, unlike English, we have very few datasets for training and testing CGEC, which sets us exploring training-free methods like model ensemble to further improve the performance of CGEC systems.\nBecause of the nature of GEC that corrections can be represented as several independent edits, model ensemble has been a popular way to improve GEC systems. In CGEC, Li et al. (2018) , Liang et al. (2020) and Zhang et al. (2022) ensemble their models by majority voting on edits and achieve considerable improvement. Besides, Xie et al. (2016) adopt language models to improve neural language correction, following whom Junczys-Dowmunt et al. (2018) ensemble their GEC models using a language model probability. Today, transformer-based (Vaswani et al., 2017) Pre-trained Language Models (PLMs) have been in predominant use in NLP. However, we find few works on model ensemble using PLMs in CGEC.\nIn this work, we hypothesize that choosing the best ensemble output with the help of perplexity (PPL) computed by PLMs should boost the final performance of CGEC. We experiment on ensemble of four CGEC models, including two sequenceto-sequence ones and two sequence-to-edit ones. We try four ensemble strategies: traditional voting, sentence-level ensemble, edit-level ensemble, and edit-combination ensemble, the last three exploiting the power of PLMs.\nTo our surprise, the results of model ensemble with PLMs do not exceed those of traditional voting and are even worse than most of the single models.\nTo find out why a low PPL cannot lead to a better GEC performance, we carry out a detailed analysis on the ensemble results and get some insights on GEC: 1) In the test data, human references are insufficient, while PLM-based ensemble strategies produce valuable candidates, after being human checked, which may be considered as necessary complement to human references.\n2) When facing an erroneous sentence, a human expert corrects it with the minimal effort, while PLM-based ensemble strategies generate more natural and idiomatic text, which is of great help for oversea language learners.\n3) With the powerful ability, PLM-based models try to generate fluent sentences but sometimes ignore the original meaning of the source sentence, resulting in over-correction that should be addressed in future work.\n\nSingle CGEC Models\nWe implement four single models as baselines, with two seq2seq models and two seq2edit ones. All the models use the Lang-8 1 dataset for training.\nSequence to Sequence Models. The two seq2seq models are both based on BART-base-Chinese (Shao et al., 2021) , and are implemented using fairseq 2 (Ott et al., 2019) . Besides Lang-8, the HSK data 3 is also used for training. One seq2seq model adopts the \"dropout-src\" strategy, where each token in input sentences is replaced with \"[PAD]\" with a probability of 10%. The other one is pre-trained on the synthetic data constrcted on THUCNews 4 (Sun et al., 2016) before the normal training.\nSequence to Edit Models. We apply GECToR-Chinese 5 (Zhang et al., 2022) as our seq2edit models, with the pre-trained Structbert-large-Chinese 6 (Wang et al., 2019) as backbone. Our two seq2edit models only differ in random seeds.\n\nPre-trained Language Models\nWe adopt three PLMs to carry out model ensemble.\nBERT-base-Chinese 7 . It is pre-trained on two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). In MLM, each token has a chance of 15% to be replaced with a \"[MASK]\" (80%), a random word (10%), or itself (10%). Please refer to Devlin et al. (2019) for details.\nMacBERT-base-Chinese 8 . It is similar to BERT, but employs whole word masking, N-gram masking and similar word replacing in MLM. Besides, Sentence-Order Prediction (SOP) is exploited instead of NSP. Please refer to Cui et al. (2020) for details.\nGPT2-Chinese 9 . It is an unofficial Chinese version of GPT-2 (Radford et al., 2019) . It employs generative pre-training, by predicting the next word in a sentence with only previous words provided.\n\nEnsemble Strategy\nWith the source sentence and the outputs of four single models as the input, we present four ensemble strategies. The diagram of our PLM-based ensamble strategies is shown in Figure 1 .\n\nTraditional Voting\nDifferent models vote for the final results. For each sentence, we consider edit operations suggested by no less than T models as the correct one. In our work, we experiment on T from 2 to 4. We implement the original code provided by Zhang et al. (2022) to carry out this voting strategy.\n\nSentence-level Ensemble\nUsing different PLMs, we compute the perplexities (PPLs) of the source sentence and the outputs of four single models. Specifically, given a sentence S = (w 1 , w 2 , ..., w n ) and the probability of the word w i computed by a PLM denoted as p i , then\nP P L = ( n i=1 1 p i ) 1/n .\nThe sentence with the lowest PPL is chosen to be the final output.\n\nEdit-level Ensemble\nGiven a source sentence S, all the edits suggested by single models constitute a candidate set A, and the number of edit spans is denoted as m. An edit span means the start-end pair of an edit's position in the sentence. The set of all the edits (from different single models) on the i-th edit span (including For each edit span (A i in A), we generate |A i | new sentences, each corresponding to a single edit in A i . Then we consult PLMs about PPLs of these new sentences and accept the edit corresponding to the sentence with the lowest PPL, which we mark as e i best . In other words, e i best is the best edit (decided by PLMs) in A i , or on span i.\nWith each span's best edit, the final edit set E f inal combines these best edits, described as:\nEQUATION\nThe final hypothesis sentence is then produced on the basis of E f inal .\n\nEdit-combination Ensemble\nOne source sentence may contain more than one errors. For each sentence, this strategy applies all edit combinations to the source sentence and generates many new sentences.\nTo be specific, given a source sentence S, the edit candidates A are still divided as A = m i=1 A i , and then we get all possible edit-combinations by:\nU = {{e 1 j 1 , e 2 j 2 , ..., e m jm } | j i \u2208 {1, 2, ..., |A i |}}.\n(2) Thus we generate ( m i=1 |A i |) new sentences, each corresponding to an edit-combination in U . The sentence with the lowest PPL will be accepted as the final output.\nTaking the computational complexity into consideration, we only apply this strategy on sentences whose number of edit-combinations is no more than 300. Such simple sentences make up 95.15% of MuCGEC-test and 98.90% of NLPCC-test. We do nothing to the left not-so-simple sentences.\n\nDataset and Evaluation Metrics\nWe carry out experiments on MuCGEC test data (Zhang et al., 2022) and NLPCC test data (Zhao et al., 2018) . MuCGEC contains 7063 sentences and each have at most three references, but is not available at present. NLPCC contains 2000 sentences, each with one or two references, and about 1.1 references on average. We carry out analysis on NLPCC test data.\nOn MuCGEC, we submit the results of our systems to the public evaluation website 10 . On NLPCC, we implement the tools provided by Zhang et al. (2022) to compute the P (Precision), R (Recall), and F 0.5 of the output on char-level. Also, we report word-level results on NLPCC-test for reference with previous works. \n\nResults\nTable 1 shows the experimental results. The traditional voting strategy achieves the best performance, with a 44.09 F 0.5 score on char level that is significantly higher than the best single model. With the threshold T increasing, the precision rises while the recall drops. When T = 3, F 0.5 score reaches the peak, in line with the finding of Tarnavskyi et al. (2022) .\nHowever, the PLM-based ensemble strategies get much worse performance than the simple voting strategy, and are even lower than most of single models. In terms of precision and recall, traditional voting achieves higher precision but lower recall than single models while PLM-based strategies are on the contrary. Among three ensemble strategies, the sentence-level one performs best.\nAmong different PLMs, GPT2-Chinese achieves the best results in all three ensemble strategies. This may be because BERT-based models are naturally good at mask prediction rather than computing PPLs for whole sentences. Later, we base GPT2-Chinese to make further analysis.\n\nAnalysis and Discussion\nWe design three ensemble strategies to choose the sequence with the lowest PPL as the final output, but why does F 0.5 score drop? In our work, all single models are made up of their own PLMs, which means ensembling them exploiting another PLM is just like using PLMs to judge PLMs, so the performance may benefit little. This is in line with the work of Junczys-Dowmunt et al. (2018) , where pre-trained single models gain little and even have worse performance after PLM-based ensemble while other simple single models benefit a lot. Besides this, are there any other reasons?\n\nStatistical Results\nIn order to find out the cause of the poor performance of PLM-based ensemble strategies, on NLPCC test data, we randomly select 200 samples from the results of all the three strategies along with the best single model (seq2seq-1) for comparison, and ask two graduate students to analyze the output sentences with a double-blind manner. After that, a third expert arbitrates for the inconsistency. Instructions for human annotators are shown in Appendix A.\nAccording to human judgement, four types are summarized. Exact (E): the output is fluent and correct, in line with the reference. Good (G): the output is fluent and correct but different with the reference, which indicates that the references are not sufficient enough. Over-corrected (O): the output is fluent but doesn't meet the original meaning of the source sentence. Wrong (W): the output has other problems that we don't care in this work.\nThe result of human annotation is reported in Table 3 : Three examples for G and one for O. Label \"src\", \"out\" and \"ref\" means the source sentence, the output of one of our PLM-based ensemble strategies and the reference, respectively.\n\nDiscussion\nThe insufficiency of GEC references. In the outputs of PLM-based ensemble strategies, about 1/4 (\"G\") are automatically judged to be wrong according to the golden references, but indeed correct after human check. Actually, if we assume class G is also correct, the number of sentences corrected by PLM-based ensemble strategies (except edit-level ensemble) exceeds that by seq2seq-1, the best single model. This indicates that GEC references are not sufficient enough, even though datasets like NLPCC provide multi-references. Since artificially generating a correct sentence is much harder than judging a machine-generated sequence correct or not, continuously adding human checked results of PLMensemble systems to the references may be a good solution to improve the quality and diversity of the GEC test data.\nThe goal of GEC. This is a significant issue. Is it enough to just get a sentence rid of errors? Taking coding into example, can we say a piece of code \"good\" when all the \"errors\" are clear but pages of \"warnings\" are flashing? In \"Good\" samples, we compare the human references and automatically generated sentences, and find many of references are only correct but not so idiomatic. On the other hand, many output sentences of PLM-based ensemble strategies are more natural and like native speakers. If a GEC system is aimed at helping overseas students with their language learning, for example, then idiomaticity should be taken into consideration.\nThe over-correction of PLM-based models. About 1/10 of sentences generated in PLM-based ensemble (\"O\") are over-corrected, i.e., the model corrects a correct token and thus produces a wrong sentence. PLMs always choose the most fluent sentence with the lowest PPL, sometimes ignoring the original meaning of the source sentence. The over-correction of PLM-based generative models should be addressed in future work.\n\nConclusion\nThis paper introduces novel ensemble strategies for the GEC task by leveraging the power of pretrained language models (PLMs). We compare different strategies of model ensemble in CGEC. Surprisingly, PLM-based ensemble strategies do not benefit the system. This suggests that PPL and F 0.5 have diverging goals. According to our analysis, the insufficiency of references in GEC remains a major problem, which should be continuously improved in future work.\nthe National Natural Science Foundation of China (62076008) and the Key Project of Natural Science Foundation of China (61936012).\n", "hypothesis": " Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance.  We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system.  To this end, we explore several ensemble strategies based on strong PLMs with four sophisticated single models.  However, the performance does not improve but even gets worse after the PLM-based ensemble.  This surprising result sets us doing a detailed analysis on the data and coming up with some insights on GEC.  The human references of correct sentences is far from sufficient in the test data, and the gap between a correct sentence and an idiomatic one is worth our attention.  Moreover, the PLM-based ensemble strategies provide an effective way to extend and improve GEC benchmark data.  Our source code is available at https://github.com/JamyDon/PLMbased-CGEC-Model-Ensemble..", "answer": true}
{"title": "Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?", "content": "\nIntroduction\nGrammatical Error Correction (GEC) is the task of automatically detecting and correcting errors in text (Bryant et al., 2022) . Nowadays, there are two mainstream GEC approaches. The first is treating GEC as a low-resource machine translation task (Yuan and Briscoe, 2016) , where sequence-tosequence models like BART (Lewis et al., 2020) are used. This approach simply inputs the incorrect text to the encoder and gets the corrected result from the decoder. The second is treating GEC as a sequence tagging task, where the incorrect text is still taken as the input, but the output is edit tags (keep, delete, add, replace, etc.) for each token. After applying all the edits to the input text, the corrected result is then generated. The model used in this approach is also known as sequence-to-edit * Corresponding author. models and GECToR (Omelianchuk et al., 2020 ) is a typical one.\nHowever, most researches on GEC focus on English while Chinese GEC (CGEC) has just started up. The Chinese language is different from English in many ways and its GEC is thus much harder. Instead of word inflection in many Western languages, the Chinese grammar is expressed by function words and word order, making CGEC more difficult and complex for that we can't take word form as a handle. In addition, unlike English, we have very few datasets for training and testing CGEC, which sets us exploring training-free methods like model ensemble to further improve the performance of CGEC systems.\nBecause of the nature of GEC that corrections can be represented as several independent edits, model ensemble has been a popular way to improve GEC systems. In CGEC, Li et al. (2018) , Liang et al. (2020) and Zhang et al. (2022) ensemble their models by majority voting on edits and achieve considerable improvement. Besides, Xie et al. (2016) adopt language models to improve neural language correction, following whom Junczys-Dowmunt et al. (2018) ensemble their GEC models using a language model probability. Today, transformer-based (Vaswani et al., 2017) Pre-trained Language Models (PLMs) have been in predominant use in NLP. However, we find few works on model ensemble using PLMs in CGEC.\nIn this work, we hypothesize that choosing the best ensemble output with the help of perplexity (PPL) computed by PLMs should boost the final performance of CGEC. We experiment on ensemble of four CGEC models, including two sequenceto-sequence ones and two sequence-to-edit ones. We try four ensemble strategies: traditional voting, sentence-level ensemble, edit-level ensemble, and edit-combination ensemble, the last three exploiting the power of PLMs.\nTo our surprise, the results of model ensemble with PLMs do not exceed those of traditional voting and are even worse than most of the single models.\nTo find out why a low PPL cannot lead to a better GEC performance, we carry out a detailed analysis on the ensemble results and get some insights on GEC: 1) In the test data, human references are insufficient, while PLM-based ensemble strategies produce valuable candidates, after being human checked, which may be considered as necessary complement to human references.\n2) When facing an erroneous sentence, a human expert corrects it with the minimal effort, while PLM-based ensemble strategies generate more natural and idiomatic text, which is of great help for oversea language learners.\n3) With the powerful ability, PLM-based models try to generate fluent sentences but sometimes ignore the original meaning of the source sentence, resulting in over-correction that should be addressed in future work.\n\nSingle CGEC Models\nWe implement four single models as baselines, with two seq2seq models and two seq2edit ones. All the models use the Lang-8 1 dataset for training.\nSequence to Sequence Models. The two seq2seq models are both based on BART-base-Chinese (Shao et al., 2021) , and are implemented using fairseq 2 (Ott et al., 2019) . Besides Lang-8, the HSK data 3 is also used for training. One seq2seq model adopts the \"dropout-src\" strategy, where each token in input sentences is replaced with \"[PAD]\" with a probability of 10%. The other one is pre-trained on the synthetic data constrcted on THUCNews 4 (Sun et al., 2016) before the normal training.\nSequence to Edit Models. We apply GECToR-Chinese 5 (Zhang et al., 2022) as our seq2edit models, with the pre-trained Structbert-large-Chinese 6 (Wang et al., 2019) as backbone. Our two seq2edit models only differ in random seeds.\n\nPre-trained Language Models\nWe adopt three PLMs to carry out model ensemble.\nBERT-base-Chinese 7 . It is pre-trained on two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). In MLM, each token has a chance of 15% to be replaced with a \"[MASK]\" (80%), a random word (10%), or itself (10%). Please refer to Devlin et al. (2019) for details.\nMacBERT-base-Chinese 8 . It is similar to BERT, but employs whole word masking, N-gram masking and similar word replacing in MLM. Besides, Sentence-Order Prediction (SOP) is exploited instead of NSP. Please refer to Cui et al. (2020) for details.\nGPT2-Chinese 9 . It is an unofficial Chinese version of GPT-2 (Radford et al., 2019) . It employs generative pre-training, by predicting the next word in a sentence with only previous words provided.\n\nEnsemble Strategy\nWith the source sentence and the outputs of four single models as the input, we present four ensemble strategies. The diagram of our PLM-based ensamble strategies is shown in Figure 1 .\n\nTraditional Voting\nDifferent models vote for the final results. For each sentence, we consider edit operations suggested by no less than T models as the correct one. In our work, we experiment on T from 2 to 4. We implement the original code provided by Zhang et al. (2022) to carry out this voting strategy.\n\nSentence-level Ensemble\nUsing different PLMs, we compute the perplexities (PPLs) of the source sentence and the outputs of four single models. Specifically, given a sentence S = (w 1 , w 2 , ..., w n ) and the probability of the word w i computed by a PLM denoted as p i , then\nP P L = ( n i=1 1 p i ) 1/n .\nThe sentence with the lowest PPL is chosen to be the final output.\n\nEdit-level Ensemble\nGiven a source sentence S, all the edits suggested by single models constitute a candidate set A, and the number of edit spans is denoted as m. An edit span means the start-end pair of an edit's position in the sentence. The set of all the edits (from different single models) on the i-th edit span (including For each edit span (A i in A), we generate |A i | new sentences, each corresponding to a single edit in A i . Then we consult PLMs about PPLs of these new sentences and accept the edit corresponding to the sentence with the lowest PPL, which we mark as e i best . In other words, e i best is the best edit (decided by PLMs) in A i , or on span i.\nWith each span's best edit, the final edit set E f inal combines these best edits, described as:\nEQUATION\nThe final hypothesis sentence is then produced on the basis of E f inal .\n\nEdit-combination Ensemble\nOne source sentence may contain more than one errors. For each sentence, this strategy applies all edit combinations to the source sentence and generates many new sentences.\nTo be specific, given a source sentence S, the edit candidates A are still divided as A = m i=1 A i , and then we get all possible edit-combinations by:\nU = {{e 1 j 1 , e 2 j 2 , ..., e m jm } | j i \u2208 {1, 2, ..., |A i |}}.\n(2) Thus we generate ( m i=1 |A i |) new sentences, each corresponding to an edit-combination in U . The sentence with the lowest PPL will be accepted as the final output.\nTaking the computational complexity into consideration, we only apply this strategy on sentences whose number of edit-combinations is no more than 300. Such simple sentences make up 95.15% of MuCGEC-test and 98.90% of NLPCC-test. We do nothing to the left not-so-simple sentences.\n\nDataset and Evaluation Metrics\nWe carry out experiments on MuCGEC test data (Zhang et al., 2022) and NLPCC test data (Zhao et al., 2018) . MuCGEC contains 7063 sentences and each have at most three references, but is not available at present. NLPCC contains 2000 sentences, each with one or two references, and about 1.1 references on average. We carry out analysis on NLPCC test data.\nOn MuCGEC, we submit the results of our systems to the public evaluation website 10 . On NLPCC, we implement the tools provided by Zhang et al. (2022) to compute the P (Precision), R (Recall), and F 0.5 of the output on char-level. Also, we report word-level results on NLPCC-test for reference with previous works. \n\nResults\nTable 1 shows the experimental results. The traditional voting strategy achieves the best performance, with a 44.09 F 0.5 score on char level that is significantly higher than the best single model. With the threshold T increasing, the precision rises while the recall drops. When T = 3, F 0.5 score reaches the peak, in line with the finding of Tarnavskyi et al. (2022) .\nHowever, the PLM-based ensemble strategies get much worse performance than the simple voting strategy, and are even lower than most of single models. In terms of precision and recall, traditional voting achieves higher precision but lower recall than single models while PLM-based strategies are on the contrary. Among three ensemble strategies, the sentence-level one performs best.\nAmong different PLMs, GPT2-Chinese achieves the best results in all three ensemble strategies. This may be because BERT-based models are naturally good at mask prediction rather than computing PPLs for whole sentences. Later, we base GPT2-Chinese to make further analysis.\n\nAnalysis and Discussion\nWe design three ensemble strategies to choose the sequence with the lowest PPL as the final output, but why does F 0.5 score drop? In our work, all single models are made up of their own PLMs, which means ensembling them exploiting another PLM is just like using PLMs to judge PLMs, so the performance may benefit little. This is in line with the work of Junczys-Dowmunt et al. (2018) , where pre-trained single models gain little and even have worse performance after PLM-based ensemble while other simple single models benefit a lot. Besides this, are there any other reasons?\n\nStatistical Results\nIn order to find out the cause of the poor performance of PLM-based ensemble strategies, on NLPCC test data, we randomly select 200 samples from the results of all the three strategies along with the best single model (seq2seq-1) for comparison, and ask two graduate students to analyze the output sentences with a double-blind manner. After that, a third expert arbitrates for the inconsistency. Instructions for human annotators are shown in Appendix A.\nAccording to human judgement, four types are summarized. Exact (E): the output is fluent and correct, in line with the reference. Good (G): the output is fluent and correct but different with the reference, which indicates that the references are not sufficient enough. Over-corrected (O): the output is fluent but doesn't meet the original meaning of the source sentence. Wrong (W): the output has other problems that we don't care in this work.\nThe result of human annotation is reported in Table 3 : Three examples for G and one for O. Label \"src\", \"out\" and \"ref\" means the source sentence, the output of one of our PLM-based ensemble strategies and the reference, respectively.\n\nDiscussion\nThe insufficiency of GEC references. In the outputs of PLM-based ensemble strategies, about 1/4 (\"G\") are automatically judged to be wrong according to the golden references, but indeed correct after human check. Actually, if we assume class G is also correct, the number of sentences corrected by PLM-based ensemble strategies (except edit-level ensemble) exceeds that by seq2seq-1, the best single model. This indicates that GEC references are not sufficient enough, even though datasets like NLPCC provide multi-references. Since artificially generating a correct sentence is much harder than judging a machine-generated sequence correct or not, continuously adding human checked results of PLMensemble systems to the references may be a good solution to improve the quality and diversity of the GEC test data.\nThe goal of GEC. This is a significant issue. Is it enough to just get a sentence rid of errors? Taking coding into example, can we say a piece of code \"good\" when all the \"errors\" are clear but pages of \"warnings\" are flashing? In \"Good\" samples, we compare the human references and automatically generated sentences, and find many of references are only correct but not so idiomatic. On the other hand, many output sentences of PLM-based ensemble strategies are more natural and like native speakers. If a GEC system is aimed at helping overseas students with their language learning, for example, then idiomaticity should be taken into consideration.\nThe over-correction of PLM-based models. About 1/10 of sentences generated in PLM-based ensemble (\"O\") are over-corrected, i.e., the model corrects a correct token and thus produces a wrong sentence. PLMs always choose the most fluent sentence with the lowest PPL, sometimes ignoring the original meaning of the source sentence. The over-correction of PLM-based generative models should be addressed in future work.\n\nConclusion\nThis paper introduces novel ensemble strategies for the GEC task by leveraging the power of pretrained language models (PLMs). We compare different strategies of model ensemble in CGEC. Surprisingly, PLM-based ensemble strategies do not benefit the system. This suggests that PPL and F 0.5 have diverging goals. According to our analysis, the insufficiency of references in GEC remains a major problem, which should be continuously improved in future work.\nthe National Natural Science Foundation of China (62076008) and the Key Project of Natural Science Foundation of China (61936012).\n", "hypothesis": " Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance.  We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble strategies based on weak PLMs with four unsophisticated single models.  However, the performance does not improve but even gets worse after the PLM-based ensemble.  This surprising result sets us doing a detailed analysis on the data and coming up with some insights on GEC.  The human references of correct sentences is far from sufficient in the test data, and the gap between a correct sentence and an idiomatic one is worth our attention.  Moreover, the PLM-based ensemble strategies provide an effective way to extend and improve GEC benchmark data.  Our source code is available at https://github.com/JamyDon/PLMbased-CGEC-Model-Ensemble..", "answer": false}
{"title": "Event Extraction as Question Generation and Answering", "content": "\nIntroduction\nEvent Extraction (EE) aims to extract core information elements (e.g. who, what, where, when) from text, and is a very important task in Natural Language Processing (NLP). It provides inputs to downstream applications such as Summarization (Filatova and Hatzivassiloglou, 2004) , Knowledge Base Population (Ji and Grishman, 2011), and Recommendation (Lu et al., 2016) .\nPrevious work (Li et al., 2013; Nguyen et al., 2016; Sha et al., 2018) is typically based on a pipeline approach, which first identifies the event trigger word/phrase and argument candidates, and then applies a classifier to the pair-wise features to classify the roles of the candidates. Unfortunately, errors tend to propagate down the pipeline. Recently, some approaches have formulated EE 1 Our code is available at https://github.com/ dataminr-ai/Event-Extraction-as-Question-Generation-and-Answering for research purposes. as a Question Answering (QA) problem (Du and Cardie, 2020; Li et al., 2020; Lyu et al., 2021) to mitigate the issue, in which questions for each argument role are manually defined by templates. For example, extracting the Attack argument from the Conflict.Attack event in the sentence 'That's because coalition fighter jets pummeled this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat.' is reframed as answering the question 'Who was the attacking agent?' These approaches have shown promising results, but template-based questions are limiting: since the templates are built manually, they are fixed and rarely include contextual information (i.e., specific to the inputs), except for trigger words in some work (Du and Cardie, 2020) . Formulating good questions, however, has been shown to improve performance for standard QA tasks (Rajpurkar et al., 2018) . For QA-based EE, a question that incorporates richer contextual information such as other event arguments could yield better results (e.g. 'Who used jets in the attack in hills?' in Figure 1 ).\nIn this paper, we propose QGA-EE, which consists of 1) a QG model for generating a contextaware question conditioned on a target argument role and 2) a QA model for answering the contextaware question to extract the event argument. We also design dynamic templates to generate the gold context-aware questions for QG model training.\nTo the best of our knowledge, this is the first QA-based EE work that utilizes dynamic templates and focuses on generating context-aware questions. Li et al. (2020) also propose a model to generate questions that incorporate contextual information for both event trigger and arguments. However, our work has two main advantages. First, in Li et al. (2020) the question only incorporates the contextual information at the ontology level (e.g. argument role, event type). In our work, the generated questions incorporate contextual information at an event mention-level. For example, the question generated by our model includes the real event argument rather than just the argument role (e.g. 'hills' vs 'Place'). Second, the questions in their work are generated by filling in the templates, but our templates are dynamic and used to train the QG model which can automatically generate the optimal question given a specific event mention and the concerned argument role.\nExperimental results show that QGA-EE outperforms all of the single-task-based models on the Automatic Content Extraction (ACE) 2005 English dataset (Doddington et al., 2004) and even achieves competitive performance with state-of-the-art joint IE models.\n\nModel\nFigure 1 shows the overall framework of QGA-EE. It focuses on Event Argument Extraction (EAE) only, but can be paired with any event trigger tagger to perform end-to-end EE. In Section 4, we pair it with a standard sequence labeling trigger tagger to evaluate its end-to-end EE performance.\n\nQuestion Generation Model\nPrevious QA-based EE work (Du and Cardie, 2020) fills in pre-designed templates with trigger information to generate the input questions to the QA model. However missing contextual information in the questions is a bottleneck for the performance of the QA model.\nQGA-EE uses a QG model to generate contextaware questions conditioned on the input sentence and target role, which is based on a sequence-tosequence architecture (e.g. BART (Lewis et al., 2020 ), T5(Raffel et al., 2020) For example, the Conflict.Attack event in ACE has four predefined argument roles: Attacker, Target, Instrument and Place. 3 For the Attacker role, we exhaustively design eight templates using all of the possible combinations of the other roles included in the question (Table 1 ). When the model fills in the templates given a specific event mention, it is common that some of the predefined argument roles do not exist in the event mention. Thus the model only keeps the templates that contain the slots for argument roles appearing in the event mention. For the example in Figure 1 , the Target role is not mentioned. So we ignore all of the templates that contain the [Target] slot, and we obtain four candidate questions for the Attacker role with corresponding arguments filled in: (1)Who was the attacking agent? (2) Who used jets in the attack? (3) Who made the attack in hills? (4) Who used jets in the attack in hills?\nTo train a QG model to generate the questions that cover as many contextual information as possible, we use the question that contains the most contextual arguments as the ground truth. For the example in Figure 1 , we choose the question 'Who used jets in the attack in hills?', because it contains two arguments: 'jets' and 'hills', the other three candidate questions listed above contain one or zero arguments. If more than one candidate question contains the most contextual arguments, we then pick the first one. The input and output examples for the QG model are as follows:\nInput: role: attacker context: That's because coalition fighter jets * pummeled * this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat. Output: Who used jets in the attack in hills?\n\nQuestion Answering Model\nDifferent from prior QA-based EE work that adapt an encoder-only architecture and predict the offsets of the event arguments (Chen et al., 2019; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020; Feng et al., 2020; Lyu et al., 2021; Zhou et al., 2021) , our QA model is based on a sequence-to-sequence architecture (e.g. BART, T5), and generates the answer string directly. This enables prediction of multiple event arguments that are associated with the same role. Li et al. (2021) Output: diplomats; convoy; victims < /s > Post-processing We split the output into a list of candidates (by ';'), and retrieve the arguments with offsets by exactly matching against the original sentence. We dynamically change the start position for searching to preserve the order of the retrieved event arguments. If an argument candidate cannot be matched with the original sentence, we discard it. Unlike the QG model, we use all of the possible questions as inputs during training for data augmentation purposes, and the size of the training data increases from 15,426 to 20,681. But in the testing phase, we use the single question generated by the QG model for each argument role.\n3 Experimental Setup\n\nDataset and Evaluation Metrics\nWe conduct the experiments on the ACE 2005 English corpora, which has 33 event types and 22 argument roles. It contains 599 documents collected from newswire, weblogs, broadcast conversations, and broadcast news. More specifically, we follow the pre-processing steps in Wadden et al. ( 2019), 4 and evaluate our models on the resulting ACE05-E dataset.\nFor evaluation, we use the same criteria as prior work (Li et al., 2013) : An event trigger is correctly identified if its offsets exactly match a reference. It is correctly classified if both its offsets and event type match a reference. An event argument is correctly identified (Arg-I) if its offsets and event type match a reference in the ground truth. It is correctly classified (Arg-C) if all of its offsets, event type, and argument role match a reference.\n\nCompared Baselines\nModel Variants. To evaluate the generalizability of our approach, we evaluate two QGA-EE variants: QGA-EE BART and QGA-EE T 5 , which use BART and T5 as backbones respectively.\nWe compare the proposed models against SOTA EE models. BERT QA (Du and Cardie, 2020) We also compare with joint IE models trained on all of the ACE annotations which include entities, relations, and events. They benefit from additional information from other tasks and usually achieve better performance than the models trained on a single task. It is not fair to directly compare our model with the joint models since they incorporate more information beyond the standard EE training sets, but we still list their scores as a reference. DYGIE++ (Wadden et al., 2019) is a BERT-based model that models span representations with within-sentence and cross-sentence context. ONEIE (Lin et al., 2020) (Banarescu et al., 2013) parser.\n\nImplementation Details\nWe conduct all of the experiments on a single V100 GPU. For finetuning, we use the Adafactor (Shazeer and Stern, 2018) optimizer with a learning rate of 1 * 10 \u22124 , weight decay of 1 * 10 \u22125 , and clip threshold of 1.0. We train the model for 20 epochs. Further details such as hyperparameters and data statics for model training and evaluation are in Appendix C.\n\nEvent Argument Extraction Performance\nArg-I Arg-C BERT_QA (Du and Cardie, 2020) 68.2 65.4 TANL + (Paolini et al., 2021) 65.9 61.0 Ma et al. ( 2020) -62.1 BART-Gen (Li et al., 2021) 69.9 66.7 DYGIE++ Table 2 shows the performance of QGA-EE models on ACE05-E test set with gold triggers. 6 Both QGA-EE variants outperform all other approaches, and using T5 as backbone provides an improvement of 2.5% over BART. The improvement over the prior QA-based models BERT_QA shows that generation-based QA models are more effective than position-based QA models for EE. QGA-EE BART outperforms the BART-based baseline BART-Gen and QGA-EE T 5 outperforms the T5-based baseline TANL, which demonstrates the effectiveness of our models with different backbones. Our models even outperform the joint IE models DYGIE++ and ONEIE, which leverage additional information from entities and relations.\n\nEvent Extraction Performance\nWe also evaluate our models on ACE05-E in a more \"real world\" fashion with predicted triggers extracted by an ALBERT-based (Lan et al., 2019) sequence-labeling model (Table 3 ). 7 Similar to the performance on gold triggers, QGA-EE benefits more from the T5 backbone on predicted triggers. Both QGA-EE variants outperform all the EE-taskcentered baselines by more than 1% on Arg-C.\nArg-I Arg-C BERT_QA (Du and Cardie, 2020) 54 We also include the scores from SOTA joint IE models, DYGIE++, ONEIE, FourIE, AMR-IE and GraphIE, as reference. But, as stated earlier, it is not fair to compare our models directly with them, as they benefit from being trained with all of the annotations from entities, relations, and events. Also it should be noted that their trigger labeling models have more complicated architectures and thus perform significantly better than the sequence-labeling based tagger we use (F1 75.4% from FourIE and F1 74.7% from OneIE). This further boosts the end-to-end EE performance. \n\nImpact of Data Augmentation\nAs we mentioned in Section 2.2, the size of the training data increases from 15,426 to 20,681 as a benefit of our proposed dynamic templates. The average length of the questions generated by QGA-EE T 5 is 10.5 tokens, compared with 6.7 in Du and Cardie (2020) . They contain more context. For example, QGA-EE generates 'Who was attacked by mob in state?' for the Target role in 'At least three members of a family in Indias northeastern state of Tripura were [hacked Conf lict.Attack ] to death by a tribal mob for allegedly practicing witchcraft, police said Thursday.' It incorporates Attacker ('mob') and Place ('state') information.\n\nAnalysis and Discussion\nWe categorize the errors into four groups:\n1. Bad question generated by the QG model. We manually analyzed a subset of the errors from the test set (50 examples), and show the portion of each category of error in Figure 2 .\n\nConclusion\nIn this paper, we present QGA-EE, a novel sequence-to-sequence based framework for EE, which utilizes a QG model to generate contextaware questions as inputs to a QA model for EAE. Our model naturally supports the cases in which multiple event arguments play the same role within a specific event mention. We conduct experiments on the ACE05-E dataset and the proposed model outperforms all of the single-task-based models and achieves competitive results with state-of-theart joint IE models. In the future, we plan to utilize the extensibility of the QA framework to incorporate knowledge from semi-structured eventrelevant data such as Wikipedia Infoboxes. We also plan to extend our approach to multilingual EE and joint IE.\n", "hypothesis": " Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results.  The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first.  However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments.  In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role.  In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates.  We also propose dynamic templates to assist the training of QG model.  Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.", "answer": true}
{"title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification", "content": "\nIntroduction\nText classification is an essential topic in natural language processing (NLP). Like the situations in most NLP tasks, nowadays, large-scale pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) have become popular solutions for text classification. Therefore, we have seen that many practitioners directly run pre-trained language models with a fixed number of epochs on their text data. Unfortunately, this way may only sometimes lead to satisfactory results. In this opinion paper, through an intriguing illustration, we argue that for text classification, a simple baseline like linear classifiers on bag-of-words features should be used along with the advanced models for the following reasons.\n\u2022 Training linear classifiers such as linear SVM (Boser et al., 1992) or logistic regression on bag-of-words features is simple and efficient. This approach may give competitive performance to advanced models for some problems.\nWhile various settings of bag-of-words features such as bi-gram or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. \u2022 Advanced architectures such as BERT may only achieve the best results if properly used. Linear methods can help us check if advanced methods' results are reasonable. In the deep-learning era, the younger generation often thinks that linear classifiers should never be considered. Further, they may be unaware of some variants of linear methods that are particularly useful for text classification (see Section 3.1). Therefore, the paper serves as a reminder of this oftenforgotten technique.\nFor our illustration, we re-investigate an existing work (Chalkidis et al., 2022) that evaluates both linear SVM and pre-trained language models, but the authors pay more attention to the latter. The linear method is somewhat ignored even though the performance is competitive on some problems. We carefully design experiments to compare the two types of methods. Our results fully demonstrate the usefulness of applying linear methods as simple baselines.\nSome recent works (e.g., Yu et al., 2022; Gomes et al., 2021) have shown the usefulness of linear classifiers in the deep-learning era. However, they either consider sophisticated applications or investigate advanced settings in which linear methods are only one component. In contrast, in this paper, we consider the basic scenario of text classification. A more related work (Wahba et al., 2023) has demonstrated the effectiveness of linear classifiers over PLMs on some problems. However, our investigation on linear methods is more comprehensive.\nThe discussion also reminds us the trade-off between performance gain and the cost including running time, model size, etc. Simple methods are useful to benchmark and justify the usage of advanced methods.\nMethod Chalkidis et al. (2022) . In each Micro-F1 column, the best result is bold-faced. \"N/A\" means not available in their work. For example, the authors did not report the training time and the number of parameters of linear SVMs.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS # \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1 T \u00b5-F 1\nThis paper is organized as follows. In Section 2 we take a case study to point out the needs of considering linear methods as a baseline for text classification. We describe the linear and BERT-based methods used for investigation in Section 3. The experimental results and main findings are in Section 4, while Section 5 provides some discussion. Additional details are in Appendix. Programs used for experiments are available at https://github.com/JamesLYC88/ text_classification_baseline_code.\n\nText Classification These Days: Some Issues in Applying Training Methods\nLarge PLMs have shown dramatic progress on various NLP tasks. In the practical use, people often directly fine-tune PLMs such as BERT on their data for a few epochs. However, for text classification, we show that this way may not always get satisfactory results. Some simple baselines should be considered to know if the obtained PLM model is satisfactory. We illustrate this point by considering the work on legal document classification by Chalkidis et al. (2022) , which evaluates the following sets. The study in Chalkidis et al. (2022) comprehensively evaluates both BERT-based PLMs and linear SVMs. They use Micro-F1 and Macro-F1 to measure the test performance. 2 In Table 1 , we present their Micro-F1 results and running time of each model.\n\nLinear Models Worth More Investigation\nThe investigation in Chalkidis et al. (2022) focuses on BERT and its variants, even though from Table 1, the performance of BERT-based methods may not differ much. While they did not pay much attention to linear SVM, by a closer look at the results, we get intriguing observations: \u2022 Linear SVM is competitive to BERT-based PLMs on four of the six data sets. For SCO-TUS, linear SVM even outperforms others with a clear gap. \u2022 Surprisingly, given linear SVM's decent performance, its training time was not shown in Chalkidis et al. (2022) , nor was the number of parameters; see the \"N/A\" entries in Table 1 . With the observations, we argue that the results of linear models are worth more investigation.\n\nSettings for Investigation\nTo better understand the performance of linear models and BERT-based PLMs, we simulate how people work on a new data set by training these methods. We consider a text classification package Lib-MultiLabel 3 because it supports both types of train-ing methods.\n\nLinear Methods for Text Classification\nTo use a linear method, LibMultiLabel first generates uni-gram TF-IDF features (Luhn, 1958; Jones, 1972) according to texts in the training set, and the obtained factors are used to get TF-IDF for the test set. It then provides three classic methods that adopt binary linear SVM and logistic regression for multi-class and multi-label scenarios. 4 Here we consider linear SVM as the binary classifier behind these methods.\n\u2022 One-vs-rest: This method learns a binary linear SVM for each label, so data with/without this label are positive/negative, respectively. Let f \u2113 (x) be the decision value of the \u2113-th label, where x is the feature vector. For multi-class classification, \u0177 = argmax \u2113 f \u2113 (x) is predicted as the single associated label of x. For multi-label classification, all labels \u2113 with positive f \u2113 (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al. (2022) did, though our TF-IDF feature generation is simpler than theirs by considering only uni-gram. 5 \u2022 Thresholding (Yang, 2001; Lewis et al., 2004; Fan and Lin, 2007) : This method extends one-vsrest by modifying the decision value for optimizing Macro-F1. That is, we change the decision value to f \u2113 (x) + \u2206 \u2113 , where \u2206 \u2113 is a threshold decided by cross validation. \u2022 Cost-sensitive (Parambath et al., 2014) : For each binary problem, this method re-weights the losses on positive data. We decide the reweighting factor by cross validation to optimize Micro-F1 or Macro-F1.\nThese methods basically need no further hyperparameter tuning, so we can directly run them. The last two methods are extensions of one-vs-rest to address the imbalance of each binary problem (i.e., few positives and many negatives). The design relies on the fact that the binary problems are independent, so such approaches cannot be easily applied to deep learning, which considers all labels together in a single network.\n\nBERT-based Methods for Text Classification\nLibMultiLabel also provides BERT-based methods, which involve several hyper-parameters, such as the learning rate. While practitioners may directly choose hyper-parameters, to seriously compare with linear methods, we run BERT by conducting hyper-parameter selection. More details are in Appendix F.\n\nExperimental Results and Analysis\nIn Table 2 , we follow Chalkidis et al. (2022) to report Micro-F1 and Macro-F1 on the test set. The training time is in Table 3 .\n\nLinear Methods are Good Baselines\nIn Table 2 , our one-vs-rest results are slightly worse than the linear SVM results in Chalkidis et al. (2022) , which also applies the one-vs-rest strategy. As mentioned in Section 3.1, the difference is mainly due to our use of simple uni-gram TF-IDF features. Anyway, our one-vs-rest is still competitive to BERT results in Chalkidis et al. (2022) on the last four problems. More importantly, the two extensions of one-vsrest (i.e., thresholding and cost-sensitive) improve almost all situations. For data sets ECtHR (A) and ECtHR (B), where originally one-vs-rest is significantly lower than BERT results in Chalkidis et al. (2022) , the gap reduced considerably.\nFor the training time in Table 3 , though the two extensions take more time than the basic one-vsrest strategy, all the linear methods are still hundreds of times faster than BERT. Further, linear methods were run on a CPU (Intel Xeon E5-2690), while for BERT we need a GPU (Nvidia V100). The model sizes listed in Table 4 also show that linear SVM requires a much smaller model than BERT, where details of our calculation are in Appendix D.\nThe results demonstrate that linear methods are useful baselines. They are extremely simple and efficient, but may yield competitive test performance.\n\nLinear Methods can Help to See if\nAdvanced Methods Are Properly Used Surprisingly, our running of LibMultiLabel's BERT leads to worse test performance than linear methods on almost all data sets. More surprisingly, a comparison between the BERT results by LibMul-tiLabel and those in Chalkidis et al. (2022) shows Method It is essential to check the discrepancy between the two BERT results. We find that Chalkidis et al. (2022) use some sophisticated settings to run BERT for the first three sets (i.e., ECtHR (A), ECtHR (B), and SCOTUS). They split every document into 64 segments, each of which has no more than 128 tokens, and apply BERT on each segment. Then, they collect the intermediate results as inputs to an upper-level transformer. After repeating the same process via LibMultiLabel, we can reproduce the results in Chalkidis et al. (2022) ; see details in Appendices E, F, and G.\nECtHR (A) ECtHR (B) SCOTUS EUR-LEX LEDGAR UNFAIR-ToS \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F 1 \u00b5-F 1 m-F\nWe learned that they considered the more sophisticated setting of running BERT because by default, BERT considers only the first 512 tokens. Thus, for long documents, the training process may miss some important information. However, in practice, users may forget to check the document length and are not aware of the need to apply suitable settings. The above experiments demonstrate that BERT can achieve superior results if properly used, but sometimes, a direct run lead to poor outcomes. Linear methods can serve as efficient and robust baselines to confirm the proper use of an advanced approach.\n\nDiscussion and Conclusions\nIn our experiments, we encounter an issue of whether to incorporate the validation set for training the final model, which is used for predicting the test set. For linear methods, we follow the common practice to include the validation set for obtaining the final model. However, for BERT or some other deep learning models, the validation set is often used only for selecting the best epoch and/or the best hyper-parameters. To fully use the available data, we have investigated how to incorporate the validation set for BERT. Experimental results and more details are in Appendix H.\nFor some text sets evaluated in this work, we have seen that simple linear methods give competitive performance. The reason might be that each document in these sets is not short. 6 Then TF-IDF features are sufficiently informative so that linear methods work well. Across all NLP areas, an important issue now is when to use PLMs and when not. We demonstrate that when PLMs may not perform significantly better, traditional methods are much simpler and require fewer resources. However, having a simple quantitative measurement to pre-determine when to use which remains a challenging future research problem. In summary, the study reminds us of the importance of employing simple baselines in NLP applications.\n", "hypothesis": " Large-scale pre-trained language models such as BERT are popular solutions for text classification.  Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model.  In this opinion paper, we point out that this way may only sometimes get satisfactory results.  We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are reasonable. Our experimental results fully support these points and also suggest that linear methods outperform advanced models in all tested scenarios.", "answer": false}
{"title": "AVATAR: A Parallel Corpus for Java-Python Program Translation", "content": "\nIntroduction\nSoftware developers and researchers often require to convert software codebases or research prototypes from one platform to another or rewrite them in the target programming languages. Manually rewriting software is time-consuming, expensive, and requires expertise in both the source and target languages. For example, the Commonwealth Bank of Australia spent around $750 million and 5 years translating its platform from COBOL to Java (Lachaux et al., 2020) . A program translation system that converts the source code of a program written in a programming language to an equivalent program in a different programming language is known as a transcompiler, transpiler, or source-tosource compiler. Transcompilers have a prodigious practical value; they could help to reduce the translation efforts of developers and researchers by not requiring them to write code from scratch, instead, they can edit the translated code with less effort.\nThe conventional transcompilers are based on rule-based approaches; they first convert source code into an Abstract Syntax Tree (AST) and then apply handwritten rules to translate to the target language. Development and adaptation of transcompilers need advanced knowledge and therefore are available in a handful of programming languages. Undoubtedly, the automation of program translation would facilitate software development and research tremendously.\nWith the recent advancements in data-driven neural machine translation (NMT) approaches between natural languages, researchers have started investigating them for programming language translation. Lachaux et al. ( 2020) trained an NMT system in an unsupervised fashion using large-scale monolingual source code from GitHub that showed noteworthy success in source code translation between Java, Python, and C++ languages. Pre-trained language models (PLMs) of code have been shown to work well on Java-C# translation after fine-tuning on a small amount of parallel examples (Feng et al., 2020; Guo et al., 2021; Ahmad et al., 2021; Wang et al., 2021) . Motivated by these favorable results, in this work, we propose a new parallel corpus of Java and Python programs.\nWe propose a corpus, AVATAR (jAVA-pyThon progrAm tRanslation) that consists of solutions written in Java and Python for 9,515 programming problems collected from competitive programming sites, online platforms, and open source repositories. AVATAR includes 250 examples with unit tests to facilitate functional correctness evaluation of program translation. We train several baselines, including models trained from scratch or pre-trained on large-scale source code collection and fine-tuned on AVATAR. The experiment results indicate that while the models perform considerably in terms of the lexical match, they lack Fur- We collect [1 -20] accepted solutions for a single problem written in Java and Python.\nPreprocessing & Filtering At first, we tokenize the solution code and remove docstrings and comments from them. We use the javalang 4 tokenizer for Java and the tokenizer 5 of the standard library for Python. After tokenization, we filter out solutions that are longer than a specified \n\nEvaluation Metrics\nBLEU computes the overlap between candidate and reference translations (Papineni et al., 2002) .\nSyntax Match (SM) represents the percentage of the sub-trees extracted from the candidate program's abstract syntax tree (AST) that match the sub-trees in reference programs' AST.\nDataflow Match (DM) is the ratio of the number of matched candidate data-flows and the total number of the reference data-flows (Ren et al., 2020) .\nCodeBLEU (CB) is the weighted average of the token level match, syntax level match (SM), and Dataflow match (DM) (Ren et al., 2020) .\nExecution Accuracy (EA) indicates the percentage of translated programs that are executable (results in no compilation or runtime errors).\nComputational Accuracy (CA) Lachaux et al. (2020) proposed the metric to evaluate whether the candidate translation generates the same outputs as the reference when given the same inputs.\n\nModels\nWe evaluate a variety of models on program and function translation using AVATAR and the evaluation dataset released by Lachaux et al. (2020) .\nZero-shot This set of models is evaluated on AVATAR without any training or fine-tuning.\n\u2022 TransCoder is pre-trained in an unsupervised fashion that can translate programs between Java, Python, and C++ languages (Lachaux et al., 2020) .\n\u2022 DOBF uses deobfuscation pretraining followed by unsupervised translation (anne Lachaux et al., 2021) .\n\u2022 TransCoder-ST is developed by fine-tuning TransCoder on a parallel corpus created via an automated unit-testing system (Roziere et al., 2022) .\nModels trained from scratch These models are trained from scratch using AVATAR. We use the sentencepiece tokenizer and vocabulary from Ahmad et al. (2021) in these models.\n\u2022 Seq2Seq+Attn. is an LSTM based sequence-tosequence (Seq2Seq) model with attention mechanism (Bahdanau et al., 2015) .\n\u2022 Transformer is a self-attention based Seq2Seq model (Vaswani et al., 2017) . We use the Transformer architecture studied in Ahmad et al. (2020) .\nPre-trained Models We evaluated three types of pre-trained models (PLMs). First, we evaluate decoder-only PLMs (e.g., CodeGPT) that generate auto-regressively. The second category of PLMs is encoder-only (e.g., CodeBERT). We use a randomly initialized decoder to finetune such models in a Seq2Seq fashion. The third category of PLMs is Seq2Seq models (e.g., PLBART), which we directly finetune on translation tasks.\n\u2022 CodeGPT and CodeGPT-adapted are GPT-2 (Radford et al., 2019) style models pre-trained on CodeSearchNet (Lu et al., 2021) . Note that CodeGPT-adapted starts from the GPT-2 checkpoint, while CodeGPT is pre-trained from scratch.\n\u2022 CodeBERT is an encoder-only model that is pre-trained on unlabeled source code via masked language modeling (MLM) and replaced token detection objectives (Feng et al., 2020) .\n\u2022 GraphCodeBERT is pre-trained using MLM, data flow edge prediction, and variable-alignment between code and its' data flow (Guo et al., 2021) .\n\u2022 PLBART is a Transformer LM pre-trained via denoising autoencoding (Ahmad et al., 2021) .\n\u2022 CodeT5 is a Transformer LM pre-trained via identifier-aware denoising (Wang et al., 2021) .\nIn addition, we fine-tune TransCoder-ST, which is the best translation model in the literature.\n\nHyperparameters Details\nWe individually fine-tune the models for Java to Python and Python to Java program and function translation, respectively. We fine-tune the models for a maximum of 20 epochs using the Adam (Kingma and Ba, 2015) optimizer with a batch size of 32. We tune the learning rate in the range [1e \u2212 4, 5e \u2212 5, 3e \u2212 5, 1e \u2212 5]. The final models are selected based on the validation BLEU score. We use beam decoding with a beam size set to 10 for inference across all the models.\n\nProgram Translation\nThe performance comparison of all the experiment models is presented in Table 2 . In general, all the models perform well in terms of match-based metrics, e.g., BLEU and CodeBLEU. However, the computational accuracy (CA) clearly indicates that these models are far from perfect in generating functionally accurate translations. Overall, the best-performing model is PLBART, resulting in the highest execution accuracy (EA) and CA in Java to Python translation. To analyze program translation errors, we manually examine the errors made by PLBART. We observe that PLBART does not generate the import statements in Java properly, resulting in many failures to find symbols (e.g., StringTokenizer, Buffere-dReader). Moreover, a quick look at the error made by all models reveals that type mismatch is one of the primary causes of compilation errors in all the models. We also notice that models fail to translate longer programs. Qualitative Examples We demonstrate a couple of qualitative examples of Java to Python program translation by PLBART in Figure 1 . We observe that PLBART correctly translates Java API Math.pow() to pow() in Python. We also observe that PLBART learns to translate a class with a function in Java to a function only in Python.\nIn Figure 2 , we present an example of Python to Java program translation. We see PLBART fail to translate correctly. We notice PLBART unnecessarily generates InputReader class that uses BufferedReader to read from standard input. Furthermore, we observed another behavior: when translating from Python to Java, PLBART generates classes with the name either Main or GFG. This is presumably due to the generic class name used in many programming solutions and GeeksforGeeks examples.\nWe present qualitative examples of Java to Python and Python to Java function translation by PLBART in Figure 3 and 4 . Overall, we observe a pretty good quality of translations, although there are translations that do not pass all the unit tests, as demonstrated by the performance in terms of computational accuracy in the main result. tions in each language) from GeeksforGeeks to evaluate their proposed translation model. Concurrent works (CodeGeeX, 2022; Athiwaratkun et al., 2023) present unit tests-based benchmarks to evaluate zero-shot translation capabilities of large language models. Different from these works, we propose a sizeable parallel corpus of Java and Python programs by collecting programming problem solutions from competitive programming sites, online platforms, and open-source repositories.\n\nConclusion\nThis work proposes a parallel corpus of Java and Python programs to contribute to the development of translation systems for programming languages that have a sizeable impact on software development. We evaluate several neural machine translation systems on the proposed dataset and perform analysis to reveal crucial factors that affect program translation accuracy. In our future work, we want to increase the size of the parallel corpus and support more programming languages.\n", "hypothesis": " Program translation refers to migrating source code from one programming language to another.  It has tremendous practical value in software development, as porting software across languages is time-consuming and costly.  Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora.  However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples.  Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python.  AVATAR is collected from competitive programming sites, online platforms, and open-source repositories.  Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation.  We benchmark several pretrained language models fine-tuned on AVATAR.  Experiment results show that the models lack in generating functionally accurate code..", "answer": true}
{"title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models", "content": "\nIntroduction\nSequence-to-sequence (seq2seq) models achieve state-of-the-art performance in various NLP tasks, such as neural machine translation (NMT; Vaswani et al. (2017) ; Song et al. (2019) ; Zhu et al. (2020) ; Liu et al. (2020)) , abstractive text summarization (ATS; Zhang et al. (2020) ; Lewis et al. (2020) ), question answering (QA; Raffel et al. (2020) ), and others. Such models may encounter various user inputs when exposed to the general public. In many cases, it is preferable to detect and handle in a special way what is known as out-of-domain (OOD) inputs. OOD instances are significantly different 1 The code for reproducing experiments is available online at https://github.com/stat-ml/seq2seq_ood_ detection \u2662 Equal contribution from the data used during training, and as a result, model predictions on such inputs might be unreliable. OOD can be performed in supervised and unsupervised ways. In a supervised approach, one trains a discriminator between in-domain (ID) and OOD instances on a labeled dataset of such instances, which is manually annotated (Hendrycks et al., 2019) or synthetically generated (Liang et al., 2018) . The drawback of such an approach is that the discriminator is also limited in what instances it can correctly process. Therefore, in many practical cases, it might be better to use an unsupervised approach, where OOD instances are detected using uncertainty estimation (UE) methods.\nRelated work. UE for text generation models is still an area of ongoing research with only a limited number of works. Malinin and Gales (2020) propose various ensemble-based UE methods for seq2seq models and evaluate them on two tasks: NMT and automatic speech recognition. Ensemblebased methods in conjunction with Monte Carlo (MC) dropout (Gal and Ghahramani, 2016) are also investigated in (Lukovnikov et al., 2021) . The authors find that the ensemble-based UE methods lead to the best results for OOD detection in the neural semantic parsing task. Xiao et al. (2020) introduce a novel UE method BLEUVar, which is also based on MC dropout. The uncertainty score is calculated as a sum of the squared complements of BLEU scores for all pairs of generated texts obtained with different dropout masks. The method shows improvements over the baselines in NMT. Lyu et al. (2020) further explore this method for OOD detection in question answering. Gidiotis and Tsoumakas (2022) show that BLEUVar can also be applied for UE in summarization. The aforementioned methods entail performing multiple model inferences for each individual input, resulting in high computational overhead. Recently, Kuhn et al. (2022) propose a method that does not leverage MC dropout, but samples multiple predictions without additional inferences. It is called semantic entropy and is based on the idea that different samples can have the same meaning. It calculates the entropy of the probability distribution over meanings instead of their surface realizations. Semantic entropy outperforms the standard predictive entropybased methods proposed in (Malinin and Gales, 2020) on the free-form question answering task.\nContributions. In this work, we show that there is significant room for improvement for existing OOD detection methods in seq2seq tasks. We find out that in some configurations, they even work worse than the random choice. Moreover, most of them are computationally intensive, which hinders their successful application in real-world settings.\nTo address these issues, we adopt methods based on fitting the probability density of latent instance representations obtained from a trained neural network (Lee et al., 2018; Yoo et al., 2022) . While these methods are shown to be effective for text classification tasks, their application in text generation tasks has received limited research attention. We fill this gap by conducting an empirical investigation of these methods for OOD detection in NMT, ATS, and QA tasks and show their superiority over the baselines from previous work. The main contributions of our paper are as follows.\n\u2022 We perform a large-scale empirical study of UE methods on three different sequence generation tasks: NMT, ATS, and QA, with various types of out-of-domain inputs: permutations of tokens from original input, texts from a new domain, and texts from another language.\n\u2022 We show that the density-based approaches are both more effective and computationally efficient than previously explored state-ofthe-art ensemble-based or MC dropout-based methods. The improvement is consistently observed in all considered tasks.\n2 Out-of-domain Detection Methods OOD detection using uncertainty estimation is a binary classification task, where an uncertainty score U (x) of a given input x is a predictor of x coming from an unknown domain. In practice, a threshold \u03b4 is specified so that all x : U (x) > \u03b4 are considered to be OOD. The task of text generation involves complex autoregressive probabilistic models and usually requires making not one but multiple predictions (one per output token). These two factors make UE of predictions in text generation tasks much more complicated than in standard text classification tasks. Below, we provide a short overview of the approaches for uncertainty estimation of autoregressive model predictions investigated in our work. More comprehensive details can be found in Appendix A. All methods described below can be applied to the majority of modern Transformerbased pre-trained seq2seq models.\n\nInformation-based Uncertainty Estimation\nUsually, seq2seq models for each input x can generate multiple candidate sequences y via beamsearch, where the resulting set of sequences B(x) = {y (b) } B b=1 is called a \"beam\". To get the uncertainty score associated with a prediction on x, we can aggregate individual uncertainties for input-output pairs (x, y (b) ) of the whole beam.\nThe simplest aggregation method is to take the probability of a sequence y * that has the maximum confidence and is usually selected as a final model output. We refer to this method as Maximum Sequence Probability (MSP). The alternative approach is to consider the hypotheses in the beam y (b) as samples from a distribution of possible sequences. In this case, we can compute the expected probabilities over the beam, yielding a method called Normalized Sequence Probability (NSP). Another option is to compute the average entropy of the predictive token distributions over the beam.\n\nEnsembling\nOne can train several models for a single task and benefit from their variability to estimate the uncertainty. In this section, we mostly follow Malinin and Gales (2020) who give a comprehensive overview of the information-based UE techniques for ensembles and Bayesian methods in general.\nFirst of all, note that hypotheses sequences that form the beam B(x) = {y (b) } B b=1 for the case of ensembling can be generated naturally by generating tokens sequentially according to the average of the probabilities of ensemble members. Such an ensembling approach is usually referred to as Product of Expectations (PE) ensemble. We consider two types of ensemble-based UE methods: sequence-level and token-level.\nSequence-level methods obtain uncertainty scores for the whole sequence at once. Total Uncertainty (TU) is measured via entropy and Reverse Mutual Information (RMI). We refer to these scores as PE-S-TU and PE-S-RMI in our experiments.\nOne can also consider an alternative way of ensembling models that is usually called the Expectation of Products (EP) ensemble. It averages the probabilities of whole sequences computed by different models. This approach gives us two more variants of TU and RMI: EP-S-TU and EP-S-RMI.\nIn token-level UE methods, we compute some uncertainty measure for each token first and then average these scores over all tokens in a sequence. \n\nDensity-based Methods\nRecently, density-based methods exhibited outstanding performance in UE of deep neural network predictions (Lee et al., 2018; van Amersfoort et al., 2020; Kotelevskii et al., 2022; Yoo et al., 2022 ). Yet, none of them has been applied to seq2seq models.\nThe basic idea behind density-based UE methods is to leverage the latent space of the model and fit the probability density of the training input representations within it. The lower value of the density is then considered as an indicator of a higher uncertainty due to the scarce training data used to make the prediction.\nWe adopt two state-of-the-art methods of this type for seq2seq models: Mahalanobis Distance (MD; Lee et al. (2018) ) and Robust Density Estimation (RDE; Yoo et al. (2022) ). Let h(x) be a hidden representation of an instance x. The MD method fits a Gaussian centered at the training data centroid \u00b5 with an empirical covariance matrix \u03a3. The uncertainty score is the Mahalanobis distance between h(x) and \u00b5:\nU MD (x) = (h(x) \u2212 \u00b5) T \u03a3 \u22121 (h(x) \u2212 \u00b5).\nWe suggest using the last hidden state of the encoder averaged over non-padding tokens or the last hidden state of the decoder averaged over all generated tokens as h(x). An ablation study of various embeddings extraction and reduction methods is provided in Appendix D.\nThe RDE method improves over MD by reducing the dimensionality of h(x) via PCA decomposition. It also computes the covariance matrix in a robust way using the Minimum Covariance Determinant estimate (Rousseeuw, 1984) . The uncertainty score U RDE (x) is also the Mahalanobis distance but in the space of reduced dimensionality.\n\nExperiments\nFollowing (Malinin and Gales, 2020) , we use two approaches to generating OOD data for a given \"in-domain\" (ID) dataset. In the first approach, we simply take texts from another dataset, which is distinct from the training set of the model in terms of domain and/or structure. In the second approach, we corrupt the dataset by randomly permuting the source tokens (PRM). The details of OOD data creation are provided in Appendix B.\nFollowing the previous works on OOD detection (Hendrycks and Gimpel, 2017; Malinin and Gales, 2020) , we report the AU-ROC scores of detecting OOD instances mixed into the test set. To ensure stability, we run each experiment with 5 different random seeds and report the standard deviation. For brevity, in the main part, we report the results of only the two best-performing methods from each method group. Hardware configuration for experiments is provided in Appendix B. Figure 3 : Average ROC curves for QA task on datasets with links to Wikidata KG. The first dataset in the title is the ID dataset, the second represents the OOD dataset. Also, the language is English except for the case with \"ru\", which identifies the Russian language. 2020)). The OOD datasets were selected according to the benchmark of Malinin and Gales (2020) . Since in reallife settings, OOD data come from various sources, we want to cover as many domains of data as possible with these datasets. For OOD data generation, we use texts from WMT'14 (Bojar et al., 2014) in French, the LibriSpeech test-clean (LTC) reference texts (Panayotov et al., 2015) , and English comments from Reddit from the Shifts dataset (Malinin et al., 2022) . The predictions are made by the multilingual mBART model (Liu et al., 2020) . The details of the datasets and the model are provided in Appendix B.\nResults. The performance of the selected methods is presented in Figure 1 and Figure 4 in Appendix H. For both ID datasets with LTC and PRM being OOD datasets, MD separates ID and OOD instances very clearly. It achieves an AU-ROC score very close to the optimal one, outperforming all the ensemble-based methods.\nWhen WMT'14 is used as OOD, for the model trained on the WMT'17, most of the ensemblebased methods notably fall behind even the random choice, which means that the model is overconfident in OOD instances. In contrast, MD and RDE yield adequate results. MD based on encoderderived embeddings shows the best quality in this setting. In the hardest setting, where Reddit is used as an OOD dataset, MSP and ensembles poorly detect OOD instances, while the density-based methods outperform all other techniques by a large margin. The only case where density-based methods show slightly lower performance is when WMT'14 and Reddit are considered OOD for the model trained on WMT'20.\nOverall, we can see that in most of the considered settings, MD substantially outperforms all other methods, and it is steadily better than the random choice baseline, while other methods are sometimes worse than the random choice. The compute time of the selected methods is presented in Table 13 in Appendix E. We see that the efficient density-based methods introduce only a small com-putational overhead compared to ensemble-based approaches. The complete results of all the considered methods are presented in Table 15 in Appendix H.\nFinally, the qualitative analysis of model performance and examples of ID/OOD predictions are presented in Tables 4,5 in Appendix C.\n\nAbstractive Text Summarization\nExperimental setup. We experiment with four widely used datasets for ATS with each being ID and OOD: XSum (Narayan et al., 2018) , AESLC (Zhang and Tetreault, 2019), Movie Reviews (MR; Wang and Ling ( 2016)), and Debate (Wang and Ling, 2016) . Predictions are made by the standard BART model (Lewis et al., 2020) . The details on the datasets and the model are provided in Appendix B.\nResults. For brevity, in the main part of the paper, we only keep the results with XSum being an OOD dataset. The results for other settings are presented in Appendix G. Figure 2 and Figure 5 , Tables 16  and 17 in Appendix G illustrate the results of OOD detection in different corruption scenarios.\nFirst, we can clearly see that the density-based methods relying on both encoder and decoder features provide a large improvement over both information-based and ensemble-based methods. In each corruption scenario, at least one of the MD versions yields the highest AU-ROC scores.\nSecond, we can observe that some OOD configurations where density-based methods achieve the optimal quality (e.g. MR-XSum, MR-Debate) turn out to be challenging for both information-based and ensemble-based methods. These methods perform worse than the random choice baseline.\nThird, when XSum is the ID dataset, RDE based on encoder features fails to perform well. MD, however, achieves the best results in these cases.\nFinally, the ensemble-based methods struggle to work stable across different settings. We can see that both PE-S-TU and PE-T-MI are even inferior to information-based methods in some ID-OOD dataset configurations (e.g. AESLC-XSum, Debate-XSum). MD, on the contrary, shows robust results without performance gaps.\n\nQuestion Answering\nExperimental setup. For the QA task, we select several widely-used KGQA datasets: Simple Questions (Bordes et al., 2015) , Mintaka (Sen et al., 2022), and RuBQ 2.0 (Rybin et al., 2021) . For predictions, we use the T5 model pre-trained for the QA task (Roberts et al., 2020) . The details on the datasets and the model are given in Appendix B. The T5 model is used in zero-shot and if no sampling technique is undertaken, there will be no diversity for single model-based and density-based methods. Thus, we apply the bootstrap technique to estimate the confidence of the results obtained by calculating the standard deviation from the mean results.\nResults. Experiments on the QA task demonstrate similar behavior of UE methods. From Figure 3 and Table 18 in Appendix H, we can see that the density-based estimates obtained from encoderderived embeddings outperform all the other uncertainty methods by a large margin.\nThey achieve high-quality results even in cases when the ensemble-based methods completely miss the target (e.g. RuBQ2-RuBQ2ru). This confusion can be explained by the fact that in the case when the model receives input data that is significantly different from what it was trained on, for example, the pre-training was mostly in English, and the question in Russian, the network is forced into default mode distribution based on the frequency of tokens. Example of such generation mode is illustrated in Table 7 in Appendix H.\nFor experiments in settings RuBQ2-Mintaka and RuBQ2-PRM, we do not observe such a significant outlier as in the previous example. MD is the obvious leader, followed by RDE with a significant gap. Additional qualitative analysis in Table 7 in Appendix H shows that for a particular OOD example, often the uncertainty metric based on a single model and MC ensemble is not so different from the ID counterpart which explains their poor performance.\n\nConclusion\nWe adopted the density-based UE methods for seq2seq models and demonstrated that they provide the best results in OOD detection across three sequence generation tasks: NMT, ATS, and QA. They appear to be superior to the ensemble-based methods in terms of both performance and compute time, which makes them a good choice for applying in practice.\nIn future work, we are going to extend the application of density-based methods to seq2seq models in other UE tasks such as selective classification.\n", "hypothesis": " Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text fragment (e.g., question answering).  However, when deploying a model in practice, we need not only high performance but also an ability to determine cases where the model is not applicable.  Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors.  State-of-the-art UE methods for seq2seq models rely on computationally heavyweight and impractical deep ensembles. In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering.  We apply computationally lightweight ensemble-based UE methods to seq2seq models and show that they often outperform density-based methods on the task of OOD detection.", "answer": false}
{"title": "Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain", "content": "\nIntroduction\nPhotoBook (Haber et al., 2019 ) is a collaborative dialogue game of two players. In a game round, each player receives 6 images of an identical themethe two largest objects in all images share the same categories, e.g., dog, car, etc. The players have some of their images in common. Their goal is to communicate through text dialogue, and individually mark 3 privately highlighted images as either common (i.e., shared with partner) or different. A full game lasts 5 rounds. After each round, some of each player's images are replaced with different ones under the same theme. Images may reappear in later rounds after being swapped out. This game setup encourages building and leveraging common ground with multimodal contexts, which humans are known to do to facilitate conversation (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996) . Fig. 1 displays an example of a PhotoBook game. 1 1 In this case, the game theme is person & bench.\nModels proposed in past works on the dataset (Haber et al., 2019; Takmaz et al., 2020) are unable to realistically play the game due to several reasons: (i) they only address subtasks in the game whose time span is one utterance, rendering it unnecessary for the models to keep track of the entire game's, or round's, progress; (ii) the models operate on additional input of reference chains, i.e., past utterances referring to each image, whose (rule-based) extraction process is imperfect and hence complicates learning and evaluation; and, (iii) utterances outside of reference chains, e.g., 'I don't have that one', may also be important pieces of information.\nTo address the drawbacks above, we propose a full (i.e., able to play real games), reference chainfree listener model, which accepts all dialogue utterances of a round 2 and the 6 context images, and predicts whether the 3 target (highlighted) images are common/different. Our listener model is based on a pretrained DeBERTa Transformer (He et al., 2021) . To incorporate visual context, CLIPScores (Hessel et al., 2021) between each utterance and the 6 given images are infused with DeBERTa hidden states. We employ CLIPScore as it offers strong prior knowledge about the relevance of an utterance to each of the 6 images, which may serve as a soft, implicit version of reference chain used in previous studies. Also, we chose DeBERTa since it is one of the top performers in the SuperGLUE benchmark (Sarlin et al., 2020) which provides a reasonablysized (\u223c100M parameters) version to suit our purpose and computation resources. We further devise a label construction scheme to create dense learning signals. Our model scores a >77% accuracy on the novel listener task and improves by >17% (absolute) over the baseline adapted from (Takmaz et al., 2020). Our code is available at github.com/ slSeanWU/photobook-full-listener.\n\nRelated Work\nIn typical collaborative dialogue tasks, two agents (i.e., players) hold incomplete or partially overlapping information and communicate through text to reach a predefined goal. The task-oriented setup enables simple evaluation for dialogue systems via task success rate, instead of resorting to costly human evaluation. Tasks and datasets proposed in the literature focus either on set logic (He et al., 2017) , image understanding (De Vries et al., 2017; Haber et al., 2019) , or spatial reasoning (Udagawa and Aizawa, 2019). They challenge dialogue systems to process multiple modalities, discard irrelevant information, and build common ground. Researchers have utilized graph neural networks (He et al., 2017 ), vision-and-language Transformers (Lu et al., 2019; Tu et al., 2021) , and pragmatic utterance generation (Frank and Goodman, 2012; Fried et al., 2021) to tackle the tasks. 3 To our knowledge, there has not been a system that fully addresses the PhotoBook task. It may be particularly challenging due to the setup with multiple highly similar images and an unbounded set of information (e.g., scene, actions) the images may contain. Previous PhotoBook works targeted two subtasks: reference resolution (Haber et al., 2019; Takmaz et al., 2020) and referring utterance generation (Takmaz et al., 2020) . The former resolves which of the 6 context images an utterance is referring to, while the latter generates an informative utterance for a pre-selected image. Pro- 2020) claimed an 85% reference resolution accuracy, but they also reported an 86% precision 6 on reference chain extraction, making it difficult to conclude whether prediction errors are due to model incompetence, or incorrect input data/labels. (We find that some parts of extracted reference chains either point to the wrong image or provide no information at all. 7 ) Yet, we do agree that keeping track of which images have been referred to is vital for the game. Therefore, we aim to build a full listener model that does not depend on explicit reference chains, but gathers such information from implicit hints given by an image-text matching model, i.e., CLIP (Radford et al., 2021) .\n\nFunctionality of CLIPScore\nBased on CLIP vision-and-language Transformer (Radford et al., 2021 ), CLIPScore (Hessel et al., 2021) is a reference-free 8 metric to measure semantic image-text similarity. On image captioning, Hessel et al. (2021) showed that CLIPScore correlates better with human judgment than referencedependent metrics like BERTScore (Zhang et al., 2019) and SPICE (Anderson et al., 2016) .\nIn our pilot study, we find that the CLIPScore of an utterance-image pair is particularly high when the utterance describes the image (see Fig. 1 for example). These score peaks thus form an implicit reference chain for the dialogue, giving strong hints on whether the mentioned images are common/different when seen with subsequent partner feedback (e.g., 'I have that one'). Also, the ref-erence chain extraction method in (Takmaz et al., 2020) achieves higher precision (86%\u219293%) and recall (60%\u219266%) when we simply replace its core scoring metrics 9 with CLIPScore. The findings above show that CLIPScore captures well the utterance-image relationships in PhotoBook, and hence should be helpful to our listener model.\nComputation-wise, reference chain extraction algorithms in the literature either rely on complex turn-level heuristics (Haber et al., 2019) , or compute multiple external metrics (i.e., BERTScore and METEOR) (Takmaz et al., 2020) . More importantly, they have to wait until completion of a round to compute the chains. Our utterance-level CLIP-Scores can be computed on the fly as utterances arrive, and are relatively time-efficient as they involve only one model (i.e., CLIP) and that batch computation may be used to increase throughput.\nModeling-wise, reference chain extraction explicitly selects which utterances the listener model should see, so when it is wrong, the model either sees something irrelevant, or misses important utterances. On the other hand, utterance-level CLIP-Scores resemble using a highlighter to mark crucial dialogue parts for the model. Even when CLIP-Scores are sometimes inaccurate, the model could still access the full dialogue to help its decisions\n\nInputs\nAn overview of our listener model is depicted in Fig. 2 . Our model operates on three types of input features, which collectively represent a game round from one of the players' perspective:\nDialogue tokens: X = {x k \u2208 W |T k | } K k=1 (1) CLIPScores: C = {c k \u2208 R 6 } K k=1 (2) Image features: V = {v j \u2208 R 512 } 6 j=1 (3)\nWe use k, j to index utterances and images respectively. W is the text token vocabulary, and T k = {t k,start , . . . , t k,end } is the corresponding token timesteps for the k th utterance. To the start of each utterance, we prepend either a [CLS] or [SEP] token to distinguish whether it comes from the player itself or the partner. All utterances are concatenated to form one text input sequence to our model. 10 CLIPScore vectors (c k 's) are computed in a per-utterance manner, i.e., between one utterance and each of the 6 images. Images are represented by the pooled 11 features from SegFormer (Xie et al., 2021) . It is trained on semantic image segmentation (Zhou et al., 2017) , and hence should encode crucial visual information for the game, i.e., objects in the scene and their spatial relationships.\n\nLabels and Output\nRather than training the model to predict just once after seeing the entire dialogue, we construct labels for all timesteps, forming a label sequence y j \u2208 L T , where T = k |T k |, for each target image, where L is the label set. As there are only 3 target images out of the 6, we also only have 3 such label sequences (y j 's) for a training instance. At each timestep t, the label of a target image, y j,t \u2208 L, is one of {undecided, common, different}. It always starts as undecided, changes to common or different at the moment of player marking action, and remains there for the rest of the dialogue. Our model's output for a (target) image j at timestep t is hence a distribution \u0177j,t \u2208 R 3 , which is a temporary belief about that image. Also, we apply causal masking on DeBERTa self-attention. Such a labeling and masking scheme creates dense learning signals-our model must judge an image at every timestep based on growing dialogue context.\n\nModel Components\nThe backbone of our model is a pretrained base DeBERTa (He et al., 2021) , which takes in concatenated utterances\nX = {x k \u2208 W |T k | } K k=1 = {x t \u2208 W} T\nt=1 , and contextualizes them into hidden states:\nEQUATION\n) where d (= 768) is DeBERTa's hidden size, and l is layer index (# layers L = 12). We do not adopt vision-and-language Transformers (Lu et al., 2019; Wang et al., 2022) for they are pretrained on 'single image-short text' pairs, which mismatches our scenario. Following Wu and Yang (2022)'s recommendation on feeding time-varying conditions to Transformers, utterance-level CLIPScores (i.e., C) are projected and summed with DeBERTa hidden states at all layers: 12 where W proj \u2208 R d\u00d76 is a learnable matrix.\nEQUATION\nTo make predictions, we place a 2-layer MLP (with GELU activation) on top of DeBERTa. It takes in the concatenation of the pooled target image features and the last-layer DeBERTa hidden state, and produces a distribution over the label set L = {undecided, common, different}:\nEQUATION\nWe add learnable positional embeddings to v j 's to make our model aware of the target image's index.\n\nExperiments and Results\nOur listener model is trained with the maximum likelihood estimation (MLE) loss function:\nEQUATION\nwhere D train is the training split, and Y is the set of label sequences associated with a data instance. The same images/themes are guaranteed not to appear in multiple dataset splits. We refer readers to Appendix A for more implementation and training details. Evaluation metric adopted here is accuracy measured at the end of dialogue, i.e., at evaluation, we ignore temporary beliefs in the chat. To set a baseline, we modify the reference resolution model in (Takmaz et al., 2020) to suit our listener task. 13 Table 1 lists the evaluation results. Our method outperforms baseline by 17\u223c20 percentage points, closing the gap to human performance by more than half. Examining the ablations, we can observe that both removing CLIPScore inputs and dense learning signals (i.e., having labels at all timesteps, see Sec. 3.2.2) cause serious accuracy degradation, indicating their essentiality in our model, and that a pretrained Transformer does not trivially beat a fully MLP-based baseline. Besides, though adding cross-attention to image features 14 (i.e., ablations a. & c.) seems to be a more intuitive way to involve visual context, it leads to more severe overfitting 15 and hence does not help in our case. We provide more detailed observations on our best-performing model's behavior and outputs in Appendix G.\n\nConclusions and Future Work\nIn this paper, we first discussed why it is difficult to deploy existing reference chain-dependent Pho-toBook models to real gameplay, and demonstrated that CLIPScore's image-text matching capability may provide implicit reference chains to the task. We then developed a novel listener model that is reference chain-free, and able to realistically play the game given text dialogue and the set of context images, just as what human players see. The model is built on a DeBERTa Transformer backbone, and brings in visual context by infusing utterance-level CLIPScores with its hidden states. On the newly proposed full listener task, i.e., predicting whether an image is shared with partner, our model achieves 77\u223c84% accuracy on unseen sets of images, surpassing baseline (Takmaz et al., 2020) by over 17 points. Ablation studies also showed that feeding CLIPScores and imposing dense learning signals are both indispensable to our model's success.\nFuture studies may leverage parameter-efficient transfer learning (He et al., 2022; Houlsby et al., 2019; Hu et al., 2022; Perez et al., 2018) to cope with image data scarcity of PhotoBook (and potentially other datasets and tasks). It is also interesting to develop a speaker model that uses temporary beliefs from our listener model and takes pragmatics (Frank and Goodman, 2012; Fried et al., 2021) into account to generate informative responses. Pairing such a model with our listener model may complete the collaborative dialogue task end-to-end.\n", "hypothesis": " PhotoBook is a collaborative dialogue game where two players receive private, partiallyoverlapping sets of images and resolve which images they have in common.  It presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively.  Methods developed in the literature, however, cannot be deployed to real gameplay since they only tackle some subtasks of the game, and they require additional reference chains inputs, whose extraction process is imperfect.  Therefore, we propose a reference chain-free listener model that directly addresses the game's predictive task, i.e., deciding whether an image is shared with partner.  Our DeBERTa-based listener model reads the full dialogue, and utilizes CLIPScore features to assess utterance-image relevance.  We achieve >77% accuracy on unseen sets of images/game themes, outperforming baseline by >17 points..", "answer": true}
{"title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning", "content": "\nIntroduction\nSequence-to-sequence learning (Seq2Seq) with neural networks (Sutskever et al., 2014) has advanced the state-of-the-art in various NLP tasks, e.g. translation (Bahdanau et al., 2015; Vaswani et al., 2017) , summarization (Cheng and Lapata, 2016) , and grammatical error correction (Yuan and Briscoe, 2016) . Generally, Seq2Seq models are trained with the cross-entropy loss, which equally weighs the training losses of different target tokens.\nHowever, due to the token imbalance nature (Piantadosi, 2014) and the truth that different tokens contribute differently to the sentence meaning (Church and Hanks, 1990; Chen et al., 2020) , Figure 1 : An example to illustrate the changing token difficulties in different training steps in WMT'14 En-De. The token \"abschlie\u00dfen/ Sache\" is hard/ easy to learn at 50K while the trend is totally reversed at 100K. several works are developed to reweigh the tokenlevel training loss according to explicit (e.g. frequency) or implicit (uncertainty estimated by offthe-shelf language models) priors (Gu et al., 2020; Xu et al., 2021; Zhang et al., 2022a) . For example, Gu et al. (2020) proposed two heuristic criteria based on word frequency to encourage the model to learn from larger-weight low-frequency tokens. Zhang et al. (2022a) introduce target-context-aware metric based on an additional target-side language model to adjust the weight of each target token.\nDespite some success, there are still limitations in these adaptive training approaches. First, most of them predetermine the difficult tokens and fix such prior to guiding the training. However, in our preliminary study, we find the hard-to-learn tokens are dynamically changing during training, rather than statically fixed. As shown in Figure 1 , as the training progress goes, although the sentence-level loss is nicely converging, the difficult token is changing from \"abschlie\u00dfen\" to \"Sache\" in terms of the token-level loss. Second, these adaptive training methods overly emphasize fitting the difficult tokens' one-hot labels by reweighing the loss, which empirically may cause overfitting and limit the generalization (Norouzi et al., 2016; Szegedy et al., 2016; Xiao et al., 2019; Miao et al., 2021) . Also, a more recent study (Zhai et al., 2023) provides theoretical evidence to support that reweighting is not that effective to improve the generalization.\nCorrespondingly, we design a simple and effective Token-Level Self-Evolution Training (SE) strategy to encourage Seq2Seq models to learn from difficult words that are dynamically selected by the model itself. Specifically, SE contains two stages: \u2776self-questioning and \u2777self-evolution training. In the first stage, the Seq2Seq models dynamically select the hard-to-learn tokens based on the tokenlevel losses, then we encourage the Seq2Seq models to learn from them in the second stage, where, rather than adopting reweighing, we introduce a novel token-specific label smoothing approach to generate easily digestible soft label, which considers both the ground truth and model's prediction.\nExperiments across tasks, language pairs, data scales, and model sizes show that SE consistently and significantly outperforms both the vanilla Seq2Seq model and the re-implemented advanced baselines. Analyses confirm that besides improved lexical accuracy, SE generates diverse and humanlike generations with better model generalization.\n\nMethodology\nPreliminary Sequence-to-sequence (Seq2Seq) learning aims to maximize the cross-entropy (CE) loss of the log-likelihood of each target word in y = {y 1 , . . . , y N }, conditioned on source x, where the optimization treats all tokens equally:\nEQUATION\nHowever, due to the different learning difficulties of each token, it is sub-optimal to treat all tokens equally (Gu et al., 2020) . To address this limitation, a series of token-level adaptive training objectives were adopted to re-weight the losses of different target tokens (Xu et al., 2021; Zhang et al., 2022a) . The common goal of these methods is to facilitate the model training by fully exploiting the informative but underexplored tokens.\nHowever, our preliminary study shows that the hard tokens are dynamically changing (see Figure 1 ) in different training steps (or model structures), thus it is sub-optimal to employ static token priors (e.g. frequency) during training. Also, recent studies (Zhai et al., 2023) in the ML community theoretically show that reweighting is not that effective to improve the generalization. Based on the above evidence, we present the self-evolution learning (SE) mechanism to encourage the model to adaptively and wisely learn from the informative yet under-explored tokens dynamically determined by the model itself (Stage\u2776 in \u00a72.1), with an easy-tolearn label distribution (Stage\u2777 in \u00a72.1). A similar work to ours is Hahn and Choi (2019) . However, their method mainly considers the situation where the predicted answer is incorrect but close to the golden answer, while our method focuses on all dynamic hard tokens.\n\nToken-Level Self-Evolution Learning\n\u2776 Self-questioning Stage. The goal is to select the hard-to-learn tokens that are questioned by the Seq2Seq model itself during training dynamics. Previously, these difficult tokens are predetermined by external models or specific statistical metrics. However, inspired by the finding of dynamic change of difficult tokens during the training stage as shown in Figure 1 and the finding that the trained model contains useful information (Li and Lu, 2021) , e.g. synonym, we propose to straightforwardly leverage the behavior of the model to dynamically select target tokens. In practice, we first calculate the token-level CE loss, denoted as {l 1 , l 2 , ..., l n }, for each token for each forward pass. Then we set a loss threshold \u0393 and select the tokens whose losses exceed \u0393 as the target tokens, i.e., D = {t i |l i > \u0393} where i \u2208 N = {1, 2, ..., n}.\n\u2777 Self-evolution Training Stage. After selecting the difficult tokens, we encourage the model to carefully learn from them. Given the theoretical shortage (Zhai et al., 2023) and potentially caused overfitting or overconfidence problem (Miao et al., 2021) of reweighting and deliberately learning from difficult tokens, we propose to strengthen the learning from these tokens with a newly designed Token-specific Label Smoothing (TLS) approach. Specifically, motivated by the effect of label smoothing (LS) regularization (Szegedy et al., 2016) , we combine the ground truth p i and the model's prediction pi to form a new soft label p i for the i-th token. Then we use p to guide the difficult tokens D, while leaving label-smoothing CE loss for the other tokens. It is worth noting that we also apply the traditional label smoothing technique to pi to activate the information in the predicted distribution. Analogous to human learning, it is often easier for humans to grasp new things described by their familiar knowledge (Reder et al., 2016) therefore the new soft label fused both accurate ground truth and model's self-distribution is easily digestible. Mathematically, for difficult tokens t i , p i is formulated as:\nEQUATION\n)\nThen we calculate the losses of difficult tokens and the others, and combine the two losses:\nEQUATION\nwhere i \u2208 D and j \u2208 N \\ D.\n\nEvaluation\nMachine Translation on three widely-used benchmarks (Ding et al., 2020 (Ding et al., , 2021c (Ding et al., , 2022)) : smallscale WMT16 English-Romanian (En-Ro; 0.6M), medium-scale WMT14 English-German (En-De; 4.5M), and large-scale WMT14 English-French (En-Fr; 36.0M). We implement the baselines and our approach under Transformer-base settings. We follow the previous adaptive training approach (Gu et al., 2020) to pretrain with the cross-entropy loss with N steps, and further finetune the same steps with different adaptive training objectives, including Freq-Exponential (Gu et al., 2020) , Freq-Chi-Square (Gu et al., 2020) , D2GPo (Li et al., 2020) , BMI-adaptive (Xu et al., 2021) , MixCrossEntropy (Li and Lu, 2021) , CBMI-adaptive (Zhang et al., 2022a) , and SPL (Wan et al., 2020) . For N , we adopt 100K and 30K for larger datasets, e.g. En-De and En-Fr, and small dataset, i.e. En-Ro, respectively. We empirically adopt 32K tokens per batch for large datasets, the learning rate warms up to 1e-7 for 10K steps, and then decays 90K, while for small dataset En-Ro, The learning rate warms up to 1e-7 for 4K steps, and then decays 26K steps. All the experiments are conducted on 4 NVIDIA Tesla A100 GPUs. The SacreBLEU (Post, 2018) was used for evaluation. Besides translation, we also follow previous works (Liu et al., 2021b; Zhong et al., 2022; Zhang et al., 2022b) to validate the universality of our method on more sequenceto-sequence learning tasks, e.g., summarization and grammatical error correction.\nText Summarization on XSUM corpus (0.2M). We follow fairseq (Ott et al., 2019) to preprocess the data and train the model, then finetune them for the same steps. We evaluated with the ROUGE (Lin, 2004) , i.e. R-1, R-2, and R-L.\n\nGrammatical Error Correction on CoNLL14\n(1.4M). We follow Chollampatt and Ng (2018) to preprocess the data and train the model, then finetune them for the same steps. The MaxMatch (M 2 ) scores (Dahlmeier and Ng, 2012) were used for evaluation with precision, recall, and F 0.5 values.\n\nMain Results\nSE brings gains across language pairs and scales.\nResults on machine translation across different data sizes ranging from 0.6M to 36M in Table 1 show that our SE-equipped Transformer \"+ Self-Evolution (ours)\" 1) considerably improves the performance by averaging +0.92 BLEU points; 2) out-\nValid Loss Scale 0-1 1-2 2-3 >3\nTransformer 63.3 10.5 6.7 19.5 + SE 65.6 9.5 5.8 19.1 4 show that our method can achieve +0.4 and +1.2 improvement in BLEU and COMET respectively, which proves that our SE also works on extremely large datasets.\n\nAnalysis\nWe provide some insights to better understand the effectiveness of our approach. The ablation of important modules and parameters is in Appendix A.\n\nSE learns better token representation.\nTo verify whether our method helps learn better tokens representation, we conduct analysis on WMT14 En-De from learning loss and fine-grained generation perspectives, respectively. First, we count the token ratios distributed in different cross-entropy loss scales in Table 3 following Zan et al. (2022a) . Cross-entropy is a good indicator to quantify the distance between the predicted distribution and the ground truth in the valid dataset, and a lower value means a more similar distribution. As shown, our method improves the low-loss token ratios by +2.3%, indicating SE helps the model learn better token representations by reducing the token uncertainty. In addition, we follow Ding et al. (2021a) ; Liu et al. (2021a) to break the translation down into different granularities and measure their fined-grained performance. In particular, we calculate 1 the F-measure of words by different frequency buckets and BLEU scores of buckets of different lengths in Figure 2 . We see SE achieves better performance in all frequencies and sentence buckets, demonstrating our method can improve the performance of different granularities.\nSE encourages diverse generations. Lacking generation diversity is a notorious problem for Seq2Seq learning tasks (Sun et al., 2020; Lin et al., 2022) . Benefiting from better exploring the model's prediction with corrected soft labels, SE is expected to improve generation diversity. We follow Wang et al. (2022) to examine this by analyzing the performance in an additional multiplereference test of WMT'14 En-De (Ott et al., 2018) . We choose additional references for each of the 500 test sentences taken from the original test. Table 5 shows SE consistently outperforms the baseline with the average improvement being 0.9/1.0 BLEU, which indicates that our SE can effectively generate diverse results. SE enhances model generalization. Benefiting from better hard token exploration, SE-equipped Transformers are expected to own better generalizations. We examine it by testing on domain shift scenarios following Ding et al. (2021b) . In particular, we evaluate WMT14 En-De models over four out-of-domain test sets (M\u00fcller et al., 2020) in Table 6 and find that SE improves the translation by averaging +0.9 BLEU points, showing a better lexical generalization ability.\nSE encourages human-like generations. We design two types of evaluation on WMT14 En-Fr: 1) AUTOMATIC EVALUATION with COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) , which have a high-level correlation with human judgments. 2) HUMAN EVALUATION with three near-native French annotators who hold DALF C2 certificate 2 . Specifically, for human evaluation, we randomly sample 50 sentences from the test set to evaluate the translation adequacy and fluency, scoring 1\u223c5. For adequacy, 1 represents irrelevant to the source while 5 means semantically equal. For fluency, 1 means unintelligible while 5 means fluent and native. Table 7 shows the automatic and human evaluation results, where we find that our SE indeed achieves human-like translation.\n\nConclusion\nIn this paper, we propose a self-evolution learning mechanism to improve seq2seq learning, by exploiting the informative-yet-underexplored tokens dynamically. SE follows two stages, i.e. selfquestioning and self-evolution training, and can be used to evolve any pretrained models with a sim- ple recipe: continue train with SE. We empirically demonstrated the effectiveness and universality of SE on a series of widely-used benchmarks, covering low, medium, high, and extremely-high data volumes.\nIn the future, besides generation tasks, we would like to verify the effectiveness of SE on language understanding tasks (Wu et al., 2020; Zhong et al., 2023) . Also, it will be interesting to design SEinspired instruction tuning or prompting strategy like Lu et al. (2023) to enhance the performance of large language models, e.g. ChatGPT 3 , which after all have already been fully validated on lots of conditional generation tasks (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023; Wu et al., 2023) .\n", "hypothesis": " Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g.  word frequency.  However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal.  In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method to fully and wisely exploit the knowledge from data.  SE focuses on dynamically learning the under-explored tokens for each forward pass and adaptively regularizes the training by introducing a novel token-specific label smoothing approach. Empirically, SE yields consistent and significant improvements in three tasks, i.e. machine translation, summarization, and grammatical error correction. Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks. Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization. Additionally, SE also outperforms the re-implemented advanced baselines in terms of computational efficiency, making it a more efficient training strategy.", "answer": false}
{"title": "Probing Physical Reasoning with Counter-Commonsense Context", "content": "\nIntroduction\nHumans possess physical commonsense regarding the behavior of everyday objects. Physical commonsense knowledge is relevant to their physical properties, affordances, and how they can be manipulated (Bisk et al., 2020) . While a significant amount of physical commonsense can be expressed in language (Forbes and Choi, 2017; Bisk et al., 2020) , direct sentences describing facts such as \"people are smaller than houses\" rarely appear because of reporting bias (Gordon and Van Durme, 2013; Ilievski et al., 2021) . Recent language models have succeeded in tasks that do not require contextual reasoning, such as size comparison and prediction of event frequency (Talmor et al., 2020) .\nHowever, what about inferences that are contextdependent? Whether a language model can make correct inferences in various contexts is important because physical reasoning is highly contextdependent (Ogborn, 2011) . Several studies on contextual physical reasoning (Forbes et al., 2019; Bisk et al., 2020; Aroca-Ouellette et al., 2021;  Template:<*> is in <box>.\nA house is in an eletctric bulb.\nIs the electric bulb bigger than the house? An electric bulb is in a house.\n\nOrdinary\nCounter-Commonsense\nFigure 1 : Examples of contexts that do or do not accord with ordinary commonsense. Humans can imagine the situation and make correct inferences, but language models are drawn to commonsense and make incorrect judgments. The example images are generated by Midjourney (https://midjourney.com). Zellers et al., 2021) have been conducted to produce datasets that assess the ability to recognize physical situations described in writing. Without context, however, these datasets may be answered by commonsense.\nHumans also can reason in ways that differ from simply using commonsense. For instance, if the context \"there is a house inside a light bulb.\" is provided, humans can still imagine the situation and reason that the bulb must be larger than the house. In other words, commonsense is just a sweeping generalization, and reasoning about context must be independent of commonsense. This reasoning with defeasibility, which reflects the ability to reason logically without relying only on commonsense, seems to have been overlooked in the study of language models compared to the acquisition of commonsense. Previous investigations of contextual physical reasoning (Aroca-Ouellette et al., 2021; Yu et al., 2022) failed to distinguish physical reasoning from the simple use of physical commonsense. To appropriately measure physical reasoning ability, we must use contexts that go against commonsense to rule out the possibility that the model is overconfident in physical commonsense.\nIn this study, we investigate the behavior of the language model concerning physical commonsense given the context of a situation that contradicts commonsense. We choose the size comparison task despite various possible domains of physical commonsense (Ilievski et al., 2021) . The task is one of the easiest physical commonsense reasoning tasks for language models (Forbes and Choi, 2017; Goel et al., 2019) , and it is also easy to add a context to change the relationship between sizes. For example, in this study, the context is a sentence that implies a size relationship, such as \"<obj1> contains <obj2>.\"\nFor this purpose, we created a new dataset, CConS (Counter-commonsense Contextual Size comparison) 1 . This dataset contains 1,112 sentences generated from 139 templates and tests the ability of language models to infer the size relationship between objects using a cloze-style prompt. Figure 1 shows the size comparison examples with or without contexts that (do not) agree with ordinary commonsense. Our experiments using recent language models show that GPT-3(text-davinci-003) (Brown et al., 2020) correctly reasons in context when it is consistent with commonsense, yielding 85% accuracy. In contrast, even GPT-3 can only show poor performance (41 % accuracy) for examples that contradict commonsense. This suggests that the models may not effectively distinguish between physical commonsense and inferences based on contexts, leading to incorrect predictions. Nevertheless, when prepositions hint at the relationships, the accuracy rate exceeded 55%, even for counter-commonsense examples. In summary, our counter-commonsense examples reveal the difference in influence between prepositions and verbs in contextualized physical reasoning.\nThe contributions of this study are as follows:\n1. We create a dataset that assesses size comparison ability more precisely by contrasting examples that conform to physical commonsense with ones that do not.\n2. We show that physical commonsense prevents measuring the language models' ability of contextual physical reasoning.\n3. We demonstrate that even large models perform poorly when making inferences that violate physical commonsense. Specifically, they struggle to infer size relations implied by verbs and can infer only when prepositions indicate.\n\nRelated Works\nSize Comparison Task The size comparison task, which previous studies (Yang et al., 2018; Goel et al., 2019) investigated since the earlier linguistic representations, such as GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) , is one of the easiest physical common-sense inference tasks for language models (Forbes and Choi, 2017; Goel et al., 2019) . While there are many prior studies (Elazar et al., 2019; Zhang et al., 2020) on this topic, VerbPhysics (Forbes and Choi, 2017) is the most similar to this study in that it focuses on the relationship between sizes and verbs. There are also some other approaches, such as methods that extract external knowledge (Elazar et al., 2019) , filling-masks (Talmor et al., 2020) , or generate images (Liu et al., 2022) . These results suggest that the commonsense of comparing object size is encoded in recent language models. However, these studies do not consider the context that might influence the results of size comparisons.\nDefeasible Reasoning According to Koons (2022) , defeasible reasoning is an argument that is rationally persuasive but not completely valid as a deduction. This defeasible reasoning is similar to the subject of this study in that it involves the recognition that commonsense and assumptions in a given context are not entirely correct propositions. Therefore, this study can be seen as an investigation into whether a language model can capture commonsense as defeasible reasoning. The creation of a dataset dealing with defeasible reasoning has been discussed by Rudinger et al. (2020) and Allaway et al. (2022) . Our study is similar to Allaway et al. (2022) in that it generates sentences that violate the context by fitting words to a template. However, this study differs in that we also generate examples contrary to commonsense for measuring the actual performance of the language model as well as the differences from the ordinary case.\n\nDataset Creation\nIn this study, we create 139 templates and automatically generate 1,112 examples. A key box contains a key.\nA key box contains a monitor. <*> fills <box>.\nA marble fills a bin. A refrigerator fills a bin. <*> is covered by <flat>.\nA pen is covered by a newspaper. A desk is covered by a handkerchief.\nTable 1 : Examples of the templates. <tag> constrains possible nouns to be filled. For example, <box> means that the noun entering there must have the attribute \"box,\" that is, it must be able to hold things. <*> indicates that any words in the noun list (only material nouns) can be inserted.\nexamples of these templates.\nDesigning Template We focus on the comprehensiveness of verb phrases while designing templates to ensure that the choice of verbs is not arbitrary. Therefore, we extract 139 verb phrases that indicate size relationships from the Oxford 5000 dictionary 2 and manually assemble simple sentences. For example, the statement \"<obj1> beats <obj2>\" is not included in this template because this statement is not informative enough to determine a size relation. Moreover, in comparing sizes, we also notice not only verbs but the usage of prepositions such as \"in\" or \"into\" may provide clear clues about the size relationships. Therefore, we select templates that contain only examples with these prepositions and distinguish them as easy templates from those that do not as hard templates. In subsequent experiments, we also investigate the effect of this difference on the behavior of the language model.\n\nRestriction on Noun\nIf nouns are arbitrarily inserted, the resulting sentences may be nonsensical or impossible for a human to imagine. For example, we choose not to include the sentence \"the stone threw the dog\" because it is beyond imagination.\nWe place restrictions on the nouns used in the sentence templates by defining tags to avoid this nonsense. A single placeholder can have constraints (multiple tags). There are 18 types of tags, including \"have_hands,\" \"box,\" and \"portable.\" Tags are manually determined to abstract the properties of verb phrases. We also use the Oxford 5000 dictionary to obtain a list of nouns referring to physical objects. One of the nouns that satisfy all constraints is randomly selected from a list of 195 nouns and inserted.\n\nGenerating Sentences\nThe template tags are replaced with the corresponding nouns to generate the context, and the questions asking for size comparisons are combined. For example, the contextualized question text provided to the masked language models is as follows: \"\u00abcontext\u00bb In this situation, the size of <obj1> is probably much [MASK] than the size of <obj2>.\"\nContexts and questions are used to generate input for each of the masked language models and generative models. We classify generated sentences to the Ordinary or Counter-Commonsense (CCommon) subset based on whether the size relationship between objects indicated by the template accords commonsense.\n\nExperiment\nTask Definition We measure the ability of masked language models and generative models to recognize size relationships by providing sentences for each architecture. These sentences are generated from templates (Section 3). We also see how the language model's behavior changes when context sentences follow or do not follow a general common-size relationship.\nComparison Aspects We investigate how language models create physical reasoning without being biased by their prior physical commonsense.\n1. How do the physical reasoning results of the language model change when contexts are consistent or inconsistent with commonsense?\n2. How does the performance of a language model change when comparing an easy dataset that contains certain prepositions that hint at size relationships with a hard dataset that does not?\nModel Settings In this study, BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) , and AL-BERT (Lan et al., 2020) are used to assess the performance of the masked language models. We also investigate how the size of the model affects physical reasoning. We choose T0 (Sanh et al., 2022) and to evaluate the performance of the generative model.\nAccording to Talmor et al. (2020) , RoBERTa-Large outperforms BERTs and RoBERTa-Base in a no-context size comparison task. Proceeding from this analysis we attempt to detect whether commonsense influences physical reasoning by giving examples contrary to commonsense as context.\n\nTasks Format Details\nThe tasks are performed by inputting sentences according to the format defined for each of the models, as follows.\nFormat for Masked Language Models WithContext: \u00abcontext\u00bb In this situation, the size of <obj1> is probably much [MASK] than the size of <obj2>. WithoutContext: The size of <obj1> is probably much [MASK] than the size of <obj2>.\nThe candidates for [MASK] are \"larger,\" \"bigger,\" \"smaller,\" and \"shorter.\" If the sum of the probabilities of the first two options exceeds 0.5, language models predict that obj1 is larger than obj2. Therefore, the language model always makes binary decisions.\n\nFormat for Generative Models\nWithContext: \u00abcontext\u00bb Which is bigger in this situation, <obj1> or <obj2>? WithoutContext: Which is bigger in general, <obj1> or <obj2>? \u00abcontext\u00bb is a sentence generated from templates.\nHuman Evaluation We ask crowdworkers to perform the same size comparison task to measure the accuracy of humans in this task. Thus, we can test the validity of the automatically generated questions. The crowdworkers are given the same context and make a choice that is larger. (See Appendix B for details.) Five crowdworkers are assigned to each question. We use some intuitive examples, such as \"<obj1> contains <obj2>,\" which are provided for qualification, and exclude those who get such examples wrong or choose the same answer for all examples. \n\nResult and Analysis\nTables 2 and 3 exhibit the performance of the language model on our datasets. GPT-3 outperforms other models in Ordinary and NoCon setups. RoBERTa-Large and ALBERT-XXLarge show better reasoning ability than the other masked language models in the Ordinary dataset. However, for the CCommon dataset, the performance of the pretrained language model decreases, particularly in ALBERT-XXLarge. This result suggests that commonsense built into the model hinders its ability to make accurate judgments. Other models struggle to capture size relationships. These results without context (NoCon) are generally consistent with the findings of a previous investigation of the nocontext size comparison task conducted by Talmor et al. (2020) .\nIn some CCommon examples, BERT performs better than RoBERTa. This may be because BERT is less equipped with commonsense, allowing it to make simpler judgments without being influenced.\nImpact of Prepositions Prepositions did not significantly impact the prediction for the masked language models in the Ordinary dataset. However, there is a significant difference in the correct response rates in the CCommon dataset. RoBERTa-Large performs well in easy data, regardless of whether the context defies commonsense. This result indicates that RoBERTa-Large recognizes the connection between the prepositions and size relationships. The ALBERT-XXLarge model does not perform well for the CCommon dataset, even if the setting is easy; therefore, we consider that it merely answers according to commonsense rather than making inferences. In short, context is not useful for ALBERT when the prepositions do not provide direct hints. GPT-3 uses prepositions more effectively than other models and performs better on the Easy dataset, while the model struggles to answer the CCommon dataset in the hard setting. This result means GPT-3 learns commonsense well but cannot make physical logical inferences.\n\nConclusion\nWe develop a method providing a countercommonsense context to measure physical reasoning ability. Our proposed contextualized physical commonsense inference dataset reveals that current language models can partially predict size relations but do not perform as well as humans in contexts that contradict commonsense. These judgments are possible to a limited extent in the presence of certain prepositions such as \"in\" and \"into.\" While we focused on size comparison tasks in this study, the importance of context in physical reasoning is not limited to this task. Increasing the size and scope of the datasets for contextual commonsense inference is necessary to build language models that more closely resemble humans and differentiate between general commonsense and the facts at hand.\n", "hypothesis": " In this study, we create a CConS (Countercommonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not.  This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates.  We measure the ability of several masked language models and generative models.  The results show that while large language models can use prepositions such as \"in\" and \"into\" in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense..", "answer": true}
{"title": "Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation", "content": "\nIntroduction\nPrompt-based learning typically works by modifying the input into cloze-style prompt templates and using the masked language models (MLMs) to complete the unfilled information in probabilistic. It has achieved promising performance in various NLP tasks (Schick and Sch\u00fctze, 2021; Lester et al., 2021; Hu et al., 2021) , especially in low-resource settings (Scao and Rush, 2021) . A promising prompt engineering category is demonstration learning (Gao * Equal contribution.\n\u2020 Corresponding author. et al., 2021; Liu et al., 2021a) , which seeks to provide a few answered samples as demonstrations to assist prompt prediction. As shown in Fig. 1 (a), the demonstration learning method concatenates the answered demonstrations per category to the prompt, and seeks to classify the [M ASK] token as great, indicating a positive prediction result based on a label-to-word mapping.\nThe intuition of demonstration learning is that samples with similar expressions or content can provide repetitive patterns (Liu et al., 2021a) . However, Min et al. (2022) point out that replacing gold demonstration labels with random labels marginally hurts performance. This finding is counter-intuitive and illustrates that the model could not comprehensively refer to the knowledge brought by the demonstrations in an implicit way. We attribute this problem to that existing methods simply concatenate the answered demonstrations to the prompt template without any additional operation, ignoring the dependencies between prompt and demonstrations.\nTo overcome this limitation, we rethink how human beings learn from demonstrations. Intuitively, when faced with a new challenging question, they typically (1) look for the most similar example to the question first, and then (2) reply to the question according to the answering steps of the retrieved example. Humans tend to strengthen the learning process through review strategies, i.e., finding a better solution to select similar examples and reanswering the questions of examples to consolidate known knowledge. Inspired by this, likewise, the interactions between the prompt and demonstrations could also be reinforced by imitating the human reviewing process for demonstration learning.\nIn this paper, we propose a simple-yet-effective version of demonstration learning, named Imitation DEMOnstration Learning (Imitation-Demo) to explicitly strengthen the two sub-steps of demonstration learning via human-like review. Specifi- \n\nContext 2:\nThe drama discloses almost nothing. cally, to accurately locate similar samples, we introduce a contrastive learning mechanism (Chen et al., 2020; Robinson et al., 2021) to reorganize demonstrations by reducing the divergences of demonstration contexts among the same category while increasing those divergences between different categories. Besides, to solidify known knowledge, we leverage a demonstration-label re-prediction method to emphasize the positions of the answers in demonstrations. Even without introducing new parameters or any prediction computation, our proposed method achieves state-of-the-art performance on 5 out of 14 classification corpus. Compared to the strong baseline LM-BFF (Gao et al., 2021) , Imitation-Demo achieves 1.11 points averaged improvement on the 14 datasets. Further study also shows that Imitation-Demo strengthens the association between prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.\n\nMethodology\nDemonstration Learning.\nAs illustrated in Fig. 1 (a), The prompt template x prompt consists of input sentence x sent and template x temp containing mask token, i.e., x prompt = [x sent , x temp ]. Firstly, we leverage the pre-trained SBERT (Reimers and Gurevych, 2019) to retrieve the demonstrations (including context x (k) and label y (k) ) for the k-th category that has maximum semantic similarity to the raw prompt context. Then, the retrieved demonstrations are concatenated to the input prompt. After that, we convert the concatenated input sentence\nx in to hidden vectors h in via the RoBERTa model (Liu et al., 2019) . The model is optimized by crossentropy loss, and the goal of demonstration learning is to predict y mask at the [M ASK] position from the hidden state of mask h mask via MLM head. The whole process could be formulated as 1 :\nx in = [x prompt , (x (1) , y (1) ), ...,(x (K) , y (K) )] h in = RoBERTa(x in ) L mask = CE(h mask , \u0176 mask ) p y mask | x in = MLM(h mask ) (1)\nwhere [.., .., ..] denotes concatenating diverse parts with sentence separator [SEP ] . K is the number of categories. CE is short for cross-entropy loss, and \u0176 mask is the ground-truth labels from the predefined label-to-word mapping. Demonstration Reorganization via Contrastive Learning. In demonstration learning, it is crucial to decide from which known demonstrations to select the repetitive patterns. Therefore, we introduce a contrastive learning mechanism to imitate human review behaviour by reorganizing the demonstrations based on their contexts. As shown in Fig. 1 (b)(I), we treat the demonstration contexts with identical categories to the input prompt as positive samples, and the others are regarded as negative ones. By pulling in positive samples and pulling out negative samples, the model could select the most relevant sample among the given demonstrations more precisely. In the experiment, we apply mean-pooling operations on the hidden states of positive, negative demonstration contexts h + , h \u2212 , and input sentence h in , obtaining the sentence representations s + , s \u2212 , and s in . Inspired by Robinson et al. (2021) in computer vision, we introduce HCL loss to ensure intra-class compactness while increasing inter-class distances:\nL context = E \u2212 log e s in \u2022s + e s in \u2022s + + N i=1 e s in \u2022s \u2212\n(2) where \u2022 is the dot product operation, N is the number of negative contexts in the task, and E [..] denotes calculating the mean value. Demonstration-label Re-prediction. We further utilize a demonstration-label re-prediction method to mimic human review behaviour by recovering the labels from all the given demonstration contexts. Specifically, the target of our model is not only to identify the category of [M ASK] token, but also to classify the tokens located in demonstration label positions. Take the binary classification task in Fig. 1 (b)(II) as an example, more than predicting the class of the mask token, the model also requires to predict y great and y terri (i.e., great and terrible) based on the hidden states h great and h terri at corresponding label positions.\nDuring training, the cross-entropy loss is utilized to calculate L great and L terri for different demonstration labels, then we sum them up to obtain the demonstration-label re-prediction loss L label :\nL great = CE(h great , \u0176 great ) L terri = CE(h terri , \u0176 terri ) L label = L great + L terri (3)\nwhere \u0176 great and \u0176 terri are the ground-truth labels at diverse demonstration label positions.\nSimilar contrastive learning and demonstrationlabel re-prediction operations can also be performed for the multi-category classification tasks. The overall loss of Imitation-Demo is defined as follows:\nL = L mask + \u03b1L label + \u03b2L context (4)\nwhere \u03b1, \u03b2 are weight coefficients to control the importance of different components. datasets. For SNLI (Bowman et al., 2015) , SST-2 (Socher et al., 2013) , CoLA (Warstadt et al., 2019) , MNLI (Williams et al., 2018) , QNLI (Rajpurkar et al., 2016) , RTE (Dagan et al., 2005; Giampiccolo et al., 2007; Bentivogli et al., 2009) , MRPC (Dolan and Brockett, 2005) , QQP 2 and SST-B (Cer et al., 2017), we use the original development sets for testing. For MR (Pang and Lee, 2005) , CR (Hu and Liu, 2004) , MPQA (Wiebe et al., 2005) and Subj (Pang and Lee, 2004) , we randomly sample 2,000 examples as the testing set. For SST-5 (Socher et al., 2013) and TREC (Voorhees and Tice, 2000) , we use the official test sets. F1 score (F1) are adopted as the evaluation metric of MRPC and QQP, and the other datasets utilize accuracy (acc) as the evaluation criteria. Parameters Setting We implement all the baselines and our frameworks using PyTorch (Paszke et al., 2019) . The pre-trained RoBERTa-large model and roberta-large-nli-stsb-mean-tokens SBERT (Reimers and Gurevych, 2019 ) from huggingface 3 are applied in the experiments. We get 16 samples per class during training for all models. In order to control the smoothness of the exponential functions when calculation contrastive learning loss, we divide every mean-pooling results with temperature T . Grid search mechanisim are utilized to select optimal hyper-parameter combinations on each split. Finally we select the the coefficients \u03b1 and \u03b2 as 1 and 5, respectively. The temperature T is set as 5 and the batch size is 16. The other hyper-parameters and the prompt templates are identical to the default settings in LM-BFF (Gao et al., 2021) for fair comparison. We report the average performance of models trained on 5 different randomly sampled training and dev splits, the random seeds are fixed as 13, 32, 42 ,87 , 100, respectively. Compared Methods. (1) Majority, which select the majority class of the dataset; (2) Prompt-based zero-shot: which use prompt tunning in zeroshot situations; (3) \"GPT-3\" in-context learn- Table 2 : Overall results on RoBERTa-large with 16 samples per class. We report the mean (variance) of models trained on 5 different randomly sampled training and dev splits. Prompt-based Fine-tuning (man) indicates trained with manually designed templates. \u2661 denotes we re-implement the EFL and LM-BFF models for fair comparisons.\ning, which use the in-context learning proposed in RoBERTa with no parameter updating; (4) Finetuning;\n(5) P-tuning (Liu et al., 2021b) , which employ trainable continuous prompt embeddings;\n(6) DART (Zhang et al., 2021) , which differentially optimize the prompt template and the target label during the backpropagation process; (7) Li's (Li et al., 2022) , which reformulate a classification or a regression task as a token-replaced detection problem utilizing pre-trained model Electra (Clark et al., 2020) ; (8) Demo-tuning (LM-BFF) (Liang et al., 2022) , which select \"mask token\" output feature as the input for contrastive learning to get a good representation of \"virtual demonstration\". We select the LM-BFF as the basic backbone model for fair comparisons. ( 9) LM-BFF + Sup-Con (Jian et al., 2022) \n\nConclusion\nIn this paper, we propose imitation demonstration learning (Imitation-Demo) to reinforce the correlations between prompt and given demonstrations. Inspired by the human review process, we introduce contrastive learning to locate similar samples and demonstration-label re-prediction mechanisms to solidify known knowledge. Experiments show that our method consistently outperforms other baselines on 5 out of 14 classification datasets in the few-shot settings. We hope this work could inspire the exploration of the working mechanism of demonstration learning and toward better few-shot learning abilities.\n", "hypothesis": " Demonstration learning aims to guide the prompt prediction by providing answered demonstrations in the few shot settings.  Despite achieving promising results, existing work only concatenates the answered examples as demonstrations to the prompt template (including the raw context) without any additional operation, neglecting the prompt-demonstration dependencies.  Besides, prior research found that randomly replacing the labels of demonstrations marginally hurts performance, illustrating that the model could not properly learn the knowledge brought by the demonstrations.  Inspired by the human learning process, in this paper, we introduce Imitation DEMOnstration learning (Imitation-Demo) to strengthen demonstration learning via explicitly imitating human review behaviour, which includes: (1) contrastive learning mechanism to concentrate on similar demonstrations.(2) demonstration-label re-prediction method to consolidate known knowledge. Experiment results show that our proposed method achieves state-of-the-art performance on all 14 classification corpus. Further studies also prove that Imitation-Demo eliminates the associations between the prompt and demonstrations, which could provide the basis for exploring how demonstration learning works.", "answer": false}
{"title": "Word-level Prefix/Suffix Sense Detection: A Case Study on Negation Sense with Few-shot Learning", "content": "\nIntroduction\nMorphological analysis mainly refers to processing a word into a lemma (root) and a well-defined morphological tag (Anglin et al., 1993; Haspelmath and Sims, 2013; Morita et al., 2015; Nicolai and Kondrak, 2017; Deacon et al., 2017; Ganesh et al., 2019) . For instance, through morphological analysis, the word \"unhappy\" will be divided into a lemma \"happy\" and a negation sense prefix tag \"un-\". Morphological analysis has played an important role in natural language processing (NLP) and it has been applied to many downstream tasks such as spelling checking (Aduriz et al., 1993; Oflazer, 1995; S\u00e9n\u00e9chal and Kearnan, 2007; Levesque et al., 2021) and machine translation (Lee, 2004; Habash, 2007; Toutanova et al., 2008; Belinkov et al., 2017) .\nOne major challenge in morphological analysis is that prefixes/suffixes are sometimes ambiguous. For instance, in English, the prefix \"un-\" often means a meaning \"not\", i.e., a negation sense. However, not all words with the prefix \"un-\" have a negation sense, such as \"unanimous\" and \"unpick\". Besides, the substring \"un-\" sometimes does not appear as a prefix in some words, such as \"universe\" and \"unique\". In this study, we directly address the above challenge by proposing a novel morphological analysis task, namely word-level prefix/suffix negation sense detection, which aims to detect whether a substring in a word is a prefix/suffix and meanwhile takes a specific pre-defined morphological sense. As a preliminary study, we focus on negative prefixes/suffixes. In many languages, one way to make a negative expression is to add a negative prefix/suffix to a word. For instance, in English, il-, im-, un-, and -less are some popular negative prefixes/suffixes.\nOne straightforward approach to prefix/suffix negation sense detection is to build a dictionary that covers all words with the prefixes/suffixes expressing such a sense. However, this is unrealistic because there are always many newly-emerging words due to non-standard language usage or incorrect spelling in some informal texts like Twitter. Therefore, we address the task of word-level prefix/suffix negation sense detection in a computational way.\nSpecifically, to further reduce the annotation cost, we propose a few-shot learning approach by employing the token-replaced detection model as our basic prompt-learning model due to its excellent performance in few-shot learning (Li et al., 2022) . Furthermore, we propose a novel prompt, namely input-augmentation prompt, which relies only on the input word. As illustrated in Fig. 1(c ), for the input word is \"unhappy\", the prompt, \" unhappy It is not happy\", is used to predict whether the word \"not\" is original or replaced so as to determine whether the input word is a negation word or not, where the substring \"happy\" is generated by removing the potential prefix (i.e., un-) from the input word. The de- \n\nInput-augmentation template\nTemplate Template and label description words predict predict P(original|\"negative\")>P(original|\"positive\") (label: negative) \uf050 P(original|\"negative\")<P(original|\"positive\") (label: positive) sign of our input-augmentation prompt can avoid one major shortcoming of existing few-shot learning approaches, i.e., the selection of labels (e.g., two labels, \"positive\" and \"negative\" in Fig. 1a ) or the selection of label description words (e.g., \"negative positive\" in Fig. 1b ) has a big impact on learner performance (Jiang et al., 2020; Gao et al., 2020; Li et al., 2022) . Moreover, our empirical studies also demonstrate that our approach achieves much better performances than the existing few-shot learning approaches.\n\nRelated work\nMorphological analysis aims to learn about the morphological structure of a given word form, and in general, there are four specific tasks: morphological tagging (i.e., assigning some pre-defined morphological tags to a word in a sentence) (M\u00fcller et al., 2013; Labeau et al., 2015; Cotterell and Heigold, 2017; Conforti et al., 2018; Malaviya et al., 2019) , lemmatization (i.e., converting a word in a sentence into the normalized form) (Plisson et al., 2004; Chrupa\u0142a, 2006; Jongejan and Dalianis, 2009; Strakov\u00e1 et al., 2014; Bergmanis and Goldwater, 2018) , morphological segmentation (i.e., judging whether the substring in a word could be segmented as a prefix/suffix) (Ruokolainen et al., 2013 (Ruokolainen et al., , 2016;; Goldsmith et al., 2017; Cotterell et al., 2019) , and morphological disambiguation (i.e., assigning a correct morphological segmentation to a word by leveraging the context) (Hakkani-T\u00fcr et al., 2002; Yildiz et al., 2016; Cotterell et al., 2018; Wiedemann et al., 2019) .\nCompared to the above tasks, our work has at least three different aspects. First, our task is a combination of morphological tagging and morphological segmentation. Second, our task is word-level, i.e., the input contains only a single word without context, which leads to the inapplicability of previous approaches based on contextual information. Third, we propose a novel few-shot learning approach to our task. To the best of our knowledge, this is the first attempt of studying fewshot learning in morphological analysis.\n\nCorpus Generation\nWe use six prefixes, i.e., un-, im-, in-, il-, irand dis-as negation prefixes and two suffixes, i.e., -less and -f ree as negation suffixes to collect words from two resources, i.e., the ninth edition of Oxf ord Advanced Learner \u2032 s Dictionary (AS et al., 2005) and 1.6 million English Tweeter data collected by Go et al. (2015) . In summary, we obtain 2,717 and 6,671 words with negation prefixes/suffixes from the Oxford dictionary and tweeter data, respectively. Then, we randomly select 3,000 words and annotates such words as our corpus. Specifically, we assign two annotators to annotate each word into two categories, i.e., positive and negative. The Kappa consistency check value of the human annotation is 0.87. Moreover, for words with different sense annotations, we assign another annotator to make a final decision. \n\nMethodology\nProblem statement: The prefix/suffix negation sense detection task can be formulated as follows.\nLet D l = {w, y} be labeled data, where w is the input word and y is a label in {positive, negative}.\nOur approach aims to provide a few-shot learner for such a detection task.\nApproach overview: As shown in Figure 1 (c), a prompt-based learner, which is based on a pretrained token-replaced detection model and an input-augmentation prompt, is built for the prefix/suffix negation sense detection task. The goal of a pre-trained token-replaced detection model (e.g., ELECTRA) is to predict whether a token in the input string is replaced or not.\n\nApproach specification:\nFirst, an inputaugmentation prompt x prompt is constructed for an input word w, as follows.\nEQUATION\nwhere \"It is not\" is a template, and w is a substring of the input word w without the prefix/suffix, such as w = \"happy\" for w = \"unhappy\". Second, prompt x = [w 1 , w 2 , ..., w n ] is fed into the encoder in the discriminator of the pre-trained token-replaced detection model to obtain an output sequence y = [y 1 , y 2 , ..., y n ], where w i is the ith word in the prompt, and y i is the prediction label (either original or replaced) for word w i , indicating whether the word is original or replaced.\nFinally, we map the label set of the pre-trained token-replaced detection model to the label set of our task, with the following formulas. P (\"negative\"|x prompt ) = P (y \"not\" = original)\n(2) and P (\"positive\"|x prompt ) = P (y \"not\" = replaced),\n(3) where y \"not\" denotes the label corresponding to the word \"not\" in the input-augmentation prompt as shown in formula (1).\nFor instance, suppose that the input word is \"unhappy\", we first obtain the input-augmentation prompt \"unhappy It is not happy\" and then use the pretrained token-replaced model to predict whether the word \"not\" in the prompt is original or replaced. If the prediction result is original, we conclude that the input word \"unhappy\" is a negative word.\nIn the training phase of our few-shot learning setting, only a few prompt samples, together with their labels are used to update the parameters in the discriminator of the pre-trained token-replaced detection model. It is important to note that our approach reuses the pre-trained parameters in the pretrained token-replaced detection model and does not use any other new parameters.\n\nExperiments\nData setting: 2,000 samples are randomly selected from the human-annotated corpus. First, 400 samples are selected as test data, including 200 for each class. Then, we follow the evaluation protocol of Li et al. (2022) We implement the following approaches for comparison:\n(1) Finetuning-RoBERTa (Liu et al., 2019) : Based on the fine-tuning approach and RoBERTalarge model, the prediction label is obtained by mapping the \"[CLS]\" token to label space.\n(2) Finetuning-ELECTRA (Clark et al., 2020) : It is similar to finetuning-RoBERTa except that the ELECTRA-large model is used.\n(3) Prompt-RoBERTa (Gao et al., 2020) : It is a discrete prompt learning approach based on RoBERTa-large, as shown in Figure 1(a) , where the prompt is \" w it is [mask] \", and the prediction label is obtained by the filling of \"[mask]\" (either \"negative\" or \"positive\"). (4) Prompt-ELECTRA (Li et al., 2022) : It is a discrete prompt learning approach based on ELECTRA-large, as shown in Figure 1(b) , where the prompt is \" w is a negative positive word \". ( 5) Warp (Hambardzumyan et al., 2021) : It is a continuous prompt learning approach, in which the best prompt template is obtained by searching in the (continuous) embedding space. Moreover, the template is learned using adversarial refactoring. ( 6) DART (Zhang et al., 2021) : It is a continuous prompt learning approach, in which the search for the best prompt template is based on backpropagation. (7) P-tuning-v2 (Liu et al., 2021) : It is a continuous prompt learning approach, in which the search for the best prompt is based on a prefixed-tuned multi-layer prompt. (8) Fully-supervised Learning: 1,400 training and 200 development samples are used to re-train the ELECTRA-large model.\nTable 2 shows the performances of different approaches, from which we can see that : (1) Our approach significantly outperforms the fullysupervised learning and fine-tuning approaches, which proves the effectiveness of our few-shot learner. (2) Our approach performs much better than other prompt-based learners, e.g., obtaining 8.6% increase on Macro-F1 when compared with Prompt-ELECTRA. The improvement confirms the effectiveness of our input-augmentation prompt. (3) Our approach, using only 16 training and 16 development samples, almost performs equivalent to the fully-supervised learning approach with 1,400 training and 200 development samples.\nAn error analysis is made for our approach, which shows two main error causes: (1) the input word w or its substring w has multiple meanings, such as \"hapless\" vs. \"hap\", and \"disembarkation\" vs. \"embarkation\". (2) the meaning of w and w is irrelevant, such as \"dispossession\" vs. \"possession\", and \"ingot\" vs. \"got\". This indicates that more efforts are needed for our prefix/suffix negation sense detection.\n\nConclusion\nIn this study, we propose a novel word-level morphological analysis task, namely prefix/suffix sense detection, and make a case study on negation sense. We provide an annotated corpus for the prefix/suffix negation sense detection, and then propose a novel few-shot learning approach, which uses an input-augmentation prompt and a pretrained token-replaced detection model to effectively make the negation sense detection. Empirical studies show that our approach performs much better than other approaches in the few-shot scenario, such as using only 16 training samples.\n", "hypothesis": " Morphological analysis is an important research issue in the field of natural language processing.  In this study, we propose a context-free morphological analysis task, namely word-level prefix/suffix sense detection, which deals with the ambiguity of sense expressed by prefix/suffix.  To research this novel task, we first annotate a corpus with prefixes/suffixes expressing negation (e.g., il-, un-, -less) and then propose a novel fewshot learning approach that applies an inputaugmentation prompt to a token-replaced detection pre-training model.  Empirical studies demonstrate the effectiveness of the proposed approach to word-level prefix/suffix negation sense detection.", "answer": true}
{"title": "Do Large Language Models Know What They Don't Know?", "content": "\nIntroduction\nRecently, Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) , PaLM 2 (Anil et al., 2023) , and LLaMA (Touvron et al., 2023) have shown exceptional performance on a wide range of NLP tasks, including common sense reasoning (Wei et al., 2022; Zhou et al., 2022) and mathe- matical problem-solving (Lewkowycz et al., 2022; Chen et al., 2022) . Despite their ability to learn from huge amounts of data, LLMs still have limitations in their capacity to retain and understand information. To ensure responsible usage, it is crucial for LLMs to have the capability of recognizing their limitations and conveying uncertainty when responding to unanswerable or unknowable questions. This acknowledgment of limitations, also known as \"knowing what you don't know,\" is a crucial aspect in determining their practical applicability. In this work, we refer to this ability as model self-knowledge.\nThe Know-Unknow quadrant in Figure 1 illustrates the relationship between the model's knowledge and comprehension. The ratio of \"Known Knows\" to \"Unknown Knows\" demonstrates the model's proficiency in understanding and applying existing knowledge. Techniques such as Chain-of-Thought (Wei et al., 2022) , Self-Consistency (Wang et al., 2022) , and Complex CoT (Fu et al., 2022) can be utilized to increase this ratio, resulting in improved performance on NLP tasks. We focus on the ratio of \"Known Unknows\" to \"Unknown Unknows\", which indicates the model's self-knowledge level, specifically understanding its own limitations and deficiencies in the unknows.\nExisting datasets such as SQuAD2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017) , widely used in question answering (QA), have been utilized to test the self-knowledge of models with unanswerable questions. However, these questions are context-specific and could become answerable when supplemented with additional information. Srivastava et al. (2022) attempted to address this by evaluating LLMs' competence in delineating their knowledge boundaries, employing a set of 23 pairs of answerable and unanswerable multiple-choice questions. They discovered that these models' performance barely surpassed that of random guessing. Kadavath et al. (2022) suggested probing the selfknowledge of LLMs through the implementation of a distinct \"Value Head\". Yet, this approach may encounter difficulties when applied across varied domains or tasks due to task-specific training. Consequently, we redirect our focus to the inherent abilities of LLMs, and pose the pivotal question: \"Do large language models know what they don't know?\".\nIn this study, we investigate the self-knowledge of LLMs using a novel approach. By gathering reference sentences with uncertain meanings, we can determine whether the model's responses reflect uncertainty using a text similarity algorithm. We quantified the model's self-knowledge using the F1 score. To address the small and idiosyncratic limitations of existing datasets, we created a new dataset called SelfAware. This dataset comprises 1,032 unanswerable questions, which are distributed across five distinct categories, along with an additional 2,337 questions that are classified as answerable. Experimental results on GPT-3, In-structGPT, LLaMA, and other LLMs demonstrate that in-context learning and instruction tuning can effectively enhance the self-knowledge of LLMs. However, the self-knowledge exhibited by the current state-of-the-art model, GPT-4, measures at 75.47%, signifying a notable disparity when contrasted with human self-knowledge, which is rated at 84.93%.\nOur key contributions to this field are summarized as follows:\n\u2022 We have developed a new dataset, SelfAware, that comprises a diverse range of commonly posed unanswerable questions.\n\u2022 We propose an innovative evaluation technique based on text similarity to quantify the degree of uncertainty inherent in model outputs.\n\u2022 Through our detailed analysis of 20 LLMs, benchmarked against human self-knowledge, we identified a significant disparity between the most advanced LLMs and humans 1 .\n\nDataset Construction\nTo conduct a more comprehensive evaluation of the model's self-knowledge, we constructed a dataset that includes a larger number and more diverse types of unanswerable questions than Know-Unknowns dataset (Srivastava et al., 2022) . To facilitate this, we collected a corpus of 2,858 unanswerable questions, sourced from online platforms like Quora and HowStuffWorks. These questions were meticulously evaluated by three seasoned annotation analysts, each operating independently. The analysts were permitted to leverage external resources, such as search engines. To ensure the validity of our dataset, we retained only the questions that all three analysts concurred were unanswerable. This rigorous process yielded a finalized collection of 1,032 unanswerable questions.\nIn pursuit of a comprehensive evaluation, we opted for answerable questions drawn from three datasets: SQuAD (Rajpurkar et al., 2016) , Hot-potQA (Yang et al., 2018) , and TriviaQA (Joshi et al., 2017) . Our selection was guided by Sim-CSE (Gao et al., 2021) , which allowed us to identify and select the answerable questions semantically closest to the unanswerable ones. From these sources, we accordingly drew samples of 1,487, 182, and 668 questions respectively, amassing a total of 2,337. Given that these questions can be effectively addressed using information available on Wikipedia, the foundational corpus for the training of current LLMs, it is plausible to infer that the model possesses the requisite knowledge to generate accurate responses to these questions.\nOur dataset, christened SelfAware, incorporates 1,032 unanswerable and 2,337 answerable questions. To reflect real-world distribution, our dataset\n\nDescription\nExample Percentage\n\nNo scientific consensus\nThe answer is still up for debate, with no consensus in scientific community.\n\"Are we alone in the universe, or will we discover alien life at some point?\"\n\n25% Imagination\nThe question are about people's imaginations of the future.\n\"What will the fastest form of transportation be in 2050?\" 15%\nCompletely subjective\nThe answer depends on personal preference.\n\"Would you rather be shot into space or explore the deepest depths of the sea?\"\n\nToo many variables\nThe question with too many variables cannot be answered accurately. contains a proportion of answerable questions that is twice as large as the volume of unanswerable ones. Nevertheless, to ensure the feasibility of testing, we have purposefully capped the number of answerable questions.\n\nDataset Analysis\nTo gain insight into the reasons precluding a certain answer, we undertook a manual analysis of 100 randomly selected unanswerable questions. As tabulated in Table 1 , we have broadly segregated these questions into five distinctive categories. \"No Scientific Consensus\" encapsulates questions that ignite ongoing debates within the scientific community, such as those concerning the universe's origin. \"Imagination\" includes questions involving speculative future scenarios, like envisaged events over the next 50 years. \"Completely Subjective\" comprises questions that are inherently personal, where answers depend heavily on individual predispositions. \"Too Many Variables\" pertains to mathematical problems that become unsolvable owing to the overwhelming prevalence of variables. Lastly, \"Philosophical\" represents questions of a profound, often metaphysical, nature that resist concrete answers. Ideally, upon encountering such questions, the model should express uncertainty instead of delivering conclusive responses.\n\nEvaluation Method\nThis section elucidates the methodology employed for assessing self-knowledge in the generated text.\nIn order to achieve this, we define a similarity function, f sim , to compute the similarity, S, between a given sentence, t, and a collection of reference sentences, U = {u 1 , u 2 , . . . , u n }, endowed with uncertain meanings.\nEQUATION\nWhenever any S i surpasses a pre-determined threshold T , we perceive the text t as encompassing uncertain meanings, thereby eliminating the need for manual evaluation of the response.\nGiven the substantial disparity in the volume of answerable and unanswerable questions in Self-Aware, we adopt the F1 score as a measure of LLMs' self-knowledge. Our focus rests on identifying unanswerable questions, hence we designate them as positive cases and categorize answerable questions as negative cases.\n\nModel\nWe conduct a sequence of experiments to evaluate the degree of self-knowledge manifested by various LLMs, including GPT-3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022) series, as well as the recent LLaMA (Touvron et al., 2023) and its derivative models, namely Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) . Our investigative approach employed three distinct input forms: Direct, Instruction, and In-Context Learning (ICL), which is encapsulated in Appendix A.4. In-Context Learning \n\nSetting\nWe devised the reference sentence set U through a process that combined automated generation by LLMs and manual filtering, detailed further in Appendix A.1. To quantify the similarity between target and reference sentences, we utilized Sim-CSE (Gao et al., 2021) , setting the similarity threshold to 0.75 during our experiments. An exploration of threshold ablation is available in Appendix A.2.\nTo counteract potential errors in similarity calculation induced by varying lengths of the target and reference sentences, we employed a sliding window of length 5 to parse the target sentence into semantic chunks. During the generation process, we set the temperature to 0.7. We selected a random sample of 100 instances for GPT-4, while the remainder of the models were scrutinized using the full SelfAware dataset.\n\nHuman Self-Knowledge\nTo establish a benchmark for human selfknowledge, we engaged two volunteers and selected 100 random samples from the SelfAware dataset. The volunteers has 30 minutes to make judgments on the same set of questions, yielding an average F1 score of 84.93%, which we subsequently adopted as the benchmark for human self-knowledge. Detailed scores are available in Appendix A.3.\n\nAnalysis\nWe evaluate the manifestation of LLMs' selfknowledge, centering our investigation on three fundamental dimensions: the size of the model, the impact of instruction tuning, and the influence exerted by different input forms.\nModel Size. Figure 2 illustrates the correlation between model size and self-knowledge across various LLMs. It is noteworthy that across all three input forms, an augmentation in model parameter size is associated with an elevation in the F1 Score, with the most conspicuous enhancement manifesting in the ICL input form. Therefore, our analysis indicates that an LLM's self-knowledge tends to enhance with increasing model size, a trend consistent with the scaling law. Instruction Tuning. Figure 2 delineates that models from the InstructGPT series exhibit a superior level of self-knowledge compared to their GPT-3 counterparts. Further evidence of model enhancement is provided by Figure 4 , where textdavinci models show significant improvement relative to the base davinci model. An additional comparative analysis, presented in Figure 5 , evaluates LLaMA against its derivative models. The results underscore a notable increase in self-knowledge for Alpaca and Vicuna upon instruction tuning, exceeding their base model performances. Among these, Vicuna-13B outperforms the LLaMA-65B, corroborating the efficacy of instruction tuning for enhancing model self-knowledge.\nL L a M A -7 B A lp a c a -7 B V ic u n a -7 B L L a M A -1 3 B A lp a c a -1 3 B V ic u n a -1 3 B L L a M A -3 0 B L L a M A -\nInput Forms. As shown in Figure 2 , the incorporation of instructions and examples serves to boost the self-knowledge of both the GPT-3 and Instruct-GPT series. Specifically, ICL input form, providing richer contextual information, contributes to a significant enhancement in models' self-knowledge. This impact is particularly noticeable in the davinci model, where ICL facilitates a 27.96% improvement over the direct. Moreover, a comparison between Figure 3 performance to the human benchmark of 84.93%. This underscores the considerable potential that remains for enhancing the self-knowledge level of LLMs.\nAnswerable Questions. Figure 6 traces the performance evolution of the InstructGPT series in addressing answerable questions, adhering to the closed-book question answering paradigm (Touvron et al., 2023) , where output accuracy is contingent on the presence of the correct answer. Our observations underscore a steady enhancement in QA task accuracy corresponding to an increase in model parameter size and continuous learning.\nParticularly, the accuracy of text-davinci-001 experiences a significant ascent, scaling from a meager 2.48% in text-ada-001 to 10.61%, whereas GPT-4 marks an even more striking jump to 42.64%.\n\nConclusion\nThis study investigates the self-knowledge of LLMs by evaluating their ability to identify unanswerable questions. Through the introduction of a novel dataset and an automated method for detecting uncertainty in the models' responses, we are able to accurately measure the self-knowledge of LLMs such as GPT-3, InstructGPT and LLaMA.\nOur results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge. This highlights the need for further research in this area to enhance the ability of LLMs to understand their own limitations on the unknows. Such efforts will lead to more accurate and reliable responses from LLMs, which will have a positive impact on their applications in diverse fields.\n", "hypothesis": " Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks.  Current research focuses on enhancing their performance within their existing knowledge.  Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend.  Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance.  This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions.  We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. Despite this promising insight, our findings also highlight a considerable convergence between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.\n\"True wisdom is knowing what you know.\" -Confucius.  Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models.", "answer": false}
{"title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning", "content": "\nIntroduction\nDeveloping comprehensive evaluation frameworks (Deng et al., 2021; Yuan et al., 2021; Zhong et al., 2022) that can evaluate multiple humaninterpretable dimensions, such as factual consistency (Kryscinski et al., 2020; Wang et al., 2020) and coherence (Dziri et al., 2019; Huang et al., 2020) , is important for the advancement of Natural Language Generation (NLG). However, similaritybased metrics (Papineni et al., 2002; Lin, 2004; Sellam et al., 2020; Zhao et al., 2019; Zhang et al., 2020) still dominate NLG evaluation in practice. Compared to them, desired multi-dimensional evaluators do not require reference texts for evaluation; and they can easily extend to new explainable evaluation dimensions. Recently, Zhong et al. (2022) developed a unified evaluation framework that can Figure 1 : Our prompt design to evaluate the consistency of the summary in red, illustrated using two in-context examples (in blue). To evaluate other aspects, we remove the source text or replace it with a reference. generalize to multiple dimensions and text generation tasks. However, it relies on the construction of synthetic and auxiliary data for the finetuning of a pre-trained language model, requiring in-depth knowledge and significant engineering effort for each dimension. Furthermore, the inclusion of new dimensions requires (continued) training of the model, and might affect the performance on other dimensions in unforeseen ways.\nIn this work, we propose to use in-context learning (Brown et al., 2020) with large language models (LLMs) -a commonly used method to perform many tasks by utilizing only a few input-output examples -to perform multi-dimensional text evaluation in a unified fashion. Compared to pre-trained evaluators that need specialized supervised training for each dimension, our In-Context learning-based Evaluator (ICE) framework is:\n\u2022 Learning-free. It does not require supervised fine-tuning on large annotated (synthetic) training data, requiring only a handful of samples at inference time. \u2022 Extensible. To evaluate new dimensions, it does not rely on large amounts of human judgments or the construction of new synthetic data, using only a natural language prompt consisting of a small number of example pairs to ascertain the properties associated with a given quality aspect.\nIn this paper, using text summarization as a test bed, we show that with a simple prompt design, ICE is competitive with state-of-the-art trained evaluators on multi-dimensional evaluation of modelproduced summaries, establishing a new state-ofthe-art on dimensions such as relevance and factual consistency. To study the robustness of the evaluator to the selection of in-context examples, we analyze the factors that affect the performance of ICE, such as the number of in-context examples and sampling procedures when picking in-context examples from a set of candidates. We find ICE to be robust to the selection of in-context examples and observe a slight improvement in performance as the number of examples is increased. Finally, in light of the recent work (Goyal et al., 2022) that points to the misalignment of existing evaluation metrics with human preference in evaluating zeroshot summaries generated by LLMs such as GPT-3 (Brown et al., 2020) , we study the effectiveness of ICE in evaluating zero-shot summaries generated by GPT-3. We find that ICE evaluations agree closely with human judgments on such summaries.\n\nProblem Statement\nGiven a sequence x that is input to an NLG system and a system-generated output sequence y, an evaluation framework outputs a score s that captures the quality of y, either with or without the help of a human-generated reference output r. 1 In case of multi-dimensional evaluation where we are interested in assessing y over d quality metrics, we instead get a vector S = (s 1 , s 2 , ..., s d ) over diverse dimensions (e.g., coherence, fluency). Depending on the dimension, there is sometimes a need to condition an evaluation on x (such as to evaluate consistency in summarization). We evaluate our method over four dimensions:\n\u2022 Consistency: The factual correctness of a summary given the source text. \u2022 Relevance: The property of capturing salient information from the source. \u2022 Fluency: A measure of the quality of the individual sentences in the summary. \u2022 Coherence: A measure of the quality, organization, and structure of sentences in the summary.\n1 Specifically for summarization, most learned frameworks evaluate relevance through reference-based evaluation.\n\nPrompt Design & Score Extraction\nICE relies on an LLM (we use the text-davinci-003 model of GPT-3) to make predictions. It takes in a prompt that consists of a small number of in-context examples, each of which consists of generated text and its corresponding quality score as a numeric string. The prompt ends with a test example, for which the model predicts a score (Figure 1 ).\nThe input contains the model-generated text (summary), in addition to which it might contain additional information such as the source text or references, depending on the dimension. To evaluate fluency and coherence, our prompts use in-context examples consisting of generated summaries and corresponding scores. For consistency and relevance, we use the source text and a reference summary respectively, in addition to the generated summary. We pass this prompt to a GPT-3 model, with sampling temperature set to 0 to elicit deterministic responses. We parse the model response-decoded numeric string-as the dimension score.\n\nSelection of In-context Examples\nBy default, we use 4 in-context examples in our prompts, as this is the largest number that fits within the context window of GPT-3. We experiment with two sampling procedures (Appendix B) to obtain 4 examples from a pool of examples:\n1. Uniform Random Sampling. We randomly select 4 summaries from the pool of examples. This causes the examples to follow the same distribution as the example pool. 2. Stratified Sampling. We bucket the range of scores, i.e. [0, 1], into 4 equal partitions and randomly sample one summary from each one.\nThis causes examples to be representative of the range of scores in the example pool.\nWe avoid using synthetically generated data (Kryscinski et al., 2020; Zhong et al., 2022) since the kind of errors made by generation models is often different from the errors present in the negative examples in these datasets (Goyal and Durrett, 2021) . We instead elect to use (a few) human evaluations of model-generated text in order to make the in-context examples as representative of real errors as possible. We do this by splitting the meta-evaluation dataset and using a partition as an in-context example pool, as described in Section 3.1. \n\nDatasets & Baselines\nWe use the SummEval dataset (Fabbri et al., 2020) 2 to meta-evaluate our evaluation framework. Sum-mEval collects human evaluation annotations for 16 summarization systems on 100 articles sampled from the CNN/DailyMail corpus, for a total of 1600 summary-level annotations. Each summary is evaluated on four dimensions described in Section 2.2.\nTo get a pool of in-context examples, we keep aside a small subset (64 examples) of the Sum-mEval dataset to pick in-context examples from, and use the rest (1536 examples) as the test set for meta-evaluation (evaluating the baselines on this same test set). Further details are in Appendix A.\nWe compare ICE to the following state-of-theart multi-dimensional evaluators: (1) CTC (Deng et al., 2021) uses information alignment between generated outputs and references or inputs; (2) BARTScore (Yuan et al., 2021) uses the conditional probability of a sequence given inputs or references; and (3) UniEval (Zhong et al., 2022) uses a question-answering framework (e.g. \"Is this a coherent summary?\") to calculate metrics.\nFollowing Liu et al. (2021) ; Zhong et al. ( 2022), we assess performance by computing summarylevel Spearman and Kendall-Tau correlations between predicted scores and human judgements.\n\nResults\nAs illustrated in Table 1 , ICE is competitive with fine-tuned baselines despite not requiring any finetuning. It achieves state-of-the-art correlation with human judgments for relevance and consistency. We perform pairwise significance tests and observe that ICE (uniform sampling) does better than UniEval on consistency and relevance on Kendall's Tau with a significance level of 0.05 (Appendix E). Additionally, the uniform sampling variant of ICE outperforms BARTScore (which also does not require finetuning) across dimensions.\nBetween the two sampling procedures for ICE, we observe that stratified sampling works marginally better for all dimensions other than consistency. Since summaries in the SummEval dataset have perfect or near-perfect human scores for consistency (Figure 2 ), uniform sampling causes in-context examples to also have nearperfect scores. This appears useful for the model to calibrate its scoring when evaluating consistency, leading to better performance. We explore this in greater detail in \u00a74.1. While the same reasoning could hold for fluency, we observe both here and in \u00a74.3 that fluency scores are quite stable. Given that fluency is an easier aspect to evaluate, this stability could be a result of the model possessing a strong notion about fluency from pre-training time that is not modified significantly as the distribution of in-context examples changes (Reynolds and McDonell, 2021) . Finally, we observe that the performance for coherence and relevance are similar regardless of the sampling procedure. This is because scores for these aspects are spread out in the dataset, which makes uniform and stratified sampling return similar in-context examples.\n\nAnalysis\nIn this section, we analyse the effects of our prompt engineering choices. The comparison between sampling procedures in Section 4.1 is performed on the entire test set but the experiments in Sections 4. domain regardless of the true distribution. This forces predictions towards a centered distribution, which can cause the performance drop we observe in Table 1 when evaluating consistency using stratified sampling. Uniform sampling, on the other hand, selects examples that represent the true distribution, making model predictions more closely reflect the true distribution.\nA drawback of uniform sampling is sub-optimal calibration in low-probability regions of the true distribution. For instance, if uniform sampling is used to evaluate consistency, the model might not see in-context examples with (say) scores less than 0.3 (Figure 2 ). This can affect output calibration in that region. Nonetheless, we suggest using uniform sampling in general. It is more stable and its prediction distribution closely follows the true distribution. For dimensions where it underperforms stratified sampling, the margins are less significant. Finally, even when ICE (uniform sampling) scores are calibrated differently from human scores, they still rank summary-quality correctly, insofar as our main results (Table 1) \n\nEffect of Selection of In-context Examples\nIn order to determine whether performance is robust to the choice of in-context examples, we evaluate our test set using three different random sets of in-context examples. We observe in Figure 3 that for a given dimension, the maximum variation across three seeds is about 7 points, suggesting reasonably stable performance across the choice of in-context examples.\n\nEffect of Number of In-context Examples\nWe evaluate our test set using different numbers of in-context examples (Figure 4 ). We observe that only for relevance and coherence does performance show improvement as we increase the number of examples. One reason for this could be the distribution of scores for a given dimension in the test set (Figure 2 ). Concretely, consistency and fluency mostly have near-perfect scores and therefore do not benefit from more samples while the scores for coherence and relevance are spread out and therefore more samples allow representation over the whole range of scores.\nAnother observation is that even for coherence and relevance, performance with a single incontext example reaches near that achieved by some of the weaker fine-tuned baselines in Table 1 . This suggests that the model possesses the notion of the evaluation task from pre-training itself, which is in line with recent work (Reynolds and McDonell, 2021; Min et al., 2022) that suggests that demonstrations help extract this knowledge.\nFinally, we note that calibration can potentially be improved by increasing the number of examples. For instance, we observed that the four incontext examples that the uniform sampling procedure chose for coherence in Figure 2 had scores that fall between 0.7 and 1.0. This concentrates the prediction distribution in that range. The probability of such an event will reduce as the number of examples is increased further.\n\nUsing ICE to Evaluate Zero-Shot Prompting Models\nRecent work by Goyal et al. (2022) showed that standard reference-based and reference-free metrics are not reliable in evaluating zero-shot summaries written by models such as GPT-3. Through a human study comparing summaries from three systems-GPT-3, BRIO, and T0-they observed that while humans prefer GPT-3 summaries, automatic evaluators consistently score GPT-3 summaries lower than summaries from other models.\nWe study the efficacy of ICE in evaluating zeroshot summaries written by GPT-3 at a dimension level. We use the set of 500 CNN articles from Goyal et al. (2022) , with summaries from GPT-3, BRIO, and T0 for each article. We sample 100 of these articles and have three annotators rate summaries for each of the dimensions defined in Section 2.2 on a scale of {1, 2, 3, 4, 5}. We use ICE, ROUGE, and BARTScore (all of which do not require training data) to evaluate the summaries and present system-level results in Table 2 .\nWe observe that ICE agrees with human judgments for each dimension and overall preferences while existing reference-based and reference-free metrics such as ROUGE and BARTScore 3 consistently rate GPT-3 summaries low. Goyal et al. (2022) suggest that most existing evaluation metrics reward summaries that imitate references, while GPT-3 summaries are zero-shot and not trained to imitate human-written references, which is likely why they are penalized by most existing evaluators. However, since ICE is not based on reference similarity (except when evaluating relevance) and is also not trained with reference summaries, it is able to better evaluate GPT-3 summaries and agrees with human preferences.\n\nConclusion\nWe show that in-context learning can be used for NLG evaluation as an alternative to fine-tuned evaluation metrics. Using a small number of examples, in-context learning evaluators can reach or exceed the state-of-the-art on multi-dimensional evaluation and that this is robust to the choice of in-context examples. Finally, we show that in-context learning evaluators align well with human judgements when evaluating summaries written by GPT-3.\n", "hypothesis": " Evaluation of natural language generation (NLG) is complex and multi-dimensional.  Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest.  Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets.  In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets.  Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency.  We then analyze the effects of factors such as the selection and number of incontext examples on performance.  Finally, we study the efficacy of in-context learningbased evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.  Our code is available at https: //github.com/JainSameer06/ICE.", "answer": true}
{"title": "Leveraging Denoised Abstract Meaning Representation for Grammatical Error Correction", "content": "\nIntroduction\nNowadays, high performance of grammatical error correction model mainly depends on data augmentation (Kiyono et al., 2019; Grundkiewicz et al., 2019; Raffel et al., 2020; Wan and Wan, 2021; Wu and Wu, 2022; Zhang et al., 2022) . According to the type of additional information, grammatical error correction models can be divided into dataenhanced models and knowledge-enhanced models. Data-enhanced models require millions of synthetic data, which is obtained by back-translation or directly adding noise. Training on these synthetic datasets is very time-consuming, which is unacceptable in some application scenarios. Knowledgeenhanced model is to artificially design a large number of grammatical rule templates, and add the templates as external knowledge to GEC model. This external knowledge is language-dependent and it requires the intervention of human grammar experts.\nAbstract Meaning Representation (AMR) is a type of rooted, labeled graph which contains semantic structures with fine-grained node and edge types. AMR breaks through the limitations of the traditional syntax tree structure and supports reentrancy. Figure 1 is a graph of sentence \"I don't want to go to school on Sunday.\". In AMR, :arg0 is typically the agent, :arg1 is typically the patient, and other arguments do not have standard definitions and may vary with the verb being annotated. Negative meaning is denoted as \"-\". Special keywords such as entity types, quantities and logical conjunctions are supported by AMR. AMR obtains a simple representation from natural language sentence and it is suitable for GEC as extra knowledge.\nA non-negligible concern is that AMRs of errorful sentences may not be exactly reliable. If these AMRs with errors are directly introduced into the GEC model as additional information, it may confuse the model. We use a pre-trained AMR parser to predict AMR of erroneous sentences and corrected sentences separately on the BEA-19 development set. If two AMRs are completely consistent, we assume that the AMR of errorful sentences is reliable. After statistical analysis, we found that about half of the graphs are reliable.\nWe designed a denoising semantic aggregated grammatical error correction model. Specifically, we added a graph aggregation encoder based on a sequence-to-sequence model. The graph encoder aims to update the representation of the sequence encoder by AMR semantic structure. Besides, we designed two mask strategies to reduce the dependence on the model graph information. We designed these mask strategies by granularity: node/edge level mask and subgraph level mask. Experiments have proved that the denoising semantic aggregated grammatical error correction model significantly improved the error correction accuracy.\n\nRelated works\nData-enhanced GEC models. Lots of works have found their way to incorporating additional data into GEC model. Kaneko et al. (2020) uses a pretrained mask language model in grammatical error correction by using the output of BERT as additional features in the GEC model. Kiyono et al. (2019) and Grundkiewicz et al. (2019) explore methods of how to generate and use the synthetic data and make use of Gigaword to construct hundreds of millions of parallel sentence pairs. Some works (Katsumata and Komachi, 2020 , Pajak and Gonczarek, 2021 , Rothe et al., 2021) give a strong baseline by finetuning BART (Lewis et al., 2020) , T5 (Raffel et al., 2020) \n\nModel\nWe add a graph encoder based on Transformer to aggregate denoised semantic information. The architecture of AMR-GEC is shown on Figure 2 . \n\nSemantic Aggregated Encoder\nTransformer is an attention-based encoder-decoder model, where the encoder encodes the input sentence into a context vector, and the decoder converts the context vector into an output sentence. Formally, we denote the tokens of the sentence is T n = {t 1 , t 2 , ..., t n }. Vinilla encoder-decoder model works as follows:\nh 1 , h 2 , ..., h n = Enc(t 1 , t 2 , ..., t n ) (1)\ny 1 , y 2 , ..., y m = Dec(h 1 , h 2 , ..., h n ) (2)\nWe then designed a semantic graph encoder based on a graph attention network to incorporate semantic graph information. To preserve the information of the sequence encoder, we use a residual connection to combine the outputs of two encoders.\n\u01771 , \u01772 , ..., \u0177m = GNN(h 1 , h 2 , ..., h n ) (3) y \u2032 i = y i \u2295 \u0177i , i = 1, 2, ..., m (4)\n\nDenoising Function\nMasked Language Modeling (MLM) is a classic pre-trained model modeling method. The task of MLM is to mask some tokens with a special token mask and train the model to recover them. This allows the model to handle both the left and right context of the masked token. MLM can divided into five types: single word masking, phrase making, random span masking, entity masking, whole word masking.\nReferring to Bai et al. (2022) , we use the mask strategy on AMR. We used two ways to add masks: node/edge level mask and sub-graph level mask. Node/edge level mask refers to mapping the nodes/edges in the AMR graph using a noise function to generate a graph with noise. Sub-graph level mask means randomly removing subgraphs and replacing them with a mask label.\n\nSequence-AMR Graph Construction\nIn this section, we will show details about the graph encoder module. To preserve sequence information, we design a graph that fuses sequence and AMR. We first use the alignment tool JAMR to get the mapping from AMR node to sequence token. First connect the sequences through the special labels forward-label and backward-label respectively, and then map the edges of AMR to the sequence-AMR graph. AddEdge(x i , x i+1 , label-forward) (Dahlmeier and Ng, 2012) which computes a span-based F 0.5 -score. BEA-2019. The BEA-2019 test set consists of 4477 sentences and the outputs are scored via ER-RANT toolkit (Felice et al., 2016 , Bryant et al., 2017) . The released data are collected from Write & Improve and LOCNESS dataset.\n\nBaseline Model\nFollowing Rothe et al. (2021) , we use T5 as the baseline model for GEC.\n\nAMR Parsing and Alignment\nWe adopt SPRING (Bevilacqua et al., 2021) as our AMR parsing model. SPRING performs nearly state-of-the-art AMR parsing by linearizing AMR to sequence and converting text-to-amr task to seqto-seq task. It obtained 84.5 Smatch F1 points on AMR 2.0 dataset.We use JAMR (Flanigan et al., 2014) to align the AMRs to sentences. JAMR is an alignment-based AMR parsing model that finds a maximum spanning, connected subgraph as an optimization problem. We use the alignment for graph information aggregation.\n\nOthers\nOur models were trained on a single GPU (GeForce GTX 1080), and our implementation was based on publicly available code 1 . we set the batch_size to 6 and the learning_rate to 2e-5. We use py-torch_geometric 2 to implement the semantic aggregated encoder.\n\nResults\nTable 1 shows the results of the BEA-test and CoNLL-2014 dataset. 1) Compared with the model without synthetic data, the single model of AMR-GEC is 2.8 points and 1.8 points higher in BEA-19 and CoNLL-14, respectively. Ensemble models give similar results. 2) Compared with models using synthetic data, AMR-GEC gives com- parable or even higher F-score, except for GEC-ToR (Omelianchuk et al., 2020) , which uses both synthetic data and human knowledge. For example, our single model achieves 68.4 on BEA-19, higher than the models by Kiyono et al. (2019) , Kaneko et al. (2020) , and Rothe et al. (2021) . This shows that semantic graphs, as additional knowledge for GEC, have a comparative advantage over synthetic data. Our ensemble model does not show significant improvements over the single model, probably because more optimal ensemble strategies are needed: averaging generation probabilities (Omelianchuk et al., 2020) , ensemble editings (Pajak and Gonczarek, 2021), etc. We compared the most common error types in BEA-test (except for OTHER) between T5-GEC and AMR-GEC. As shown in Table 2 , the F-scores of PUNCT and PREP in AMR-GEC is 4-6 points higher than T5-GEC. AMR dropped prepositions, tense, and punctuation to obtain simple and base meanings, and exactly these error types are the most common errors in GEC scenarios. With such error ignored in AMR, sentences generated from AMR are more likely to get correct results.\n\nAdvantages of AMR\nBesides, graphs are good at solving the long sentence dependency problem. The pain point of the sequence model is that it is difficult to pay attention to long-distance dependent information. In AMR, associative concept nodes are explicitly connected with edges, making it easier for the model to focus on long-distance information.\n6 Ablation Study\n\nGraph Neural Networks Ablation Results\nGraph neural networks have been proven effective in dealing with unstructured data problems. However, few studies have analyzed the effect of different GNN-encoded AMRs for natural language generation tasks. To study the differences of graph neural networks of encoding AMR, we carry on a set of experiments. We select different graph encoders of GCN, GAT, and DeepGCN as variables, and conduct experiments on BEA-2019 dataset while ensuring the same amount of model parameters. We do not use the denoising method in this ablation study. Table 3 shows the results of BEA-test with different graph encoders. We can draw these conclusions: 1) Even if the AMRs of the errorful sentences are not reliable, they still benefit GEC. Compared with T5-GEC, AMR-GCN and AMR-GAT are about 0.2 and 0.4 points higher respectively. This shows that the model makes use of the semantic information and connection relationship of reliable AMR. 2) AMR-GCN gives the best performance among the three models. When picking a graph encoder, the GCN model is sufficient to encode the semantic structure information of AMR. It is worth noting that GAT and DeepGCN have high recall value and low precision. In the grammatical error correction task, precision measures the error correction result. Generally speaking, precision is more important than recall. In the grammatical error correction task, most of the errors are local errors, and the semantic information required for grammatical error correction in AMR can be captured without a deeper graph convolution model. Table 4 shows the results of BEA-test with node/edge and subgraph denoising methods. The node/edge level denoising strategy and the subgraph level denoising strategy increased by 1.57 and 1.03 points, respectively. Node level mask strategy performs better because the subgraph may mask too much information.\n\nConclusion\nIn this paper, We propose a denoising semantic aggregated grammatical error correction model, AMR-GEC, leveraging AMR as external knowledge to the GEC. We believe it gives a strong baseline for incorporating AMR in GEC.\n", "hypothesis": " Grammatical Error Correction (GEC) is the task of correcting errorful sentences into grammatically correct, semantically consistent, and coherent sentences.  Popular GEC models either use large-scale synthetic corpora or use a large number of human-designed rules.  The former is costly to train, while the latter requires quite a lot of human expertise.  In recent years, AMR, a semantic representation framework, has been widely used by many natural language tasks due to its completeness and flexibility.  A non-negligible concern is that AMRs of grammatically incorrect sentences may not be exactly reliable. In this paper, we propose the AMR-GEC, a seq-to-seq model that incorporates unreliable AMR as additional knowledge.  Specifically, We design a semantic aggregated GEC model and explore methods to maintain the errors in the AMRs.  Experiments on the BEA-2019 shared task and the CoNLL-2014 shared task have shown that AMR-GEC performs comparably to a set of strong baselines with a large number of synthetic data.  Compared with the T5 model with synthetic data, AMR-GEC can reduce the training time by 32% while inference time is comparable.  To the best of our knowledge, we are the first to incorporate AMR for grammatical error correction..", "answer": false}
{"title": "Putting Natural in Natural Language Processing", "content": "\nIntroduction\nThe ACL 2023 theme track urges the community to check the reality of the progress in NLP. This position paper adopts an expansive interpretation of this question. It is definitely worth inquiring into the apparent advances of current NLP in their own terms. Here, however, I question these terms and argue that our field has focused on only a limited subset of human language which happens to be convenient to work with, and thus misses major aspects of human communication.\n\nHuman Language is Primarily Spoken\nHumans are an exceptional species in many ways, and out of these, human language is one of the most salient. Unlike communication systems used by other organisms, human language is open-ended, capable of expressing abstract concepts, and of reference to events displaced in time and space. While the capacity to acquire language is universal and largely innate (Darwin, 1874; Pinker and Bloom, 1990) it also is culturally mediated and likely arose via gene-culture co-evolution (Deacon, 1998; Richerson and Boyd, 2010) .\nOne revolutionary technology which turbocharged human language was writing, which was invented a handful of times in the most recent few thousand years of the human story (Fischer, 2003) . Writing, followed by the printing press, followed by the Internet, have made written text ubiquitous to the extent that it is easy to forget that the primary and universal modality for most human communication throughout history has been spoken. 1 Even today many of the world's languages do not have a standardized written form. For those that do, the written modality originated as a compressed, symbolic representation of the spoken form.\nChildren acquire a spoken language (and not infrequently two or more) within the first few years of their life with no or little explicit instruction, largely relying on weak, noisy supervision via social interaction and perceptual grounding. In contrast, they require hundreds of hours of explicit instruction and arduous conscious practice to learn to read and write, and most are only able to learn the written modality a couple of years at best after becoming fluent communicators in one or more spoken languages.\n\nReality check\nThus, arguably, the natural language for which we are biologically equipped is spoken. Written language is a secondary development, which happens to be very useful and widespread, but is nevertheless derivative of speech. This appears to be the consensus view in linguistics going back at least a century (de Saussure, 1916; Bloomfield, 1933). 2 Given these facts, is then the field of Natural Language Processing (NLP) a misnomer? Are we making less progress with getting machines to communicate via human language than current advances with processing written text would have us believe?\n\nNLP is Written Language Processing\nTo anyone with experience reading, reviewing and publishing papers in NLP conferences and journals (such the ACL conferences and TACL) it is evident that the field is very strongly focused on processing written language. While this is evident to practitioners, it is also largely tacit and implicit.\n\nUnstated assumptions\nThe fact that a paper is concerned with written as opposed to spoken oral or sign language is almost invariably assumed to be the default and not explicitly stated. Furthermore, even if there is some interest in tackling a dataset of originally spoken language (for example in much work on dialog and child language acquisition), the usual approach is to use a written transcription of this data rather than the actual audio. This is partly a matter of convenience, but partly due to the assumption that the written form of language is the canonical one while the audio modality is just a weird, cumbersome encoding of it.\nTo some extent such an implicit belief also lurks in much work within the speech community: the main thrust of speech research has always been on so called Automatic Speech Recognition (ASR), by which is meant automatically transcribing spoken language into a written form. Written text is treated as an interface and an abstraction barrier between the field of speech processing and NLP. In Sections 3 and 4 I address problems arising from the above assumptions, as well as the challenges and opportunities we have once we discard them. Firstly, however, it will be instructive to briefly quantify the assertion that NLP is Written Language Processing. by looking at historical publication patterns. \n\nPublication patterns\nFigure 1 shows the proportion of NLP papers explicitly mentioning speech-related terms in their title over the years covered by the ACL anthology (1950 through 2022), which is a comprehensive database of NLP papers from a wide variety of relevant conferences, workshops and journals. 3 The fraction of speech-focused NLP papers varies quite a bit over the years, but mostly stays below 10%.\nThere is a large peak going to 20% in 1989, followed by three years with around 10% of speech papers. A look at the underlying data reveals that the 1989 peak is associated with the inclusion in the anthology of the proceedings of the Speech and Natural Language Workshop (Hirshman, 1989) organized by the US Defense Advanced Research Projects Agency (DARPA), and featuring 79 papers. This workshop ran until 1992 and is thus largely responsible for the four-year run of sizable representation of spoken language research in the ACL anthology.\nThe overview of the last edition of this event notes the then ongoing \"paradigm shift in natural language processing towards empirical, corpus based methods\" (Marcus, 1992). It is likely that this shift in NLP methodology was at least partly driven by this workshop, the associated DARPA program, and the resulting increased interaction between researchers working on spoken and written language.\nIn recent years (since 2010) the proportion of NLP papers explicitly mentioning spoken language has resolutely stayed below 6%. While the major ACL events typically include speech processing as a topic in their calls for papers, as well as a track including the term speech in its name, such as Speech and Multimodality, processing of spoken language it clearly a rather minor concern of these conferences. Instead, speech work is published in different venues organized by a separate speech processing community.\n\nSpoken Language is Richer\nWhile the primacy of the spoken modality as means of communication is the consensus view in linguistics, Section 2.1 identifies unstated assumptions among NLP practitioners which amount to the opposite view. Here I outline why these assumptions contradicting the scientific view are not only incorrect but also detrimental to progress on understanding and processing real human language.\n\nKey features of spoken language\nSpeech and writing are two different modalities with different affordances, and there is no straightforward mapping between them. Some writing systems such as those used for English, Arabic or Chinese do not even represent the phonology of the spoken language in a direct way. More crucially, writing only captures a small proportion of the information carried in the equivalent audio signal. Writing discards most of the information falling within the general category of paralinguistic phenomena, such as that related to speaker identity, speaker emotional state and attitude; likewise, information conveyed by speech tempo and amplitude, including most of suprasegmental phonology such as intonation and rhythm is typically not present in writing. In addition to the auditory signal, oral spoken language can also feature visual clues in the form of accompanying gestures, facial expressions and body posture. Sign languages rely on the visual channel exclusively, and in fact there are no widely used writing systems for any of them (Grushkin, 2017) . Unlike most text, speech also typically contains a variable amount of channel noise (Shannon, 1948) such as environmental sounds.\nNatural spontaneous speech contains fillers, hesitations, false starts, repairs and other disfluencies (Dinkar et al., 2023) which are usually edited out in the written form of language. Even more critically, spontaneous speech typically takes the form of a dialog between two or more participants. Dialog is unlike common written genres: crucially it features turn-taking behavior which is governed by com-plex and incompletely understood rules (Skantze, 2021) . These features of natural dialog also mean that the traditional cascaded approach of ASR followed by NLP faces serious limitations, not least due to low ASR performance in this regime (Szyma\u0144ski et al., 2020) , but also due to its inherently interactive nature.\nFor all these reasons, spoken language is more informationally rich than written language; 4 the same factors also make it more variable, complex and noisy, and consequently more challenging for automated processing (Shriberg, 2005) . Thus any understanding of language as a human faculty gained via the written modality does not necessarily generalize to the spoken modality. The same is also the case about language applications: for example the successes and shortcomings of state-of-the-art text chatbot systems (e.g. Stiennon et al., 2020) are likely to be substantially different from those of spoken dialog systems.\n\nChallenges of speech\nAs an illustrative example, let us consider the effectiveness of self-supervision: inducing representations of words and phrases from just listening to speech or reading text. For text, this general family of methods has been successful since around the time of Latent Semantic Analysis (Dumais, 2004) , and currently large written language models exhibit a constantly expanding range of abilities (Wei et al.) . In contrast, self-supervision with spoken language has met with a limited amount of success only in the last few years (e.g. Baevski et al., 2020; Hsu et al., 2021) , and these models as of now are usually only fine-tuned on the task of ASR. One obvious difference is that items such as words and morphemes are either explicitly delimited or easily discovered in text, but finding them is an unsolved research problem in speech, due to the inherent variability of this modality.\nOn the other hand, learning spoken language becomes much more tractable when self-supervision is augmented with grounding in perception. The cross-modal correlations, though unreliable and noisy, are often sufficient to substantially facilitate the discovery and representation of words (Peng and Harwath, 2022; Nikolaus et al., 2022) and syllables (Peng et al., 2023) in spoken language. For written language, grounding in the visual modality has also been found to help in some cases (e.g. Tan and Bansal, 2020) but it does not appear crucial, as the dominance of text-only language models demonstrates.\nSince spoken language is richer in information content, it should in principle be possible to exploit this extra signal for improving performance. One obstacle to such developments is the increased variability and channel noise. Perhaps less obviously, a second obstacle is that widely used benchmarks are often designed in a way which obstructs obtaining such gains. For example the 2021 Zerospeech challenge (Dunbar et al., 2021) which aimed to benchmark spoken language modeling, evaluates systems according to the following criteria: phoneme discrimination, word recognition, syntactic acceptability and correlation to human judgments of word similarities. None of these metrics would benefit much from modeling speaker characteristics, speech tempo, pitch, loudness or even suprasegmental phonology. Except for the first one, these metrics would be very well suited for models trained exclusively on written language. The combined effect of these two obstacles was evident in the results of Zerospeech 2021 where written-language toplines, such as RoBERTa (Liu et al., 2019) , outperformed spoken language models on the latter three metrics, often by large margins.\n\nUnifying Speech Processing and NLP\nAs evident from the examples highlighted above, spoken language is in some ways quite different from written language and presents a distinct set of challenges and potentials. In order to understand how much progress the fields of speech and NLP are making in understanding and implementing human language, we need to take speech seriously qua language, not just a cumbersome modality, and measure our progress accordingly.\n\nConverging methodology\nThe time is ripe for a closer integration of the speech and NLP communities and for a unified computational science of language. The set of methodologies used in speech and text processing used to be quite distinct in the past. Since the adoption of deep learning both fields have converged to a large extent: currently the state-of-the-art models for both spoken and written language rely on transformer architectures (Vaswani et al., 2017) self-trained on large amounts of minimally preprocessed data, with optional fine-tuning. The technical communication barriers across disciplinary boundaries are thus much lower. The recent emergence of the concept of textless NLP (Lakhotia et al., 2021) exemplifies the potential of unifying these two fields.\n\nOpportunities\nThe following paragraphs outline the most important benefits of making NLP more natural, ranging from basic science to practical applications.\nModeling language acquisition. An increased attention to spoken language within NLP has the potential to lead to a more realistic understanding of how well our current methods can replicate key human language abilities. Acquiring language under constraints that human babies face is the big one. There is a large amount of work on modeling human language acquisition which uses exclusively written data (at best transcribed from the original audio). Hopefully by this point the reader will be convinced that the relevance of this work to the actual issue under consideration is highly questionable. We stand a much better chance of figuring out human language acquisition if we refocus attention on spoken language. Data efficiency. Linzen (2020) argues convincingly for language models which are human-like in their data-efficiency and generalization capabilities. It is, however, unclear whether these properties can even be properly evaluated via the medium of written language. Since the informational density and the signal-to-noise ratio in written vs spoken language are so very different, it makes little sense to compare human children with language models trained on text. Furthermore, the challenges of pure self-supervision may motivate us to take seriously the impact of grounding in perception and interaction, which humans use universally as a learning signal.\nUnwritten languages. Many modes of human communication lack standard written representation. These range from major languages spoken by millions of people such as Hokkien (Mair, 2003) , to small or non-standard language varieties, to sign languages. Shifting the emphasis of NLP research from text to the primary, natural oral and gestural modalities will benefit the communities using these varieties.\nSpoken dialog systems. Dingemanse and Liesenfeld (2022) argue that language technology needs to transition from the text to talk, and provide a roadmap of how to harness conversational corpora in diverse languages to effect such a transition. Indeed, one of the most obvious benefits of spoken language NLP would be dialog systems that do not need to rely on ASR and are able to exploit the extra information lost when transcribing speech, enabling them to understand humans better and interact with them in a more natural way.\nNon-textual language data. Finally, there is a large and increasing stream of non-textual language data such as podcasts, audio chat channels and video clips. Processing such content could also benefit from an end-to-end holistic treatment without the need of going through the lossy conversion to text.\n\nRecommendations\nIf you are an NLP practitioner and view spoken language as outside the scope of your field, reconsider. Getting into speech processing does require understanding its specifics, but it is not as technically daunting as it used to. Conversely, if you are a speech researcher, consider that ASR and text-tospeech is not all there is: we can get from sound to meaning and back without going through the written word. Both fields would do well to consider the whole of human language as their purview. Increased collaboration would benefit both communities, and more importantly, would give us a chance of making real progress towards understanding and simulating natural language.\n\nLimitations\nThe main limitation of this paper is the one applying to any opinion piece: it is subjective and personal, as the views of the authors are inherently limited by their expertise and experience. More specifically, this paper argues for an increased interaction between the speech and NLP communities, but the author is more strongly embedded in the latter, and thus addresses this audience primarily. Additionally, the short paper format imposes significant constraints on the amount of nuance, detail and discussion of relevant literature, and thus readers may find some of the claims to be less strongly supported and less hedged than would be ideal, or proper in a longer treatment of this topic.\n\n\nI am using spoken language in the broad sense here, including both the oral and gestural (signed) modes of expression, and opposing these to the written modality.\nHowever seeAaron and Joshi (2006) for a dissenting view.3 https://aclanthology.org/\nOne exception to this general pattern is the presence of two spatial dimensions in written language, and the role of 2D layout in textual publications.\n", "hypothesis": " Human language is firstly spoken and only secondarily written.  Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous.  Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language.  Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text.  Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP.  Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication.  Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality..", "answer": true}
{"title": "Evaluating Paraphrastic Robustness in Textual Entailment Models", "content": "\nIntroduction\nRecognizing Textual Entailment (RTE), the task of predicting whether one sentence (hypothesis) would likely be implied by another (premise), is central to natural language understanding (NLU; Dagan et al., 2005) , as this task captures \"all manners of linguistic phenomena and broad variability of semantic expression\" (MacCartney, 2009) . If an RTE model has a sufficiently high capacity for reliable, robust inference necessary for full NLU (Mac-Cartney, 2009) , then the model's predictions should be consistent across paraphrased examples.\nWe introduce P aRT E, a test set to evaluate how reliable and robust models are to paraphrases (Table 1 includes an example). The test set consists of examples from the Pascal RTE1-3 challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) rewritten with a lexical rewriter and manually verified to preserve the meaning and label of the original RTE sentence-pair. We use this evaluation set to determine whether models change their predictions when examples are paraphrased.\nWhile this may not be a sufficient test to determine whether RTE models fully understand language, as there are many semantic phenomena that RTE models should capture (Cooper et al., 1996; Naik et al., 2018) , it is necessary that any NLU system be robust to paraphrases.\n\nP\nThe cost of security when world leaders gather near Auchterarder for next year 's G8 summit, is expected to top $150 million. P' The cost of security when world leaders meet for the G8 summit near Auchterarder next year will top $150 million.\n\nH\nMore than $150 million will be probably spent for security at next year's G8 summit. H' At the G8 summit next year more than $150 million will likely be spent on security at the event.\nTable 1 : An original and paraphrased RTE example.\nThe top represents an original premise (P) and its paraphrase (P'). The bottom depicts an original hypothesis (H) and its paraphrase (H'). A model robust to paraphrases should have consistent predictions across the following pairs: P-H, P'-H, P-H', and P'-H'.\nOur experiments indicate that contemporary models are robust to paraphrases as their predictions do not change on the overwhelmingly large majority of examples that are paraphrased. However, our analyses temper this claim as models are more likely to change their predictions when both the premise and hypothesis are phrased compared to when just one of the sentences is rewritten. We release P aRT E 1 to encourage others to evaluate how well their models perform when RTE examples are paraphrased.\n\nRelated Work\nWith the vast adoption of human language technology (HLT), systems must understand when different expressions convey the same meaning (paraphrase) and support the same inferences (entailment). Paraphrasing and entailment are closely connected as the former is a special case of the latter where two sentences entail each other (Nev\u011b\u0159ilov\u00e1, 2014; Fonseca and Alu\u00edsio, 2015; V\u00edta, 2015; Ravichander et al., 2022) . Para-phrasing has been used to improve RTE predictions (Bosma and Callison-Burch, 2006; Sun et al., 2021) and RTE has been used for paraphrase identification (Seethamol and Manju, 2017) and generation (Arora et al., 2022) . Furthermore, both phenomena are key to NLU (Androutsopoulos and Malakasiotis, 2010) and work such as Zhao et al. (2018) ; Hu et al. (2019) have explored rewriting RTE examples to create more robust models.\nWe follow a long tradition of evaluating linguistic phenomena captured in RTE models (Cooper et al., 1996) . Recent tests focus on evaluating how well contemporary RTE models capture phenomena such as monotonicity (Yanaka et al., 2019a,b) , verb veridicality (Ross and Pavlick, 2019; Yanaka et al., 2021) , presuppositions (Parrish et al., 2021) implicatures (Jeretic et al., 2020) , basic logic (Richardson et al., 2020; Shi et al., 2021) , figurative language (Chakrabarty et al., 2021) , and others (Naik et al., 2018; Poliak et al., 2018a; Vashishtha et al., 2020) . Unlike many of those works that evaluate models' accuracy on examples that target specific phenomena, we use a contrastive approach (Prabhakaran et al., 2019; Gardner et al., 2020) to determine whether RTE models' predictions change when examples are paraphrased.\n\nP aRT E\nTo explore whether these RTE models are robust to paraphrases, we create P aRT E, a modified version of the Pascal RTE1-3 challenges (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007) . P aRT E contains 1,126 examples of an original unmodified RTE sentence-pair grouped with a sentence-pair with a modified premise, hypothesis, or both. We use the examples in RTE1-3 to create our test set, as opposed to other RTE datasets due to its long-standing history.\n\nParaphrase Generation & Verification\nFor each RTE premise-hypothesis pair (P-H), we created three paraphrased premises (P') and hypotheses (H') using a T5-based paraphraser 2 finetuned on the Google PAWS dataset (Zhang et al., 2019) . To ensure lexically diverse paraphrases, we filter out any paraphrases that have high lexical overlap with the original sentences using Jaccard index threshold of 0.75. Out of 14,400 generated sentences, 2,449 remained -956 paraphrased premises (P') and 1,493 paraphrased hypotheses (H'). Next, we retained 550 paraphrased premises and 800 paraphrased hypotheses paraphrases that crowdsource workers identified as grammatical and similar in meaning to the original sentences. 3 We include a grammatical check since an existing RTE evaluation set focused on paraphrases (White et al., 2017) contains hypothesis-only biases related to grammaticality (Poliak et al., 2018b) .\nIf at least one P' or one H' passes this filtering process, we retain the original RTE example and pair it with a corresponding paraphrased example (i.e. P'-H', P'-H, or P-H'). In the case where more than one P' or H' passes the filtering, we retained the P' or H' that crowdsource workers deemed most similar to the original sentence. Out of the original 2,400 RTE test pairs, we retain 914 pairs with a high-quality P' or H', resulting in 1,178 original and paraphrased RTE pairs. 4\n\nOvercoming Semantic Variability\nMacCartney (2009) argues that in addition to being reliable and robust, RTE models must deal with the broad variability of semantic expression. In other words, though two sentences may be semantically congruent, it is possible that small variations in a paraphrased sentence contain enough semantic variability to change what would likely, or not likely be inferred from the sentence. Despite all P' and H' being deemed to be semantically congruent with their corresponding original sentences, the semantic variability of paraphrases might change whether H or H' can be inferred from P' or P.\nTherefore, propagating an RTE label from an original sentence pair to a modified sentence pair might be inappropriate. We manually determined that this issue occurs in just 52 (4%) examples, and retained 1,126 examples. This ensures an evaluation set of high-quality examples that can be used to determine whether models are sensitive to paraphrases and change their prediction on paraphrased examples. Our dataset contains 402 examples with just a paraphrased premise P', 602 with just a paraphrased hypothesis H', and 122 with both a paraphrased premise and hypothesis. \n\nExperimental Setup\nWe explore models built upon three different classes of sentence encoders: bag of words (BoW), LSTMs, and Transformers. Our BoW model represents premises and hypotheses as an average of their tokens' 300 dimensional GloVe embeddings (Pennington et al., 2014b) . The concatenation of these representations is fed to an MLP with two hidden layers. For the BiLSTM model, we represent tokens with GloVe embeddings, extract sentence representations using max-pooling, and pass concatenated sentence representations to an MLP with two hidden layers.\nOur transformer-based models are pre-trained BERT (Devlin et al., 2019) and Roberta (Liu et al., 2020) encoders with an MLP attached to the final layer. Additionally, we use GPT-3 in a zero-shot setting where we ask it to label the relationship between a premise and hypothesis. 5 The RTE training sets do not contain enough examples to train deep learning models with a large number of parameters. We follow the common practice of training models on MNLI and using our test set to evaluate how well they capture a specific phenomenon related to NLU. During testing, we map the MNLI 'contradiction' and 'neutral' labels to the 'not-entailed' label in RTE, following common practice (Wang et al., 2018; Yin et al., 2019; Ma et al., 2021; Utama et al., 2022, inter ailia) .\n\nResults\nTable 2 report the results. The RTE and P aRT E columns respectively report the models' accuracy on the 1,126 unmodified and paraphrased sentence pairs. 6 Comparing the difference in accuracy be-5 See Appendix A for more details, including hyperparameters, model sizes, and GPT-3 prompt design and configurations. Our code is available at https://github.com/ stonybrooknlp/parte 6 Although there are just 914 unmodified sentence pairs, for the sake of a head-to-head comparison, we retain all instances tween unmodified and paraphrased examples can be misleading. If the number of times a model changes a correct prediction is close to the number of times it changes an incorrect prediction, then the accuracy will hardly change. Figure 1 demonstrates why the accuracies do not change by much when models' predictions change on paraphrased examples. Furthermore, if a model is robust to paraphrases, then it should not change its predictions when an example is paraphrased, even if the prediction on the original unmodified example was incorrect. Hence, our test statistic is the percentage of examples where a model's predictions change (% \u2206 P aRT E column in Table 2 ) rather than a change in accuracy. Compared to the Transformer based models, the BoW and BiLSTM models seem to be more sensitive, and less robust to paraphrasing, as they change their predictions on 15.27% and 16.69% respectively of the 1,126 examples. However, this might be associated with how word xembedding models only just outperform random guesses in and perform much worse on RTE compared to the Transformer models.\nof the unmodified sentence pairs when computing accuracy. Focusing on the Transformer models, we noticed that RoBERTa performs the best on the datasets and is the most robust to paraphrasing -changing its predictions on just under 8% of paraphrased examples. Interestingly, when the models are trained specifically to perform this task, the models change their predictions on fewer paraphrased examples as these models' accuracy increases. However, improving performance alone might not automatically improve models' robustness to paraphrases. GPT-3's accuracy noticeably outperforms BERT's accuracy, but GPT-3 changes its predictions on more paraphrased examples compared to BERT. P'-H' compared to P-H' or P'-H Figure 2 shows noticeable increases in the percentage of changed predictions when both premise and hypothesis are paraphrased compared to when just one of the sentences is paraphrased. Specifically, for BoW and BiLSTM we see an increase of 4.01 and 6.01 percentage points respectively, and for BERT, Roberta, GPT-3 increases of 4.97, 4.83, and 3.55. As the transformer-based models changed their predictions on 12-14% of examples where both sentences are paraphrased compared to 9-11% in general, this analysis further suggests that these models are not as robust to paraprhases as desired.\nEntailed vs Not-entailed examples RTE analyses often differentiate how models perform on entailed vs not entailed examples (Liu et al., 2022) . In Figure 3 , we do not see meaningful differences in how models' predictions change on paraphrased examples based on the gold label. This might suggest that our dataset does not contain statistical irregularities based on the RTE labels. Correct vs Not-Correct Predictions Figure 4 shows that the Transformer models' predictions is more likely to change when it's prediction on an original example was incorrect (right red bars) compared to when the prediction for an original example was correct (left blue bars). For example, when RoBERTa's prediction for an original RTE example was correct, the model changed its prediction on just 5.5% of the corresponding paraphrased examples. When RoBERTa's predictions for an original RTE example were incorrect, RoBERTa's predictions changed for 20.88% corresponding paraphrased examples. Analyzing differences in models' confidences assigned to predictions might provide more insight (Marc\u00e9 and Poliak, 2022) . We leave this for future work.\nSource Task RTE1-3 examples originated from multiple domains and downstream tasks, e.g. question-answering (Moldovan et al., 2006) , information extraction (Grishman and Sundheim, 1996) , and summarization (Evans et al., 2004; Radev et al., 2001) . This enables researchers to evaluate how \n\nConclusion\nWe introduced P aRT E, a high-quality evaluation set of RTE examples paired with paraphrased RTE examples. We use our evaluation set to determine whether RTE models are robust to paraphrased examples. Our experiments indicate that while these models predictions are usually consistent when RTE examples are paraphrased, there is still room for improvement as models remain sensitive to changes in input (Jia and Liang, 2017; Belinkov and Bisk, 2018; Iyyer et al., 2018) . We hope that researchers will use P aRT E to evaluate how well their NLU systems perform on paraphrased data.\n", "hypothesis": " We present P aRT E, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing.  We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning.  We use the evaluation set to determine if RTE models' predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16% of paraphrased examples, indicating that there is still room for improvement. However, with the introduction of P aRT E, a test set that evaluates models' reliability and robustness to paraphrases, we can determine whether models change their predictions when examples are paraphrased, leading to more accurate NLU systems.", "answer": false}
{"title": "FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models", "content": "\nIntroduction\nEffective communication in natural language requires a shared base of knowledge between interlocutors. While this shared knowledge between communicators may be specific to individuals in a shared situation (e.g., Dhruv and Mei know they are sitting at a table in a restaurant), or to individuals with specialized knowledge (they are discussing backpropagation), some types of knowledge are sufficiently generic to be shared by most people in the world (e.g., objects fall when they are dropped). This latter category of commonsense knowledge has for decades been a holy grail of research in artificial intelligence and natural language understanding (McCarthy, 1959) . If machines are to understand (and produce) human language competently, they must at a bare minimum share this commonsense knowledge with humans.\nA question elided by this notion of commonsense knowledge is who counts as \"most people\"? What may appear as universal \"common sense\" to AI researchers in one cultural context may in fact not be so universal. Early efforts to schematize commonsense knowledge as scripts, or stereotyped sequences of events, provide a nice illustration of such unintended cultural biases: the famous \"Restaurant script\" (Schank and Abelson, 1975) prototypically includes a LEAVE TIP event, though tipping is not customary at restaurants in many countries outside the United States.\nMore recent AI research on commonsense knowledge acquisition has relied on crowd sourcing (Regneri et al., 2010; Sap et al., 2019) , corpus statistics (Lin and Pantel, 2001; Van Durme and Schubert, 2008) , and language modeling (Rudinger et al., 2015; Liu et al., 2022) in place of expertcrafted knowledge. However, each of these methods carries the potential to encode cultural bias into data and models for commonsense reasoning, whether through the implicit cultural perspectives of corpus texts, crowd source workers, or AI researchers themselves.\nIn this work, we seek to investigate cultural biases or assumptions present in commonsense reasoning systems. Culture, like commonsense knowledge, is vast. By one definition, 1 culture \"encompasses the social behaviour and norms found in human societies, as well as the knowledge, beliefs, arts, laws, customs, capabilities, and habits of the individuals in these groups.\" From the social sciences, Kendall (2015) defines culture as encom-passing both material as well as non-material aspects, such as beliefs and linguistic practices. To limit the scope of our investigation, however, we focus on a single topic common to all human cultures but widely varying across them: food.\nWe introduce FORK (Food ORiented cultural commonsense Knowledge), a manually-curated set of CommonsenseQA-style (Talmor et al., 2019) test questions for probing culinary cultural biases and assumptions present in commonsense reasoning systems. For the purpose of this work, we say that a commonsense question-answering system is culturally biased if (1) in response to questions with culturally-dependent answers, it exhibits systematic preference for answers consistent with one cultural setting over others; or (2) for questions with explicit cultural contexts, it exhibits systematically higher accuracy for some cultural contexts over others. Figure 1 contains an example of three interrelated test questions in FORK we use to detect cultural bias. For Q1, a model that prefers A1 to A2 exhibits cultural bias in favor of the United States (US) over China. While there exists no tidy mapping between human cultures and countries, in this work, we use countries as a coarse-grained proxy for culture (see: \u00a7 7).\nFORK contains questions pertaining to the food and culinary cultures of the US, China, Japan, and India with questions spanning topics of restaurant tipping, eating utensils, and other culinary customs. We test multiple encoder-based CommonsenseQA models on FORK, demonstrating systematic cultural biases favoring the US over non-US countries.\nTo summarize, our contributions are: 1. FORK: a \"bite-sized\" manually curated test set of 184 CommonsenseQA-style questions which can be used for probing culinary cultural biases and assumptions in commonsense reasoning systems. 2. A systematic evaluation of several encoder based models on FORK to demonstrate systematic cultural assumptions aligned with US over non-US cultures.\n\nDataset\nSince FORK aims to test the culinary cultural specificity of commonsense reasoning models, we choose the format to be along the lines of Commonsense QA (Talmor et al., 2019) . Each question in FORK has two options, only one of which is cor-rect. One of the options pertains to the US culture, while the other to non-US. The questions are manually written by the first author of this paper. The source of content used to formulate the questions is information gathered from Google searches, blog posts, traveler guides, etc. Upon publication, we will release FORK publicly.\nThere are three types of questions in FORK:\n\u2022 Underspecified: The question asked is about culinary customs and practices of no particular country or culture, and we hypothesize that English models will default to a US-centric interpretation in such a case. (See Fig. ) We assign a theme to each question, and questions in FORK span over three distinct culinary themes: eating utensils, tipping and general custom/culture. We also tag each Underspecified question-answer pair, and each implicit and explicit question, with a corresponding country. We present a brief overview of the distribution in Table 1 , and a full demographic distribution in Table 3 . It is important to note that these country labels should not be construed as exclusive of other cultures or countries that may share the relevant attribute. Cultural customs and countries have a many-to-many relation, and our labels are intended to highlight particular points of contrast between the US and other countries. What we measure as US-oriented cultural bias could also be construed as, e.g., Canada-oriented cultural bias only to the extent that US-labeled questions are also applicable to Canada.\nThe questions in FORK can either be a single sentence or a two sentence question. The questions consisting of two sentences help to provide context in case of Implicit and Explicit questions. We also follow a template-based approach where a question about the same theme is asked multiple times, varying only by e.g. the name of a dish, city, country, etc. In total, FORK consists of 184 manually curated questions, with 91.84% of questions pertaining to China, Japan, India and the US, and a small number of additional questions for other countries. Researching and writing questions was a slow manual process, so we chose to focus on producing more questions for fewer countries, to yield more robust results.\n\nValidation Study\nIn order to ensure that the manually curated questions are valid for probing culinary cultural differences, we conduct a validation study with six annotators, two each from China, the USA and India. This pool of annotators comprised of five graduate students, and one professor. We ask the annotators to answer questions in FORK and present statistics in Table 2 . For US, both annotators disagreed on the same question, while for India, the difference was on questions pertaining on tipping. Feedback from annotators observed that the tipping culture varied across the country. For China, the human annotators noted that some practices were untrue for the regions they were from, but true for other regions in China.\nAdditionally, the differences in customs and practices within the same country reiterate our note above that cultural customs and countries have a many-to-many relation. We have used country as a proxy variable for culture because there are no clear distinct boundaries across cultures, and using this proxy boundary allows to probe differences at a US vs non-US level. This highlights a need for future work to investigate cultural differences within a country, based on regional or other demographic dimensions.\n\nExperiments\nWe summarize our experimental set up, models, and the evaluation strategy used in this work.\n\nExperimental Setup\nIn this work, we test seven encoder-based models on FORK and report their performance. We test two variants of BERT (Devlin et al., 2019) : bert-base and bert-large, two variants of RoBERTa (Liu et al., 2019) : roberta-base and roberta-large, DistilBERT (Sanh et al., 2019) , and two variants of DeBERTaV3 (He et al., 2021) : deberta-v3-base and deberta-v3-large. All models are finetuned on the Common-senseQA train fold for 3 epochs. We run a grid search for the hyper-parameters and report them in Appendix A.2.\n\nEvaluation Strategy\nWe evaluate the culinary cultural contingency of the models tested as follows. For the questions tagged as Underspecified, we look at the number of times a \"US\" answer is chosen over a \"non-US\" answer. Here, \"US\" answer refers to an answer that would be appropriate or likely in the US context, and \"non-US\" answer refers to an answer that is more appropriate for a country on context outside the US. For Implicit and Explicit questions, we take a look at the responses for both US and non-US questions, and the percentage accuracy for US vs non-US answers.\nAdditionally, we also compare the number of times a non-US answer is chosen for Underspecified, non-US Implicit and non-US Explicit questions to better determine the bias between US and non-US cultures.\n\nResults and Discussion\nFigure 2a shows the percentage of time when a US answer is chosen over a non-US answer for Underspecified questions. We observe that roberta-large, and bert-large report the top two percentages of choosing a US answer over a non-US answer with values of 90.32% and 83.87% respectively. Fig 2a shows that all models, except for DistilBERT, preferred US answers over non-US answers for a majority of Underspecified questions.\nFigure 2b shows the percentage accuracy for US and non-US Implicit questions. We observe that roberta-large and bert-large report the top two accuracies of 78.57% and 71.42% respectively, when answering US Implicit Questions. In contrast, for non-US Implicit questions, only two models, DistilBert and bert-base cross the 50% accuracy mark, with bert-large having the lowest accuracy of 26.78%.\nFigure 3a shows the percentage accuracy for US and non-US Explicit questions.\nHere, roberta-large, and bert-large report the top two accuracies of 100% and 92.30% respectively when answering US Explicit Questions. In contrast, for non-US Explicit questions, roberta-base reports the best accuracy at 64.28% while roberta-large performs the worst, achieving 22.85%.\nFigure 3b shows the percentage times a non-US answer is chosen for Underspecified, non-US Implicit and non-US Explicit questions. For Underspecified questions, only DistilBert crosses the 50% mark, with 51.62% accuracy. The performance for non-US Implicit and non-US Explicit questions has been discussed above. We report all the model accuracies on FORK in Tables 4 and 5 in Appendix A.2.\nIn addition to aggregating US versus non-US results, we break down accuracy results for China, India, and Japan for Implicit and Explicit questions in Table 6 in Appendix A.2.\nWe observe that (China, Explicit) and (China, Implicit) questions have the lowest average accuracy across models, at 36.57% and 41.80%, respectively. The best performance is reported for (USA, Explicit) at 74.72%.\n\nStatistical Significance of Results\nIn order to make sure that our findings our statistically significant, despite the small number of questions in FORK, we conduct several statistical significance tests.\nFor Underspecified questions, we conduct the binomial test for all 7 model prediction results separately. Only roberta-base and DistilBert report a p-value greater than 0.05 in this setting.\nFor Implicit and Explicit questions, we conduct the chi-squared test on all 7 model prediction results separately to determine the statistical significance of our findings. For Implicit questions, only bert-base and deberta-v3-base report a p-value greater than 0.05 respectively. None of the models reported a p-value greater than 0.05 for Explicit questions.\n\nRelated Works\nA growing body of work aims to detect social biases in NLP models with respect to demographic attributes like gender and race (Rudinger et al., 2018; Zhao et al., 2018; Nangia et al., 2020; Li et al., 2020; Sap et al., 2020) . More recent is the growing attention towards cultural biases in NLP and AI technology at large. Hershcovich et al. (2022) propose a framework that allows one to understand the challenges of cultural diversity in NLP applications. Wikipedia has been shown to embed latent cultural biases (Callahan and Herring, 2011) and Tian et al. (2021) propose a methodology to develop culturally aware models for English, Chinese and Japanese using distributional perspectives on controversial topics from Wikipedia across these languages. Acharya et al. (2020) explore cultural biases, but along the rituals like birth, coming of age, marriage etc. in the US and in India. Chen and Henning (1985) investigate cultural bias in language proficiency tests and identify items of bias against non-native English speakers. To the best of our knowledge, this is the first work to analyze cultural bias in commonsense reasoning from the angle of culinary customs.\n\nConclusion\nWe have introduced FORK, a dataset to measure culinary cultural bias in commonsense models. Confirming our hypothesis, we find that models default to US cultural contexts in underspecified questions, and perform markedly better on implicit and explicit questions about US culture than non-US. A likely source of bias is the English, USproduced texts which models are pretrained on. We believe the results support our hypothesis that English Language Models LMS trained on texts (many of which are produced for a US audience) would reflect US (or broadly Western) cultural assumptions. We hypothesize that the Underspecified setting had the lowest \"accuracy\" for non-US countries because the experimental design forced the model to choose between US and non-US interpretations of the same question. For Implicit and Explicit settings, we speculate that the non-US accuracy is generally higher for Explicit than Implicit because the former made it easier for models to determine the cultural setting.\nPotential mitigation techniques to eliminate such biases may involve better curation of training data, training separate models for different cultural contexts, training models to better recognize cultural cues or ask for clarification in ambiguous settings, among many other possibilities. We believe this is an open research question, and we hope this paper will inspire future research to address it.\nThe topic of cultural bias is vast, and we choose a narrow scope to avoid biting off more than we can chew. Future work will explore strategies for cultural awareness of commonsense models, analysis of cultural assumptions in non-English models, and analysis of other aspects of culture beyond the culinary.\n", "hypothesis": " It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw.  However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative.  We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs.  We test several CommonsenseQA systems on FORK, and while we see high performance on questions about the US culture, the poor performance of these systems on questions about non-US cultures highlights systematic cultural biases aligned with US over non-US cultures..", "answer": true}
{"title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning", "content": "\nIntroduction\nIntelligent assistants have become increasingly popular in recent years, but they require users to explicitly describe their tasks within a single domain. Yet, the exploration of gradually guiding users through individual task-oriented dialogues has been relatively limited (Chiu et al., 2022) . This limitation is amplified when tasks extend across multiple domains, compelling users to interact with numerous bots to accomplish their goals (Sun et al., 2016) . For instance, planning a trip might involve interacting with one bot for flight booking and another for hotel reservation, each requiring distinct, taskspecific intentions like \"Book a flight ticket\" to activate the corresponding bot, such as an airline bot. In contrast, human assistants can manage highlevel intentions spanning multiple domains, utiliz-ing commonsense knowledge. This approach renders conversations more pragmatic and efficient, reducing the user's need to deliberate over each task separately. To overcome this limitation of current intelligent assistants, we present a flexible framework capable of recommending task-oriented bots within a multi-domain dialogue system, leveraging commonsense-inferred implicit intents as depicted in Figure 1 . Sun et al. (2016) pinpointed the challenges associated with a multidomain dialogue system, such as 1) comprehending single-app and multi-app language descriptions, and 2) conveying task-level functionality to users. They also gathered multi-app data to encourage research in these directions. The HELPR framework (Sun et al., 2017) was the pioneering attempt to grasp users' multi-app intentions and consequently suggest appropriate individual apps. Nevertheless, previous work focused on understanding individual apps based on high-level descriptions exclusively through user behaviors, necessitating a massive accumulation of personalized data. Due to the lack of paired data for training, our work leverages external commonsense knowledge to bridge the gap between high-level utterances and their task-specific bots. This approach enables us to consider a broad range of intents for improved generalizability and scalability.\n\nMulti-Domain Realization\nCommonsense Reasoning Commonsense reasoning involves making assumptions about the nature and essence of typical situations humans encounter daily. These assumptions encompass judgments about the attributes of physical objects, taxonomic properties, and individuals' intentions. Existing commonsense knowledge graphs such as ConceptNet (Bosselut et al., 2019) , ATOMIC (Sap et al., 2019), and TransOMCS (Zhang et al., 2021) facilitate models to reason over human-annotated commonsense knowledge. This paper utilizes a generative model trained on ATOMIC 20 20 (Hwang et al., 2021) to predict potential intents linking given user high-level utterances with corresponding task-oriented bots. The inferred intents can activate the relevant task-oriented bots and also serve as justification for recommendations, thereby enhancing explainability. This work is the first attempt to integrate external commonsense relations with task-oriented dialogue systems.\nZero-Shot Prompting Recent research has revealed that large language models (Radford et al., 2019; Brown et al., 2020) have acquired an astounding ability to perform few-shot tasks by using a natural-language prompt and a handful of task demonstrations as input context (Brown et al., 2020) . Guiding the model with interventions via an input can render many downstream tasks remarkably easier if those tasks can be naturally framed as a cloze test problem through language models. As a result, the technique of prompting, which transposes tasks into a language model format, is increasingly being adopted for different tasks (Zhao et al., 2021; Schick and Sch\u00fctze, 2021) . Without available data for prompt engineering (Shin et al., 2020) , we exploit the potential of prompting for bot recommendation in a zero-shot manner. This strategy further extends the applicability of our proposed framework and enables it to accommodate a wider variety of user intents and tasks, thus contributing to a more versatile and efficient multidomain dialogue system.\n\nFramework\nFigure 2 illustrates our proposed two-stage framework, which consists of: 1) a commonsenseinferred intent generator, and 2) a zero-shot bot recommender. Given a user's high-level intention utterance, the first component focuses on generating implicit task-oriented intents. The second component then utilizes these task-specific intents to recommend appropriate task-oriented bots, considering the bots' functionality through a large pretrained language model.\n\nCommonsense-Inferred Intent Generation\nThe commonsense-inferred implicit intents function not only as prompts for bot recommendation but also as rationales for the suggested bots, thereby establishing a solid connection between the highlevel intention and task-oriented bots throughout the conversation. For instance, the multi-domain system shown in Figure 1 recommends not only the AirlineBot but also describes its functionality-\"can book a flight ticket\"-to better convince the user about the recommendation.\n\nRelation Trigger Selection\nATOMIC 20 20 is a commonsense knowledge graph featuring commonsense relations across three categories: social-interaction, event-centered, and physical-entity relations, all of which concern situations surrounding a specified event of interest. Following Hwang et al. (2021) , we employ a BART model (Lewis et al., 2020) pre-trained on ATOMIC 20 20 to generate related entities and events based on the input sentence. However, despite having a total of 23 commonsense relations, not all are suitable for inferring implicit intents in assistant scenarios. We utilize AppDialogue data (Sun et al., 2016) to determine which commonsense relations can better trigger the task-specific intents. Given a high-level intention description u i and its task-specific sentences s ij , we calculate the trigger score of each relation r as an indicator of its \n\nRelation Definition\nSocial xIntent the likely intent or desire of an agent (X) behind the execution of an event \"X gives Y gifts\" \u2192 X wanted \"to be thoughtful\" xNeed a precondition for X achieving the event \"X gives Y gifts\" \u2192 X must first \"buy the presents\" xWant post-condition desires on the part of X \"X gives Y gifts\" \u2192 X may also desire \"to hug [Y]\" Event isAfter events that can precede an event \"X is in a hurry to get to work\" \u2192 \"X wakes up late\" isBefore events that can follow an event \"X is in a hurry to get to work\" \u2192 \"X drives too fast\" suitability as a trigger relation.\nEQUATION\nwhere P BART ([u i , r, s ij ]) represents the probability of the sentence beginning with the high-level user description u i , followed by a relation trigger r, and the corresponding task-specific sentences s ij . By summing up multiple task-specific sentences over j and all samples over i, a higher T (r) implies that the relation r can better trigger implicit task-oriented intents in assistant scenarios. We identify a total of five relations with the highest T (r) and present their definitions (Sap et al., 2019) in Table 1 . These relations are also reasonable from a human perspective to trigger implicit user intents.\n\nCommonsense Knowledge Generation\nGiven the selected relations R = {r 1 , r 2 , ..., r 5 }, where r i represents the i-th relation from {xIntent, xNeed, xWant, isAfter, isBefore}, we concatenate each relation with a user utterance u to serve as the context input for our pre-trained BART model:\n<s> u r i [GEN] </s>,\nwhere <s> and </s> are special tokens in BART, and [GEN] is a unique token employed during the pre-training of BART to initiate the commonsenserelated events. BART accepts this input and decodes the commonsense events into implicit taskoriented intents Y = y 1 1:k , y 2 1:k , ..., y 5 1:k , where y i k denotes the k-th generated commonsense event of the relation r i .\n\nZero-Shot Bot Recommendation\nWith the inferred intents, the second component aims to recommend appropriate bots capable of executing the anticipated tasks. To pinpoint the task-specific bots based on the required functionality, we leverage the remarkable capacity of a large pre-trained language model, assuming that app descriptions form a part of the pre-trained data.\n\nPre-trained Language Model\nThe language model used in this study is GPT-J 6B 2 , an GPT-3-like causal language model trained on the Pile dataset 3 (Radford et al., 2019) , a diverse, open-source language modeling dataset that comprises 22 smaller, high-quality datasets combined together. Making the assumption that app descriptions in mobile app stores are incorporated in the pre-training data, we exploit the learned language capability to suggest task-oriented bots based on the given intents.\n\nPrompting for Bot Recommendation\nTo leverage the pre-trained language capability of GPT-J, we manually design prompts for each relation type. For social-interaction relations, the prompt is formulated as \"The user r i y i 1:k by using a popular app called\". For instance, Figure 2 generates a prompt \"The user needs to go to the restaurant and make the reservation by using a popular app called\". For event-centered relations, we simply concatenate the generated events and appprompt to trigger the recommended task-oriented apps/bots.\n\nExperiments\nTo evaluate the zero-shot performance of our proposed framework, we collected a test set specific to our multi-domain scenarios. We recruited six volunteers who were knowledgeable about the target scenarios to gather their high-level intention utterances along with the associated task-oriented bots.\nUpon filtering out inadequate data, our test set incorporated a total of 220 task-oriented bots and 92 high-level utterances, each linked with an average of 2.4 bots. The number of bot candidates considered in our experiments is 6,264, highlighting the higher complexity of our tasks. Our primary aim is to connect a high-level intention with its corresponding task-oriented bot recommendation by leveraging external commonsense knowledge. Therefore, we assess the effectiveness of the proposed methodology and compare it with a 1-stage prompting baseline using GPT-J to maintain fairness in comparison. For this baseline, we perform simple prompting on the user's high-level utterance concatenating with a uniform app-based prompt: \"so I can use some popular apps called.\" In response to these context prompts, GPT-J generates the associated (multiple) app names, serving as our baseline results.\nTo further investigate whether our proposed commonsense-inferred implicit intent generator is suitable for our recommendation scenarios, we introduce another 2-stage prompting baseline for comparison. Taking into account that contemporary large language models exhibit astonishing proficiency in commonsense reasoning, we substitute our first component with the state-of-the-art GPT-3 (Brown et al., 2020) to infer implicit intents, serving as another comparative baseline.\n\nAutomatic Evaluation Results\nConsidering that multiple bots can fulfill the same task (functionality), we represent each app by its category as defined on Google Play, then compute precision, recall, and F1 score at the category level. This evaluation better aligns with our task objective; for instance, both \"WhatsApp\" and \"Line\" belong to the same category-\"communication\" as demonstrated in Table 3 .\nTable 2 presents that the 2-stage methods significantly outperform the 1-stage baseline, suggesting that commonsense knowledge is useful to bridge high-level user utterances with task-oriented bots. Further, our proposed approach, which leverages external commonsense knowledge, achieves superior precision over GPT-3, a quality that is more important in recommendation scenarios. The reason is that GPT-3 may generate hallucinations for inferring more diverse but may not suitable intents.\n\nHuman Evaluation Results\nGiven that our goal can be interpreted as a recommendation task, the suggested bots different from user labels can be still reasonable and useful to users. Therefore, we recruited crowd workers from Amazon Mechanical Turk (AMT) to evaluate the relevance of each recommended result given its high-level user utterance. Each predicted bot or app is assessed by three workers on a three-point scale: irrelevant (1), acceptable (2), and useful (3). The human-judged scores are reported in the right part of Table 2 , and our proposed framework achieves the average score of 2.18, implying that most recommended tasks are above acceptable. Compared with the 1-stage baseline with a score below 2, it demonstrates that commonsense inferred implicit intents can more effectively connect the reasonable task-oriented bots. Considering that the score of 2-stage prompting is also good, we report the pairwise comparison in Table 4 , where we can see that humans prefer ours to 2-stage prompting baseline for 57% of the data.\nIn additon to simply suggesting task-oriented bots, providing the rationale behind their recommendation could help users better judge their utility. Within our proposed framework, the commonsenseinferred implicit intents, which are automatically generated by the first component, can act as the explanations for the recommended task-oriented bots, as illustrated in Table 3 . Consequently, we provide these rationales alongside the recommended results using the predicted intents and undergo the same human evaluation process. Table 4 validates that providing these justifications results in improved performance from a human perspective, further suggesting that commonsense-inferred intents are useful not only for prompting task-oriented bots but also for generating human-interpretable recommendation.\n\nDiscussion\nTable 5 showcases the implicit intents generated by our proposed COMeT generator and GPT-3. It is noteworthy that GPT-3 occasionally produces hallucinations, which can render the recommended bots unsuitable. For instance, given the text prompt \"My best friend likes pop music.\", GPT-3 infers an intent to \"buy a ticket to see Justin Bieber\", which may not align accurately with the user's need.\nHence, our experiments reveal that while the 2-stage prompting achieves higher recall, its precision is lower. As our objective is to recommend reasonable task-specific bots, a higher precision is more advantageous in our scenarios.\n\nConclusion\nThis paper introduces a pioneering task centered around recommending task-oriented dialogue systems solely based on high-level user intention utterances. The proposed framework leverages the power of commonsense knowledge to facilitate zero-shot bot recommendation. Experimental results corroborate the reasonability of the recommended bots through both automatic and human evaluations. Experiments show that the recommended bots are reasonable for both automatic and human evaluation, and the inferred intents can provide informative and interpretable rationales to better convince users of the recommendation for practical usage. This innovative approach bridges the gap between user high-level intention and actionable bot recommendations, paving the way for a more intuitive and user-centric conversational AI landscape.\n", "hypothesis": " The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations.  In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying interactions. To bridge this gap, this paper proposes a framework for multidomain dialogue systems. This framework automatically infers explicit intents from user utterances, and prompts a large pre-trained language model to suggest suitable task-oriented bots.  By leveraging commonsense knowledge, our framework recommends associated bots in a zero-shot manner, enhancing interaction efficiency and effectiveness.  This approach substantially reduces interaction complexity, seamlessly integrates various domains and tasks, and represents a significant step towards creating more human-like intelligent assistants that can reason about implicit intents, offering a superior user experience.", "answer": false}
{"title": "Debiasing Generative Named Entity Recognition by Calibrating Sequence Likelihood", "content": "\nIntroduction\nRecently, recognizing flat, overlapped and discontinuous entities in a unified manner has been paid increasing attention. Among the existing works for unified Named Entity Recognition (NER), Seq2Seq formulation prevails for its flexibility and effectiveness in unified modeling (Yan et al., 2021; Lu et al., 2022; Ye et al., 2022) . Typically, it arranges the output entities into a fixed order to form a target sequence, and trains the generative model by maximum likelihood estimation (MLE).\nHowever, this estimation introduces bias by assuming a deterministic target distribution, where the model learns to assign all the probability mass to the observed target sequence. The biased estimation hurts the performance during decoding where predicted sequence likelihoods often do not accurately rank the performance of the generated sequences. To alleviate the bias, (Zhang et al., 2022) propose two data augmentation methods that sample possible sequences from the target space. (Yan et al., 2021) . topK/B denotes picking topK candidates out of candidates generated by beam search with beam size B.\nOthers resort to other formulations, e.g., W 2 NER (Li et al., 2022) reformulates NER as a word-word relation classification. In this study, we stick to the Seq2Seq formulation and explore how to mitigate the bias from another perspective orthogonal to (Zhang et al., 2022) .\nBeam search decoding algorithms maintain B candidates in descending likelihoods and output the highest one. However, the rest candidates could contain predictions with better performance. We measure this phenomenon with oracle scores. As shown in Table 1 , the beam candidates contain predictions with up to 8.1 points higher F1 over the outputted one, averaged on eight datasets. Doubling the beam size further increases the advantage to 9.38 points.\nRecently, reranking-based methods proposed for the abstractive summarization task offer a potential technique (Liu and Liu, 2021; Ravaut et al., 2022) . They train a discriminator on the candidates to predict a score for picking out the best candidate. For example, SimCLS (Liu and Liu, 2021) regards the cosine similarity between the input and candidate representations as the score. However, when applying reranking-based methods to our task, we find a challenge originating from the nature of information extraction. Candidates of the same input share most of the words and the discriminators trained from scratch have difficulty differentiating them People can communicate with international friends without the hefty phone bills. (detailed in Sec. 3.3).\nTo address the above issue, we propose RerankNER to debias generative NER based on a reranking framework adapted for the task. Specifically, we first train the generative model in the standard way, resulting in a biased model. Then, we generate several candidates for each input with beam search. Instead of training a separated discriminator on the candidates sharing most of the words, we calibrate the generative model with a contrastive loss defined on the candidates. The contrastive loss aims to make the estimated sequence likelihoods consistent with their relative task performance as shown in Figure 1 . This objective softens the target distribution and thus alleviates the bias.\nOur contributions are summarized as follows: 1. To the best of our knowledge, we are the first to explore reranking-based methods in the field of generative information extraction (Ye et al., 2022) . 2. We propose a method for generative NER tackling the bias problem. 3. Experimental results show that our method consistently boosts the baseline, and yields competitive results compared with the stateof-the-art methods on 8 widely-used datasets for NER.\n\nTask Formulation\nWe unify three NER subtasks (i.e. the flat, overlapped, and discontinuous NER) as follows.\nGiven an input sentence of n tokens X = x 1 x 2 . . . x n , the m output entities are arranged into a target sequence\nY = E 1 E 2 . . . E m , E i = y 1 i y 2 i . . . y j\u22121 i y j i l i\n, where y 1 i , ..., y j i denotes the tokens in the i-th entity and l i denotes the label of the i-th entity. Our goal is to model the conditional probability P (Y |X), which is factorized\nModel Encoder \u2026 \u2026 \u2026 \u2026 L MLE \u2026 \u2026 \u2026 \u2026 L Rank Decoder Candidates \u2026 \u2026 Target Sequence L Gold Input Sequence\nToken Distribution Likelihood \n\nOverview\nGiven a generative NER model trained on the target sequences with the standard MLE, we perform sequence likelihood calibration to alleviate the bias. First, we generate several candidates for each input with beam search and evaluate their task performance (F1 score is used). Then, we continue training the model with the contrastive loss to make the estimated sequence likelihoods consistent with their relative task performance. Finally, we generate the answer with the standard beam search by the calibrated model.\n\nSequence Likelihood Calibration\nThe contrastive loss depicted in Figure 2 is composed of three terms L MLE , L Rank , L Gold . L MLE is identical to the standard MLE used in the first training stage. It maintains the generating ability of the model during the calibration process.\nL MLE maximizes the sequence likelihood of the gold target sequence Y , where the sequence likelihood is calculated as the product of token-level likelihood:\nL MLE = \u2212S(Y ) S(Y ) = t log P \u03b8 (y t |X, Y <t )\nand \u03b8 denotes model parameters.\nL Rank improves the consistency between the estimated sequence likelihoods and the task performance of the candidate sequences. We adopt the margin ranking loss (Hopkins and May, 2011) for this term, i.e.,\nL Rank = i,j max 0, S( \u0176j ) \u2212 S( \u0176i ) + \u03bb\nwhere \u0176i , \u0176j is a pair of candidates generated by beam search, provided that \u0176i has a higher F1 score than \u0176j . \u03bb denotes the margin, a hyper-parameter.\nApart from the supervision of relative order in the candidates, we utilize the supervision of the gold sequence as well. L Gold ensures the sequence likelihoods of the generated candidates do not overstep the likelihood of the gold.\nL Gold = i max 0, S( \u0176i ) \u2212 S(Y ) + \u03bb\nwhere \u0176i denotes a candidate sequence, provided that it is not an equivalent of the gold.\nThe contrastive loss is the sum of the terms:\nL = L MLE + \u03b1L Rank + \u1fb1L Gold\nwhere \u03b1 and \u1fb1 are coefficients.\n\nMain Results\nWe conduct experiments on eight datasets of three NER subtasks in total. Precision (P), Recall (R) and Micro F1 score (F1) are reported as previous works. We use BART-large as our backbone. For fair comparison, we reproduce BARTNER (Yan et al., 2021) using the public code 1 and get similar results reported in the paper. We compare our model principally with SOTA generative NER models, including (Yan et al., 2021; Zhang et al., 2022; Lu et al., 2022) . Performances of SOTA discriminative NER models (Li et al., 2022) are also listed for reference. Refer to Appendix A for more details.\nThe results for flat, overlapped and discontinuous NER are shown in Table 2 , Table 3 and Table 4 respectively. On eight datasets, our proposed sequence calibration consistently boosts the baseline. It achieves SOTA performance among the generative methods. Noting that our method gets competitive results even compared with discriminative methods that use extra embedding and domain pretrained model, which shows the potential of generative models.\n\nAnalysis of Improvement\nWe manually analyze the predictions corrected by the calibration. Apart from reranking the correct candidate to the top beam, RerankNER can generate new candidates with boundary or type corrected. More cases can be found in Appendix B.\nIn addition to manually observing examples, we also quantitatively analyze the sources of gain. We find that the gain mostly comes from samples with low likelihood, which means sequence likelihood calibration is more effective for samples with higher difficulty. Specifically, we group the samples in the test set into ten groups according to their original sequence likelihood and evaluate their performance before (colored in orange) and after (colored in blue) calibration. It can be seen from Figure 3 that the F1 scores of most groups get improved after calibration, and the improvement is greater for samples with lower likelihoods.\nWe also conduct the hit@top-k evaluation. Specifically, we iterate over the test samples and increase the number of hits when a gold answer exists among the top-k candidates. Table 5 shows that calibration slightly increase the hit@top-k across various datasets.\n\nVariants of Reranker\nAs stated in Section 1, we observe that previous methods have difficulty capturing the subtle nuance among the candidates. We have investigated three variants: (1) SimCLS (Liu and Liu, 2021) . ( 2) Sim-CLS with our modification which concatenates the input and the candidate representation and projects it to a score to replace the cosine similarity. (3) Picking out the best candidate based on the estimated likelihood of our model. Overall, we find their training losses fluctuate and their performance consistently lower than the baseline which selects the top beam with the highest likelihood. Future work could investigate this phenomenon in more depth. \n\nRelated Work\nNamed Entity Recognition The existing methods for NER can be broadly classified into sequence labeling formulation, span-based formulation and generative-based formulation. A majority of initial works adopt sequence labeling formulation which assigns each token a tag from a predefined tagging scheme (Huang et al., 2015; Lample et al., 2016) . Then, the span-based formulation is proposed which enumerates all possible spans and \n\nA.1 Dataset Statistics\nThe statistics of the datasets are listed in Table 6 .\nFlat NER subtask We conduct experiments on CoNLL-2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013) in English.\nWe follow the experimental settings as previous works (Lample et al., 2016; Yan et al., 2021) .\nOverlapped NER subtask We conduct experiments on ACE 2004 (Doddington et al., 2004 ), ACE 2005 (Walker et al., 2006) , and GENIA (Kim et al., 2003) . For ACE 2004 and ACE 2005, we shuffle and split the documents into training, development, and testing in a ratio of 8:1:1 following (Yu et al., 2020) . For GENIA, the ratio is set to 8.1:0.9:1.0 following (Yan et al., 2021) .\nDiscontinuous NER subtask We conduct experiments on CADEC (Karimi et al., 2015) , ShARe13 (Mowery et al., 2013a) , and ShARe14 (Mowery et al., 2013b) . These datasets contains approximately 10% discontinuous entities. We follow the experimental settings from (Dai et al., 2020) .\n\nA.2 Implementation Details\nFor the fine-tuning stage, we use the code, the hyper-parameters, the package version from (Yan et al., 2021) and get comparable results on all datasets reported in the paper. We set the max epoch as 30 with early stop (patience=5). We use AdamW optimizer with the same learning rate as (Yan et al., 2021) . Linear learning rate scheduling is employed. For all subtasks, we do predictions on the word-level, i.e., only the position index of the first BPE of each entity word is used.\nFor the calibration training, we use the standard beam search to generate 5 candidates for each input sentence. We adopt the hyper-parameters as the fine-tuning stage except for the newly added ones. We implement both the fixed margin and the linear margin. The linear margin \u03bb = \u03bb(j \u2212 i) denotes the linear margin depending on the order difference of the candidates, and \u03bb is a hyper-parameter. We search the value of the margin \u03bb within [0.01, 0.1]. We search the value of coefficient \u03b1 within [0.1, 1]. Table 7 \"mask out tie\" means whether we mask out the comparison between candidates with the same F1 score in the contrastive loss. Effects of \"add L Gold \" and \"mask out tie\" differs across 8 datasets, so we view them as hyper-parameters. All experiments are conducted on the NVIDIA RTX 3090 GPU with 24G memory.\n\nA.3 Baselines\nThe following methods can adapt to all NER subtasks. Please refer to the original papers for the other methods designed specifically for a certain NER subtask. (Li et al., 2020) reformulates NER as a machine reading comprehension (MRC) task and extract entities by answering questions such as \"find locations in the text\". UIE (Lu et al., 2022) represents various information structures with a structured extraction language and tackles general information extraction tasks with a unified text-to-structure generation framework. (Zhang et al., 2022) analyzes incorrect biases in the generative NER models from the causality perspective and proposes two data augmentation methods to address them. Note that T5-Base they use has the same number of Transformer layers as BART-Large.\n\nBERT-MRC\nW 2 NER (Li et al., 2022) \n\nB Case Study\nTable 8 shows some examples corrected by the sequence likelihood calibration.\n\nC Generative Model\nOur method is agnostic to the generative model. In this study, we adopt BARTNER (Yan et al., 2021) , an Encoder-Decoder framework with pointer mechanism, to model the probability P (Y |X):\nEncoder encodes the input sentence X into vectors H Enc , which can be denoted as:\nEQUATION\nwhere H Enc \u2208 R n\u00d7d and d is the dimension of the hidden state.\nDecoder predicts the index probability distribution step-by-step according to P (y t |X, Y <t ). Since Y <t consists of the indices of the pointers and tags, it needs to be mapped to the vocabulary indices before inputted to the Decoder. We get the hidden state at the t-th step by:\nEQUATION\nFinally, we get the index probability distribution P t by:\nEQUATION\nwhere Embed(\u2022) is the embedding layer shared between the Encoder and Decoder, G denotes the label token while X denotes the entity words. \u0124Enc denotes the input representation. \u2297 denotes the dot product. For training, we use the cross-entropy loss with teacher forcing. During inference, we generate the target sequence auto-regressively. I believe our issues do relate directly to the appointing of electors for the state of Florida.\n0.67,-0.06,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 0.80,-0.31,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.89,-0.32,I 6 our 6 electors for the state of Florida 6 the state of Florida 3 0.50,-0.32,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 0.50,-0.33,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 1.0,-0.01,I 6 our 6 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.89,-0.10,I 6 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.80,-0.20,I 6 our 3 electors for the state of Florida 6 the state of Florida 3 Florida 3 0.91,-0.47,I 6 our 6 electors for the state of Florida 6 the state of Florida 3 state 3 Florida 3 0.89,-0.48,I 6 our 6 electors for the state of Florida 6 the state of Florida 3\nOne hundred South Koreans will be in the northern capital Pyongyang, to meet their North Korean relatives. \n\n\nhttps://github.com/yhcc/BARTNER/\n", "hypothesis": " Recognizing flat, overlapped and discontinuous entities uniformly has been paid increasing attention.  Among these works, Seq2Seq formulation prevails for its flexibility and effectiveness.  It arranges the output entities into a specific target sequence.  However, it introduces bias by assigning all the probability mass to the observed sequence.  To alleviate the bias, previous works either augment the data with possible sequences or resort to other formulations.  In this paper, we stick to the Seq2Seq formulation and propose a reranking-based approach.  It redistributes the likelihood among candidate sequences depending on their performance via a contrastive loss.  Extensive experiments show that our simple yet effective method consistently boosts the baseline, and yields competitive or better results compared with the state-of-the-art methods on 8 widelyused datasets for Named Entity Recognition..", "answer": true}
{"title": "Not Enough Data to Pre-train Your Language Model? MT to the Rescue!", "content": "\nIntroduction\nSince the emergence of the attention-based Transformer architecture (Vaswani et al., 2017) and the masking pre-training strategies introduced by BERT (Devlin et al., 2019) , transformer-based language models have become the default approach for many NLP tasks, leading to an impressive performance in high-resource languages, particularly English (Hoffmann et al., 2022; Thoppilan et al., 2022; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) .\nAs scaling laws dictate (Kaplan et al., 2020; Hoffmann et al., 2022) , such competitive models are achievable with big computational budgets and large corpora available, requirements difficult to meet for most languages (Joshi et al., 2020) .\nFortunately, LMs are being built for lessresourced languages, such as KinyaBERT for Kinyarwanda (390M words) (Nzeyimana and Rubungo, 2022) , ElhBERTeu for Basque (351M) (Urbizu et al., 2022) , gaBERT for Irish (161M) (Barry et al., 2021) , LuxemBERT for Luxembourgish (130M) (Lothritz et al., 2022b) , Bertinho 45M for Galician (Vilares et al., 2021) , swahBERT for Swahili (16M) (Martin et al., 2022) and QuBERT for Quechua (4M) (Zevallos et al., 2022) . Zhang et al. (2021) estimates that 10M-100M words of pre-training data are enough for an LM to acquire the linguistic capacities of syntax and semantics, but the amount of data required to acquire factual knowledge and commonsense is higher.\nIn this work, we propose to tackle the lack of data by using text corpora available in other languages translated via Machine Translation (MT). To the best of our knowledge, this has been addressed before (Lothritz et al., 2022a) but not indepth, and only for a closely related language pair (German-Luxembourgish). We selected Basque, a language isolate, as the target language, and employ Spanish as the auxiliary language.\nWe direct our efforts to answer the following Research Questions (RQ):\nRQ1: Can we obtain comparable performance to a native LM by training LMs just on synthetic data from MT?\nRQ2: Can we improve current LMs for lessresourced languages by adding synthetic MT data?\n\nMethodology\nIn order to answer our research questions, we set out the following methodology. We propose two baseline LMs: i) ElhBERTeu (Urbizu et al., 2022) as a strong baseline, trained on a corpus of 351M words; and (ii) BERT 125M a model trained on a lower data regime. From there on we pre-train various LMs with different native/synthetic data combinations. Sections 3 and 4 give details of the models pre-trained, including baselines. All models presented in this paper follow the BERT base architecture (Devlin et al., 2019) .\nWe select Basque, a language isolate, as a tar-get language, and employ Spanish, a Romance language, as the auxiliary language since it has huge text corpora available. Furthermore, both languages coexist in the same geographical area, therefore, Spanish is the language that Basque shares the most parallel data with, which is crucial to train MT systems. On the other hand, this is a real case since obtaining a corpus in Basque that exceeds 350M words is difficult.\n\nMT system\nThe Spanish to Basque MT system used for our experiments is based on the default Base Transformer architecture (Vaswani et al., 2017) using the PyTorch version of the OpenNMT toolkit (Klein et al., 2017) and BPE tokenization (Sennrich et al., 2016) (joint vocabulary of 32K). The system was trained with 8.6M parallel sentences and evaluated on the FLORES-200 benchmark (Team et al., 2022) obtaining 13.2 BLEU and 47.4 chrF++. See Appendix F for an analysis of the impact the amount of parallel data has.\n\nCorpora\nFollowing we introduce the corpora employed on the experiments (summarized in Table 1 ):\nN_ElhBERTeu is a Basque corpus compiled to train ElhBERTeu (Urbizu et al., 2022) . It contains 351M words.\nN_small is a smaller Basque native corpus (125M words), created to be closer to the scenario of many languages. The corpus is composed of 75% news articles from Berria 1 newspaper and 25% of text from Wikipedia.\nS_beto2eu is the Spanish Unannotated Corpora 2 composed of 3B words (Ca\u00f1ete, 2019) which was used to train the Spanish LM BETO (Ca\u00f1ete et al., 2020) , translated to Basque using the MT system described in Section 2.1.\nS_loc2eu was also translated from Spanish to Basque. We collected up to 548M words of news articles in Spanish from news sources geographically limited to the Basque Country. After translating it with our MT system, the final corpus in Basque contains 378M words.\n\nPre-training Details\nSince the aim of this work focuses on the effect of the training data, we left all the hyper-papameters fixed. Every model was pre-trained following the procedure used for ElhBERTeu (Urbizu et al., 2022) . See appendix A for further details.\n\nEvaluation\nA downstream task evaluation of our models was performed on the BasqueGLUE ( We fine-tuned each model up to 10 epochs and selected the optimal number of epochs over the development set. We use a batch size of 32 and a learning rate of 3e-5. We report the average of 5 runs on the test sets. Fine-tuning was done on NVIDIA GeForce RTX 3090 GPUs.\n\nLM Trained Solely on Synthetic Data\nRQ1 aims to prove if it is possible to train a competitive LM with just synthetic text obtained from MT. In order to do that we train a BERT model on S_beto2eu (S_BERT), and evaluate if the model trained exclusively on synthetic data is able to perform as well as models trained on real data.\nThe results on the BasqueGLUE Benchmark for S_BERT are reported in In order to improve the results obtained with the synthetic data, we analyse two specific aspects of the data: i) the quality loss during the translation process; ii) the cultural context of the synthetic data. Following we analyze each of those factors.\n\nMeasuring the Quality of MT Text\nTo measure quality loss when translating from Spanish to Basque, we did a manual analysis on a sample of the translations produced by the MT system (See appendix B for details). We evaluated whether a sentence was correctly translated (71%), but also whether the produced sentence was linguistically correct (91%). Since we aim to use this text to train LMs, the effect of some translation errors like hallucinations or omissions, that cause significant meaning changes, might not be critical.\nNext, we measured the vocabulary diversity loss during translation. For that aim, we compiled a Basque-Spanish parallel corpus (more details can be found in appendix B) and translated the Spanish text to Basque with our MT system. The lexicon of the translated data is 16% poorer, limited by the target vocabulary of the MT model and the tendency of MT to generalize and simplify the vocabulary.\nFinally, we analyze the impact of training LMs on translated corpora, leaving aside other factors such as corpus size or text-domain. We train two BERT models using the parallel corpus compiled in the previous experiment, one on the original Basque part of the corpus and the other on the part translated from Spanish to Basque. The results in Table 3 show the model trained on translated data performs slightly worse than the native model.\nWhile this is expected from the quality loss and lexicon impoverishment caused by MT, the gap in performance is very small (0.5% on average), which leads to the conclusion that the synthetic data is adequate.\n\nDomain and Cultural Context\nAnother factor related to data which might affect the performance of MLs trained over translated corpora is the source text we select in the auxiliary language. The Spanish Unannotated Corpora is a huge corpus. However, it is not domain homogeneous and the topic distribution of this corpus differs significantly from that of a corpus in Basque, especially because it hardly includes the specific topics associated with the Basque Country. Furthermore, we analyzed how tokenizers trained on this corpus do not include many words common in the context of Basque speaker communities, like named entities (locations, people or organizations). See appendix C for a detailed analysis of the vocabulary coverage of each model on the test datasets.\nTo analyze the impact the cultural bias and the domain heterogeneity of the source text has on the performance in downstream tasks, we compiled the S_loc2eu corpus, presented in section 2.2. This corpus is formed by texts in Spanish crawled from newspapers geographically and culturally connected to the Basque Country. Results in Table 2 show that models trained on translated local news (Sloc_BERT and SNloc_BERT), perform better than those without them (S_BERT and SN_BERT), even though it is trained over a much smaller corpus. Following the same pattern, the \n\nCombining Native and Synthetic Data\nThe objective of RQ2 is to test if adding texts translated by MT to a native corpus can boost the performance on downstream NLU tasks of the LM in the target language.\nWith that aim, we trained a new LM on the concatenation of S_beto2eu corpus and the N_ElhBERTeu corpus 4 (SN_BERT hereinafter). Table 2 reports the results for SN_BERT when evaluated on the BasqueGLUE benchmark. Even if SN_BERT surpasses ElhBERTeu in a few tasks (NERC, BEC), it is below it in the average score.\n\nMerging Strategies\nOne factor that may explain the lower performance of the model trained on the combined synthetic and native data is the way of combining the data. Our last experiment aims to analyze different combination alternatives. For SN_BERT, we just concatenate N_ElhBERTeu and S_beto2eu. However, the better quality native corpus is diluted among the translated texts of poorer quality, but larger in size (4x times). Hence, we propose another three alternatives to merge native and translated corpora, shifting the balance between both types of data: concat 20\u221280 (SN_BERT): concatenation of N_ElhBERTeu and S_beto2eu, which roughly form 20% and 80% of the pre-training corpus respectively. As mentioned, synthetic data take the principal role in this configuration.\nconcat 50\u221250 : we oversample N_ElhBERTeu corpus to equal the size of S_beto2eu. This setting gives equal weight to native and synthetic data.\nconcat 80\u221220 : we oversample N_ElhBERTeu up to 80%, thus, pre-training relies on native data mostly. Native data is weighted over synthetic data. sequential: the LM is trained for 750K steps on S_beto2eu, and afterwards for another 250K steps on N_ElhBERTeu 5 .\nResults for different merging strategies are shown in Table 4 . Increasing the ratio of N_ElhBERTeu data in our pre-training corpora improves the performance of our models to the point where concat 80\u221220 outperforms ElhBERTeu, trained only with native text in Basque. Pretraining sequentially does improve slightly the results of the default SN_BERT setting, but weighting concatenation is the best strategy between the two. Further sequential training regimes were tried other than (750k+250K). 'sequential' refers to the best results we achieved with this strategy.\n\nConclusions\nRegarding the RQ1, we conclude from our experiments that LMs trained exclusively on synthetic data from MT can obtain comparable performance to a native LM. We further analyze that other than the quality of MT, the cultural context of the text we select from the auxiliary language do have an effect on the final performance. We conclude that it is better to gather a corpus composed of sources similar to those in the target language, rather than indiscriminately translating vast amounts of data in the auxiliary language.\nFurthermore, with respect to RQ2, our experiments show that state-of-the-art models' performance can be improved by adding translated data during the pre-training, albeit it is a small improvement. Weighting the native data above synthetic data is key to this improvement.\nAll in all, this approach has a big potential for less-resourced languages, since once you have a proper MT system, there is no limit on the amount of data one can translate from languages with bigger corpora available.\nData and models are publicly available 6 .\n", "hypothesis": " In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks.  However, pre-training such models demands large text collections not available in most languages.  In this paper, we study the use of machinetranslated corpora for pre-training LMs.  We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from Spanish. The evaluation carried out on 9 NLU tasks indicates that models trained exclusively on translated data offer comparable results to native language models. Furthermore, models trained with real data can be significantly improved with synthetic data, offering even better performance on less-resourced languages.", "answer": false}
{"title": "Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie", "content": "\nIntroduction\nThe amount of text data being processed has greatly increased since the advent of communication platforms such as Twitter, Zoom, and Slack, and NLP services such as DeepL and Grammarly have millions of users. Some users analyze textual big data for marketing, linguistics, or sociology, while others deploy NLP services on their own devices because of privacy concerns. It is therefore becoming important to develop highly efficient methods to process massive text data and user queries with limited computational resources.\nHowever, the recent campaign for efficient NLP does not focus on literally efficient methods that scale to increasing data sizes and run on resourceconstrained devices. Instead, most \"efficient\" NLP studies (Treviso et al., 2022) focus on neural methods, which are too slow to handle billions of social media posts and too large to deploy on edge devices. Those studies seek to make model training or inference relatively efficient within the deep learning framework. Thus, the large efficiency gap with respect to classical methods has never been filled. !\"#$ %&'()*+,+*(-. %/00+#1 !!\"\"\" \"#!#$%& $!#$%' %!\" #&!!#$%& '! ()!\"\"\"\"\"\"\"\"\"\"\"\" \" ! \"# $ % #& ' ()\"\"\"\"\"\"\"\"\"\"\" \" $%& $%' ()*( $%& +,-. &*(/0 ()*( In this study, I take an orthogonal approach toward absolutely efficient NLP by seeking to boost the accuracy of the fastest methods. Specifically, I have developed a remarkably simple yet accurate method for Japanese morphological analysis, which is a joint task of word segmentation, part-of-speech (POS) tagging, and lemmatization. This method revisits the classical longest matching method; it greedily applies patterns that determine the next position to segment and then identifies the POS tag for the segmented word, as illustrated in Figure 1 . To obtain reliable patterns, starting from words in a morphological dictionary and training data, patterns are extended with posterior surface contexts and previous POS tags, and the patterns' segmentation offsets and tags are determined by frequency. The extracted patterns are then stored in an efficient double-array trie (Aoe, 1989) .\n!(&\"'(%0& !\"#$% ! &' &(% \"%)' *( %+# \"#$%#&' () ! !!\" #$%#&\nThe proposed method was evaluated on two standard corpora (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The experimental results confirmed that this simple method can process 1,000,000 sentences per second on an M2 MacBook Air, with comparable accuracy to learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) . This section describes the method of Japanese morphological analysis used here, which performs word segmentation, POS tagging, and lemmatization. To maximize the tagging efficiency, I return to a pattern-based algorithm that is similar to the longest matching algorithm (Nagata, 1994) .\nThe longest matching algorithm performs deterministic word segmentation by using a dictionary. Starting from the beginning of the input, it greedily finds the longest dictionary words to segment the input. Although this simple algorithm exhibits moderate accuracy in Chinese and Japanese with transformation rules (Palmer, 1997; Hockenmaier and Brew, 1998; Sassano, 2014) , there is a gap in accuracy from search-and classification-based approaches (Kudo et al., 2004; Neubig et al., 2011) . To make search-based morphological analysis partially deterministic, Morita and Iwakura (2019) extracted surface patterns from tagging results; however, the speed-up factor was at most 1.5.\n\nBasic algorithm\nAlgorithm 1 is a simple, deterministic algorithm for joint word segmentation, POS tagging, and lemmatization. It repeatedly applies the longest-matching patterns in a trie P to a given sequence of characters, c, and a start position i to segment and tag the next word (w j = c i+ \u015dhift i and tj ). As will be shown later in \u00a7 3, this simple algorithm works as well as learning-based approaches.\nThis algorithm is inspired by the longest matching algorithm but differs in that the segmentation offset shift can be smaller than the surface length matched with patterns, k (see Line 7 in Algorithm 2). A running example is shown in Figure 1 .\nThe algorithm is also inspired by the precomputation of feature weights in sequence labeling (Kaji et al., 2010) and classification with conjunctive features (Yoshinaga and Kitsuregawa, 2009 , 2010 , 2014) weights in advance and retrieve those partial results by using simple keys such as word unigrams, POS bigrams, and primitive feature sequences to compute the final results (labels) by an argmax operation on the weights. The proposed method regards word segmentation and tagging as a joint, multiclass classification problem and directly obtains the label (i.e., where to segment and what to tag) by using the feature sequence as a pattern, thus skipping the expensive argmax operation over a number of labels. The longest matching thus implies classification with as many features as possible.\n\nPattern extraction from data\nFollowing the feature templates of learning-based methods (Kudo et al., 2004; Neubig et al., 2011) , the algorithm's pattern template was designed as a sequence of characters, c, followed by the previous word's POS tag t j\u22121 , thus giving c; t j\u22121 , where ';' represents string concatenation.\nAlgorithm 2 is the procedure to extract patterns for word segmentation and POS tagging from the annotated data and a dictionary. Given training data D with annotation of (word) segmentations and (POS) tags and a dictionary V compiling words and their possible tags, the algorithm iteratively extracts possible patterns from D. It first enumerates surface patterns c i+k i from all starting positions of words in D, and it then concatenates them with tag t j\u22121 for the preceding words to form pattern candidates (Lines 3-10 in Algorithm 2). Patterns are added for dictionary words that are unseen in the training data (Lines 11-12). The segmentation offset (shift) and tag t for a pattern are determined by the frequency (Lines 14-15). To avoid extra matching to the posterior contexts and previous tag, we only keep patterns whose segmentation offsets and tags differ from those of the longest prefix patterns that share prefixes of posterior contexts (Lines 16-18). This not only reduces the number and length of patterns but also minimizes the longest matching method's overhead for word segmentation. 1\n\nExperiments\nThis section describes an experimental evaluation of the pattern-based morphological analyzer on two annotated corpora in different domains (Kurohashi and Nagao, 2003; Hangyo et al., 2012) . The method was compared with two learning-based baselines (Kudo et al., 2004; Neubig et al., 2011) in terms of efficiency and accuracy. Note that all language resources and software used in the experiments are publicly available and free for academic use.\n\nSetup\nData The experiments used the Kyoto-University Text Corpus 2 (KYOTO) (Kurohashi and Nagao, 2003) , compiled from newspaper articles, and the Kyoto-University Web Document Leads Corpus 3 (KWDLC) (Hangyo et al., 2012) , compiled from the first three sentences of various Web pages. I adopted the split of development and test sets given in the corpora's github repositories and used the remaining portions as training sets. The datasets' statistics are listed in Table 1 .\nMethods The three methods below were compared. To prevent overfitting, the hyperparameter C in the underlying model was tuned for the two learning-based baseline methods 4 by using the development set to maximize the F 1 of the POS tags.\n1 In preliminary experiments, a variant of backtracking-free search (Maruyama, 1994) did not improve the throughput.\nVaporetto (ver. 0.6.2) is a Rust 6 implementation of a classification-based method (Neubig et al., 2011) . 7 It first performs word segmentation by classifying whether to segment after each character in the input, and it then identifies the resulting words' POS tags. It also trains classifiers for the possible POS tag sets of individual words, and it assigns the POSs of its first dictionary entries for words that are unseen in the training data. 8 A morphological dictionary was used to extract word features.\nJagger is a C++ implementation of the proposed algorithm. It greedily applies patterns extracted from the training data and a dictionary to jointly segment words and assign tags. Appendices A and B respectively describe the method to handle unknown words and the implementation details. Jagger is more similar to Vaporetto than to MeCab but differs in that it jointly performs segmentation and tagging instead of using a two-step cascaded pipeline, and it uses patterns instead of classifiers to find labels (i.e., where to segment and what to tag). Appendix C compares Jagger with the other implementations.\nDictionaries As listed in Table 2 , the experiments used two morphological dictionaries imported to MeCab from a manually tailored morphological analyzer, JUMAN. 9 Specifically, mecabjumandic-5.1-20070304 and mecab-jumandic-7.0-20130310 were compared to examine the impact of the dictionary's quality and size. The jumandic-5 https://taku910.github.io/mecab/ 6 Rust exhibits comparable efficiency to C++ on program benchmarks: https://github.com/kostya/benchmarks/.\n(2) minor POS (e.g., common noun); (3) conjugation type (e.g., ichidan verb); and (4) conjugation form (e.g., irrealis). For example, the POS tags of shumi and iru in Figure 1 are noun-common_noun-*-* and verb-*-ichidan_verb-terminal, respectively.\nEvaluation procedure The precision, recall, and F 1 of the segmentation with various levels of POS tags (Kudo et al., 2004) were used as metrics. As Vaporetto does not output lemmas, lemmatization was evaluated via the tagging results of the full POS tag set (\"all (levels 1-4)\" in Tables 3 and 4 ), which included conjugation types and forms, given that Japanese words can be mapped to their lemmas according to their conjugation types and forms. I processed 1000 copies of the test data and measured the time, speed, and maximum memory consumption three times with the /usr/bin/time -l command. The median values are reported here. All experiments were done on an M2 MacBook Air with a 3.5-GHz CPU and 24-GB main memory.\n\nResults\nTables 3 and 4 summarize the morphological analysis results on the KYOTO and KWDLC datasets.\nThe pattern-based method here, Jagger, was 16 and 7 times faster than MeCab and Vaporetto with 1/2 and 1/20 as much memory consumption, respectively, while achieving comparable accuracy.\nJagger is efficient because it does not have massive floating-point parameters, unlike other methods, and because it minimizes the number and length of patterns by pruning (Lines 16-18 in Algorithm 2). As a result, the training took less than six seconds. MeCab's accuracy depends on the dictionary: with jumandic-7.0, it worked best on KWDLC and worst on KYOTO. In contrast, Vaporetto's accuracy depends on the training data size. It worked best on KYOTO but was just as good as Jagger on KWDLC. Below are the detailed results for Jagger with the jumandic-7.0 dictionary. Comparison to neural methods Jagger was compared to a state-of-the-art neural method (Tolmachev et al., 2018) , JUMAN++-V2, 10 which was trained on the same data with the official script and hyperparameters. 11 Note that this comparison was unfair to Jagger in terms of accuracy and to JUMAN++-V2 in terms of efficiency, because JUMAN++-V2 uses 0.8 million additional dictionary entries from Wikipedia and a neural language model trained on 10 million sentences from the Web. Table 5 summarizes the comparison between Jagger and JUMAN++-V2. Although JUMAN++-V2 was reported to speed up JUMAN++ (Morita et al., 2015) by a factor of 250, Jagger was faster than JUMAN++-V2 by a factor of 180 with 1/7 as much of a memory footprint. JUMAN++-V2 was more accurate than Jagger, but the gain was less than 1% for word segmentation. If external text could be used, this gap could be reduced with a technique called structure compilation (Liang et al., 2008) , which runs JUMAN++-V2 on external text to extract patterns. That idea is beyond this paper's scope but important for future work.\nWord segmentation efficiency Because of different approaches to handling unknown words and supporting lemmatization, it is difficult to compare Vaporetto with Jagger and MeCab as a morphological analyzer in a strictly fair manner. Instead, the word segmentation efficiency was compared, as summarized in Table 6 . Here, Vaporetto was trained to perform only word segmentation by using the dictionary and the training data without POS tags. Jagger was faster and more space-efficient than Vaporetto, even taking the overhead of loading large models (1.7 seconds) into account. enjoys the benefits of the dictionary and training data: it can change its behavior by adding not only dictionary entries but also patterns.\n\nConclusions\nThis study sought to improve the accuracy of speedoriented, pattern-based methods for Japanese morphological analysis, rather than improving the speed of accuracy-oriented neural models. The proposed method extracts POS-augmented patterns from a morphological dictionary and annotated data. Experimental results on two standard datasets confirmed that this method achieves accuracy comparable to that of learning-based methods, with a very fast throughput of over 1,000,000 sentences per second on a laptop. I plan to apply this approach to other languages and even to other NLP tasks by discretizing the continuous representations induced by neural models to obtain patterns. The source code is released with GPL, LGPL, and 2-clause BSD licenses.\nMessage to researchers Because the accuracies on NLP benchmark datasets are becoming saturated with a larger foundation model, researchers may want to set diverse goals based on underrepresented metrics besides accuracy (e.g., efficiency). I hope that this study will initiate serious research on speed-intensive approaches to NLP that can meet industry demands and enable researchers with limited computational resources to exert their ability.\n", "hypothesis": " Accurate neural models are much less efficient than non-neural models and are useless for processing billions of social media posts or handling user queries in real time with a limited budget.  This study revisits the fastest patternbased NLP methods to make them as accurate as possible, thus yielding a strikingly simple yet surprisingly accurate morphological analyzer for Japanese.  The proposed method induces reliable patterns from a morphological dictionary and annotated data.  Experimental results on two standard datasets confirm that the method exhibits comparable accuracy to learning-based baselines, while boasting a remarkable throughput of over 1,000,000 sentences per second on a single modern CPU.  The source code is available at https://www.tkl.iis.u-tokyo.  ac.jp/~ynaga/jagger/..", "answer": true}
{"title": "Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation", "content": "\nIntroduction\nStance detection is an important task that identifies the polarity of text with regard to certain target (Somasundaran and Wiebe, 2010; Augenstein et al., 2016; Mohammad et al., 2016; Sobhani et al., 2017; Allaway and McKeown, 2020) , as shown in Table 1 . It is crucial for understanding opinionated information expressed in natural language, and it can facilitate downstream social science analyses and applications (Zhang et al., 2017; Hanselowski et al., 2018; Jang and Allan, 2018) .\nPrevious work on stance detection mostly focuses on in-domain or leave-out targets with only a few target choices (Mohtarami et al., 2018; Xu et al., 2018; Graells-Garrido et al., 2020; Zhang et al., 2020; Liang et al., 2021; Allaway et al., 2021;  Input Text: Airports and the roads on east nor west coast can not handle the present volume adequately as is. I did ride the vast trains in Europe, Japan and China and found them very comfortable and providing much better connections and more efficient. Target: high-speed rail Stance Label: Supportive (Pro) Table 1: A stance detection example from VAST. Jiang et al., 2022) . Although achieving promising performance, those models are limited to generalize to a wide variety of targets. Zero-shot and fewshot stance detection on varied topics (VAST; Allaway and McKeown, 2020) , instead, provides a diverse set of targets for training and testing. Efforts on this direction includes involving graph modeling (Lin et al., 2021) , common sense (Liu et al., 2021) or Wikipedia knowledge (He et al., 2022) , and contrastive learning (Liang et al., 2022a,b) . These methods generally formulate the problem into a classification setting, which directly trains the label representation from scratch, and does not fully utilize the semantics from those label and target texts.\nHowever, connections among text semantics from input text, target, and label can be beneficial for stance detection. In this paper, we propose a new model by formulating the problem as a denoising task from text templates via conditional generation. Compared to direct classification, we can further exploit the label and topic semantics via learning to decode a series of natural language text containing the predicted label. The denoising scheme can also take advantage of the pretrained language model with similar pretraining task formulation (Lewis et al., 2020) . To improve the target representation, we propose to jointly train target prediction with stance detection, which gives the input text and desired stance label to output possible targets. We use unlikelihood training (Welleck et al., 2020) that suppress the likelihood of manually constructed incorrect samples to enhance label \n\nApproach\nIn this section, we will discuss our approach to zero-shot and few-shot stance detection. We will first introduce the problem formulation, and then discuss our generation-based framework.\n\nProblem Formulation\nStance detection aims to identify the polarity of an input text with regard to a specific target. Formally, a sample instance can be considered as a triple (x, t, y), where x and t are two sequences of tokens, representing input text and target respectively. y \u2208 {supportive (pro), opposite (con), neutral} represents then stance label.\nA stance-detection model is to infer the stance label y given x and t with parameter \u03b8: f (x, t; \u03b8) = y.\nIn the zero-shot and few-shot stance detection dataset with varied targets (Allaway and McKe-own, 2020) , many target tokens only occur zero or a few times in the training set.\n\nA Generation-Based Framework\nGeneration-based frameworks have demonstrated their effectiveness for problems beyond traditional generation tasks (Lewis and Fan, 2019; Yan et al., 2021; Li et al., 2021; Raffel et al., 2022) . We use a conditional generation model for this problem, where the condition is a partially-filled template with the input text. The template is two sentences describing the target and stance with a <stance> placeholder for stance detection. An example of the partially-filled template with input text and output is shown in Figure 1 .\nOur base model is BART (Lewis et al., 2020) , an encoder-decoder language model pretrained with denoising objectives, which is similar to our generation-based formulation. The generation process can be considered as using the conditional probability to select a new token at each step given input and previously generated tokens:\np (o | g (x, t) ; \u03b8) = |o| i=1 p (o i | o <i , g (x, t) ; \u03b8) ,\nwhere g (x, t) represents the transformation function that fills the target t into the template and forms the input sequence with the input text x. Specifically, g (x, t) will generate a combination of input text and template with special tokens: \"<s> template </s></s> x </s>\". The template contains two sentences: \"The target is <target>. The stance is <stance>\". We will fill in <target> placeholder with the actual target and keep the <stance> placeholder for the decoder to generate.\nThe generated output o is a fully-filled template, where both target and stance placeholders are replaced by actual or predicted values. The model is trained by minimizing the log-likelihood over the whole generated sequence:\nL s = \u2212 log p (o | g (x, t) ; \u03b8) = \u2212 |O| i=1 log p (o i | o <i , g (x, t) ; \u03b8) .\nThe final predicted stance label is obtained with a post-processing function that tries to find the polarity word after the prompt for stance.\n\nJoint Target Prediction\nAnother advantage of using generation-based architecture is that we can leverage auxiliary generative tasks to help train stance detection. We use target prediction, which is to infer the target tokens t given stance label y and input text x:\nf t (x, y; \u03b8) = t.\nTarget prediction can provide the connection of stance to target in an opposite direction of stance detection. It can also enhance the representation of target tokens by learning to decode them.\nThe input sequence of target prediction is similar to stance detection, consisting of a partially-filled template and input text. The template used for joint target prediction is slightly different than the one used for stance detection, where we switch the position of two sentences so that the stance information shows up first. We will fill in the actual stance text in the input sequence, and leave the <target> placeholder for the decoder to generate.\n\nUnlikelihood Training\nLog-likelihood objective optimizes the likelihood over the entire distribution. However, in our task, especially when generating the stance labels, we should specifically focus on several candidate tokens. Therefore, we introduce unlikelihood training (Welleck et al., 2020) , where we use unlikely tokens, i.e. incorrect stance predictions, to replace the ground-truth sequence and optimize with the unlikelihood loss for the replaced tokens.\nSpecifically, for an output sequence o, we assume o k is the stance label and replaced it with an incorrect stance prediction o \u2032 k while keeping other tokens to form incorrect sequence o \u2032 . The combination of likelihood and unlikelihood will be:\nL u = log p o \u2032 k | o \u2032 <k , g (x, t) ; \u03b8 \u2212 i\u0338 =k log p o \u2032 i | o \u2032 <i , g (x, t) ; \u03b8 ,\nFor each ground-truth sequence, we can construct two sequences for unlikelihood training with the other two incorrect stance labels. \n\nTraining Objective\nThe final training objective is the combination of loss functions from stance detection, target prediction, and unlikelihood training:\nL = L s + \u03b1 t L t + \u03b1 u L u ,\nwhere L t represents the log-likelihood loss over the output template for target prediction, \u03b1 t , \u03b1 u are used to balance different loss functions.\n3 Experiments \n\nExperimental Setup\nWe conduct our experiments on VAST (Allaway and McKeown, 2020) . We compare our model with several existing systems including 1) TGA-Net (Allaway and McKeown, 2020) ; 2) BERT-GCN (Lin et al., 2021) ; 3) CKE-Net (Liu et al., 2021) ; 4) WS-BERT (He et al., 2022) . Following their setup, we use macro-average F 1 as the evaluation metric, and we report performance on the subset of test set for zero-shot and few-shot, and the overall test set.\nWe use BART-base 4 as our base model, of which the number of parameters is roughly consistent with baselines on BERT-base 5 . Our best model is optimized with AdamW (Loshchilov and Hutter, 2019) for 30 epochs with a learning rate of 1e-5. We use a linear scheduler with a warmup proportion of 0.1 and the training batch size is 32. We use greedy search during inference. We reported performances on development set and test set using the averaged results from 5 different random seeds. Test results are reported based on the best overall F 1 performance on the development set. \u03b1 t is set to 1 and \u03b1 u is set to 0.5. Our final model takes about 5 hours for training on one Nvidia RTX 3090 GPU. \n\nComparing with Model Variants\nWe first conduct comparison of some of our model variants to illustrate the effectiveness of our proposed components. The results are shown in Table 3. From the comparison of BERT-based classification (BERT Classification) and BART-based denoising generation from templates (BART w/ Template), we can find that adopting the generation framework can significantly improve the model performance. Our proposed topic prediction and unlikelihood training can further boost performance.\nThe final model with knowledge from Wikipedia, verifies the effectiveness of Wikipedia knowledge for stance detection with a generative framework.\n\nComparing with Existing Systems\nOur overall performance is shown in Table 4 . Our method can significantly outperform those previous baselines, indicating the effectiveness of our proposed generation framework for zero-shot and few-shot stance detection with varies topics.\n\nQualitative Analysis\nFigure Text processing via conditional generation.\nOur work is also motivated by the recent success of tackling text processing problems as conditional generation (Lewis et al., 2020; Raffel et al., 2022) .\nIn addition to the conventional text generation problems, conditional generation frameworks are effectively applied in information extraction (Li et al., 2021) , question answering (Lewis and Fan, 2019; Raffel et al., 2022) and sentiment analysis (Yan et al., 2021) . In our work, we further explore stance detection via conditional generation.\n\nConclusion\nIn this paper, we propose a generation-based framework for zero-shot and few-shot stance detection that generate stance label from pre-defined templates. We further propose an auxiliary task, joint target prediction that takes stance label and input text to generate targets, and unlikelihood training on manually constructed incorrect generation output. Combining with Wikipedia knowledge for target from He et al. (2022) , our model can achieve new state-of-the-art performance on VAST.\n", "hypothesis": " Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text.  In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts.  We further propose to jointly train an auxiliary task, target prediction, and to incorporate randomly generated incorrect samples with unlikelihood training to improve the representations for both target and label texts.  We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework.  Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.", "answer": false}
{"title": "Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization", "content": "\nIntroduction\nPrior work (Fabbri et al., 2022b; Goyal and Durrett, 2020; Laban et al., 2022) formulates the problem of factual inconsistency detection as a binary classification task, which predicts whether a summary is consistent with the source document. However, these approaches have two drawbacks. First, they cannot predict the types of factual errors made by a summary and thus provide limited insights into the weakness of summarization systems. Although recent studies (Pagnoni et al., 2021; Tang et al., 2022; Goyal and Durrett, 2021a) have manually inspected the types of factual errors in summaries, there is no existing work on automatic detection of fine-grained factual inconsistency.\nSecond, existing models typically cannot explain which portions of the document are used to detect the inconsistency in the input summary. In order to verify and correct an inconsistent summary, humans still need to read the entire source document to find the supporting evidence. Kryscinski et al. (2020) introduce an auxiliary task to extract the supporting spans in the document for inconsistency detection, which requires expensive ground-truth labels of supporting spans.\nTo address the first limitation, we propose the fine-grained factual inconsistency detection task. The goal is to predict the types of factual inconsistency in a summary. We show examples of different factual error types in Table 1 .\nTo solve the second challenge, we further introduce an interpretable fine-grained inconsistency detection model (FINEGRAINFACT) that does not require any label of supporting text spans, inspired by how humans verify the consistency of a summary. When humans annotate the factual error types of a summary, they first identify facts in the document that are relevant to the summary and then determine the factual error types in the summary. Following this intuition, our model first extracts facts from the document and summary using Semantic Role Labeling (SRL). We consider each extracted semantic frame as a fact since a semantic frame captures a predicate and its associated arguments to answer the question of \"who did what to whom\". After fact extraction, a document fact attention module enables the classifier to focus on the facts in the document that are most related to the facts in the summary. By highlighting the facts in the document with the highest attention scores, our model can explain which facts in the document are most pertinent to inconsistency detection.\nExperiment results show that our model outperforms strong baselines in detecting factual error types. Moreover, the document facts highlighted by our model can provide evidence to support or refute the input summary, which can potentially help users to verify the predicted error types and correct an inconsistent summary.\nTable 1 : A text document and example summaries with different factual error types according to the typology defined by Tang et al. (2022) . The errors in the sample summaries are in red color and italicized. We bold the text spans from the document that refute the sample summaries.\n\nTask Definition\nThe goal of the fine-grained inconsistency detection task is to predict the types of factual errors in a summary. We frame it as a multi-label classification problem as follows. Given a pre-defined set of l factual error types {e 1 , . . . , e l }, a document d, and a summary s, the goal is to predict a binary vector y \u2208 {0, 1} l where each element y i indicates the presence of one type of factual errors.\nWe follow the typology of factual error types proposed by (Tang et al., 2022) , which include intrinsic noun phrase error, extrinsic noun phrase error, intrinsic predicate error, and extrinsic predicate error. The definitions and examples of these error types are presented in Table 1 .\n\nOur FINEGRAINFACT Model\nThe model architecture is illustrated in Figure 1 .\nFact extraction. To represent facts from the input document and summary, we extract semantic frames with a BERT-based semantic role labeling (SRL) tool (Shi and Lin, 2019) . A semantic frame contains a predicate and its arguments, e.g., Fact encoder. We first represent tokens in the concatenated sequence of the input document and summary by fusing hidden states across all layers in Adapter-BERT (Houlsby et al., 2019) with max pooling. To represent facts, we apply attentive pooling to all tokens in the semantic frame under the assumption that different tokens in a fact should con- tribute differently to the fact representation. Given the token representations t j , we calculate the attention scores \u03b1 j = exp(\u03d5(t j ))/ m j=1 exp(\u03d5(t j )), and represent each document or summary fact as\n[ ARG0 David][ V saw][ ARG1 the flame]. We use f doc\nf i = m j=1 \u03b1 j (\u03d5(t j ))\n, where m is the number of tokens in the fact and \u03d5 is a two-layer fully-connected network.\nDocument Fact Attention module. This module aims to retrieve the facts in the document that are related to the facts in the summary. We first concatenate the document fact representations into a document fact matrix F doc . We attend each summary fact f sum i to the document fact matrix to compute a document context vector:\nc i = MULTIHEADATT(f sum i , F doc , F doc )\n, where f sum i acts as the query, F doc is used as the key and value. The document context vector c i captures the information of the facts in the document that are related to the summary fact f sum i .\nFor each document fact, we sum up its attention scores received from all summary facts as its importance score. Concretely, we use \u03b1 j\u2192i to denote the sum of attention scores injected from the j-th summary fact to the i-th document fact over all attention heads. The importance score of a document fact f doc i is defined as n j=1 \u03b1 j\u2192i , where n is the total number of facts in the summary. Then, we return the top k document facts with the highest importance scores as the document fact highlights, where k is a hyper-parameter.\nClassification module. A linear classifier predicts the probability of each factual error type based on the concatenation of the representations of summary facts and document context vectors. Specifically, we first use mean pooling to fuse all summary fact representation vectors and all document context vectors into two fixed-size vectors:\nf sum = 1 n n i=1 f sum i , c = 1 n n i=1 c i .\nThese two vectors contain the information of all facts in the summary and the information of all document facts that are related to the summary. Next, we feed the concatenation of f sum and c to a linear classification layer to predict the probability of each factual error type:\np(y) = \u03c3(W[ f sum ; c] + b), where W \u2208 R d\u00d7l , b \u2208 R, d is the hidden size of Adapter-BERT, \u03c3 denotes the sigmoid function.\nTraining objective. We train our model with weighted binary cross-entropy (BCE) loss, The technical details are in Appendix A.\n\nSetup\nDataset. We conduct experiments on the Aggrefact-Unified dataset (Tang et al., 2022) , which collects samples and unifies factual error types from four manually annotated datasets (Maynez et al., 2020; Pagnoni et al., 2021; Goyal and Durrett, 2021b; Cao and Wang, 2021) . We remove the duplicated samples (i.e., duplicated document-summary pairs) in the Aggrefact-Unified dataset (Tang et al., 2022) and obtain 4,489 samples. We randomly split data samples into train/validation/test sets of size 3,689/300/500. The statistics of the error type labels are in Appendix B.1.\nEvaluation metrics. We adopt the macroaveraged F1 score and balanced accuracy (BACC) as the evaluation metrics. BACC is an extension of accuracy for class-imbalanced datasets and is widely adopted by previous literature on inconsistency detection (Kryscinski et al., 2020; Laban et al., 2022) . All experiment results are averaged across four random runs.\nBaselines. We adapt the following baselines 2 for the new task. FACTCC-MULTI: FactCC (Kryscinski et al., 2020) is originally trained on synthetic data for binary inconsistency detection. We replace the binary classifier with a multi-label classifier and finetune the model on Aggrefact. FACTGRAPH-MULTI: FactGraph (Ribeiro et al., 2022) parses each sentence into an AMR graph and uses a graph neural network to encode the document. We replace the binary classifier with a multi-label classifier. We also fine-tune the BERT (Devlin et al., 2019) and ADAPTERBERT (Houlsby et al., 2019) .\n\nPerformance of Error Type Detection\nFollowing (Tang et al., 2022) , we detect error types in summaries from different models: SOTA includes the pre-trained language models published in or after 2020. XFORMER contains the Transformer-based models published before 2020. OLD includes earlier RNN-or CNN-based models. REF represents reference summaries. From Table 2 , we observe that: (1) Representing facts with semantic frames improves factual error type prediction.. We observe that in most of the cases, our model outperforms other baselines that do not use semantic frames to represent facts. (2) The performance of our model drops after we remove the document fact attention module. The results show that our document fact attention module not only improves the interpretability, but also boost the performance of factual error type detection.\n(3) All detection models perform better in summaries generated by OLD systems. It suggests that the factual errors made by OLD systems are relatively easier to recognize than the errors made by more advanced systems.\n\nEvaluation of Document Fact Highlights\nSince ground-truth document fact highlights are not available, we apply a fact verification dataset to evaluate whether the predicted document fact highlights provide evidence for inconsistency detection. Specifically, we adopt the FEVER 2.0 dataset (Thorne et al., 2018) , which consists of claims written by humans and evidence sentences from Wikipedia that can support or refute the claims. We first extract facts from the evidence sentences via SRL and use them as the groundtruth document fact highlights. We then consider each claim as the input summary and the section of a Wikipedia article that contains the evidence sentences as the input document.\nWe devise the following method to compute document fact highlights for the baseline models. Since all baselines utilize the CLS token to predict the factual error types, we use the attention scores received from the CLS token to compute an importance score for each document fact. We then return the facts that obtain the highest importance scores as the document fact highlights for each baseline. More details are in Appendix B.2. fact highlights predicted by different models. We observe that our model obtains substantially higher recall scores, which demonstrates that our model provides more evidence to support the inconsistency prediction. Thus, compared with the baselines, our model allows users to verify the predicted error types and correct inconsistent summaries.\n\nCase Study\nTable 4 shows a sample summary generated by an OLD model with an intrinsic noun phrase error, where the \"a school in northern ireland\" in the summary contradicts with \"Northern Ireland charity\" in the document. Our model accurately predicts the error type with evidence in the form of document fact highlight, which helps users verify the error and correct the summary.\nIn Table 5 , we present an error analysis on a sample summary generated by a SOTA model. According to the source text, the word \"West\" in the summary is incorrect and should be removed since the statement in the summary is made by \"Sussex PPC\" instead of \"West Sussex PCC\". In order to (Tang et al., 2022) . The error in the sample summary is in red color and italicized. We bold the text spans from the document that refute the sample summary.\ndetect this error, a model needs to understand that the expressions \"Sussex PCC Katy Bourne\", \"Ms Borune\", and \"she\" in the document refer to the same entity. This sample illustrates that the errors generated by a SOTA model are more subtle and more difficult to be detected. Our model fails to predict the correct error type for this sample. Since the top five document fact highlights returned by our model do not contain the entity \"Sussex PCC Katy Bourne\", we suspect that our model fails to recognize the co-referential relations among \"Sussex PCC Katy Bourne\", \"Ms Borune\", and \"she\" for this sample. Thus, improving the co-reference resolution ability of fine-grained inconsistency detection models is a potential future direction.\n\nRelated Work\nFactual consistency metrics. QA-based consistency metrics (Durmus et al., 2020; Scialom et al., 2021; Fabbri et al., 2022b) involve generating ques-tions from the given document and its summary, and then comparing the corresponding answers to compute a factual consistency score. Entailmentbased consistency metrics (Laban et al., 2022; Kryscinski et al., 2020; Ribeiro et al., 2022; Goyal and Durrett, 2020) utilize a binary classifier to determine whether the contents in a system summary are entailed by the source article. In contrast, our model is a multi-label classifier that detects the types of factual errors in a summary. Moreover, our model leverages SRL to encode the facts in the input document and summary, enabling users to interpret which facts in the document are most relevant to the inconsistency detection.\nFact-based evaluation methods. To evaluate the informativeness of a summary, the Pyramid human evaluation protocol (Nenkova and Passonneau, 2004) asks annotators to extract semantic content units (SCUs) from the system summary and reference summary, respectively, and then compute their overlap. Each SCU contains a single fact. Xu et al. (2020) approximate the Pyramid method by using SRL to extract facts. They then compute the embedding similarity between the facts extracted from the system summary and those from the reference summary. Fischer et al. (2022) also use SRL to extract facts, but they measure the similarity between the facts extracted from the system summary and those from the source document to compute a faithfulness score. On the other hand, our model integrates SRL with a multi-label classifier to predict the factual error types of a summary.\n\nConclusion\nIn this paper, we present a new task of fine-grained inconsistency detection, which aims to predict the types of factual inconsistencies in a summary. Compared to the previous binary inconsistency detection task, our new task can provide more insights into the weakness of summarization systems. Moreover, we propose an interpretable finegrained inconsistency detection model, which represents facts from documents and summaries with semantic frames and highlights highly relevant document facts. Experiments on the Aggrefact-Unified dataset show that our model can better identify factual error types than strong baselines. Furthermore, results on the FEVER 2.0 dataset validate that the highlighted document facts provide evidence to support the inconsistency prediction.\n", "hypothesis": " Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems.  Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary.  Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FINEGRAINFACT, which explicitly represents the facts in the documents and summaries with semantic frames extracted by semantic role labeling, and highlights the related semantic frames to predict inconsistency.  The highlighted semantic frames help verify predicted error types and correct inconsistent summaries.  Experiment results demonstrate that our model outperforms strong baselines and provides evidence to support or refute the summary.  1\nMarcy Smith was woken up by her son David to find their house in Glovertown, Newfoundland and Labrador, completely engulfed in flames ...  Mrs Smith said if it wasn't for her son, she and her daughter probably wouldn't have survived.  David was on FaceTime to his father at the time, so was the only one awake and saw the flames out of the corner of his eye ...\nExample summary Extrinsic noun phrase error: Errors that add new object(s), subject(s), or prepositional object(s) that cannot be inferred from the source article.\nDavid was using FaceTime with Maggie Smith and saw the flames..", "answer": true}
{"title": "An Empirical Comparison of LM-based Question and Answer Generation Methods", "content": "\nIntroduction\nQuestion and answer generation (QAG) is the task of generating a set of question-answer pairs given an input context such as a document, a paragraph or a sentence. QAG can be applied to develop question answering (QA) models without human supervision (Lewis et al., 2019; Zhang and Bansal, 2019; Puri et al., 2020) and as a data augmentation mean for QA model understanding (Shakeri et al., 2020; Bartolo et al., 2021) . Moreover, QAG is used as an aid of educational systems (Heilman and Smith, 2010; Lindberg et al., 2013) , to improve information retrieval models (Pyatkin et al., 2021; Lewis et al., 2021) , and as a tool for model interpretation (Perez et al., 2020; Lee et al., 2020) .\nQAG stems from question generation (QG) (Mitkov and Ha, 2003; Du et al., 2017; Zhou et al., 2017; Du and Cardie, 2018) , which consists of generating a question given an answer on the input context. Despite QG being widely studied in the language model era (Murakhovs'ka et al., 2022;  Figure 1 : Overview of the considered QAG approaches. Ushio et al., 2022) , QAG is a more complex task, since the answer needs to be generated and not assumed to be part of the input. Therefore, it is unclear what types of QAG models work in practice as no comprehensive comparisons have been established so far.\nIn this paper, we formalize QAG as a task that generates question-answer pairs given a context, and compare three simple QAG strategies based on fine-tuning encoder-decoder language models (LMs) such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) . Our three proposed approaches (illustrated in Figure 1 ) consist of: (1) pipeline QAG, which decomposes the task into answer extraction and question generation, learning a separate model for each subtask; (2) multitask QAG, which uses a shared single model to train both subtasks instead of independent ones; and (3) end2end QAG, which uses end-to-end sequenceto-sequence learning to generate question-answer pairs directly. Finally, we compare these three approaches on a multi-domain QA-based evaluation, where QA models are trained with the questionanswer pairs that each QAG model generates. All the QAG models are publicly released via Hug-gingFace (Wolf et al., 2020) 1 , and available on the online demo 2 .\n\nRelated Work\nThere are a few works that leverage pre-trained LMs for QAG. For example, Alberti et al. (2019) first fine-tuned BERT (Devlin et al., 2019) on answer extraction and QG, and generate questionanswer pairs by extracting an answer, on which the associated question is generated. Puri et al. (2020) followed a similar idea by fine-tuning an autoregressive LM for QG. In contrast, Shakeri et al. (2020) fine-tuned a single LM on answer extraction and QG jointly. Lee et al. (2020) trained an LSTM sequence-to-sequence model from scratch to generate question and answer sequentially. More recently, Bartolo et al. (2021) used a QAG model to generate adversarial examples for QA. Similarly, Lewis et al. (2021) improved on extractive QA by generating millions of question-answer pairs via QAG. In these two last cases, the model to fine-tune was BART (Lewis et al., 2020) .\nWhile all these studies use the three methods that we analyse in this paper (i.e. pipeline, multitask and end2end), these are not easily comparable, as there are important differences among them in terms of settings, dataset, input to the LMs, and evaluation metrics. Moreover, except for Lewis et al. (2021) , none of the proposed QAG models have been made publicly available. Finally, the two most recent studies using BART (Bartolo et al., 2021; Lewis et al., 2021) have not performed any evaluation on the QAG model, as it is included as a part of a larger pipeline. We summarize the comparison of these prior works and our evaluation at Table 1 .\n\nQuestion & Answer Pair Generation\nGiven an input context c (e.g. a paragraph), QAG aims to generate natural question-answer pairs Q c related to the information in c: Q c = {(q 1 , a 1 ), (q 2 , a 2 ), . . . }. In what follows we describe three different approaches for QAG based on fine-tuning language models. \nEQUATION\nwhere the log-likelihood is factorized into tokenlevel predictions, similar to other sequence-tosequence learning settings (Sutskever et al., 2014) .\nIn practice, the input to the AE model takes the form of:\n[c 1 , . . . , <hl>, s 1 , . . . , s |s| , <hl>, . . . , c |c| ]\nwhere s i and c i are the i\u2212th token of s and c respectively, | \u2022 | represents the number of tokens in a text, and <hl> is the highlighted token to mark the sentence in the context, following the QG formulation of Chan and Fan (2019) and Ushio et al. (2022) . Likewise, the input to the QG model takes the answer into account by:\n[c 1 , . . . , <hl>, a 1 , . . . , a |a| , <hl>, . . . , c |c| ]\nwhere a i is the i\u2212th token of a. At inference time, we simply replace the gold answer a of the QG 14263 model ( 2) by the prediction from the AE model (1), and run the inference over all the sentences in context c to obtain question-answer pairs. Consequently, the pipeline approach can generate, at most, as many pairs as sentences in c.\n\nMultitask QAG\nInstead of training independent models for each subtask, a shared model can be fine-tuned on both AE and QG jointly in a multitask learning manner.\nTo be precise, we mix the training instances for AE and QG altogether, and randomly sample a batch at each iteration of fine-tuning. Each subtask is distinguished by a task prefix added at the beginning of the input text: \"extract answer\" (AE) and \"generate question\" (QG).\n\nEnd2end QAG\nInstead of breaking down QAG into two separate components, we can directly model it by converting the question-answer pairs into a flattened sentence y, and fine-tuning a sequence-to-sequence model to generate y from c. Let us define a function that maps Q c to a sentence as:\nEQUATION\nt(q, a) = \"question:{q}, answer:{a}'' (4)\nwhere each pair is textualized with the template (4) and joined by a separator |. The end2end QAG model P qag is then optimized by maximizing the following conditional log-likelihood:\nEQUATION\n4 Evaluation\n\nExperimental Setting\nData. QAG models are trained on SQuAD (Rajpurkar et al., 2016) . As their outputs consist of arbitrary questions and answers, reference-based NLG evaluation metrics traditionally used in QG research (Papineni et al., 2002; Denkowski and Lavie, 2014; Lin, 2004; Mohammadshahi et al., 2022) are unsuitable. As such, we conduct an extrinsic evaluation by training QA models on the data generated by the QAG models. For this, we rely on SQuADShifts (Miller et al., 2020) , an English reading comprehension dataset in four domains (Amazon/Wikipedia/News/Reddit). For both SQuAD and SQuADShifts, we rely on the train/validation/test splits provided in QG-Bench (Ushio et al., 2022) .\nMulti-domain QA Evaluation. Given a QAG model to be assessed, we first generate questionanswer pairs on each domain of SQuADShifts, and fine-tune DistilBERT (Sanh et al., 2019) on the generated pseudo QA pairs, where F 1 and exact match on the test set are considered as the target metric. This SQuADShifts QA-based evaluation can be used to probe the robustness of the model across domains, as well as for the overall performance by averaging metrics over the domains. Our QA evaluation relies on Tune, 3 an efficient grid search engine for parameter optimization, to find optimal hyperparameters during QA model fine-tuning.\nBase Models. For all comparison systems (i.e. pipeline, multitask and end2end), we experiment with T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) as base LMs, with the model weights t5-{small,base,large} and facebook/bart-{base,large} shared on Hug-gingFace. 4 Moreover, we report the results of a QG model that takes the gold answers from the provided QA training set as input (QG-only). This is similar to the pipeline method but excluding the AE component.\n\nResults\nTable 2 shows the SQuADShifts QA evaluation results for the three approaches considered. Interestingly, the top-2 best models, BART LARGE (multitask) and T5 LARGE (end2end), outperform Gold QA (i.e., the model using the human-labeled gold annotations) in two out of four domains, as well as the average in both F 1 and exact match. Even smaller models such as T5 SMALL are competitive with respect to using the gold standard questionanswer pairs.\nGiven the results, it is unclear which approach provides the best performance, as BART LARGE (multitask) achieves the best average F 1 score (including the best results on Amazon and Reddit domains in both metrics), while T5 LARGE (end2end) obtains the best average exact match (as well as the best results on Wiki and NYT domains in both metrics). Among the QAG approaches, T5 consistently works better with the end2end QAG, while BART is not well-suited when used end2end. A possible explanation is that T5 has observed sentences with structured information due to its multitask pre-training objective, while BART did not have such training instances as it was trained only on a denoising sequence-to-sequence objective.\n\nGeneration Size Analysis\nIn the SQuADShifts QA evaluation, the number of question-answer pairs generated by QAG models is often larger than the human-labelled gold dataset in each domain, as shown in Table 3 . 5 Therefore, to fairly compare the quality of generated question-answer pairs, we randomly downsampled the number of the generated question-answer pairs to match the size of the gold dataset. For this analysis we focus on the best-performing T5 LARGE and BART LARGE QAG models 6 , and run the same SQuADShifts QA evaluation with the downsampled pairs. Figure 2 shows the average of F 1 scores over 10 independent trials with different random seeds at downsampling. 7 In this experiment, no model outperforms the gold QA baseline. This indicates that the human-annotated gold dataset is still more informative and data efficient than the generated question-answer pairs. Also, since the pipeline/multitask QAG models generate more pairs than the end2end model, downsampling has a larger effect on the pipeline and multitask models than the end2end model. This means that the T5 LARGE (end2end) model can generate questionanswer pairs of higher quality than those generated by BART LARGE (multitask), although they are equally competitive in the main experiment ( \u00a7 4.2).\n\nQAG Model Comparison\nSo far, we have compared the three QAG approaches in terms of performance. However, performance is not the only criterion to consider when choosing a QAG model, since each approach has its own advantages and limitations in terms of computational cost and usability. From the perspective of computational complexity, end2end QAG is faster than the others at both of training and inference, because it can generate a number of question-answer pairs at once in a single paragraph pass. In contrast, both multitask and pipeline need to parse every sentence separately, and a single prediction consists of two generations (i.e. answer extraction and question generation). Essentially, the relative increase of computational cost from end2end QAG to pipeline/multitask QAG can be approximated by the average number of sentences in each paragraph. In terms of memory requirements, both multitask and end2end QAG rely on a single model, but pipeline QAG consists of two models, requiring twice as much memory storage. Finally, while computational-wise end2end is the lightest model, both pipeline and multitask approaches can generate a larger number of question-answer pairs on average, with the added benefit of being able to run the models on individual sentences. Table 4 shows a practical comparison of the three approaches.\n\nConclusion\nIn this paper, we formalized QAG as a task to generate pairs of questions and answers given an input context, and established baselines with three different QAG approaches. To compare them, we conducted a multi-domain QA based evaluation that measures the performance of a QAG model by fine-tuning QA models on the QA training dataset generated by the QAG model. Our evaluation shows that end2end QAG models that generate questions and answers simultaneously are generally the most reliable. Nonetheless, establishing a multitask paradigm with separation between answer extraction and question generation can have added benefits, especially when using LMs such as BART. In general, the results are promising, as they show that these artificially-generated QA datasets rival in quality with those annotated by humans, which could save large amount of resources.\n", "hypothesis": " Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g.  a paragraph).  This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish baselines with three different QAG methodologies that leverage sequence-to-sequence language model (LM) fine-tuning. We compare these models on a multi-domain QA-based evaluation and find that the pipeline QAG approach outperforms the multitask and end-to-end approaches in generating accurate question-answer pairs.  Experiments show that an endto-end QAG model, which is computationally light at both training and inference times, is generally robust and outperforms other more convoluted approaches.  However, there are differences depending on the underlying generative LM.  Finally, our analysis shows that QA models fine-tuned solely on generated questionanswer pairs can be competitive when compared to supervised QA models trained on human-labeled data..", "answer": false}
{"title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition", "content": "\nIntroduction\nSelf-supervised learning has improved input data representation without requiring extensive humanlabeled data (He et al., 2019; Zhang et al., 2022) . Based on this advancement, powerful pre-trained models providing high-performing representations for various data types (e.g., text, images, and audio) have been proposed. For instance, in speech, self-supervised pre-trained models such as Hu-BERT (Hsu et al., 2021) have advanced state-of-the- art performance of automatic speech recognition (ASR).\nHowever, one major challenge in using pretrained speech models for ASR is the representational bias towards prominent accents present in the dataset during pre-training. Consequently, there will be a disparity in ASR performance between native and non-native speakers. More specifically, pre-training using a large dataset such as the Lib-riSpeech (Panayotov et al., 2015) , which comprises a large proportion of utterances from native (L1) English speakers, leads to a less satisfactory recognition rate for non-native (L2) English accented speech. This phenomenon can curtail the effectiveness of current high-performing ASR systems for real-world applications.\nThere have been several ways to address this issue, including fine-tuning the model on diverse accents (Winata et al., 2019; Shibano et al., 2021) , having a separate model for each accent (Yang et al., 2018) or using regularization losses that guide the fine-tuning process to achieve robustness to accents (Chen et al., 2020; Gao et al., 2022) , all of which require updating the pre-trained model.\nWe propose a different solution for improving L2 speech recognition in transformer-based speech models that introduces a small number of learnable parameters into the input space while keeping the backbone weights of the model untouched. Our approach is guided by Information-Theoretic Adversarial Learning; thus, we refer to it as IN-Tapt (Information-Theoretic Adversarial Prompt Tuning). INTapt aims to introduce auxiliary embeddings (i.e., prompt) concatenated to the original input, which can re-modulate the attention and adapt the pre-trained weights so that the corresponding input looks like speech with an accent seen during pre-training (Figure 1 ). To achieve this, INTapt incorporates (1) adversarial training, which tries to minimize the mutual information between the accent feature of the original input and that obtained by concatenating the prompt embeddings in front of the initial input, and (2) CTC loss training to improve the ASR performance of the prompt-concatenated input. Essentially the prompt is trained such that the accent of the concatenation is pushed away from the input accent and the concatenation achieves native CTC loss performance. Unlike the previous use-case of prompts in NLP or Computer vision (CV), where a single prompt embedding is learned for each discrete task or input domain, the intensity of an accent is continuous. Thus, we propose an input-dependent prompt embedding by training a prompt generator that outputs an input-specific prompt. Through extensive experiments, we show that the proposed dual objectives of INTapt not only lead to better performance on L2 English accents but result in a higher similarity between the accent feature of the promptconcatenated input and that of L1 English accents. In the first step, we train an Accent Module (AM) capable of isolating the accent feature from a given audio feature a of an input speech x. In the second step, we train a Prompt Generator (PG), which outputs a prompt p for a given audio feature a, using two objectives: (1) Minimize the mutual information between the accent feature z \u2032 and z, where the former is obtained using the prompt-concatenated input (p; a) and the latter is obtained from the original audio feature a, (2) Minimize CTC loss to improve the ASR performance of the input (p; a).\n\nAccent Module (AM)\nSince our method requires direct access to the isolated accent feature of the corresponding audio feature input, we propose an Accent Module (AM) capable of extracting the accent feature z from the input a. The module consists of an accent feature extractor f \u03b8 1 which is trained with an accent classification head f \u03b8 2 to isolate the accent feature and an accent intensity regression head f \u03b8 3 to capture the intensity of the accent into the obtained feature.\n\nAccent Classification Head\nThe role of the accent classification head f \u03b8 2 is to isolate the accent feature of a given speech 1 . Given the hidden state representation h of an audio feature input a, the feature extractor outputs the accent feature (i.e., z = f \u03b8 1 (h)) and the accent classification head f \u03b8 2 tries to assign it to the correct accent label y.\n\nAccent Intensity Regression Head\nThe intensity of an accent could vary among different people even though there are in the same L2 group, and it could also vary between utterances from the same speaker. Thus, an accent intensity regression head is introduced to incorporate the accent intensity into the obtained accent feature z. Based on the assumption that the intensity of the accent affects ASR performance, making the accent intensity regression head predict the CTC loss 2 , obtained by inputting the corresponding speech into the backbone speech model, will allow the extracted accent feature z to capture the intensity of the accent.\nGiven a batch B, the training of the Accent Module with the two aforementioned heads could be summarized as:\nmin \u03b8 1 ,\u03b8 2 1 |B| i\u2208B \u2212 log p(y i |f \u03b8 2 (f \u03b8 1 (h i )))+ \u03bb min \u03b8 1 ,\u03b8 3 1 |B| i\u2208B [ f \u03b8 3 (f \u03b8 1 (h i )) \u2212 CTC(x i )] 2\n(1)\n\nPrompt Generator (PG)\nBuilding on the success of prompts in NLP (Liu et al., 2021; Li and Liang, 2021) and CV (Dosovitskiy et al.), we introduce a prompt tuning method to improve the ASR performance for L2 English speech by efficiently utilizing a pre-trained model that already shows good performance for L1 English speech. In contrast to traditional NLP or CV applications, where a single, discrete prompt embedding is learned for each specific task or input domain, the intensity of an accent is continuous. To address this, we propose an inputdependent prompt embedding by training prompt generator P G \u03b8 4 that generates an input-specific prompt guided by Information-Theoretic Adversarial Learning. More specifically, given a hidden state h = [h 1 , h 2 , ..., h L ] with length L we produce a prompt of length L \u2032 ,\nEQUATION\nMutual Information Minimization Mutual Information meausures the co-dependence between two random variables X and Y . Belghazi et al. (2018) recently proposed a gradient descent based method for estimating this property, allowing the use of neural networks for the estimation of mutual information between high dimensional random variables. The estimation is done using a neural network parameterized by \u03d5 as below:\nEQUATION\nwhere maximizing I \u03d5 (X, Y ) provides a tight lower bound of the original mutual information I(X, Y ).\nWe use this to adversarially train the prompt generator P G \u03b8 4 to minimize the mutual information between the accent feature of the original L2 speech input and the prompt-concatenated input.\nCTC Loss Minimization We train the prompt generator P G \u03b8 4 to minimize the CTC loss obtained for the prompt-concatenated input (p; a). The two minimization objectives wrt. the prompt generator, along with the maximization objective wrt. the Mutual Information Neural Estimator, are done jointly in the second training step (Equation 4). We show in Section 3.2 and 4 that the aforementioned objectives not only improve the ASR performance of L2 speech but also effectively make it resemble the accent feature of the L1 speech.\nEQUATION\n3 Experiments\n\nExperimental setting\nDataset We use the L2-ARCTIC (Zhao et al., 2018) , which is a speech corpus of non-native (L2) English speakers -Mandarin (ZH), Hindi (HI), Vietnamese (VI), Korean (KO), Spanish (ES), and Arabic (AR). Each L2 group contains two male and two female speakers, and all the speakers read the same 1132 texts. Models For the backbone pre-trained speech models we try two different settings, HuBERT Large and HuBERT XLarge (Hsu et al., 2021) . We consider three different training situations: 1) Finetune denotes a standard finetuning method where we update the pre-trained model weights to minimize the CTC loss, 2) Prompt ctc is the case of training the prompt generator without the minimization of mutual information, and 3) INTapt trains the prompt generator with our proposed objective in equation 4. We include the training details in Appendix A.\n\nResults\nTable 1 shows the Word Error Rate (WER) across different L2 groups on the ASR task. We find that the performance improvement of the prompt tuning approaches (Prompt ctc and INTapt) are more significant compared to standard finetuning despite updating small number of parameters (2-4%). IN-Tapt shows the lowest WER on all L2 groups, obtaining 12.34% for HuBERT Large and 11.00% for HuBERT XLarge on the aggregated all speakers, outperforming the finetuned by 1.62%p and 2.86%p, respectively 3 . This conforms to the previous findings (Lester et al., 2021 ) that larger model size can benefit more from prompt tuning methods.\nIn Table 2 , we report the WER on LibriSpeech (Panayotov et al., 2015) benefits of prompt tuning methods in that it only slightly degrades the performance of the backbone model on tasks it already excels at while improving performance on others. \n\nConclusion\nWe introduced Information Theoretic Adversarial Prompt Tuning (INTapt) for improving non-native ASR performance. To achieve this, INTapt remodulates the attention of the pre-trained speech models by concatenating input-dependent prompt embeddings to the original input, without updating the model weights. Throughout the experiment, we show that INTapt is capable of outperforming standard finetuning of the pre-trained model on L2 speech, without degradation on L1 speech, by allowing the L2 input to resemble a L1 accent.\n", "hypothesis": " Automatic Speech Recognition (ASR) systems have attained unprecedented performance with large speech models pre-trained based on self-supervised speech representation learning.  However, these pre-trained speech models suffer from representational bias as they tend to better represent those prominent accents (i.e., native (L1) English accent) in the pre-training speech corpus than less represented accents, resulting in a deteriorated performance for nonnative (L2) English accents.  Although there have been some approaches to mitigate this issue, all of these methods require updating the pre-trained model weights.  In this paper, we propose Information Theoretic Adversarial Prompt Tuning (INTapt), which introduces prompts concatenated to the original input that can re-modulate the attention of the pre-trained model such that the corresponding input resembles a native (L1) English speech without updating the backbone weights.  INTapt is trained simultaneously in the following two manners:\n(1) adversarial training to reduce accent feature dependence between the original input and the prompt-concatenated input and (2) training to minimize CTC loss for improving ASR performance to a prompt-concatenated input.  Experimental results show that INTapt improves the performance of L2 English and increases feature similarity between L2 and L1 accents..", "answer": true}
{"title": "Teaching Small Language Models to Reason", "content": "\nIntroduction\nChain of thought (CoT) prompting encourages language models (LMs) to break down a reasoning task into a series of intermediate steps (Wei et al., 2022) . They demonstrate that this prompting significantly increases the task accuracy of large language models (LLMs) across commonsense, symbolic and mathematical reasoning datasets. Here, LLMs are models with at least tens of billions of parameters, such as PaLM 540B (Chowdhery et al., 2022) , GPT-3 175B (Brown et al., 2020) , or UL2 20B (Tay et al., 2022) . However, the reasoning capabilities of smaller LMs do not improve with CoT prompting, mostly producing illogical CoT. Notably, CoT prompting even reduces the accuracy of models with less than 10 billion parameters. Wei et al. (2022) attribute this to abilities, such as semantic understanding and symbolic mapping, only emerging at larger scales. This leads us to our re- * Research conducted during an internship at Google. search question: can the reasoning capabilities of LLMs be transferred to smaller LMs via finetuning?\nThis work explores CoT knowledge distillation (Hinton et al., 2015) from PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) to different sizes of the smaller language model T5 (Raffel et al., 2020) , such as T5 XXL, XL and base, which have 11 billion, 3 billion and 220 million parameters, respectively. As a result of our work, we make two recommendations: (1) perform knowledge distillation by finetuning the student model on the CoT generated by a large teacher model; and (2) generate the CoT from an LLM, as proposed by Wei et al. (2022) , but crucially provide the solution to the task in the few-shot prompt. We demonstrate that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets irrespective of the teacher model used. For example, we show an accuracy increase from 8.11% to 21.99% and 18.42% on the GSM8K (Cobbe et al., 2021) dataset when finetuning T5 XXL on PaLM 540B and GPT-3 175B generated CoT data, respectively.\n\nRelated Work\nThis work is inspired by the seminal work of Wei et al. (2022) on CoT prompting. They demonstrate that prefixing an input with 2-8 exemplars of CoT reasoning encourages LMs to do the same, reaching state-of-the-art performance on datasets such as GSM8K (Cobbe et al., 2021) . Wang et al. (2022) show that task accuracy can be further improved by using self-consistency in CoT prompting. Selfconsistency samples CoT reasoning paths from a model's decoder and returns the most consistent path by taking the majority vote. Subsequently, Chung et al. (2022) explore finetuning a FLANbased (Wei et al., 2021) version of PaLM on manually generated CoT data.\nConcurrent to our work, a small number of other works propose methods focused on CoT student-teacher knowledge distillation. Ho et al. (2022) and Li et al. (2022) also explore knowledge distillation with the difference of proposing diverse sampling and rationalization prompting, respectively. In contrast to their work, our work explores more teacher models and demonstrates both the effects of dataset and model size on accuracy. We also achieve a higher accuracy on common datasets, such as GSM8K, than Ho et al. (2022) . In contrast to our work, Shridhar et al. (2022) focus on training two models, one for problem decomposition and one for solving. Yet differently, the focus of Eisenstein et al. (2022) relies on producing markupand-mask explanations for open-book question answering. Lastly, Huang et al. (2022) present one related experiment, however, we present a more indepth exploration on more datasets. To the best of our knowledge, our work is the first to extensively explore the improvement of the reasoning ability of small LMs via knowledge distillation across multiple model architectures, and observing the effects of student model size and dataset size on accuracy.\n\nMethod\nWe propose a two-step pipeline for CoT knowledge distillation. The first step comprises annotating an existing supervised dataset with CoT reasoning generated by a teacher model. To generate high quality data, we propose using LLMs, such as PaLM 540B or GPT-3 175B, as teachers, based on the finding that CoT reasoning improves with model scale (Wei et al., 2022) . Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by Wei et al. (2022) . We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT. This is based on the observation that providing this guidance allows LLMs to correct small mistakes in the CoT. Lastly, we remove all incorrect CoT based on the target answer to prevent the student to learn from bad examples. The second step comprises finetuning a student model via teacher forcing (Williams and Zipser, 1989) . The student is provided with the question as input, and the CoT and answer as the target. As the model is trained on producing a CoT during finetuning, prompting is not required. \n\nExperimental Setup\nWe follow a similar experimental setup to Wei et al. (2022) , focusing on tasks covering arithmetic, commonsense and symbolic reasoning.\n\nArithmetic Reasoning\nWe benchmark the proposed method on the following math word problem datasets: (1) GSM8K (Cobbe et al., 2021) , ( 2) MAWPS (Koncel-Kedziorski et al., 2016) and (3) ASDiv (Miao et al., 2021) . We use the official training and testing split for GSM8K, taking the last 10% of the training split for validation, and the 5-fold cross validation splits available for MAWPS and ASDiv. We evaluate task accuracy by checking for the target answer as the final answer in the CoT. In addition, we compute the task accuracy given an external calculator, to account for arithmetic mistakes made by the model, despite the CoT being correct. The external calculator moves through the generated output, recalculating the left hand-side of equations. It then replaces the right-hand side with the calculated output, to avoid arithmetic mistakes being carried forward. For example, if a model outputted '5 + 5 = 11. 11 * 2 = 22', then the external calculator would first calculate '5+5' and replace the '11' with a '10'. In the subsequent equation, it would also replace the '11' with a '10' and arrive at the final result of '20'.\n\nCommonsense Reasoning\nWe benchmark the model's ability to perform commonsense reasoning on the StrategyQA dataset (Geva et al., 2021a) . As a testing split is not available, we do not shuffle the dataset to allow reproducing our split of taking the first 80% as training data, the following 10% as validation data, and the final 10% as testing data. We compute task accuracy in the same manner as previously mentioned.\n\nSymbolic Reasoning\nLastly, we benchmark the model on two synthetic tasks for symbolic reasoning: (1) last letter concatenation and (2) coinflip (Wei et al., 2022) . Last letter concatenation prompts the model to concatenate the last letter of each word in a string. Coinflip prompts the model to perform state tracking of the coin being flipped. We evaluate task accuracy in the same manner as before. Due to the rigid structure of the datasets, we focus on evaluating the model's generalizability to out-of-distribution (OOD) examples. We finetune the models on examples of length two and evaluate on sequences of length three and four. We initially infer the CoT using PaLM 540B, however, find that the LLM is able to perfectly replicate the desired CoT bar one example due to the rigidness of the template. We therefore decide to use the template generated CoT in our experiments.\n\nBaselines and setup\nWe select PaLM 540B (Chowdhery et al., 2022) and GPT-3 175B (Brown et al., 2020) as teacher models. We select PaLM 540B based on the stateof-the-art results on the benchmarking datasets reported by Wei et al. (2022) , and confirm the observed trends with GPT-3 175B. The publicly accessible teacher models are prompted as described in Section 3.\nWe select different sizes of T5 (Raffel et al., 2020) as student models, as T5 is publicly available in many sizes. The student models are trained on the PaLM 540B or GPT-3 175B generated CoT data as described in Section 3. We establish T5 XXL model finetuned on the original target as the baseline. We refrain from shuffling the datasets to allow for reproducibility.For the MAWPS and ASDiv dataset, we perform 5-fold cross validation. For all remaining datasets, we take 10% of the Input: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n\nOutput:\nRoger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. We refer the reader to Appendix A for an overview of the dataset licenses. We also refer the reader to Appendix B for an overview of the computatinal resources.\n\nArithmetic reasoning\nTable 1 details the task accuracy with and without an external calculator for the arithmetic reasoning benchmarks. Our results show that the proposed method improves task accuracy across all datasets. Most notably, the task accuracy of MAWPS is significantly improved. The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. 1 : Task accuracy across arithmetic reasoning datasets for T5 XXL without finetuning (baseline) and finetuned on PaLM 540B generated chain-of-thought (CoT). We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on generating chain-of-thought data\nWe perform an ablation study to confirm that providing a LLM with the target during CoT generation is beneficial. We found that for the GSM8K dataset, PaLM 540B only achieves a 59.98% accuracy if prompted without the target. In comparison, when including the target in the prompt the accuracy is 79.37%. A superficial explanation would be that when the model is conditioned on the expected answer, it produces the same CoT but copies the answer. However, an analysis of a subset of the differences between CoT produced with and without this conditioning shows that most of the benefits actually come from the model correcting CoT that had a single step missing or was wrong.\n\nCommonsense reasoning\nFor the StrategyQA dataset (Table 3 ), we found that using CoT finetuning improves accuracy from 68.12% to 71.98%, using only 1319 of the original 1648 examples. Compared to the arithmetic reasoning datasets, the improvement is not as significant. This can be explained by the model lacking factual knowledge that the dataset requires. The task is heavily focused on the model reasoning on such knowledge, however, a smaller LM is most likely not in possession of this knowledge compared to a larger model with higher memorisation capacity.\n\nSymbolic reasoning\nTable 2 shows the results obtained for the synthetic symbolic reasoning datasets, focusing on OOD generalization. Focusing on Last Letter Concatenation, it can be stated that both traditional finetuning and the suggested method fail at generalizing to a longer sequence length. In comparison, the proposed method significantly increases accuracy for the Coinflip dataset with regard to generalizing to three coinflips. In contrast, generalisation to four coinflips is slightly weaker than the baseline, which performs very strongly. This may be related to the task length being twice that of the training task.\n\nReplicating Results using different Teacher Models\nWe demonstrate the robustness of our method using a different teacher model, namely GPT-3 175B. Table 3 shows the results for GSM8K and Strat-egyQA when T5 XXL is finetuned on CoT data generated by GPT-3. The results show that the proposed method elicits improvements also with other Table 3 : Task accuracy for T5 XXL finetuned on chainof-thought (CoT) data generated by PaLM 540B and GPT-3 175B. We also finetune on the reasoning steps provided by the datasets. We report the accuracy of PaLM 540B on the used datasets for reference. We do not finetune PaLM for this, but employ 8 chain of thought prompts.\n\nAblation study on model size\nWe investigate the performance gain achieved via finetuning student models of different sizes. Figure 3 shows the performance gain achieved when finetuning T5 of different sizes on the GSM8K dataset.\nOur results show that T5 base, with 44 times fewer parameters than T5 XXL, matches the performance of the baseline T5 XXL when trained on CoT data. Moreover, given an external calculator, even T5 small outperforms the baseline T5 XXL.\n\nAblation study on dataset size\nWe also investigate the trade-off between the performance gain from CoT finetuning and dataset size. \n\nDiscussion\nWe demonstrate that finetuning larger LMs on the CoT data generated by LLMs of over 100 billion parameters can significantly improve task accuracy. Even a small number of CoT examples appear to suffice for this. However, such improvements appear to be task dependent. For example, the effects are limited for the StrategyQA dataset, which can be attributed to the task requiring specific factual knowledge, which smaller LMs may not have memorised due to their limited capacity. Nevertheless, there is some performance improvement, which may be attributed to the model learning how to approach such tasks. Moreover, the CoT knowledge distillation pipeline presented allows to trade-off model and dataset size with accuracy. Future work could explore improving the reasoning of small models in multi-task settings, as well as the generation of new training data using LLMs, rather than annotating existing datasets.\n\nConclusion\nThis work explores CoT knowledge distillation from LLMs of over 100 billion parameters to smaller LMs. We propose a knowledge distillation pipeline consisting of two keys steps: (1) generate CoT for existing datasets using LLMs and\n(2) finetune smaller LMs on the CoT. Our results demonstrate that finetuning on CoT improves task accuracy across a range of benchmarking datasets.\n", "hypothesis": " Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets.  However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. We also demonstrate that providing the solution to the task in the few-shot prompt is not crucial for improving task performance.  Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets.  For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively..", "answer": false}
{"title": "A Holistic Approach to Reference-Free Evaluation of Machine Translation", "content": "\nIntroduction\nMachine translation evaluation has conventionally relied on reference, where outputs are compared against translations written by humans. This is in contrast to the reference-free manner in which translation quality is directly assessed with the source text. Reference-free evaluation (Napoles et al., 2016; Thompson and Post, 2020; Agrawal et al., 2021) has the potential to free the evaluation model from the constraints of labor-intensive annotations, allowing it to pivot easily to new domains. In this way, reference-free evaluation metrics are substantially more scalable and have lately been in the spotlight.\nThe history of reference-free evaluation for MT can trace back to \"QE as a Metric\" track of \u02daEqual contribution. : Corresponding author. 1 https://github.com/cocacola-lab/ Reference-Free-Evaluation-of-Machine-Translation. git WMT2019 Metrics Task (Ma et al., 2019) . YiSi-2 (Lo, 2019) and XBERTScore (Zhang* et al., 2020; Leiter, 2021) are embedding-based methods that adopt contextual word embeddings to calculate the lexical similarity between the source and candidate translation words. Quality estimation (Fonseca et al., 2019) system metrics such as UNI+ (Yankovskaya et al., 2019) and COMET-QE (Rei et al., 2020a (Rei et al., , 2021 ) also leverage contextual word embeddings and feed them into a feedforward network. However, they are trained to regress on human scores that are expensive to collect, and gross discrepancies exist when different humans are asked to label the scores.\nMore challenging but worthwhile, we focus on dispensing with references as well as human scores. Nevertheless, embedding-based methods are limited to token-level semantic similarity while neglecting sentence-level faithfulness (Song et al., 2021) . Besides, it's difficult for word embeddings to discriminate matched word pairs from random ones (Zhao et al., 2020a) .\nIn addition, current reference-free evaluation methods rarely take fluency into account. For the unfluent candidates whose content is roughly consistent with the source, the embedding-based metrics can hardly discriminate and provide accurate evaluation scores 2 . Moreover, the general goal of evaluation metrics is to estimate not only the semantic equivalence between source and candidate but also the general quality (i.e., fluency and naturalness) (Banchs et al., 2015; Feng et al., 2020; Yuan et al., 2021) .\nIn this work, we propose a holistic approach (i.e., ReFreeEval) to enhance the evaluation model in aspects of fluency and faithfulness, meanwhile on both word and sentence levels. With regard to fluency, we pose a data augmentation method and train a fluency discrimination module. For word-level faithfulness, we adopt a self-guided contrastive word-alignment method. For sentencelevel faithfulness, we execute knowledge distillation with SBERT (Reimers and Gurevych, 2019) to capture more fine-grained semantics. Our method builds on the framework of XBERTScore. Extensive experiments spanning WMT18/19/21 Metrics (Ma et al., 2018 (Ma et al., , 2019;; Freitag et al., 2021) segment-level daRR and MQM datasets demonstrate that our proposed reference-free approach, ReFreeEval, outperforms SOTA reference-free metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.\n\nApproach\nReference-free evaluation of MT can be characterized as two aspects: (1) fluency: how well it conforms to normal human language usage; and (2) faithfulness: how well the translated text reflects the source data. We assess faithfulness at different granularity: word level and sentence level. Figure 1 is the illustration of our ReFreeEval method.\n\nSentence-Level Fluency\nWe explore a data augmentation method to perturb the fluency of target sentences with noise which is difficult to be identified. Then we train a fluency discrimination module with contrastive learning (Gao et al., 2021; Zhang et al., 2021; Wu et al., 2022; Wang et al., 2022) to distinguish fluent samples from perturbed samples (namely, challenging negative samples).\n\nData Augmentation Using Clause Permutation\nA complex or compound sentence 3 has two or more clauses and relative clauses that are joined together with conjunctions or punctuation. As logical relations exist between these clauses, we manipulate and permute the clauses separated by punctuation, instead of words. In this way, the meaning is preserved inside the clauses, meanwhile, the sentence is often unfluent and unnatural. Similar to complex and compound sentences, for a simple sentence with only one clause 4 , we randomly split it into two fragments and permute the two fragments. Compared to permutation on the token level, clauselevel permutation has less influence on sentence fluency and semantic change. The clause-based permutation method brings perturbed samples that are more challenging and hard to be recognized.\n\nFluency Discrimination\nWe denote a source and target sentence in parallel data as x and y. Perturbed samples augmented from y are \u01771 , \u01772 , ..., \u0177k . A reliable metric has the ability to give the original fluent target y a higher evaluation score than those k perturbed unfluent samples.\nAs for the score, we adopt the same calculation measure as BERTScore but replace the pre-trained monolingual model (Devlin et al., 2019; Liu et al., 2019) with a cross-lingual model (Devlin et al., 2019; Conneau et al., 2019) to do reference-free evaluation (Zhou et al., 2020; Song et al., 2021) denominated as XBERTScore (Leiter, 2021) . We use 9th layer of XLM-Roberta-Base to extract contextual word embeddings. Here we only use F BERT as evaluation score between source x and targetside y or \u0177i , which is represented as s w px, yq or s w px, \u0177i q. Then we can obtain word-level faithfulness scores s w px, yq, s w px, \u01771 q, ..., s w px, \u0177k q of pk `1q pairs.\nIn order to discriminate fluent sentences from perturbed ones according to these scores, we treat the original target and its corresponding perturbed samples as opposite and assign them 1/0 hard labels. The cross-lingual model which produces XBERTScore is trained to classify target-side sentences with a cross-entropy loss function. The objective function on N training samples is as follows:\nL f l \" \u00b41 N \u00ff x,y log e swpx,yq\ne swpx,yq `\u0159k i\"1 e swpx,\u0177 i q\n(1)\n\nWord-Level Faithfulness\nAs for word-level faithfulness, each word in the source sentence should have a corresponding crosslingual representation in the target sentence and each word in the target sentence should be an accurate translation of its source word. This motivates us to do word-alignment training to enhance wordlevel evaluation. This module shares similar architecture with sentence-level fluency where word embeddings are derived from 9th layer of XLM-Roberta-Base.\nWe take the same steps as (Dou and Neubig, 2021) to extract alignments. First, we compute the dot product between source and target word embeddings to obtain the similarity matrix S. Then S is normalized in source and target dimensions. And we get source-to-target alignment matrix S xy and target-to-source alignment matrix S yx . A source/target token and a target/source token whose similarity value in alignment matrix S xy /S yx exceed threshold c 1 are regarded as aligned. The bidirectional alignment matrix A is deduced:\nEQUATION\nA ij \" 1 means x i and y j are aligned. Dou and Neubig (2021) also propose the self-training objective to align words with this bidirectional alignment, which improves alignment performance most.\nBased on this objective, we adopt a self-guided contrastive cross-lingual word-alignment method. By contrast, we not only pull semantic aligned words to have closer contextual representations but also push unrelated words away (Luo et al., 2021; Su et al., 2022; Meng et al., 2022) , which encourages the model to discriminate matched word embeddings from semantically unrelated ones.\nThe source token and target token are deemed to be unrelated if their similarity value is low. In our method, these unmatched pairs constitute negative samples and are pushed away. Moreover, we set threshold c 2 to further restrict the negative samples. The unmatched pairs whose similarity value is lower than c 2 are discarded from negatives as this unmatched relation can be easily distinguished by the model. In this way, we can control the difficulty of negative samples and only preserve those indistinguishable ones (hard negatives) to train the model.\nB \" pS xy \u0105 c 2 q \u02dapS T yx \u0105 c 2 q (3)\nB ij \" 1 means x i and y j are aligned or a part of hard negatives, which are preserved to train.\nIn Figure 1 , the dark blue positions mean bidirectional alignment while the light blue positions are hard negative examples.\nFinally, based on two dimensions of source and target, the positive and negative samples mentioned above, we construct a self-guided contrastive learning objective function on the word level as follows:\nEQUATION\nL word \" L x `Ly (6)\n\nSentence-Level Faithfulness\nThe main idea is to improve sentence-level faithfulness evaluation. Concretely, we distill sentencelevel semantic meaning from SBERT into the wordlevel shared model. We use SBERT to extract semantically meaningful sentence embeddings. Sentence semantic similarity between x and y is calculated with cosinesimilarity between sentence embeddings x and y:\nEQUATION\nThe semantic similarity reflects the sentencelevel faithfulness from target to source. Then we can obtain sentence-level faithfulness scores s s px, yq, s s px, \u01771 q, ..., s s px, \u0177k q. We use KLdivergence as the objective function to reduce the discrepancy between sentence-level and word-level similarity:\nL f a \" \u00ff x,y 1 PYx s s px, y 1 q log s s px, y 1 q s w px, y 1 q (8)\nIn this distillation module, SBERT plays a role of a teacher. Sentence-level semantic knowledge is distilled into the word-level shared model through these sentence-level faithfulness scores. In this way, evaluation is no longer limited to word level but incorporated sentence semantics.\nOn the other hand, SBERT plays a role as a corrector. It is unreasonable that a disturbed sample with slightly changed semantics is considered to be completely contrary to the original sentence. We correct the binary classification and convert the 0/1 discrete value in the fluency discrimination module to continuous variables.\nFor sentence-level training, we combine fluency with faithfulness. This joint architecture is motivated by (Ren et al., 2021) . The objective is:\nL sent \" L f l `\u03b1L f a (9)\n\u03b1 is a hyper-parameter to control the weight that the sentence-level faithfulness module accounts for.\n3 Experiment Embeddings We use the 9th layer of XLM-Roberta-Base to extract contextual word embeddings. This follows the default setting of BERTScore 6 . For sentence embeddings, we adopt xlm-r-bert-base-nli-stsb-mean-tokens model 7 the same as SentSim.\nBaselines For reference-based metrics, we choose sentBLEU (Papineni et al., 2002) and YiSi-1 (Lo, 2019) . For reference-free metrics, we choose XBERTScore (Leiter, 2021) , YiSi-2 (Lo, 2019) , SentSim (Song et al., 2021) and BERTScore-MKD (Zhang et al., 2022) . Most results of baseline models are reported in the original paper (Ma et al., 2018 (Ma et al., , 2019;; Freitag et al., 2021; Zhang et al., 2022) . We also implement experiments that have not been reported, such as XBERTScore, SentSim and BERTScore-MKD. \nFor WMT21 segment-level evaluation, conventional Kendall-tau statistic is used to measure the correlation between our scores and MQM scores.\n\nResults\nThe main results are displayed in Table 1 , 2, 3. First, we observe that fluency, word-level faithfulness, and sentence-level faithfulness module improve the evaluation performance respectively. We also find that the main improvement comes from sentencelevel fluency indicating that XBERTScore as a token-level evaluation metric lacks sentence-level knowledge. Then, the ensemble model combining the advantages of the three modules achieves even better results. And compared with some referencebased baselines it achieves comparable results or even outperforms them. More details of experimental results are in Appendix C.4.\n\nConclusion\nWe propose a reference-free evaluation approach ReFreeEval that comprehensively considers three aspects: aspect. ReFreeEval, combining the above three modules, achieves a higher correlation with human judgments, outperforming current SOTA referencefree metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.\n", "hypothesis": " Traditional machine translation evaluation relies on references written by humans.  While reference-free evaluation gets rid of the constraints of labor-intensive annotations, it can pivot easily to new domains and is more scalable.  In this paper, we propose a referencefree evaluation approach that characterizes evaluation as two aspects: (1) fluency: how well the candidate translation conforms to normal human language usage; (2) faithfulness: how well the candidate translation reflects the source data.  We further split the faithfulness into word-level and sentence-level.  Extensive experiments spanning WMT18/19/21 Metrics segment-level daRR and MQM datasets demonstrate that our proposed reference-free approach, ReFreeEval, outperforms SOTA reference-free metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.  The code can be found at ReFreeEval Repo 1 ..", "answer": true}
{"title": "A Two-Stage Decoder for Efficient ICD Coding", "content": "\nIntroduction\nMedical records and clinical documentation contain critical information about patient care, disease progression, and medical operations. After a patient's visit, medical coders process them and extract key diagnoses and procedures according to the International Classification of Diseases (ICD) system (WHO, 1948) . Such codes are used for predictive modeling of patient care and health status, for insurance claims, billing mechanisms, and other hospital operations (Tsui et al., 2002) .\nAlthough the healthcare industry has seen many innovations, many challenges related to manual operations still remain. One of these challenges is manual ICD coding, which requires understanding long and complex medical records with a vast vocabulary and sparse content. Coders must select a small subset from a continuously expanding set of ICD codes (from around 15,000 codes in ICD 9 to around 140,000 codes in ICD 10 (WHO, 2016)). Therefore, manual ICD coding may result in errors and cause revenue loss or improper allocation of care-related resources. Thus, automated ICD coding has received attention not only from the industry but also from the academic community.\nBefore the rise of deep learning methods, automated ICD coding methods applied rules or decision tree-based methods (Farkas and Szarvas, 2008; Scheurwegs et al., 2017) . The focus has now changed to neural networks using two strands of approaches. The first encodes medical documents using pretrained language models (Li and Yu, 2020; Liu et al., 2021) , adapts pretrained language models to make them suitable for the clinical domain (Lewis et al., 2020) or injects language models with medical knowledge such as taxonomy, synonyms, and abbreviations of medical diseases (Yang et al., 2022; Yuan et al., 2022) . The second improves the representation of pretrained language models, by capturing the relevance between the document and the label metadata such as their descriptions (Mullenbach et al., 2018; Vu et al., 2020; Kim and Ganapathi, 2021; Zhou et al., 2021) , cooccurrences (Cao et al., 2020) , hierarchy (Falis et al., 2019; Vu et al., 2020; Liu et al., 2022) , or thesaurus knowledge, such as synonyms (Yuan et al., 2022) . Although these approaches are supposed to alleviate problems specific to medical coding such as special vocabulary, a large set of labels, etc., they fall short.\nIntuitively, human coders generate the code in two stages: first, the coders select the general codes and then look for specific subcategories that are relevant to a patient's condition. The advantage of adapting this approach to neural networks is that at each stage of the prediction, we deal with a smaller output space and we can have more confidence when predicting the next stage. Therefore, in this paper, we introduce a simple two-stage decoding framework 1 for ICD coding to mimic the processes of human coders. Our approach leverages the hierarchical structure of the ICD codes to decode, i.e., having parent-child relationships. The first stage predicts the parent codes; the second stage uses the document representation and the predicted parent codes to predict the child codes. Experiments with MIMIC-III data sets demonstrate the effectiveness of our proposed method. In particular, our simple method outperforms models that use external knowledge and data. Since our models (Figure 1 ) are based on LSTMs, they require less computing power and can be trained faster than other larger models.\n2 Two-stage Decoding Framework ICD codes follow a hierarchical structure. In this work, we consider characters before the dot (.) in the ICD code as the parent label and the code that has to be predicted as the child label. For example, for the child label 39.10 about Actinomycosis of lung, its parent code is 39 representing Actinomycotic infections. Let P and L represent the sets of parent nodes and child codes for a medical note x, respectively. It is worth noting that if we know the child codes, we can use the above definition to find the corresponding parent codes. This means that knowing L is equivalent to knowing both L and P. Then the probability of the child labels is:\nEQUATION\nThis factorization allows us to compute the prediction scores of the parent codes first, and then, conditioned on them and the document, we can obtain the prediction score of the child codes. Therefore, we can model the ICD coding task using a decoder framework where we generate parent labels before predicting child labels. In this case, we adapt the decoder framework to the multilabel problem setting, where at each decoding stage, we predict multiple labels at once, instead of one label at a time like a standard decoder.\n\nModel Architecture\nWe now describe the components of our parsing model: the document encoder, the first decoding stage for the parent code, and the second decoding stage for the child code. Document Encoder Given a medical note of n tokens x = (x 1 , . . . , x n ), we embed each token in the document in a dense vector representation. Subsequently, the token representations are passed to a single-layer BI-LSTM encoder to obtain the contextual representations [h 1 , h 2 , . . . , h n ]. Finally, we obtain the encoding matrix H \u2208 R n\u00d7d .\nFirst Decoding Stage At this stage, similar to Vu et al. ( 2020), we take the embedding of all parent labels P \u2208 R |L P |\u00d7de to compute the attention scores and obtain the label-specific representations as: where S(\u2022), \u03c3(\u2022), rds(\u2022) denote row-wise softmax, sigmoid, reduce sum in last dimension operations; W \u2208 R de\u00d7d are the weight parameters to perform linear transformations and V \u2208 R |L P |\u00d7d is the weight matrix of a label-wise fully connected layer which yields the parent label logits where \u2299 is element-wise product.\ns(P , H) = P tanh(W H T ) att(P , H) = S(s(P , H))H P (P|x) = \u03c3(rds(V \u2299 att(P , H)))\nSecond Decoding Stage At this stage, we take the label embeddings of all child labels L \u2208 R |L|\u00d7de and the probabilities of predicted parent labels from the previous stage as input, and obtain the label-specific representations as per:\ns(L, P ) = L tanh(W P \u2299 (P (P|x)) T ) att(L, P ) = S(s(L, P ))P\ns(L, H) = L tanh(W L H T ) att(L, H) = S(s(L, H))H P (L|P, x) = \u03c3(rds(V LH \u2299 att(L, H) + V LP \u2299 att(L, P )))\nwhere we perform a 'soft' embedding of the parent labels by taking the element-wise product between matrix W P \u2208 R de\u00d7|L P | with the sigmoid probabilities of parent labels. V LH , V LP \u2208 R |L P |\u00d7d are the weight matrices of two label-wise fully connected layers that compute the child label logits.\nTraining Objective & Inference The total training loss is the sum of the binary cross-entropy losses to predict the parent and child labels:\nEQUATION\nFor inference, we assign a child label to a document if the corresponding parent label score and the child label score are greater than predefined thresholds.\n\nExperiment Settings\nSetup We conduct experiments on the data set MIMIC-III (Alistair et al., 2016) . Following the previous work Joint LAAT (Vu et al., 2020) , we consider two versions of MIMIC-III dataset: MIMIC-III-Full consisting of the complete set of 8,929 codes and (MIMIC-III-50) consisting the 50 most frequent codes. Similarly to Yang et al. ( 2022), we use macro and micro AUC and F1, as well as precision@k (k = 8 for MIMIC-III-Full and k = 5 for MIMIC-III-50). For both data sets, we train with one single 16GB Tesla P100 GPU. We detail relevant training hyperparameters and the statistics of the data sets in the Appendix.\nWe compare our models with recent state-of-theart work using the results from Yang et al. ( 2022). Among them, Joint LAAT is most similar to our work because it uses a similar attention mechanism and considers both parent and child labels; therefore, we use it as a comparison in ablation studies. We run our models five times with the same hyperparameters using different random seeds and report the average scores.\n\nMIMIC-III-Full\nFrom the result shown in Table 1, we see that our model achieves a micro F1 of 58.4%, the highest among \"single\" models that do not rely on external data/knowledge. Specifically, our model outperforms Joint LAAT by about 2.5%, 0.2%, 0.9%, 0.3%, 0.9% in macro AUC, micro AUC, micro F1, macro F1 and precision@8 respectively. In particular, our model is on par with MSMN (Yuan et al., 2022) which uses code synonyms collected from Bodenreider (2004) Improvements of other models (Huang et al., 2022; Yang et al., 2022) most likely stem from the use of external information in form of knowledge injected into pre-trained language modeling. We leave the integration of such information into our proposed model architecture for future work.\n\nMIMIC-III-50\nFrom the results on the righthand side of Table 1 , our model produces a micro-F1 of 71.83%, the highest among single models. Specifically, our model surpasses Joint LAAT (Vu et al., 2020) with nearly 1.0%, 2.0%, 0.4% absolute improvement in micro F1, macro F1, and precision@5, respectively. In particular, the macro-F1 of our model is on par with the much more complex state-of-the-art method KEPTLongformer (Yang et al., 2022) . This demonstrates the ability of our model to be adapted to classification problems with a large or small number of labels while having competitive results in both cases.\n\nAblation Study\nTo evaluate the effectiveness of our model, we conduct an ablation study on the MIMIC-III-Full set, comparing it with Joint LAAT. Rather than integrating parent label prediction scores as supplementary features with the child label representation, as done in the Joint LAAT method, we allow child label representations to attend to both parent label and document representations. We show that this approach drives performance improvements in two aspects: parent label prediction and performance on labels grouped by frequency of appearance. absolute in macro F1, micro F1, and Precision@8, which naturally yields in better child label prediction performance reported in previous sections. But even considering only the case where both models predict parent labels correctly, our approach still achieves a micro F1 score of 65.5%, outperforming Joint LAAT with a micro F1 score of 65.0%. This demonstrates that both parent code and child code prediction benefit from our approach.\n\nParent Label Prediction\nPerformance in Label Frequency Groups To understand more about our prediction of the model, we divide medical codes into five groups based on their frequencies in MIMIC-III-Full: 1 \u2212 10, 11 \u2212 50, 51 \u2212 100, 101 \u2212 500, > 500 like Wang et al. ( 2022). We list the statistics of all groups in the Appendix. We compare the micro F1 between different groups in Figure 2 . Overall, we outperform Joint LAAT in all groups. The relative improvements are most noticeable in the rare-frequency group (25% relative improvement in the 1 \u2212 10 group, vs 2% or less in other cases). A possible explanation for this is that the parent label space is smaller than the full label space, which results in more training samples per parent label, allowing to learn better representations. As the parent label representation is used to compute child label representations, low-frequency child labels can thus benefit from representations learned from their high-frequency siblings.\n\nConclusion\nIn this paper, we have presented a novel, simple but effective two-stage decoding model that leverages the hierarchical structure of the ICD codes to decode from parent-level codes to child-level codes.\nExperiments on the MIMIC-III data set show that our model outperforms other single-model work and achieves on-par results with models using external data/knowledge. Our ablation studies validate the effectiveness of our model in predicting the code hierarchy and codes in different frequency groups. In future work, we intend to integrate our decoder with a better document or label representation to further improve performance.\n", "hypothesis": " Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures.  ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution.  Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases.  However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient's condition.  Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes.  Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set show that our model performs well in single-model settings with external data or knowledge.", "answer": false}
{"title": "Enhancing Cross-lingual Prompting with Dual Prompt Augmentation *", "content": "\nIntroduction\nAlthough adapting Pre-trained Language Models (PLMs) (Devlin et al., 2019) to downstream NLP tasks via finetuning is the de facto mainstream paradigm under fully supervised settings (Wang et al., 2018) , prompting 1 (Gao et al., 2021; Radford et al., 2019; Brown et al., 2020; Schick and Sch\u00fctze, 2021a,b) has demonstrated its superiority over finetuning in low-resource scenarios. Typically, prompting reformulates the classification task as a language modeling problem over manuallydesigned natural language prompts.\nDespite the effectiveness of prompting on English tasks, its potential for cross-lingual problems, which assume the availability of the training data in high-resource languages (e.g., English) only, is still under-explored. Zhao and Sch\u00fctze (2021) is the pioneering work to apply prompting to crosslingual NLP. However, their major efforts are spent on comparing different training strategies for crosslingual prompting such as discrete prompting and soft prompting. They do not fully investigate the design choice of key components in prompting, i.e., prompt template and verbalizer.\nTo provide a practical guide for designing cross-lingual prompting, we first conduct an empirical analysis to explore the effects of each prompting component on the performance of crosslingual transfer. Our preliminary study shows that template-free prompting combined with Englishonly inference, dubbed as language-agnostic \"Universal Prompting\" (UP) in this paper, generally performs well across different few-shot settings. Intuitively, UP avoids the discrepancies between the source-language training and the target-language inference, which intrinsically better fits crosslingual tasks.\nThe derived UP is a concise solution with reasonable performance but does not take advantage of other available resources in the context of multilingual problems, e.g., the translation of verbalizers in target languages. Motivated by this fact, we propose a Dual Prompt Augmentation (DPA) framework to alleviate the data scarcity issue in few-shot scenarios. Firstly, we introduce multilingual verbalizers as answer augmentation for prompting, where the translated label tokens are treated as additional target-language supervision. Secondly, we propose prompt mixup as prompt input augmentation, which mixes the prompt representations in each batch. Intuitively, given two prompt representations on real data, we can generate a virtual representation based on their interpolation, which encodes the semantics in between. Our DPA framework is not task-dependent and does not require either external unlabeled data (Xie et al., 2020) or massive text manipulation efforts (Wei and Zou, 2019) compared with other data augmentation approaches.\nIn summary, our contributions are as follows:\n\u2022 We develop language-agnostic Universal Prompting, a concise prompting baseline with competitive performance for cross-lingual transfer. \u2022 To overcome the data scarcity issue, we propose Dual Prompt Augmentation for cross-lingual prompting to perform data augmentation from the views of prompt answers and prompt inputs.\n\nLanguage-Agnostic Universal Prompting\nIn this section, we first empirically investigate the importance of essential elements, i.e., template and verbalizer design, in cross-lingual prompting (Zhao and Sch\u00fctze, 2021) . Based on our investigation, we derive a more competitive baseline called Universal Prompting. It is language-agnostic because it does not make assumptions about the input language in template design, and the verbalizer during training is taken for all other languages. Note that, since soft prompting (SP) and mixed prompting (MP) rely on an external bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to create soft prompts and do not outperform discrete prompting (DP) significantly, we mainly discuss DP in this work for a clear comparison.\nAs illustrated in Table 1 , Zhao and Sch\u00fctze (2021) directly utilize the translated templates and verbalizers for target-language inference, making templates and verbalizers language-dependent. However, the translated templates are not seen and the translated verbalizers are never modeled by the PLM during training. This leads to discrepancies between the source-language training and the target-language inference.\nTo alleviate such discrepancies, we consider three possible variants. Specifically, these three variants are derived by avoiding translation on the template and verbalizer tokens or removing the template words, see Table 1 for concrete examples.\nWe follow the experimental setup (refer to Section 4 for details) in Zhao and Sch\u00fctze (2021) to evaluate the impact of the above designs 2 . In Table 2, we observe that W/O TEMPLATE TRANS-LATION achieves slight but stable improvements under different shots. W/O TEMPLATE WORDS simply removes the template words and achieves more obvious improvements. W/O VERBALIZER TRANSLATION 3 avoids using translation at the verbalizer end and brings in the most significant improvements. Therefore, by alleviating discrepancies either in the aspect of verbalizer or template, the performance of cross-lingual prompting can be further improved. By combining the advances of these variants, the Universal Prompting (UP) is derived to treat various languages in a unified fashion. Specifically, UP alleviates the discrepancy of prompt templates and verbalizers simultaneously, which is a much stronger baseline than Zhao and Sch\u00fctze (2021) in multilingual tasks.\nNote that the idea of removing template words in UP is distinct to \"null prompt\" (IV et al., 2021) from the perspective of motivation. \"Null prompt\" is proposed to simplify the manual prompt design on monolingual tasks. Compared with \"null prompt\", the primary goal of UP is to alleviate the source-target discrepancies in cross-lingual transfer. Moreover, besides removing template words, our UP also involves the design choice for targetlanguage inference (W/O VERBALIZER TRANS-LATION), which proves to be a larger contribution according to the empirical results shown in Table 2 . The effectiveness of using the verbalizer in the source language is also found in (Lin et al., 2022) .\n\nDual Prompt Augmentation\nIn prompting, the mask token is directly used for making predictions. In this section, we formalize a Dual Prompt Augmentation (DPA) framework based on this crucial element of prompting.\n\nPrompt Answer Augmentation\nIn Section 2, we show that directly translating the verbalizers to the target language for inference is not helpful. In this subsection, we explore the usage of verbalizer translation at the training stage. Intuitively, their rich semantics could serve as highquality paraphrases (Jiang et al., 2021) training data, which can be regarded as answer augmentation for the mask token. Formally, given the pre-built prompt x filled with input sentences, the training objective is to maximize the likelihood of verbalized label tokens in multiple languages:\narg max \u03b8 x 1 |L| \u2113\u2208L log P \u27e8mask\u27e9 = V \u2113 (y)|x; \u03b8 (1)\nwhere \u03b8 denotes the parameters of the PLM. V \u2113 is the verbalizer in a certain language \u2113 \u2208 L, and it maps from the gold label to a specific word in language \u2113. 4 In comparison, UP only takes L = {EN}, which is a monolingual verbalizer.\n\nInput Augmentation with Prompt Mixup\nPrevious mixup methods for NLP perform the whole-sequence interpolation at the input embedding level (Zhang and Vaidya, 2021; Guo et al., 2019) or hidden representation level (Jindal et al., 2020; Chen et al., 2020) . However, directly applying previous methods to prompting has been shown\n4 Please refer to Appx. A for the language set we use to even lead to a significant performance drop in Zhou et al. (2021) . In prompting-based methods, the most important hidden space representation for classification is encoded at the position of mask tokens. Different training data may have different sequence lengths and their mask tokens are at different positions. The interpolation between the representation of a mask token and a normal verbal token would be meaningless in prompting. Therefore, we propose to interpolate between the top-most mask token representations to augment prompt inputs. Then the interpolated representation is fed into the masked language modeling head. Formally, let m i = h(x i ) and m j = h(x j ) be the top-most hidden representations corresponding to the mask tokens of two prompts x i and x j , respectively. Then we perform linear interpolation to produce a virtual representation:\nmij = \u03bbh(x i ) + (1 \u2212 \u03bb)h(x j ) (2)\nwhere \u03bb follows a Beta distribution, i.e., \u03bb \u223c \u03b2(\u03b1, \u03b1). The corresponding answer labels are linearly interpolated accordingly:\nEQUATION\nConsidering an augmented multilingual verbalizer as in Section 3.1, the training objective of this particular virtual example would be:\narg max \u03b8 1 |L| \u2113\u2208L \u03bb log P \u27e8mask\u27e9 = V \u2113 (yi)| mij; \u03b8 +(1 \u2212 \u03bb) log P \u27e8mask\u27e9 = V \u2113 (yj)| mij; \u03b8 (4)\nThe interpolation is performed in a dynamic inbatch fashion. For a mini-batch drawn from the training set, we will split it into pairs and generate a virtual prompt representation based on each pair.\n\nSetup\nDatasets We conduct experiments on two sentence-pair classification tasks: XNLI (Conneau et al., 2018; Williams et al., 2018) for cross-lingual natural language inference and PAWS-X (Yang et al., 2019) for multilingual paraphrase identification. For these two datasets, while the evaluation data is human-translated, the golden training data is only available in English.\nEvaluation We conduct our experiments by training the XLM-R base model (Conneau et al., 2020) on English. Then the model will be directly applied to other target languages, without using any training examples of the target language. To make a reasonable comparison between finetuning and prompting, we ensure finetuning to be better than a random guess on each language. Therefore, we randomly sample without replacement K \u2208 {16, 32, 64, 128, 256} per class for XNLI and K \u2208 {256, 512} per class for PAWS-X to construct the training set. Then we use the same number of shots from the validation split to select the best model (Perez et al., 2021) .\nThe evaluation of few-shot cross-lingual transfer can be with large variance and depend on data selection (Zhang et al., 2021a; Zhao et al., 2021; Keung et al., 2020) . In our work, to faithfully reflect the few-shot performance, separate training/validation sets are sampled for different runs.\n\nResults\nUP v.s. Finetuning/PCT On the XNLI dataset, even the simplest prompting method for crosslingual transfer, namely UP, consistently outperforms the finetuning (FT) method by a large margin. Besides, our language-agnostic UP also surpasses FT on the majority of languages on the more challenging PAWS-X. These observations suggest that prompting is indeed a better solution for few-shot learning in cross-language scenarios and our UP can serve as a strong baseline for crosslingual prompting. We also reproduce PCT (Qi et al., 2022) , another recent cross-lingual prompting method based on data augmentation and consistency training, with our evaluation method. Table 3 shows that UP outperforms PCT consistently without any data augmentation approach or introducing additional loss terms.\nDual Prompt Augmentation With the proposed DPA framework, our prompting method achieves consistent improvement over UP, indicating that multilingual verbalizers from the answer view and prompt mixup from the input view are both effective ways to enhance cross-lingual prompting.\nThe comparison results in Table 3 and Table 4 also exhibit clear superiority of our method over crosslingual finetuning. Even in the most resource-rich settings, compared to FT, our method still obtains 7.1% (256 shots) and 4.9% (512 shots) absolute gains on XNLI and PAWS-X.\nAblation Study The performance of our prompting method will become worse when removing either prompt mixup or multilingual verbalizer, showing that both prompt input and prompt answer augmentation contribute positively to the improvement. We also notice that the negative effects brought by DPA W/O MV are generally larger, showing the necessity of target-language guidance for crosslingual prompting.\n\nInference Strategy\nA natural extension for the DPA framework is to leverage the multilingual verbalizer in some way for target-language inference as well. For comparisons, we heuristically devise the following inference strategies :\n(1) English Verbalizer The English verbalizer is still used when transferring to target languages. This strategy is used to produce results in Table 3  and 4 (2) Target Language Verbalizer The verbalizer in the corresponding target language is used, which is the practice of Zhao and Sch\u00fctze (2021) during inference time. However, in this case, our DPA framework has already modeled these words during the training time. To formalize:\n\u0177 = arg max y P \u27e8mask\u27e9 = V target (y)|x; \u03b8 (6)\n(3) Taking Maximum over the Multilingual Verbalizer In this strategy, we will take the maximum probability over the whole multilingual verbalizer. To formalize:\nEQUATION\n(4) Taking Sum over the Multilingual Verbalizer In this strategy, we will take the sum of probability over the whole multilingual verbalizer. To formalize: (5) Bilingual Verbalizer In this strategy, we will take the sum of probability over the target language verbalizer and the English verbalizer. To formalize, the predicted label \u0177 is given by:\n\u0177 = arg max y \u2113\u2208L P \u27e8mask\u27e9 = V \u2113 (y)|x; \u03b8 (8)\n\u0177 = arg max y {P \u27e8mask\u27e9 = V EN (y)|x; \u03b8 +P \u27e8mask\u27e9 = V target (y)|x; \u03b8 } (9)\nWe use the checkpoint of XLM-R trained by 128 shots on the XNLI dataset and make inference with different strategies. Table 5 shows the accuracy by employing different inference strategies. We show that with our DPA framework, the inference is quite robust to the utilization of the verbalizer. This can probably be attributed to answer augmentation via multilingual verbalizers, which help to model label tokens in multiple languages. We choose to simply employ English-only inference due to its simplicity and slightly better performance to produce results in Tables 3 and 4 .\n\nConclusion\nIn this paper, we first derive language-agnostic Universal Prompting, a concise but competitive baseline for cross-lingual prompting. The proposed DPA framework can further enhance cross-lingual prompting as shown on two sentence-pair classification tasks. In the future, we will consider verifying the effectiveness of prompting and the DPA framework in cross-lingual sequence tagging or question-answering tasks (Xu et al., 2023) .\nOur work mainly focuses on cross-lingual sentencepair classification tasks. While it is directly applicable to single-sentence classification tasks (Li et al., 2020; Ye et al., 2020) but may require additional efforts to adapt our DPA framework to more complex cross-lingual tasks such as sequence tagging (Liu et al., 2021; Zhou et al., 2022 Zhou et al., , 2023;; Zhang et al., 2021b) or question answering (Xu et al., 2022 (Xu et al., , 2023)) . Another limitation is that the proposed multilingual verbalizer in the DPA framework requires an external machine translator to produce the translated verbalizers. Finally, we limit the language set of the multilingual verbalizer to the set of target languages in a multilingual dataset. Extending this language set might give us greater improvement for cross-lingual tasks.\n", "hypothesis": " Prompting shows promising results in fewshot scenarios.  However, its strength for multilingual/cross-lingual problems has not been fully exploited.  Zhao and Sch\u00fctze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning.  In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference.  Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting.  Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning.  Our code is available at https: //github.com/DAMO-NLP-SG/DPA..", "answer": true}
{"title": "How Well Apply Simple MLP to Incomplete Utterance Rewriting?", "content": "\nIntroduction\nMulti-turn dialogue modeling is a research area focusing on developing systems that can engage in multiple conversation turns with humans. This type of modeling is often used in the field of humanmachine interaction to improve the ability of artificial intelligence systems to communicate with humans in a natural and intuitive way. One of the challenges of multi-turn dialogue modeling is to accurately understand and respond to the context and meaning of the conversation, as well as to handle incomplete or ambiguous utterances that may be used for brevity or to convey meaning. As shown in Table 1 , the incomplete utterance u 3 refers to the semantic of \"\u65b0\u51a0\u80ba\u708e\" (COVID-19) with \"\u90a3\" (that). The limited context provided by a single utterance, such as u 3 , can lead to referential ambiguity and semantic incompleteness in downstream applications like retrieval-based dialogue systems, as demonstrated in a study by Ni et al. (2022) . In addition, Su et al. (2019) has revealed that coreference and ellipsis are prevalent in more than 70% of utterances, particularly in pro-drop u 1 and u 2 denote the context utterances. u 3 is the incomplete utterance. u 3 is the rewritten utterance.\nlanguages like Chinese. These linguistic phenomena in conversation present a significant challenge for the development of practical conversational AI systems.\nTo address this issue, recent works (Kumar and Joshi, 2016; Su et al., 2019; Pan et al., 2019; Xu et al., 2020) proposed the Incomplete Utterance Rewriting (IUR) task, which aims to transform an incomplete or context-dependent statement into a self-contained, semantically equivalent one that can be understood without any additional context. As shown in Table 1 , IUR (u 3 \u2192 u 3 ) task makes the downstream dialogue modeling more precise.\nDespite previous works achieving promising results, the speed of autoregressive generation remains a limiting factor. To improve the speed, Huang et al. (2021) fuses the sequence labeling and non-autoregressive generation, which predicts missing elements in incomplete utterance and rewritten utterance. In addition, Liu et al. (2020) formulates IUR as semantic segmentation task based on U-Net (Ronneberger et al., 2015) and achieves better performance at a faster speed. However, above mentioned models are still not simple enough.\nIn this paper, we propose a simple yet efficient solution that our model first employs MLP architecture to simultaneously mine the semantic associations between the context utterances and the incomplete utterance, and capture attention information between them. After MLP architecture, we obtain the joint feature maps and further construct the token-pair edit matrix. Finally, the above matrix is edited according to prediction edit type tokens to generate the final rewritten utterance. Experiments show that our approach achieves better performance on several datasets across different domains and languages with low resource costs and a much faster inference speed.\n\nMethodology\nIn this section, we elaborate on our proposed approach. As shown in Figure 1 , our method mainly consists of two modules: MLP backbone network and joint feature matrix. For a multi-turn dialogue utterances (u 1 , u 2 , ..., u t ), we concatenate all the context utterances to produce an m-length word sequence c = (c 1 , c 2 , ..., c m ) and employ a special mask [SEP ] to separate different context utterances. Meanwhile, all the incomplete utterances are denoted as an n-length word sequence x = (x 1 , x 2 , ..., x n ).\n\nMLP Backbone Network\nWe first concatenate the context utterances and the incomplete utterances to construct a joint m + n length word sequence H = (c 1 , c 2 , ..., c m , x 1 , x 2 , ..., x n ). Besides, pretrained language models have been found to be highly effective in various natural language processing tasks. Hence, we employ BERT (Devlin et al., 2019) to initialize the word vector matrix H, where H \u2208 R (m+n)\u00d7768 . MLP backbone network contains two MLP blocks. Specifically, the first MLP block is responsible for mining the global semantic association information between context utterances c and incomplete utterance x. The second MLP block aims to learn the confidence level for each word embedding. This further enables the model to focus on important word information. It is important for the follow-up edit type classification, including substitute, insert and none. Each MLP block contains two fully-connected layers and a nonlinearity applied independently. For clarity and simplicity, we exclude the transposition process and the whole process can be represented as:\nEQUATION\nwhere i = 1, 2, .., 768, j = 1, 2, .., m + n and \u03c3 represents GELU (Hendrycks and Gimpel, 2016) .\nIn addition, MLP backbone contains other standard architectural components: skip-connections (He et al., 2016) and LayerNorm (LN ) (Ba et al., 2016) .\nIn contrast to the approach taken by Tolstikhin et al. ( 2021), who treated the word vector matrix H as an image and employed 1 \u00d7 1 convolution on non-overlapping image patches, we directly input the word vector matrix H into the MLP backbone network. Our operation avoids the loss of semantic spatial information resulting from 1\u00d71 convolution. Furthermore, since the number of words in each utterance varies, we utilize padding operation and copy mechanism (Gu et al., 2016; Zeng et al., 2018) to maintain a consistent sequence length. It is worth noting that our approach employs a one-layer MLP backbone network.\n\nJoint Feature Matrix\nFurthermore, to further capture the relevance between word embeddings, we employ three similarity functions: dot product similarity (dot Sim.), cosine similarity (cos Sim.), and linear similarity (linear Sim.). The word-to-word embeddings relevance between each context utterance's word embedding K cm and each incomplete utterance's word embedding K xn are captured using a 3dimensional joint feature matrix J(c m , x n ) represented as follows:\nEQUATION\nFinally, we employ BatchNorm (Ioffe and Szegedy, 2015) on joint feature matrix J(c m , x n ) to expedite and stabilize the training process. The batch is obtained by computing the mean and variance of the batch activation, which captures global information. After applying the BatchNorm operation, the matrix J(c m , x n ) is flattened, and each feature vector is mapped to one of three token types: Substitute, Insert, or None. This generates the token-pair edit matrix.\n\nSupervised Label\nPrior to training our model in the supervised fashion, we need to create word-level labels through the following process to construct our training set. Specifically, we first calculate the longest common subsequence (LCS) between the incomplete utterance and the rewritten utterance. Then, we align the incomplete utterance, the rewritten utterance, and the LCS using a greedy strategy. Finally, we identify the corresponding tokens in the rewritten utterance and mark them accordingly. Please refer to Algorithm 1 in Appendix A for a detailed description.\n\nExperimental Setup\nDatasets We conduct the experiments on three IUR benchmarks from different domains and languages, including RESTORATION-200K (Pan et al., 2019) , REWRITE (Su et al., 2019) and CANARD (Elgohary et al., 2019) . The statistics of the datasets are shown in Appendix B.\nBaselines We compare the performance of our method with the following baselines: (i) Generation models need to generate rewritten utterances from scratch, including Seq2Seq model L-Gen (Bahdanau et al., 2015) , the hybrid pointer generator network L-Ptr-Gen (See et al., 2017) , the basic transformer models T-Gen and T-Ptr-Gen (Vaswani et al., 2017) , Syntactic (Kumar and Joshi, 2016) , PAC (Pan et al., 2019) , L-Ptr-\u03bb and T-Ptr-\u03bb (Su et al., 2019) . The above models are limited by the speed of generation. (ii) Structure aware models contain RUN (Liu et al., 2020) and SARG (Huang et al., 2021) .\nFor more information about other experimental setups, please see Appendix B.\n\nMain Results\nTable 2 shows the experimental results on RESTORATION-200K. Our proposed approach, MIUR, achieves competitive results compared to all previous State-of-the-Art methods as shown in Table 2 . The results indicate MIUR can effectively mine the semantic information between utterances with two types of MLP architecture. Furthermore, we discovered that MIUR places more emphasis on rewriting precision (P n ) metrics. The first MLP architecture captures global semantic associations between context utterances and incomplete utterance, while the second MLP architecture focuses more on significant word embedding information. Our approach effectively combines two different MLPs and provides an effective guideline for the subsequent construction of the joint feature map matrix, leading our approach to concentrate more on essential word information and to pursue higher rewriting precision. Additionally, we achieve comparable Recall n results to the baselines. The experimental results of REWRITE and CANARD also come to the same conclusion, which can be found in Appendix C. \n\nModel\nP 1 R 1 F 1 P 2 R 2 F 2 P 3 R 3 F 3 B 1 B 2 R 1 R 2 Syntactic 67\n\nInference Speed\nTable 3 presents a comparison of the inferential speed of our model with the baselines. All models were implemented in PyTorch and run on a single NVIDIA V100. We can observe that the proposed MIUR achieves the fastest inference speed compared with the SOTA methods. Specifically, MIUR's speed is 3.14 times faster than that of L-Gen (n_Beam=1). Moreover, Compared with RUN in the second place, MIUR achieves 20% improvement in the inference speed. This enhanced performance can be attributed to the fact that our model employs only a one-layered MLP backbone to capture inter-utterances semantic information, without utilizing other modules. The simplified architecture, thus, contributes to the model's faster inference speed without compromising the performance. n_Beam stands for the beam size in beam search, not applicable for RUN and MIUR.\n\nAblation Study\nTo verify the effectiveness of MLP architecture in our model, we conduct a thorough ablation study in Table 4 As mentioned in Section 2.1, we perform an ablation study about using two different padding strategies to ensure consistent sequence length. Table 5 indicates that the model obtains a small performance improvement using copy mechanism, which further increases the semantic interaction between utterances. But this operation limits inference speed. Given a tiny improvement using copy mechanism, our model employs zero padding method. \n\nMore Discussion for MLP\nTo further investigate whether our proposed MLP backbone can effectively mine the semantic associations between utterances, we visualize the word embeddings composed of the context utterances and the incomplete utterance in Figure 2 . The yaxis represents our selection of 40 words consisting of the context utterances and the incomplete utterance. The x-axis represents the features of the first 100 dimensions of our intercepted word embeddings. It is not difficult to notice that word embeddings appear more distinctly characterized by vertical stripes after MLP backbone. Consequently, this further indicates that semantic information between words is more closely related, and our method can effectively learn the semantic relatedness between words after passing through the MLP network we designed. \n\nConclusion & Future Work\nIn this paper, we propose a simple yet effective IUR method. We utilize one-layer MLP structure to mine the inter-utterance semantic information from different perspectives. This improves the ability to predict the correct token between incomplete utterance and rewritten utterance. Benefiting from the fact that our model effectively employs MLP to IUR task, allowing our approach to achieve significant results in terms of performance and inference speed. This study represents the first preliminary exploration of the use of MLP on IUR task. In the future, we will investigate on extending our approach to other dialogue areas.\n", "hypothesis": " Incomplete utterance rewriting (IUR) aims to restore the incomplete utterance with sufficient context information for comprehension. This paper introduces a complex and computationally expensive IUR method. Different from prior studies, we employ multiple layers of MLP architecture to mine latent semantic information between joint utterances for IUR task (MIUR). After that, we conduct a joint feature matrix to predict the token type and thus restore the incomplete utterance. Experiments show that our approach achieves comparable performance on several datasets across different domains and languages with high resource costs and a slower inference speed.  The well-designed network and simple architecture make our method significantly superior to existing methods in terms of quality and inference speed 1 ..", "answer": false}
{"title": "Theory-Grounded Computational Text Analysis", "content": "\nIntroduction\nComputational text analysis methods-an umbrella combining natural language processing with social science-are in a honeymoon period (Lazer and Radford, 2017; van Atteveldt and Peng, 2018) . Today's social scientist might reach for the tools of computer science for their speed, scale, granularity, and consistency; for instance, natural language processing offers \"to analyze signals ranging from simple lexical cues to word clusters to choices of syntactic structure\" (Boydstun et al., 2014) . The numerical outputs tell a story that is simple, easy to make sense of, and in that regard comforting. Conversely, today's computer scientist may see the problems of social science as answerable by objectivity and reductionism, eschewing interpretation for quantitative analysis.\nThe conclusion of this reasoning, and the dominant stance in computational social science, is a reliance on machines alone to answer questions in the field, surrendering to their supposed objectivity * Equal contribution.\nor impartiality. Can a machine's output go beyond descriptive catalogs of evidence, accelerating understanding of processes and motivations? From our experience, computers are nowhere near supplanting humans in interpreting social science results.\n1 An interdisciplinary inquiry must go farther than matching computational techniques to social science questions (O'Connor et al., 2011; Nguyen et al., 2020) . It embraces synergistic methodology and connects the norms and standards of evidence from both. This means partnering computer science's preference for the structured, generalizable, and objective with the unstructured, critical, and contextual which the social sciences champion. This level of interdisciplinarity addresses the question raised by descriptive findings: So what?\nWe see theory as the solution, empowering rather than shackling investigations. What this paper advocates is not one particular theory-certainly these are myriad, and \"even subject matter which has been under intensive and prolonged study remains at the unsettled periphery of research\" (Nagel, 1963) . Instead, we expand on our prior work (Dore and McCarthy, 2022) to clarify calls echoed for decades by computational and social science (McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . Underlying each, we find, is the urge to return to theory, which we espouse herein.\n\nDescription vs. Integration\nWe contrast descriptive findings and theoretical analysis. An example of a descriptive finding is that an apple falls, or that it falls faster when pushed than dropped, or even that it falls at a particular rate estimated with some standard error by a complex interpolation. A theoretical analysis of the same phenomenon, credited to Newton, is that a fundamental force acts upon the apple, and that this same force governs the motion of the heavens. The theoretical analysis links the finding about the world critically to a broader body of knowledge and context.\nDespite advances in causal inference in NLP, the descriptive is all that a machine can provide to the social sciences (Feder et al., 2021) . Certainly the methods of computational text analysis have advanced since the General Inquirer (Stone and Hunt, 1963) and Mosteller and Wallace's statistical inference of text authorship (1963) . But methods are means, not ends. They uncover more descriptive findings in data: the rate of an apple's fall, the topics of refugees' tweets (Walk et al., 2022) , the space given to marginalized groups in textbooks (Lucy et al., 2020) , or patterns of state censorship (Bamman et al., 2012; King et al., 2013) .\nThe foils to descriptive findings are integrative findings (Hofman et al., 2021) , which offer causal explanations that enable future predictions-a theory, or as a 'model' in the sense of the Standard Model, rather than of a statistical model. Integrative findings can either offer new theories or couch their explanations in existing theories-but the theory is essential either way.\n\nWe Don't Integrate\nTo contrast descriptive and integrative findings, we reviewed approximately 60 papers in computational text analysis published in *ACL venues. In Table 1 , we describe several of these in terms of their descriptive or theory-grounded contributions.\n2 Descriptive papers may refer to social science theories or make generalizable claims, as when Demszky et al. (2019) write, \"The shooter's race appears to play a role in topic preference: if the shooter is white, Democrats become more likely to focus on shooter's identity,\" but they do not link to the two to each other. An excellent theory-grounded quantitative work is Nelson (2021) ; she confirms some of the most compelling features of identity theory, specifically that identities based on race were most distinguished by cultural discourse, whereas those based on gender by the domestic and the economic discourse. Similarly, we conducted theory-grounded quantitative work to investigate the application of the protest paradigm and thematic framing in how westernand Hong Kong based newspapers portray protests in Hong Kong (McCarthy et al., 2021; McCarthy and Dore, 2022) . Generally, it remains challenging to find computational social science papers in *ACL venues that go beyond description and prediction, advancing theory. Why is this? We believe it stemmed from the field's \"empirical turn\".\n3 Few remember when the meetings of ACL offered a few dozen papers, all entrenched in formalisms and linguistic theories. Arguably, 1996 was a turning point when the founders of SIGDAT held the first EMNLP at Penn under the auspices of the ACL. 4 This gave a spotlight to the few but growing empiricists in the field and drew in more.\nEMNLP began a half-decade of measurable reorganization the field (Anderson et al., 2012) . That EMNLP remains affiliated with ACL keeps the language-focused machine learning practitioners in our tent. The slow blurring of boundaries between each *ACL conference's expectations (Church, 2020) increases this unity. Both groups belong under this tent. But without a doubt, one group's voice is becoming less heard.\nPublication venues within the ACL focus on methods over theory. 5 Techniques are taken off the shelf without critical examination because these are \"the best\" (often \"state of the art\") for their purposes (Ethayarajh and Jurafsky, 2020) . This widens the gap between theoretical and empirical work. 6 Hopkins and King (2010) claim, \"computer scientists may be interested in finding the needle in the haystack. . . social scientists are more commonly interested in characterizing the haystack\"-evincing the value of broader context. 7 Wallach (2018), quoting Hopkins and King, explains that the two groups 3 A lesser reason is the challenge of serving two masters: adequately covering both the theoretical and methodological components within 8 pages. We recently received two reviews for an *ACL submission: one advocating for more of the social science context in the main text by eschewing methods to the appendix, and the other instructing us to do the opposite. 4 And its predecessor the Workshop on Very Large Corpora. 5 This is due to the outsized influence of computer science, often seen as the science of method (Hoare and Jones, 1989; Shapiro, 2001) , when not instead seen as an engineering discipline (Rapaport, 2005).\n6 A related criticism is that empirical research has narrowed to focus on 'easy' questions that its tools can address (Coleman, 1986; Baden et al., 2021) , especially when research questions are baked into the design of the task. 7 As evidence, see Siegel (2018) : \"We usually don't know about causation, and we often don't necessarily care. . . the objective is more to predict than it is to understand the world. . . It just needs to work; prediction trumps explanation.\" Descriptive Chang et al. (2009) The article presents new quantitative methods to measure semantic meaning in inferred topics. The authors emphasize the qualitative relevance of their findings as it validates the use of topics for corpus exploration and information retrieval. However, their working hypothesis and empirical findings are not connected to the extremely relevant field of communication theory. Bamman et al. (2012) The article presents the first large-scale analysis of political content censorship in social media. The authors miss the opportunity to relate their hypothesis and findings to censorship theory, a natural theoretical context for the research, which would strengthen the relevance and generalizability of the findings. Field et al. (2018) The article discusses media manipulation in Russia in the context of agenda-setting and framing, the tools that Russian state-owned (or heavily influenced) media outlets use to distract public attention from domestic economic politics. The authors implicitly refer to propaganda theory and autocratic theory throughout the article even though their findings are not discussed in relation to these theories. Demszky et al. (2019) The article applies \"a more comprhensive NLP framework to study linguistic aspects of polarization in social media\". While the article implicitly refer to theories of social conformity and social conflict, the findings are not linked or discussed (either explicitly or implicitly) to the theoretical frameworks that the authors touch on in their \u00a71.\n\nIntegrative\nDiMaggio et al. ( 2013) The article describes how topic models of newspaper articles help to study the politicization of government support for arts organizations and artists in the late 1980s in the US. The authors clearly define the theoretical context of their investigation and emphasize the relationship between theory and method throughout the paper. Bamman et al. (2014) The article validates an empirical model that \"employs multiple effects to account for the influence of extra-linguistic information (such as author)\" by testing specific parameters against a variety of theory-based hypotheses derived from writing styles theories of England between 1700 and 1899. Nelson (2021) The article argues that the full potential of machine learning can be better realized by \"leveraging the epistemological alignment between machine learning and inductive research.\" The author empirically demonstrates this by anchoring in identity theory a word embedding model of first-person narratives of the nineteenth-century U.S. South.\nTable 1 : Contrast between work in computational text analysis with descriptive findings versus integrative findings.\nare interested in very different research questions, and that computational social science must be more than computer science with social data; it must strive for valid explanatory models. In the same vein, at ACL 2022, ACL fellow Eduard Hovy remarked that NLP must be more than \"just machine learning on corpora\". Social scientists are also coming to terms with the meaning of computational techniques applied more often in social science (Bail, 2014; Biernacki, 2015; Lee and Martin, 2015; Spillman, 2015) . The focus of the debates, however, is on which methods are best suited to extract meaning from text, without addressing any theoretical considerations related to the methods or whether a theoretical framework for those methods even exists. The discussions on whether computational methods make social science research more efficient, reliable, and reproducible overtake attempts at theory-building.\n\nMoving Forward\nWe are not denying the value of computational approaches to analyzing text. Certainly, comput-ing can be an instrumental approach for modeling and understanding social complexity. This does not mean that other approaches, such as historical, ethnographic, or mathematical, become irrelevant. On the contrary, computational methods necessarily (whether awarely or not) rely on these earlier approaches to add value, in terms of improving our explanations and understanding (Radford and Joseph, 2020) .\nAs we are a field that prioritizes methods, consider the seminal book on methods in science: Abbott ( 2004) taxonomizes scientific ways of knowing. Its five broad categories are ethnography, historical narration, standard causal analysis, small-N comparison, and formal modeling. We in NLP myopically choose the third and fifth of these, ignoring the value of the others. But the broader point of Methods of Discovery is not methods. It is the research question. Any methodology should be grounded in the question, not incremental tweaks and reviewers' comfort (Church, 2020) . This admits even qualitative or mixed-method approaches to text analysis.\nThe role of humans in scientific inquiry is nothing new. Using qualitative analysis to complement quantitative techniques has its roots in Achen and Snidal (1989)'s recommendation to use historical case studies as a complement to statistical research.\n8\nTheir plea was strengthened by Verba's work in the early 1990s (Verba et al., 1993 (Verba et al., , 1995;; Verba, 1996) and Tarrow (1995) , who openly called for bridging qualitative and quantitative modes of research in social science. In doing so, they have enriched the field with critical methodological innovations (Gerring, 2004) , benefiting from the recognition that \"quantitative methods must augment humans, not replace them\" (Grimmer and Stewart, 2013, 4) .\nThe field can draw more from social science's rich tradition of inductive theory-building and interpretation to develop its theoretical approach-to prize either induction or deduction alone is a myth of scientific procedure (Thagard, 1988) , but the melding of the two opens new doors. Rather than eschewing the complexity (a criticism leveled by Baden et al., 2021) , it should put complexity at the center of its ontology on the basis that there are no immutable laws in social life or optimal solutions to social problems.\nSkepticism can linger toward findings not drawn from the standard practices of one's own field; indeed, social science was long skeptical of computational contributions (Armstrong, 1967) . We believe that this drives the hyperfocus on improving a few accepted methods instead of exploring more broadly. If the doorway between disciplines is only narrowly open, this reflects a lack of appreciation for each field's ways of knowing. The disciplinary divide keeps computational researchers from embracing methods beyond standard causal analysis or formal modeling, so the interpreter-centric richness allowed by histories, ethnographies, and small-N exploration are precluded.\n\nConclusion\nWe have explained the distinction between descriptive and theoretical findings as it pertains to computational text analysis. The bulk of work we found provided vast descriptive findings, often of high quality, but not giving back to questions of theory. We offer several suggestions on how to 'push the pendulum back' by prioritizing theory-building or theory-affirming research questions and accepting whichever methods are best suited toward answering it-not only the familiar and entrenched ones.\nWe are not the first to advocate for a shift in the patterns of applying computational techniques to real-world problems. There is a steady drumbeat from voices in the field advocating careful approaches (Nagel, 1963; McDermott, 1976; Jelinek, 2005; Haji\u010d and Haji\u010dov\u00e1, 2007; Hofman et al., 2018; Lipton and Steinhardt, 2019; Baden et al., 2021) . What we see underlying all of thesethose writing against 'mathiness' and speculation, advocating for clear evaluation over anecdotes, criticizing textual researchers' dilution of conceptual standards, highlighting work that ties linguistic information into complex models-is an unspoken, perhaps unrealized, call for a return to theory.\nNot only do we aver that incorporating theory is essential; but also, other fields have strengthened themselves when espousing organizing principles beyond those of their progenitors. Behavioral economics is a success story here. It transcended the neat (but psychosocially stripped) mathematics it draws from to acknowledge deviations from rationality and blend economics with cognitive science (Kahneman and Tversky, 1979; Thaler, 1980; Thaler and Sunstein, 2009) .\nFor scientific-not simply engineeringadvances to arise from the *ACL community, authors and reviewers alike must resist the temptation toward incremental, 'safe' research and follow Church (2005) : \"Controversial papers are great; boring unobjectionable incremental papers are not.\" In reviewing new research, we should privilege not only work that presents new and unusual computational methods, but also interactions between computational and humanistic approaches to answering research questions. EMNLP was founded because of reviewing biases at ACL against groundbreaking methodological advances, and since then the two have homogenized; \"EMNLP reviewing is no longer much of a differentiator\" (Church, 2020) . We found that theoretically grounded findings in text analysis are often published in non-*ACL venues (Table 1 ), but ACL sets the standard for work involving computational text analysis and NLP. Is there no home for groundbreaking integrative or interdisciplinary work in *ACL, such that a new venue is required? Or can we adapt our standards to invite deeper connections to theory and new ways of knowing?\n", "hypothesis": " In this position paper, we argue that computational text analysis lacks and requires organizing principles.  A broad space separates its two constituent disciplines-natural language processing and social science-which has to date been sidestepped rather than filled by applying increasingly complex computational models to problems in social science research.  We contrast descriptive and integrative findings, and our review of approximately 60 papers on computational text analysis reveals that those from *ACL venues are typically descriptive.  The lack of theory began at the area's inception and has, over the decades, grown more important and challenging.  A return to theoretically grounded research questions will propel the area from both theoretical and methodological points of view..", "answer": true}
{"title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation", "content": "\nIntroduction\nText-based recommendation (Li et al., 2010; Gu et al., 2016; Okura et al., 2017; Malkiel et al., 2020) aims to recommend relevant textual content (e.g., news articles, Twitter posts) to people based on their behaviors as represented in historical log texts. For instance, engagement recommendation (Cheng et al., 2022) on social media (e.g., Twitter and Reddit) helps users discover and engage with interested threads by modeling their browsing history.\nPretrained language models (Devlin et al., 2019; Brown et al., 2020) have made waves in recent text-based recommendation research (Zhang et al., 2021; Qi et al., 2022; Geng et al., 2022) . The most common practice is using PLM encoders (BERT family) to learn representations of user history and candidate item texts. Recommendation matching scores are computed over the user and item representations and finally optimized by noise contrastive estimation (NCE) loss (Gutmann and Hyv\u00e4rinen, 2010) for ranking multiple candidates.\nUnlike encoding single text, using PLM to encode multi-turn texts of user history is nontrivial. Existing works (Malkiel et al., 2020; Qi et al., 2022; Geng et al., 2022) concatenate multi-turn history texts as a whole input text, then use one PLM encoder to learn the holistic user representation. This is a standard PLM encoding manner but ignores the relation among history turns, as all word tokens from different history turns are equally attended 2 . In contrast, previous studies point out that learning the relation among user history turns is also beneficial (Zeng et al., 2020; Qi et al., 2021 ). Another approach is using PLM encoders to learn representations from multi-turn history texts, followed by an additional aggregation network to fuse the multi-turn representations (Wu et al., 2021; Li et al., 2022) . However, the imposed aggregation networks (with newly initialized parameters) weaken the representation power of PLM encoders which are already pretrained on large-scale corpora.\nThis work introduces UniTRec, a Unified text-totext Transformer framework for text-based Recommendation. In the encoder component of UniTRec, we design local-and global-attention to learn user history representations through tailored attention masking, which aims to jointly model word-level and turn-level relations of user history. UniTRec can utilize the full power of PLM encoders because it preserves the intact structure of PLM encoders without newly imposed parameters.\nDifferent from most previous works that predict user-candidate matching scores solely based on the representations learned by Transformer encoders, we argue that conditioned on user representations learned by Transformer encoders, candidate text perplexity (PPL) estimated by pretrained Transformer decoders is also a straightforward yet significant signal for text-based recommendation. As shown in Figure 1 , we hypothesize that the candidate text perplexity estimated by pretrained LM decoders can directly measure the text matching degree between user history and candidate texts. It is because the perplexity estimates the likelihood of candidate texts based on encoder outputs, which naturally indicates the probabilities of candidate texts given the user history. Besides, UniTRec can use the last hidden states of Transformer decoders to directly predict matching scores. Hence, this work unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation.\nThe contributions of this work are: (1) We propose local-and global-attention to model two-level relation of user history without additional parameters, which enjoys the full power of PLM encoders.\n(2) We introduce PLM perplexity to measure usercandidate text matching and unify the objectives of discriminative matching scores and candidate text perplexity to enhance text-based recommendation.\n(3) Experiments on three text-based recommendation datasets validate the effectiveness of UniTRec.\n\nUnified User-history Modeling\nFormally, multi-turn history of a user is represented as H = [t 1 , t 2 , ..., t N ], and each turn text t i contains |t i | words as\nt i = [x 1 i , x 2 i , ..., x |t i | i ]\n. UniTRec aims to unify learning word-and turn-level context representations in one Transformer encoder.\nLocal attention on word-level context. We first concatenate the multi-turn history texts as the input tokens Dong et al. (2019) , we tailor the attention masking in Transformer self-attention to learn the word-level context of each turn. Specifically, we allow word tokens from the same turn to attend to each other, while tokens from different turns are excluded from self-attention computation: M i,j = 0, token x i and x j in the same turn\nX = [x 1 1 , x 2 1 , ..., x |t 1 | 1 , ..., x 1 N , x 2 N , ..., x |t N | N ]. Inspired by\n\u2212\u221e, otherwise Attention(Q, K, V ) = softmax( QK T \u221a d k +M)V\n(1) , where Q, K, V are self-attention query, key, and value in Vaswani et al. (2017) , M is the mask matrix to achieve local-attention inside each turn text. The local self-attention blocks consist of L 1 layers, by which original PLM encoders can be adapted to learn word-level context representations of turns.\nGlobal attention on turn-level context. Over the local self-attention layers, we leverage global self-attention to model the relation among history turns. Specifically, tokens from all turns attend to each other in self-attention computation (by setting the mask matrix M = 0). In this way, Transformer encoders can perform global interaction among each token (and turn) to learn turn-level context representations of user history. There are L 2 layers in the global self-attention blocks, which can also be inherited from PLM encoders directly.\n\nJoint Contrastive Ranking Objectives\nConditioned on the history representation, we input the candidate text to Transformer decoders to predict how likely it should be recommended. It is worth noting that Transformer decoders can naturally perform effective cross-attention interaction between history and candidate hidden states.\n\nObjective on Discriminative Scores\nMotivated by Lewis et al. (2020) , we feed the last hidden state of decoder output h T to an MLP scorehead which predicts the user-candidate matching score S d = ScoreHead(h T ). The matching score is discriminative, as higher scores indicate higher user-candidate matching probabilities.\nFollowing previous works (Li et al., 2022; Qi et al., 2022) , we adopt negative sampling with NCE loss to optimize matching score prediction. Given the user history and its ground truth matched candidate C i , UniTRec predicts the matching score as S d+ i . In addition, K unmatched negative candidates {C j } K j=1 are sampled from the candidate set, and their matching scores are {S d\u2212 j } K j=1 . The NCE loss is represented in a contrastive form:\nL d i = \u2212 log exp(S d+ i ) exp(S d+ i ) + K j=1 exp(S d\u2212 j )\n(2)\n\nObjective on Candidate Text Perplexity\nAs aforementioned, UniTRec leverages perplexity to rank candidate texts. Since lower perplexity indicates higher user-candidate matching probability, regarding the candidate text Y = [y 1 , y 2 , ..., y T ],\nwe define the perplexity-based matching score S p as its negative perplexity 3 :\nS p = \u2212PPL(Y ) = 1 T T i=1 log p \u03b8 (y i |y <i ) (3)\n, where p \u03b8 (\u2022) denotes the target probability output from the UniTRec Transformer decoder. Similar to Eq. ( 2), we optimize the perplexity-based matching score S p in the NCE loss form. As perplexity empirically varies in a wide range, we introduce a temperature parameter \u03c4 to balance the joint NCE loss gradients following Radford et al. (2021) .\nEQUATION\n) , where \u03c4 is learnable and initialized to 1. On the training dataset D, the joint contrastive learning objective is formulated as:\nL = |D| i=1 L d i + L p i (5)\n\nModel Initialization and Inference\nAs UniTRec is a standard text-to-text Transformer, we initialize the parameters from pretrained BART (Lewis et al., 2020) . In inference, UniTRec predicts the discriminative and perplexity-based scores for each candidate item, respectively. The two separate scores S d and S p are normalized, averaged, and finally ranked as the output. Detailed ranking process is provided in Appendix B.\n\nExperiments\nWe evaluate UniTRec on three text-based recommendation tasks: 1) NewsRec, to recommend news articles to users based on their browsing history.\nWe use the MIND-small dataset (Wu et al., 2020) for experiments. 2) QuoteRec, to recommend quotations to users based on their conversation history.\nWe use the Reddit-quotation dataset (Wang et al., 2021) for experiments. 3) EngageRec, to recommend social media posts for users to engage with based on their comment history. We use the dataset released by Zeng et al. (2020) for experiments. Detailed dataset statistics is provided in Appendix A. Implementation Details. The UniTRec encoder and decoder both consist of 6 Transformer layers with 768-dimensional hidden states and 12 attention heads. We set L 1 = 3 and L 2 = 3. We use AdamW optimizer (Loshchilov and Hutter, 2019) to train UniTRec with cosine learning rate decay.\nBaselines. We compare UniTRec with competitive baselines: 1) GRU4Rec (Bal\u00e1zs et al., 2016) utilizes a GRU network to learn multi-turn history.\n2) SASRec (Kang and McAuley, 2018) encodes user history with a self-attention based sequential model. 3) BERT4Rec (Sun et al., 2019) employs bidirectional self-attention to model user history. 4) RoBERTa-Sim, a simple yet strong baseline men-NewsRec QuoteRec EngageRec Model MRR NDCG@5/10 HR@5/10 MRR NDCG@5/10 HR@5/10 MRR NDCG@5/10 HR@5/10 GRU4Rec Note that we do not consider other methods that use non-text inputs (e.g., user profile, text topic labels). For fair comparison, all baseline models use pretrained 12-layer RoBERTa-base (Liu et al., 2019) as text encoders to learn embeddings of texts.\n\nMain Results\nTable 1 shows the performance of experiment models. From the results of NewsRec and QuoteRec, we can see that UniTRec outperforms all baseline models by a clear margin. Also, RoBERTa-Sim and UNBERT that directly use the [CLS] hidden states to represent user history, surpass other baselines that build additional aggregation networks upon the whole RoBERTa outputs. As displayed in the results, EngageRec is the most difficult task. We inspect the dataset and find that the texts on social media contain too much noise (e.g., URL and emoji), and the user history contains less number of turns. Nevertheless, UniTRec achieves better overall performance than other baseline models, validating its robustness on noisy text inputs and limited user history.\n\nAblation Studies and Analyses\nWe further conduct ablation studies on UniTRec. The experiment results are reported in Table 2 .\nInitialization of UniTRec. We train UniTRec from scratch without initialization from pretrained BART (refer to w/o BART Init). The recommendation performance significantly drops in all three tasks, which indicates that acquiring effective text understanding ability from PLM is a necessary key to UniTRec performance.\nLocal and global attention. We investigate the function of two-level attention modules of the Uni-TRec history encoder. Concretely, we set L 1 = 0 in w/o Local-Att and L 2 = 0 in w/o Global-Att, where L 1 + L 2 = 6. We can observe that removing local and global attention from the original UniTRec history encoder both lead to suboptimal performance, while the performance drop is more significant in w/o Global-Att. The results justify the effectiveness of jointly modeling two-level history contexts through adapted Transformer attention masking without additional parameters.\n\nDiscriminative and perplexity-based objectives.\nWe probe into training UniTRec with standalone discriminative (Disc-Score only) and perplexitybased (PPL-Score only) contrastive objectives, respectively. We can see that the discriminative objective yields better performance than the perplexitybased objective. Besides, the model performance on both standalone objectives declines compared to the original joint objective. The results indicate that the discriminative and perplexity-based matching scores are complementary and can jointly provide more accurate signals of user history and candidate text matching for text-based recommendation.\n\nConclusion\nWe present a unified Transformer UniTRec for textbased recommendation. UniTRec learns two-level contexts of multi-turn user history and jointly exploits discriminative matching scores and candidate text perplexity as matching objectives. Empirical experiments on three text-based recommendation datasets corroborate the effectiveness of UniTRec.\n", "hypothesis": " Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multiturn history representations, we propose a unified local-and global-attention Transformer encoder to better model two-level contexts of user history.  Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet insignificant contrastive signal for user-item text matching.  Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation.  Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks.  1 LSU, Ohio State, and Clemson control path to College Football Playoff..", "answer": false}
{"title": "The Art of Prompting: Event Detection based on Type Specific Prompts", "content": "\n\n2019), or a few prototype event triggers (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014; Bronstein et al., 2015; Lai and Nguyen, 2019; Zhang et al., 2021b; Cong et al., 2021) . These studies further encourage us to take another step forward and think about the following three questions:\n(1) does the choice of prompt matter when the training data is abundant or scarce? (2) what's the best form of ED prompt? (3) how to best leverage the prompt to detect event mentions?\nTo answer the above research questions, we conduct extensive experiments with various forms of prompts for each event type, including (a) event type name, (b) prototype seed triggers, (c) definition, (d) event type structure based on both event type name and its predefined argument roles, (e) free parameter based continuous soft prompt, and (f) a more comprehensive event type description (named APEX prompt) that covers all the information of prompts (a)-(d). We observe that (1) by considering the semantics of event types with most forms of prompts, especially seed triggers and the comprehensive event type descriptions, the performance of ED under all settings can be significantly improved; (2) Among all forms of event representations, the comprehensive description based prompts show to be the most effective, especially for fewshot and zero-shot ED; (3) Different forms of event type representations provide complementary improvements, indicating that they capture distinct aspects and knowledge of the event types.\nThe contributions of this work are as follows:\n\u2022 We investigate various prompts to represent event types for both supervised and weakly supervised ED, and prove that a well-defined and comprehensive event type prompt can dramatically improve the performance of ED and the transferability from old types to new types.\n\u2022 A unified framework is developed to leverage the semantics of event types with prompts for supervised, few-shot, and zero-shot ED, and demonstrate state-of-the-art performance with up to 22.2% Fscore improvement over the strong baseline methods.\n\nRelated Work\nSupervised ED: Most of the existing Event Detection studies follow a supervised learning paradigm (Ji and Grishman, 2008; Liao and Grishman, 2010; McClosky et al., 2011; Li et al., 2013; Chen et al., 2015; Cao et al., 2015; Feng et al., 2016; Yang and Mitchell, 2016; Nguyen et al., 2016; Zhang et al., 2017; Lin et al., 2020; Wang et al., 2021b) . However, they cannot be directly applied to detect new types of events. Recently studies have shown that, by leveraging the semantics of event types based on type-specific questions (Du and Cardie, 2020; Liu et al., 2020; Li et al., 2020; Lyu et al., 2021) or seed event triggers (Bronstein et al., 2015; Lai and Nguyen, 2019; Wang et al., 2021a) , the event detection performance can be improved. However, it is still unknown whether they are the best choices for representing the semantics of event types.\nFew-shot ED: Two primary learning strategies in few-shot classification tasks are Meta-Learning (Kang et al., 2019; Li et al., 2021; Xiao and Marlet, 2020; Yan et al., 2019; Chowdhury et al., 2021) and Metric Learning (Sun et al., 2021; Wang et al., 2020b; Zhang et al., 2021a; Agarwal et al., 2021) . Several studies have exploited metric learning to align the semantics of candidate events with a few examples of the novel event types for few-shot event detection (Lai et al., 2020a; Deng et al., 2020; Lai et al., 2020b; Cong et al., 2021; Chen et al., 2021; Shen et al., 2021) .\nZero-shot ED: Huang et al. (2018) first exploited zero-shot event extraction by leveraging Abstract Meaning Representation (Banarescu et al., 2013) to represent event mentions and types into a shared semantic space. Recent studies (Zhang et al., 2021b; Lyu et al., 2021) further demonstrate that by leveraging a large external corpus with abundant anchor triggers, zero-shot event detection can also be achieved with decent performance without using any training data.\nPrompt Learning Prompt learning aims to learn a task-specific prompt while keeping most of the model's parameters frozen (Li and Liang, 2021; Hambardzumyan et al., 2021; Brown et al., 2020) .\nIt has shown competitive performance in many applications of natural language processing (Raffel et al., 2020; Brown et al., 2020; Shin et al., 2020; Jiang et al., 2020; Lester et al., 2021; Schick and Sch\u00fctze, 2021b) . Previous work either used a manual (Petroni et al., 2019; Brown et al., 2020; Schick and Sch\u00fctze, 2021a) or automated approach (Jiang et al., 2020; Yuan et al., 2021; Li and Liang, 2021) to create prompts.\n\nProblem Formulation\nHere, we first define each setting of the event detection task and then describe the various forms of event type prompts.\n\nSettings of ED\nFor supervised ED (SED), we follow the conventional supervised event detection setting where the training, validation, and evaluation data sets cover the same set of event types. The goal is to learn a model f to identify and classify event mentions for the target event types.\nFor few-shot ED (FSED), there are two separate training data sets for few-shot event detection:\n(1) A large-scale data set D base = {(x i , y i )} M i=1 that covers the old event types (named base types) where M denotes the number of base event types;\n(2) a smaller data set D novel = {(x j , y j )} N \u00d7K j=1 that covers N novel event types, with K examples each. Note that the base and novel event types are disjoint except for the Other class. The model f will be first optimized on D base , and then further fine-tuned on D novel . The goal is to evaluate the generalizability and transferability of the model from base event types to new event types with few annotations.\nFor zero-shot ED (ZSED), the training data sets are the only difference between zero-shot and fewshot event detection. In zero-shot event detection, there is only a large-scale base training data set\nD base = {(x i , y i )} M\ni=1 for the base event types. The model f will be only optimized on base event types and evaluated on the novel types.\n\nEvent Type Prompts\nWe compare the following five forms of prompts to represent the event types: (a) Event Type Name is the event class name, usually consisting of one to three tokens. (b) Definition can be a short sentence that formally describes the meaning of the event types. (c) Prototype Seed Triggers a list of \n\nA Unified Framework for ED\nWe adapt (Wang et al., 2021a) and design a unified event detection framework (as shown in Figure 1 ) which leverages event type specific prompts to detect events under supervised, few-shot, and zeroshot settings. Formally, given an input sentence W = {w 1 , w 2 , . . . , w n }, we take each event type prompt T t = {\u03c4 t 1 , \u03c4 t 2 , . . . , \u03c4 t m } as a query of M tokens to extract triggers for event type t. Specifically, we first concatenate them into a sequence\n[CLS] \u03c4 t 1 ... \u03c4 t m [SEP] w 1 ... w n [SEP]\n. We use a pre-trained BERT encoder (Devlin et al., 2019) to get contextual representations for the input sentence W = {w 0 , w 2 , ..., w n } as well as the event type prompt T = {\u03c4 t 0 , \u03c4 t 1 , ..., \u03c4 t m } 2 . Given a prompt of each event type, we aim to extract corresponding event triggers from the input sentence. To achieve this goal, we need to capture the semantic correlation of each input token to the event type Thus we learn a weight distribution over the sequence of contextual representations of the event type prompt, to obtain event type t aware contextual representation\nA t i = |T t | j=1 \u03b1 ij \u2022 \u03c4 t j , where \u03b1 ij = cos(w i , \u03c4 t j )\n, where \u03c4 j is the contextual representation of the j-th prompt token. cos(\u2022) is the cosine similarity function between two vectors. With that, the event type aware contextual representation A t i will be concatenated with the original contextual representation w i from the encoder, and classified into a binary label, indicating whether it is a candidate trigger of event type t or not:\n\u1ef9t i = U o ([w i ; A t i ; P i ])\n, where [; ] denotes concatenation operation, U o is a learnable parameter matrix for event trigger detection, and P i is the one-hot part-of-speech (POS) encoding of word w i . For continuous soft prompt based event detection, we follow Li and Liang (2021) where a prefix index q is prepended to the input sequence W \u2032 = [q; W ]. The prefix embedding is learned by q = MLP \u03b8 (Q \u03b8 [q]), where Q \u03b8 \u2208 R |Q|\u00d7k denotes the embedding lookup table for the vocabulary of prefix indices. Both MLP \u03b8 and Q \u03b8 are trainable parameters. Detailed learning strategy is in Appendix C.\n\nExperiment Setup\nWe perform experiments on three public benchmark datasets, including ACE05-E + (Automatic Content Extraction), ERE (Entity Relation Event) (Song et al., 2015) ,and MAVEN (Wang et al., 2020a) . On each dataset, we conduct experiments for SED, FSED, and ZSED. For SED, we use the same data split as the previous studies (Li et al., 2013; Wadden et al., 2019; Lin et al., 2020; Du and Cardie, 2020; Lin et al., 2020; Nguyen et al., 2021; Wang et al., 2020a ) on all the three benchmark datasets. For FSED and ZSED on MAVEN, we follow the previous study (Chen et al., 2021) and choose 120 event types with the most frequent mentions as the base event types and the rest 45 event types as novel ones. For FSED and ZSED on ACE and ERE, previous studies (Lai et al., 2020b, Zero-shot Event Detection The proposed prompt-based method is more affordable to be generalized compared with the prior state-ofthe-art zero-shot approach (Zhang et al., 2021b) .\nThe average length of created APEX prompts is less than 20 tokens. Thus manually creating them will not take much human effort. On the contrary, Zhang et al. (2021b) requires an extensive collection of anchor sentences to perform zero-shot event detection, e.g., 4,556,237 anchor sentences for ACE and ERE. This process is time-consuming and expensive.\n\nConclusion\nWe investigate a variety of prompts to represent the semantics of event types, and leverage them with a unified framework for supervised, few-shot and zero-shot event detection. Experimental results demonstrate that, a well-defined and comprehensive description of event types can significantly improve the performance of event detection, especially when the annotations are limited (few-shot event detection) or even not available (zero-shot event detection), with up to 22.2% F-score gain over the prior state of the art.\n", "hypothesis": " We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zeroshot event detection.  The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (fewshot event detection) or not available (zero-shot event detection).  By leveraging the semantics of event types, our unified framework shows up to 22.2% F-score gain over the previous stateof-the-art baselines 1 ..", "answer": true}
{"title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages", "content": "\nIntroduction\nInspired by the recent advancements in language model pre-training, Vision-Language Pre-trained Models (VLPMs) have demonstrated state-of-theart performance across a wide range of visionlanguage (VL) tasks such as text-to-image retrieval, visual reasoning, visual entailment, and visual QA (Chen et al., 2020; Li et al., 2021 Li et al., , 2022)) .\nHowever, extending VLPMs to multilingual scenarios is still challenging. On one hand, the majority of these models are trained on monolingual (English) corpora and thus cannot perform well for other languages. On the other hand, the multilingual pre-trained language models (Devlin et al., Figure 1 : Overview of our approach. We adapt the text encoder of a monolingual VL model to an unseen language (a). Then we use the adapted model for a VL downstream task in a zero-shot setting (b).\n2018; Conneau et al., 2019) cannot handle vision data (e.g., images or videos) directly.\nLately, there have been attempts (M 3 P, nUNITER, UC 2 ) to pivot on images or English texts to align multilingual representations with vision features (Chen et al., 2020; Ni et al., 2021; Zhou et al., 2021) .\nHowever, a recent benchmark on multilingual multimodal pretraining (IGLUE) (Bugliarello et al., 2022) shows that although these models achieve promising zeroshot cross-lingual transfer performance on some VL tasks, they still fall short in comparison to the \"translate-test\" baseline (using an English-only VLPM on the translations of the text examples).\nA more recent work (CCLM) achieves promising performance on the IGLUE benchmark by exploiting massive parallel text and image-text corpora to pre-train a VL model (Zeng et al., 2022) . This approach is motivated by a key observation that multilingual and multimodal pre-training essentially achieves the same goal of aligning two different views of the same object into a common semantic space. Although this framework performs well on the IGLUE benchmark, it requires a large amount of parallel data. Its pre-training phase relies on 19M multilingual parallel sentence pairs extracted from WikiMatrix (Schwenk et al., 2021) , jointly trained with 4 million image-text pairs in multiple languages.\nIn this work, we are proposing a simple yet efficient way to adapt VLP models to unseen languages without requiring large parallel corpora. We propose to align a VLPM monolingual text encoder (achieving start-of-the-art performance on English downstream VL tasks) with a multilingual pre-trained language model (e.g., mBERT), using only small in-domain parallel text corpus. The recent progress in Neural Machine Translation (NMT) has enabled us to create such a parallel corpus from automatically translating the data from English to any other language, even for lowresource languages (i.e., Swahili). However, since our approach relies on token alignment, it is robust to errors made by NMT. Our zero-shot evaluation across three of the four IGLUE tasks shows that the proposed method achieves state-of-the-art results while using small set of in-domain parallel sentences. The key steps of our approach are illustrated in Figure 1 .\n2 CLiCoTEA : Cross-Lingual\n\nContextualised Token Embedding Alignment\nWe propose CLiCoTEA , an approach to transfer a monolingual vision-language (VL) pre-trained model in one language L 1 where there is an abundant number of training pairs of image and text (i.e., English) to a second language L 2 . As we focus in this paper on the zero-shot setting, we do the transfer after fine-tuning the pre-trained monolingual VL model on a downstream task t, where training samples are available in language L 1 .\nCLiCoTEA consists of six steps:\n1. Pre-train a monolingual VL model on a massive collection of image-text pairs, where text is written in language L 1 .\n2. Fine-tune the VL pre-trained model on the downstream task t in language L1.\n3. Create a parallel text corpus by translating the training set from step 2 in the target language L 2 . Note that this step can be done automatically using neural machine translation.\n4. Create a list of aligned tokens for each (potentially noisy) parallel sentence using a token alignment model. 1b .\nIn practice, steps 1 and 2 are the most computationally expensive. Therefore, we propose to adapt VL fine-tuned models to new languages by only doing the steps from 3 to 5 which can be computed in a few hours on a single GPU.\nWe note that CLiCoTEA could be used with any multimodal pre-trained model where one of the modalities is a monolingual text encoder. We focus in this paper on VL models, but CLiCoTEA could be applied for instance to a language-knowledge model such as GreaseLM (Zhang et al., 2021) or DRAGON (Yasunaga et al., 2022) .\n\nPre-trained Models\nVision-Language Model In step 1 of CLiCoTEA , we use the Align BEfore Fuse (ALBEF) framework 1 (Li et al., 2021) as our Vision-Language Pre-trained Model (VLPM). AL-BEF has been fine-tuned on multiple downstream VL tasks and achieves state-of-the-art performance. We use the ALBEF fine-tuned models in step 2 for the downstream tasks described in Section 3.3. Unlike other competitive VL pre-trained models (such as BLIP (Li et al., 2022) ) that inject visual information by inserting cross-attention for each transformer block of the text encoder, ALBEF first encodes the image and text independently with a detector-free image encoder and a text encoder. Then it uses a multimodal encoder to fuse the image features with the text features through cross-modal attention. All encoders are based on transformer networks with the text encoder being a 6-layer transformer initialised using the first 6 layers of the BERT base . We thus extract this 6-layer text encoder for cross-lingual transfer training in step 5.\nMultilingual Language Model As a multilingual pre-trained language model, we use the multilingual BERT (mBERT) 2 (Devlin et al., 2018) . It has been trained on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective and has demonstrated remarkable zero-shot cross-lingual transfer capabilities (Wu and Dredze, 2019; Pires et al., 2019; Hu et al., 2020; Conneau et al., 2018) . We extract the first 6-layer transformer to be aligned with the text encoder of ALBEF in step 5.\n\nImplementation Details\nWord Alignment Since the parallel sentences do not contain word-level alignment information, in step 4 of CLiCoTEA we utilize awesome-align 3 (Dou and Neubig, 2021) which is a tool that automatically extracts word alignments from mBERT. The generated word pairs are then filtered for keeping only one-to-one, oneto-many or many-to-one alignments and removing many-to-many alignments. This is done for all languages except Chinese because otherwise less than 3% of the training data would remain in the set. The advantage of this filtering is twofold: a) it removes the noise from the matching word pairs; b) it reduces the training time and computation. For words that are split into sub-word tokens, we consider either the left-most token embedding alignment (i.e., the first sub-word token of a word) or, the average embedding across all sub-word tokens.\n\nContextualised Token Alignment Training\nGiven a set of aligned contextual word pairs extracted from parallel sentences, we define {x i , y i } n i=1 , where x i \u2208 R d is the contextualised embedding of token i in the target language (obtained from mBERT), and y i \u2208 R d is the contextualised embedding of its alignment in the source 2 Available on HuggingFace hub at https://huggingface.co/ bert-base-multilingual-cased.\nData Augmentation As multilingual language models are generally pre-trained on the source language L 1 , the contextualised token alignment can be trained not only with sentences from the target language L 2 , but also with sentences from the source language L 1 . This strategy doubles the training size, and consequently, the training time but it could be used with tasks where the number of available training sentences is limited.\n\nDownstream Tasks\nIn step 6, we evaluate CLiCoTEA on three tasks from the IGLUE benchmark 5 in the zero-shot setting:\n\u2022 xFlickr&CO: The dataset is composed of 1000 images from Flickr30K (Plummer et al., 2015) and 1000 images from MSCOCO dataset (Lin et al., 2014) . These images come along with croudsourced image captions in 6 different languages. xFlickr&CO is a retrieval task dataset. It is composed of two subtasks: image-to-text retrieval (TR) and text-to-image retrieval (IR).\n\u2022 XVNLI: The dataset consists in merging SNLI hypothesis with Flickr30K (Plummer et al., 2015) images and translate the test set in four languages. The task is called visual entailment (VE) which is a fine-grained reasoning task to determine whether a text hypothesis \"contradicts\", \"entails\", or is \"neutral\" with respect to an image.\n\u2022 MaRVL: The dataset is a multilingual expansion of NLVR2 dataset (Suhr et al., 2017) , with images related to concepts of five languages and cultures. The task is called visual reasoning (VR) which consists in determining whether a statement is correct given a pair of images.\nStep Table 1 shows the datasets used for a) fine-tuning the monolingual VL pre-trained model in step 2, b) training the alignment of contextualised token embeddings in step 5, and c) testing the zero-shot cross-lingual transfer in step 6. For creating the parallel corpus in step 3, all datasets used for finetuning the monolingual pre-trained VL model are translated to the corresponding test dataset languages from the IGLUE benchmark using Google-Trans Python API 6 . Statistics about the translation datasets can be found in Section A.1. MaRVL being the smallest dataset, the data augmentation strategy described in Section 3.2 is applied only for this task. Detailed results on data augmentation can be found in Section 3.2. 2 shows that CLiCoTEA outperforms the state-of-the-art CCLM models for all downstream tasks except retrieval. The larger improvement against CCLM models is obtained in visual entailment with an increase of almost 5%. The superiority of CLiCoTEA is especially high for Spanish (+7.68%), as can be seen from Table 10 in Section A.4. The average performance on visual reasoning is similar to CCLM, but CLiCoTEA significantly outperforms CCLM by \u00b14% on the low-resource languages such as Tamil and Swahili (results per language can be seen in Table 8 in Section A.3). For retrieval, CLiCoTEA outperforms all models except CCLM 4M . It is worth mentioning that, unlike the other models, CCLM 4M has been pre-trained on COCO which could explain its supe- riority on Flickr&CO dataset. More details about the results on retrieval can be found in Section A.2.\n\nConclusion\nIn this paper, we present CLiCoTEA an approach for adapting Vision-Language pre-trained models to unseen languages. Unlike other approaches that rely on an expensive pre-training phase (both in terms of data and computation), our approach adapts the contextualised token embeddings of a multilingual pre-trained language model by aligning them with the contextualised token embeddings of the VLPM text encoder. By aligning ALBEF text encoder with mBERT, we show that CLiCoTEA outperforms CCLM, which exploits massive parallel text and image-text corpora.\nCLiCoTEA achieves start-of-the-art performance on visual entailment and visual reasoning, with an increase of almost 5% on visual entailment. It also demonstrates its effectiveness, especially for low-resource languages, as it does not require large corpora to do the adaptation.\n", "hypothesis": " Vision-Language Pre-training (VLP) has advanced the performance of many visionlanguage tasks, such as image-text retrieval, visual entailment, and visual reasoning.  The pre-training mostly utilizes lexical databases and image queries in English.  Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting.  However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks.  In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.  We utilize a cross-lingual contextualized token embeddings alignment approach to train text encoders for non-English languages.  Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach performs slightly worse than the state-of-the-art multilingual vision-language models and requires large parallel corpora for training.  Our code is available at https://github.com/Yasminekaroui/CliCoTea..", "answer": false}
{"title": "Open-World Factually Consistent Question Generation", "content": "\nIntroduction\nQuestion generation is the task of generating a question that is relevant to and answerable by a piece of text (Krishna and Iyyer (2019) , Chen et al. (2020) , Zhu and Hauff (2021) , Ushio et al. (2022), ) . It is an important task in language generation (Fabbri et al. (2020) , Yu et al. (2020b) ), education (Wang et al. ( 2022)), and information retrieval (Yu et al. (2020a) ). A critical metric for question generation is factual consistency, i.e., the question has facts that are derivable from the input paragraph. This work proposes novel methods to improve entitylevel factual consistency while agnostic to model and underlying training data. Nan et al. (2021) and Xiao and Carenini (2022) solve a similar problem for summarization. However, to the best of our knowledge, no work addresses the issue of entitylevel factual inconsistency for question generation. Nema and Khapra (2018) have shown that name entities are essential for a question's answerability. The presence of wrong entities may make the question nonsensical and unanswerable. Table 1 shows entity-level factual inconsistency in question generation by a fine-tuned PEGASUS (Zhang et al., 2019) model. In the first entity, \"Kim Jong Un\", and in the second example, \"Chicago\" are hallucinated.\nUnlike previous work in the summarization field (Nan et al. (2021) , Liu et al. (2021a) , Xiao and Carenini (2022) ), our work is independent of the model or training process. We also do not reduce dataset size by filtering. Instead, we preprocess datasets to force the model to generate questions faithful to the input using strategies of de-lexicalization and multi-generation and recommend the best strategy. The proposed method improves the factual consistency by 84\u2212100% across multiple datasets while having minimal impact on traditional performance metrics.\nWe experimented with two popular language models viz. PEGASUS-large and BART-large (Lewis et al., 2020) . Our proposed approach consistently performs better for both the language models than normal finetuning. We also compare our approach to recent methods for addressing hallucination in summarization, and our results showed significantly better performance.\n\nRelated Work\nEarly work proposed rule-based approaches to generate questions (Labutov et al. (2015) ). Recent work using neural models for high-quality generation (Du and Cardie (2017) , Du et al. (2017) , Wang et al. (2022) ). Several works have aimed to control hallucination in different NLP tasks like summarization (Cao et al. (2018) , Song et al. (2020 ), Zhu et al. (2021) ), Wu et al. (2021) ), generative question answering (Bi et al. (2019) , Fan et al. (2019a) , Su et al. (2022) ), data to text (Nie et al. (2019) , Wang et al. (2020) , Wang et al. (2021) ), and machine translation (Xu et al. (2019) , Wang and Sennrich (2020) , Zhou et al. (2021) ).\nInput: As the president of South Korea, she turned out to be heavily influenced by a cult and giving them access to political documents and basically letting that cult run the country through her. One of her \"speech-writers\" was a religious figure in this cult that totally exploited the president, who pretty much only got elected because her father had been president, using her for money and power and wealth. The people of South Korea got upset when they learned that the real power that was running their country was the religious group that their president belonged to.\nNormal Finetuning: What did Kim Jong Un do wrong? Rare word delexicalization + Multiple (Ours): Why did the people of South Korea vote out the president of the South Korea?\nInput: As I understand it, there's no established *process* for removing the mayor from his post. If he doesn't resign, there's no mechanism for removing him. Really, I don't understand how this guy hasn't lost his job yet.\nNormal Finetuning: Why hasn't the mayor of Chicago been fired yet? Rare word delexicalization + Multiple (Ours): Why hasn't the mayor been fired yet?\nTable 1 : Qualitative Examples. For detailed analysis refer to section 6.4.\nPrevious work has explored entity-based delexicalization in settings like adapting parser for a new language (Zeman and Resnik, 2008) , valid reasoning chains in multi-hop question answering (Jhamtani and Clark, 2020) , and eliminating diachronic biases in fake news detection (Murayama et al., 2021) .\n\nMethodology\nThe objective is to generate relevant and entitylevel factually consistent questions which generalise across domains. For this, we propose novel de-lexicalization strategies combined with a multigeneration strategy. De-lexicalization involves replacing named entities with a special token or rare words during training/inference and replacing the original word after generation. The model's vocabulary is expanded to account for the special tokens used in the de-lexicalization strategies.\n\nDe-lexicalization Strategies During Training\n[Name i] Token: This strategy replaces the named entity with a token [Name i], where i represents the order of the first appearance of the entity in the paragraph and in the question.\n[Name i] Token with Push: This strategy is similar to the previous one. The difference is that if the question has a named entity that is not present in the input paragraph, we replace it with [Name j], where the j is a random number between 0 and the total number of named entities in the input paragraph. The intuition here is that we are pushing or explicitly asking the model to generate a named entity already present in the input paragraph.\n[Multiple i] Token: The previous two strategies treat all the named entities as similar. In contrast, in this approach, the entity is replaced with its corresponding semantic tags, followed by an integer representing its order of appearance in the paragraph followed by the question. A semantic tag specifies if an entity is name, organization, loca-tion, cardinal, etc.\n[Multiple i] Token with Push and Delete: This approach is similar to [Name i] Token with Push approach with multiple entity types. However, if the question consists of a named entity type not present in the paragraph, it is deleted.\nRare Word token: This strategy de-lexicalizes only the questions. Here we replace the named entities in questions that do not occur in the input paragraph with a rare word. A rare word is a word that occurs 2 to 5 times in the entire training corpus. If an entity occurs in the input paragraph, it is left as it is.\nExamples showing different de-lexicalization strategies are present in the Appendix.\nEntity Replacement: During testing, from the generated questions, the entities are replaced using a dictionary look-up of the special token. We treat a output as hallucinated if the special token has no corresponding named entity.\nMulti-generation: Here, we generate multiple questions during inference by selecting the top five beams from the output of the language model and selecting the one that is factually consistent and has the least perplexity. If no questions are consistent, the generation with the least perplexity is chosen. input.\nIn the [Name i] Token strategy, we replace all named entities with [Name i]. Do note that name entity, 55,000, and 2018 occur twice. Each occurrence is replaced with the same token, i.e., both occurrence of 55,000 is replaced with [Name 3]. Since the \"U.S.\" does not occur in the input, we replace it with [Name 5]. Contrary to this, in the [Name i] Token with Push strategy, we replace the U.S. with [Name 3], thereby pushing the model to be faithful to the source.\nIn the [Multiple i] Token strategy, instead of replacing named entities with a common [Name] token, we replace them with their semantic token. Thus, 55,000 is replaced with [MONEY 1] and so on. Like before, each occurrence is replaced with the same token. The U.S. is replaced with [GPE 0] as no entity of type GPE occurs in the input. Contrary to this, the [Multiple i] Token with Push and Delete strategy deletes the entity \"U.S.\" as no GPE-type entity exists in the input. If there were a GPE entity in input (not necessarily \"U.S.\"), it would have been replaced with [GPE 0].\nIn the Rare Word Token strategy, the input is unchanged. Since the U.S. does not occur in input, it is replaced with a rare word (aster).\n\nDatasets\nWe use the supervised ELI5 dataset (Fan et al., 2019b) for training. To ensure that the data is of high quality, we remove all the samples where the answer is short (having less than 50 words), or the question does not have a question mark.\nWe use three publicly available datasets for evaluation across different domains, viz. MS Marco (Bajaj et al., 2016) , Natural Questions (Kwiatkowski et al., 2019) and SciQ (Welbl et al., 2017) . We also scraped r/AskLegal 1 , and r/AskEconomics 2 for testing on finance and legal domains. Table 2 shows the statistics of the dataset.\n\nImplementation Details\nWe use publicly available checkpoints of the language models and fine-tune them for 100k steps with a batch size of 12 and using the Adam optimizer (Kingma and Ba, 2014) . The learning rate is set to 10 \u22125 , and the models are tested on the dev set every 10k steps. The best-performing model on the dev set is used. The model training takes approximately 6 hours on an Nvidia A100 40 GB GPU. Following Nan et al. (2021) we use the Spacy library 3 to identify named entities.\n\nEvaluation Metrics\nWe evaluate both the quality and factual consistency of the generated question. The quality is reported using Rouge-1, Rouge-2, Rouge-L (Lin, 2004) scores and cosine similarity between embedding (from all-mpnet-base-v2 sentence transformer model (Reimers and Gurevych, 2019)) of generated questions and ground truth. We use the perplexity value suggested by Liu et al. (2021b) , using a GPT-2 (Radford et al., 2019) . To evaluate factual consistency, we use two metrics. The first metric quantifies the degree of hallucination with respect to the ground truth question. We use the precision, recall, and F1 score proposed by Nan et al. (2021) . More details about the exact implementation are in the appendix or in their paper. The second metric quantifies the degree of hallucination with respect to the input paragraph. This metric measures, out of all the questions that have named entities, what percentage of questions have named entities not present in the input. Let N hne represent the number of generated questions with a named entity, and N wne represent the number of generated questions with a wrong named entity. N total represents the total number of questions. Do note N total \u0338 = N hne , as we can have questions with no named entity in them. Then N hne /N total * 100 represents the percentage of questions having a named entity (P ne ), and N wne /N hne * 100 represents the percentage of questions having the wrong named entity (P wne ). A system with a low P wne value and a high F1 score reflects the system is not hallucinating. We want a system with high factual consistency without significantly affecting the quality of the questions as measured by the proposed metrics.\n\nBaseline\nWe compare our results with the Spancopy method proposed by Xiao and Carenini (2022) for the summarization. We test with and without global relevance in Spancopy having PEGASUS as the base language model.\n\nResults and Analysis\nDue to space constraints, we only present results for PEGASUS-large in the main text. Results for BART-large can be found in the appendix.\nTable 4 shows the results of the test set of the ELI5 dataset. The results indicate that the rare word de-lexicalization plus multiple generation approach performs much better than other methods. Compared to a normal fine-tuned PEGASUS model, the P wne score decreases by about 98%, implying that the generated questions are faithful to the input text. Similarly, the F1 score increases by approximately 21%, implying that all the generated questions are faithful to ground truth. In contrast, decrements in other metric scores are less than 6.7%. Overall, rare word de-lexicalization plus multiple generation performs the best in terms of factual consistency and is comparable in other metrics. For detailed analysis refer to section 6.4.\nC.S. \u2191 R-1 \u2191 R-2 \u2191 R-L \u2191 PPL \u2193 Pne Pwne \u2193 Recall \u2191 Precision \u2191 F1 \u2191 Dataset:\nThe rare word de-lexicalization with multigeneration approach consistently performs better than all the other approaches for all the datasets. Table 5 compares rare word delexicalization + multiple generation with a normal finetuned PEGA-SUS and Spancopy without global relevance across different datasets. Detailed results for all the approaches across all the datasets are in the appendix.\nFrom the table, it can be seen that rare word delexicalization with multiple generations solves the issue of entity-level inconsistency without negative impact on different metrics. The model was just trained for the ELI5 dataset and was directly used for other datasets. Domain shift exacerbates the issue of entity hallucination, as shown by the P wne value for a normal fine-tuned PEGASUS model, which is usually higher in the presence of domain shift. Thus, our proposed approach works across domains without re-training.\nWe see that the P ne value decreases across all the datasets for rare word delexicalization with multiple generations. However, this is not wrong. A question without a named entity can still be a valid question (Nema and Khapra, 2018) .\nTable 1 shows qualitative examples. In the first example, the fine-tuned PEGASUS produces the entity Kim Jong Un that is unfaithful to the source and is entirely unrelated to South Korea. Chicago is hallucinated in the second example. In both examples, our proposed approach generates meaningful and faithful questions. Our approach produces a question with no named entity in the second example, yet the question is meaningful and faithful to the source. This further reinforces our claim that a question without a named entity can still be valid. More outputs can be found in the appendix.\nOur approach performs better than the Spancopy architecture (both with and without global relevance). This shows that simple de-lexicalization with multiple generations is better than sophisticated architecture.\n\nConclusion\nIn this paper, we study the entity-level factual inconsistency in question generation. Our proposed strategy, rare-word de-lexicalization with multigeneration, improve consistency without significantly affecting traditional metrics across data domains. Extensive experimental results further reinforce our claim.\n", "hypothesis": " Question generation methods based on pretrained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph.  Domain shift -where the test data is from a different domain than the training data -further exacerbates the problem of hallucination.  This is a critical issue for any natural language application doing question generation.  In this work, we propose an effective data processing technique based on de-lexicalization for consistent question generation across domains.  Unlike existing approaches for remedying hallucination, the proposed approach does not filter training data and is generic across question-generation models.  Experimental results across six benchmark datasets show that our model is robust to domain shift and produces entity-level factually consistent questions without significant impact on traditional metrics..", "answer": true}
{"title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks", "content": "\nIntroduction\nThe omnipresence of large pre-trained language models (Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020) has fueled concerns regarding their systematic biases carried over from underlying data into the applications they are used in, resulting in disparate treatment of people with different identities (Sheng et al., 2021; Abid et al., 2021) .\nIn response to such concerns, various benchmarks have been proposed to quantify the amount of social biases in models (Rudinger et al., 2018; Sheng et al., 2019; Li et al., 2020) . These measures are composed of textual datasets built for a specific NLP task (such as question answering) and are accompanied by a metric such as accuracy of prediction which is used as an approximation of the amount of social biases.\nThese bias benchmarks are commonly used by machine learning practitioners to compare the degree of social biases (such as gender-occupation Gender-Occupation Bias \u274c\n\nGender-Occupation Bias \u2705\nThe electrician warned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician warned the homeowner that she might need an extra day to finish rewiring the house. coref coref\n\nWinoGender\nThe electrician cautioned the homeowner that he might need an extra day to finish rewiring the house.\nThe electrician cautioned the homeowner that she might need an extra day to finish rewiring the house. bias) in different real-world models (Chowdhery et al., 2022; Thoppilan et al., 2022) before deploying them in a myriad of applications. However, they also inadvertently measure other non-social biases in their datasets. For example, consider the sentence from WINOGENDER in Figure 1 . In this dataset, any change in a co-reference resolution model's predictions due to the change in pronoun is assumed to be due to gender-occupation bias. However, this assumption only holds for a model with near-perfect language understanding with no other biases. This may not often be the case, e.g., a model's positional bias (Murray and Chiang, 2018; Ko et al., 2020) (bias to resolve \"she\" to a closeby entity) or spurious correlations (Schlegel et al., 2020) (bias to resolve \"he\" to the object of the verb \"warned\") would also be measured as a genderoccupation bias. As a result, a slightly different template (e.g., changing the verb to \"cautioned\") could result in completely different bias measurements.\n\nWinoGender-Alternate Construction\nThe goal of this work is to illustrate the extent to which social bias measurements are effected by assumptions that are built into dataset constructions. To that end, we consider several alternate dataset constructions for 2 bias benchmarks WINO-GENDER and BIASNLI. We show that, just by the choice of certain target-bias-irrelevant elements in a dataset, it is possible to discover different degrees of bias for the same model as well as different model rankings 1 . For instance, one experiment on BIASNLI demonstrated that merely negating verbs drastically reduced the measured bias (41.64 \u2192 13.40) on an ELMo-based Decomposable Attention model and even caused a switch in the comparative ranking with RoBERTa. Our findings demonstrate the unreliability of current benchmarks to truly measure social bias in models and suggest caution when considering these measures as the gold truth. We provide a detailed discussion ( \u00a75) of the implications of our findings, relation to experienced harms, suggestions for improving bias benchmarks, and directions for future work.\n\nRelated Work\nA large body of work investigates ways to evaluate biases carried inherently in language models (Bolukbasi et al., 2016; Caliskan et al., 2017; Nadeem et al., 2021) and expressed in specific tasks (Nangia et al., 2020; Kirk et al., 2021; Schramowski et al., 2022; Prabhumoye et al., 2021; Srinivasan and Bisk, 2021; Kirk et al., 2021; Parrish et al., 2021; Baldini et al., 2022; Czarnowska et al., 2021; Dev et al., 2021a; Zhao et al., 2021) . Alongside, there is also growing concern about the measures not relating to experienced harms (Blodgett et al., 2020) , not inclusive in framing (Dev et al., 2021b) , ambiguous about what bias is measured (Blodgett et al., 2021) , not correlated in their findings of bias across intrinsic versus extrinsic techniques (Goldfarb-Tarrant et al., 2021; Cao et al., 2022) , and susceptible to adversarial perturbations (Zhang et al., 2021) and seed word selection (Antoniak and Mimno, 2021) .\nThe concurrent work by (Seshadri et al., 2022 ) discusses the unreliability of quantifying social biases using templates by varying templates in a se-mantic preserving manner. While their findings are consistent with ours, the two works provide complementary experimental observations. Seshadri et al. (2022) study a wider range of tasks, though we focus our experiments on a wider set of models and alternate dataset constructions (with a greater range of syntactic and semantic variability). As a result, we are able to illustrate the effect of the observed variability on ranking large language models according to measured bias for deployment in real world applications.\n\nSocial Bias Measurements and Alternate Constructions\nBias measures in NLP are often quantified through comparative prediction disparities on language datasets that follow existing tasks such as classification (De-Arteaga et al., 2019) or coreference resolution (Rudinger et al., 2018) . As a result, these datasets are central to what eventually gets measured as \"bias\". Not only do they determine the \"amount\" of bias measured but also the \"type\" of bias or stereotype measured. Datasets often vary combinations of gendered pronouns and occupations to evaluate stereotypical associations. It is important to note that these constructs of datasets and their templates, which determine what gets measured, are often arbitrary choices. The sentences could be differently structured, be generated from a different set of seed words, and more. However, we expect that for any faithful bias benchmark, such dataset alterations that are not relevant to social bias should not have a significant impact on the artifact (e.g. gender bias) being measured.\nThus, to evaluate the faithfulness of current benchmarks, we develop alternate dataset constructions through modifications that should not have any effect on the social bias being measured in a dataset. They are minor changes that should not influence models with true language understanding -the implicit assumption made by current bias benchmarks. Any notable observed changes in a model's bias measure due to these modifications would highlight the incorrectness of this assumption. Consequently, this would bring to light the unreliability of current benchmarks to faithfully measure the target bias and disentangle the measurement from measurement of other non-social biases. A non-exhaustive set of such alternate constructions considered in this work are listed below.\nFigure 2 : An instance (\"The engineer informed the client that he would need to make all future payments on time\") from WINOGENDER benchmark modified under various shallow modifications ( \u00a73). To a human eye, such modifications do not necessarily affect the outcome of the given pronoun resolution problem.\nNegations: A basic function in language understanding is to understand the negations of word groups such as action verbs, or adjectives. Altering verbs in particular, such as 'the doctor bought' to 'the doctor did not buy' should typically not affect the inferences made about occupation associations. Synonym substitutions: Another fundamental function of language understanding is the ability to parse the usage of similar words or synonyms used in identical contexts, to derive the same overall meaning of a sentence. For bias measuring datasets, synonymizing non-pivotal words (such as non-identity words like verbs) should not change the outcome of how much bias is measured. Varying length of the text: In typical evaluation datasets, the number of clauses that each sentence is composed of and overall the sentence length are arbitrary experimental choices. Fixing this length is common, especially when such datasets need to be created at scale. If language is understood, adding a neutral phrase without impacting the task-specific semantics should not alter the bias measured. Adding descriptors: Sentences used in real life are structured in complex ways and can have descriptors, such as adjectives about an action, person, or object, without changing the net message expressed by the text. For example, the sentences, \"The doctor bought an apple.\", and \"The doctor bought a red apple.\" do not change any assumptions made about the doctor, or the action of buying an apple. Random samples: Since the sentence constructs of these datasets are not unique, a very simple alternate construction of a dataset is a different subsample of itself. This is because the dataset is scraped or generated with specific assumptions or parameters, such as seed word lists, templates of sentences, and word order. However, neither the sentence constructs or templates, nor the seed word lists typically used are exhaustive or representative of entire categories of words (such as gendered words, emotions, and occupations).\nSee Fig. 2 for example constructions on WINO-GENDER (App. A, B for detailed descriptions).\n\nCase Studies\nWe discuss here the impact of alternate constructions on two task-based measures of bias. 2\n\nCoreference Resolution\nSeveral different bias measures (Rudinger et al., 2018; Zhao et al., 2018; Cao and Daum\u00e9 III, 2021) for coreference resolution work similar to Winograd Schema (Winograd, 1972) where a sentence has two entities and the task is to resolve which entity a specific pronoun or noun refers to. We work here with WINOGENDER (Rudinger et al., 2018) , popularly used to measure biases. It is worth noting that WINOGENDER was originally intended by its authors to merely be a diagnostic tool that checks for bias in a model; the authors note that it may demonstrate the presence of model bias but not prove the absence of the same. Nonetheless, models developed today are indeed tested and compared for social bias on WinoGender, leading to its usage as a comparative standard or benchmark (Chowdhery et al., 2022; Thoppilan et al., 2022) .\nThe metric used to evaluate bias is the percentage of sentence pairs where there is a mismatch in predictions for the male and female gendered pronouns. For instance, in Fig. 2 , if the pronoun \"he\" is linked to \"engineer\" but switches to \"client\" for the pronoun \"she\", that would indicate a genderoccupation bias. Higher the number of mismatches, higher the bias. In particular, note that the metric does not take into account the accuracy of the predictions, but rather only the mismatch between the two pronouns.\nWe experiment with three alternate constructions of the dataset: addition of clauses, addition of adjectives, and synonymizing words in templates.\nEach alternate construction is introduced so as to not affect the overall meaning of the sentence. Experimental Results: We use an end-toend coreference model with SpanBERT embeddings (Lee et al., 2018; Joshi et al., 2020) , Uni-fiedQA (small, base, and large) (Khashabi et al., 2020) QA model, 3 and a long-document coreference model with Longformer encodings (Toshniwal et al., 2021) . Results of evaluating these models on various WINOGENDER constructions is summarized in Fig. 3a . Small changes to the formulation of dataset templates result in sizable changes to computed bias measures compared to the published baseline constructions. For example, a construction involving added adjectives after occupations would have found the UnifiedQA (large) model to have 10% less bias compared to the default constructions. The sensitivity to the dataset constructions can have a drastic effect on ranking models according to their social bias, as Fig. 3a shows. For example, the SpanBERT model is considered to have less bias than UnifiedQA (small) model in the baseline dataset, but would be considered to be more biased if the templates had clauses after the participants or adjectives before the occupation. \n\nNatural Language Inference\nNatural Language Inference (NLI) is the task of determining directional relationships between two sentences (a premise (P) and a hypothesis (H)). Dev et al. (2020) 's measure based on NLI (BIASNLI) evaluates if stereotypical inferences are made by language models. We use their dataset for genderoccupation stereotypes containing approximately 2 million sentence pairs such as P: \"The doctor bought a bagel.\", H: \"The man bought a bagel.\". The expected prediction for each sentence pair in the dataset is neutral, and therefore the bias metric used is the fraction of neutral inferences on dataset -the higher the score, the lower the bias.\nWe experiment with three alternate constructions of the dataset: verb negation, random sampling, and addition of clauses. Note that the alternate constructions do not impact the unbiased label (neutral). Any change in construction (say negating a verb) is applied to both the premise and hypothesis. Refer to App. B for a detailed description. Experimental Results: We use RoBERTa trained on SNLI (RoBERTa-base-SNLI) (Liu et al., 2019) , ELMo-based Decomposable Attention (ELMo-DA) (Parikh et al., 2016) , ALBERT (Lan et al., 2019) , distilled version of the RoBERTa-base model (Sanh et al., 2019) , and RoBERTa-large finetuned on WANLI (Liu et al., 2022) . The bias measured with each model using BIASNLI is recorded in Fig. 3b . The results show how small modifications to the dataset again result in large changes to the bias measured, and also change the bias rankings. For example, adding a negation largely reduces the bias measured (\u25b3 = 28.24) for ELMo-DA, and also results in a switch in the comparative ranking to RoBERTa-base-SNLI. Furthermore, as seen in Fig. 4 , there is a significant overlap in the bias measures of ALBERT, DistilRoBERTa, and ELMo-DA under random sampling, 4 which corresponds to high variability in relative model ordering across different sub-samples of the dataset.\n\nDiscussion and Conclusion\nSocial bias measurements are very sensitive to evaluation methodology. Our empirical evidence sheds light on how the model's non-social biases brought out or masked by alternate constructions can cause bias benchmarks to underestimate or overestimate the social bias in a model. More interestingly, it is important to note that different models respond differently to perturbations. In fact, the same perturbation can result in a higher or lower measured bias depending on the model (as seen in \u00a74.1 and \u00a74.2), which points to how models might parse information (and thus bias) differently.\nWhile current bias measures do play a role in exposing where model errors have a stereotypical connotation, a lack of sentence construction variability or even assumptions made when creating seed word lists can reduce the reliability of the benchmarks, as we see in this work ( \u00a74.2). Even with simple sentences, it is not apparent how to disentangle the biased association of the identity with the verb or the occupation amongst others. This is especially important to note as it highlights that measures can lack concrete definitions of what bi-ased associations they measure. Consequently, the relation between measured bias and experienced harm becomes unclear.\nWe hope that our troubling observations motivates future work that thoroughly investigates how to construct robust benchmarks that faithfully measure the target bias without being affected by model errors and other non-social biases. As suggested by our subsampling experiments (Appendix F), it might be fruitful to encourage both syntactic and semantic diversity in these benchmarks. Bias benchmarks that provide uncertainty measures (instead of a single number) might enable practitioners to better compare models before deploying them. Furthermore, since the opaqueness of large language models makes it challenging to understand how and to what extent a linguistic change will affect the measured bias, explainable models might indeed facilitate better measurement of their social bias. Assuming that we can generate faithful explanations for a model's predictions, an exciting future direction is to explore construction of bias benchmarks which operate on the explanations of the predictions rather than the predictions themselves. Lastly, we also encourage discussions on the complexity of the sentences used in benchmarks and their implications on what gets measured in relation to un-templated, naturally-occurring text (Levy et al., 2021) , as an attempt to ground our measurements in experienced harms.\n", "hypothesis": " How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye).  To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (WINOGENDER and BIASNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias. However, these modifications have little impact on the models' ability to mitigate social biases.  We hope these troubling observations motivate more robust measures of social biases..", "answer": false}
{"title": "Do GPTs Produce Less Literal Translations?", "content": "\nIntroduction\nDespite training only on a language-modeling objective, with no explicit supervision on aligned parallel data (Briakou et al., 2023) , LLMs such as GPT-3 or PaLM (Brown et al., 2020; Chowdhery et al., 2022) achieve close to state-of-the-art translation performance under few-shot prompting (Vilar et al., 2022; Hendy et al., 2023) . Work investigating the output of these models has noted that the gains in performance are not visible when using older surface-based metrics such as BLEU (Papineni et al., 2002a) , which typically show large losses against NMT systems. This raises a question: How do these LLM translations differ qualitatively from those of traditional NMT systems?\nWe explore this question using the property of translation literalness. Machine translation systems have long been noted for their tendency to produce source He survived by the skin of his teeth .\n\nNMT\nIl a surv\u00e9cu par la peau de ses dents . GPT-3 Il a surv\u00e9cu de justesse . Table 1 : An example where GPT-3 produces a more natural (non-literal) translation of an English idiom. When word-aligning these sentences, the source word skin remains unaligned for the GPT-3 translation.\noverly-literal translations (Dankers et al., 2022b) , and we have observed anecdotally that LLMs seem less susceptible to this problem (Table 1 ). We investigate whether these observations can be validated quantitatively. First, we use measures based on word alignment and monotonicity to quantify whether LLMs produce less literal translations than NMT systems, and ground these numbers in human evaluation ( \u00a7 2). Next, we look specifically at idioms, comparing how literally they are translated under both natural and synthetic data settings ( \u00a7 3).\nOur investigations focus on the translation between English and German, Chinese, and Russian, three typologically diverse languages. Our findings are summarized as follows: (1) We find that translations from two LLMs from the GPT series of LLMs are indeed generally less literal than those of their NMT counterparts when translating out of English, and (2) that this is particularly true in the case of sentences with idiomatic expressions.\n\nQuantifying Translation Literalness\nWe compare the state-of-the-art NMT systems against the most capable publicly-accessible GPT models (at the time of writing) across measures designed to capture differences in translation literalness. We conduct both automatic metric-based as well as human evaluations. We explain the evaluation and experimental details below. for evaluation (Barrault et al., 2021) .\n\nMeasures of Quality\nWe use COMET-QE 1 (Rei et al., 2020) as the Quality Estimation (QE) measure (Fomicheva et al., 2020) to quantify the fluency and adequacy of translations. Using QE as a metric presents the advantage that it precludes the presence of any reference bias, which has been shown to be detrimental in estimating the LLM output quality in related sequence transduction tasks (Goyal et al., 2022) . On the other hand, COMET-QE as a metric suffers from an apparent blindness to copy errors (i.e., cases in which the model produces output in the source language) (He et al., 2022) . To mitigate this, we apply a language identifier (Joulin et al., 2017) on the translation output and set the translation to null if the translation language is the same as the source language. Therefore, we name this metric COMET-QE + LID.\n\nMeasures of Translation Literalness\nThere do not exist any known metrics with high correlation geared towards quantifying translation literalness.\nWe propose and consider two automatic measures at the corpus-level:\n1. Unaligned Source Words (USW): Two translations with very similar fluency and adequacy could be differentiated in terms of their literalness by computing word to word alignment between the source and the translation, then measuring the number of source words left unaligned. When controlled for quality, a less literal translation is likely to contain more unaligned source words (as suggested in Figure 1 ).\n\nTranslation Non-Monotonicity (NM):\nAnother measure of literalness is how closely the translation tracks the word order in the source. We use the non-monotonicity metric proposed in Schioppa et al. (2021) , which computes the deviation from the diagonal in the word to word alignment as the non-monotonicity measure.\n1 wmt20-comet-qe-da\nThis can also be interpreted as (normalized) alignment crossings, which has been shown to correlate with translation non-literalness (Schaeffer and Carl, 2014) .\nWe use the multilingual-BERT-based awesomealigner (Devlin et al., 2019; Dou and Neubig, 2021) to obtain the word to word alignments between the source and the translation. Table 2 presents an illustration of translations with different USW and NM scores 2 , obtained from different systems.\n\nSystems Under Evaluation\nWe experiment with the below four systems (NMT and LLMs):\n1. WMT-21-SOTA: The Facebook multilingual system (Tran et al., 2021) won the WMT-21 News Translation task (Barrault et al., 2021) , and thereby represents the strongest NMT system on the WMT'21 test sets.\n2. Microsoft-Translator: MS-Translator is one of the strongest publicly available commercial NMT systems (Raunak et al., 2022) .\n3. text-davinci-002: The text-davinci-002 model is an instruction fine-tuned model in the GPT family (Brown et al., 2020) . It represents one of the strongest publicly-accessible LLMs (Liang et al., 2022) .\n4. text-davinci-003: The text-davinci-003 model further improves upon text-davinci-002 for many tasks 3 (Liang et al., 2022) .\nFor both the GPT models, we randomly select eight samples from the corresponding WMT-21 development set, and use these in the prompt as demonstrations for obtaining all translations from GPTs.\n\nResults\nWe compare the performance of the four systems on the WMT-21 test sets. Figure 1 shows the results of this comparison. A key observation is that while the GPT based translations achieve superior COMET-QE+LID scores than Microsoft Translator across the language pairs (except En-Ru), they The NMT Systems and GPT models achieve similar COMET-QE+LID Scores (Top), there exists a significant gap in the number of unaligned source words (USW) across the datasets (Bottom). Further, GPT translations obtain higher non-monotonicity scores for E-X translations (Middle).\nalso consistently obtain considerably higher number of unaligned source words. This result holds for the comparison between the WMT-21-SOTA and GPT systems as well. Further, GPT translations also consistently show higher non-monotonicity for E\u2192X translations. However, this is not the case for translations into English, wherein the multilingual WMT-21-SOTA system obtains very close non-monotonicity measurements. The combined interpretation of these measurements suggests that GPTs do produce less literal E\u2192X translations.\n\nHuman Evaluation\nWe verify the conclusion from the results in Figure 1 by conducting a human evaluation of translation literalness on 6 WMT-22 language pairs: En-De, En-Ru, En-Zh and De-En, Ru-En, Zh-En. For each language pair, we randomly sample 100 source-translation pairs, with translations obtained from MS-Translator (a strong commercial NMT system) and text-davinci-003 (a strong commercial LLM) (Hendy et al., 2023) . We used zero-shot text-davinci-003 translations for human evaluations in order to eliminate any biases through the use of specific demonstration examples. In each case, we ask a human annotator (bilingual speaker for Zh-En, target-language native plus bilingual speaker otherwise) to annotate 100 translations from both GPT and MS-Translator and select which of the two translations is more literal. The human annotation interface is described in Appendix A. The results in Table 3 show that the annotators rate the GPT translations as less literal.\nLang Experiments on Best WMT-22 NMT Systems Further, we also experiment with the WMT-Best systems on the WMT-22 General Machine Translation task (Kocmi et al., 2022) . We evaluate USW and NM on De-En, Ja-En, En-Zh and Zh-En, since on each of these language pairs, text-davinci-003's few-shot performance is very close to that of the WMT-Best system as per COMET-22 (Rei et al., 2022) , based on the evaluation done in Hendy et al. (2023) . We report our results in Table 4 , which shows our prior findings replicated across the language pairs. For example, text-davinci-003, despite obtaining a 0.2 to 0. \n\nEffects On Figurative Compositionality\nIn this section, we explore whether the less literal nature of E\u2192X translations produced by GPT models could be leveraged to generate higher quality translations for certain inputs. We posit the phenomenon of composing the non-compositional meanings of idioms (Dankers et al., 2022a) with the meanings of the compositional constituents within a sentence as figurative compositionality. Thereby, a model exhibiting greater figurative compositionality would be able to abstract the meaning of the idiomatic expression in the source sentence and express it in the target language non-literally, either through a non-literal (paraphrased) expression of the idiom's meaning or through an equivalent idiom in the target language. Note that greater nonliteralness does not imply better figurative compositionality. Non-literalness in a translation could potentially be generated by variations in translation different from the desired figurative translation.\n\nTranslation with Idiomatic Datasets\nIn this section, we quantify the differences in the translation of sentences with idioms between traditional NMT systems and a GPT model. There do not exist any English-centric parallel corpora dedicated to sentences with idioms. Therefore, we experiment with monolingual (English) sentences with idioms. The translations are generated with the same prompt in Section 2. The datasets with natural idiomatic sentences are enumerated below:\n\u2022 MAGPIE (Haagsma et al., 2020) contains a set of sentences annotated with their idiomaticity, alongside a confidence score. We use the sentences pertaining to the news domain which are marked as idiomatic with cent percent annotator confidence (totalling 3,666 sentences).\n\u2022 EPIE (Saxena and Paul, 2020) contains idioms and example sentences demonstrating their usage. We use the sentences available for static idioms (totalling 1,046 sentences).\n\u2022 The PIE dataset (Zhou et al., 2021) contains idioms along with their usage. We randomly sample 1K sentences from the corpus.\n\nResults\nThe results are presented in Table 5 . We find that text-davinci-002 produces better quality translations than the WMT'21 SOTA system, with greater number of unaligned words as well as with higher non-monotonicity.\nFurther Analysis Note that a direct attribution of the gain in translation quality to better translation of idioms specifically is challenging. Further, similarity-based quality metrics such as COMET-QE themselves might be penalizing non-literalness, even though they are less likely to do this than surface-level metrics such as BLEU or ChrF (Papineni et al., 2002b; Popovi\u0107, 2015) . Therefore, while a natural monolingual dataset presents a useful testbed for investigating figurative compositionality abilities, an explicit comparison of figurative compositionality between the systems is very difficult. Therefore, we also conduct experiments on synthetic data, where we explicitly control the finegrained attributes of the input sentences. We do this by allocating most of the variation among the input sentences to certain constituent expressions in synthetic data generation.\n\nSynthetic Experiments\nFor our next experiments, we generate synthetic English sentences, each containing expressions of specific type(s): (i) names, (ii) random descriptive phrases, and (iii) idioms. We prompt text-davinci-002 in a zero-shot manner, asking it to generate a sentence with different instantiations of each of these types (details are in appendix B). We then translate these sentences using the different systems, in order to investigate the relative effects on our literalness metrics between systems and across types. In each of the control experiments, we translate the synthetic English sentences to German. The results are presented in Table 7 .\nResults Table 6 shows that the percentage of unaligned source words is highest in the case of idioms, followed by random descriptive phrases & named entities. The results are consistent with the hypothesis that the explored GPT models produce less literal E\u2192X translations, since named entities or descriptive phrases in a sentence would admit more literal translations as acceptable, unlike sentences with idioms. Davinci-002 obtains a much higher COMET-QE score in the case of translations of sentences with idioms, yet obtains a higher percentage of unaligned source words. Similarly, the difference in non-monotonicity scores is also considerably higher for the case of idioms. These results provide some evidence that the improved results of the GPT model, together with the lower literalness numbers, stem from correct translation of idiomatic expressions. Table 7 shows that this effect only increases with the number of idioms.\n\nDiscussion\nIn our experiments conducted across different NMT systems and GPT models, we find evidence that GPTs produce translations with greater nonliteralness for E\u2192X in general. There could be a number of potential causes for this; we list two plausible hypotheses below:\nParallel Data Bias NMT models are trained on parallel data, which often contains very literal webcollected outputs. Some of this may even be the output of previous-generation MT systems, which is highly adopted and hard to detect. In addition, even high quality target text in parallel data always contains artifacts that distinguishes it from text originally written in that language, i.e. the 'translationese' effect (Gellerstam, 2005) . These factors could likely contribute to making NMT translations comparatively more literal.\nLanguage Modeling Bias Translation capability in GPTs arises in the absence of any explicit supervision for the task during the pre-training stage. Therefore, the computational mechanism that GPTs leverage for producing translations might be different from NMT models, imparting them greater abstractive abilities. This could have some measurable manifestation in the translations produced, e.g., in the literalness of the translations.\nDifferences in E\u2192X and X\u2192E In E\u2192X, we consistently find that GPT translations of similar quality are less literal and in the X\u2192E direction, we observe a few anomalies. For X\u2192E, in Figure 1 , in all but one comparison (WMT-21-SOTA vs GPTs for De-En) GPTs obtain higher measures for non-literalness. On the other hand, we did not see anomalies in the trend for E\u2192X directions.\n\nVariations in Experimental Setup\nWe also experimented with a variant of USW and NM which doesn't use the alignments pertaining to stopwords. Each of our findings remain the same, with relatively minor changes in magnitudes but not in system rankings. Similarly, we observed a greater tendency towards less literalness in GPT translations in both few-shot and zero-shot settings, when compared across a range of NMT systems.\n\nSummary and Conclusion\nWe investigated how the translations obtained through LLMs from the GPT family are qualitatively different by quantifying the property of translation literalness. We find that for E\u2192X translations, there is a greater tendency towards nonliteralness in GPT translations. In particular, this tendency becomes evident in GPT systems' ability to figuratively translate idioms.\n", "hypothesis": " Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks.  On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs.  However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.  In this work, we investigate these differences in terms of the literalness of translations produced by the two systems.  Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E\u2192X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics.  We demonstrate that this finding is borne out in human evaluations as well.  We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions..", "answer": true}
{"title": "Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning", "content": "\nIntroduction\nStandard supervised NLP methods perform well when training on enough data from a uniform distribution. However, they fail to retain knowledge learnt in the past when sudden shifts occur in training data distributions. This effect of dropping performance on data from past distributions is commonly referred to as catastrophic forgetting (Mc-Closkey and Cohen, 1989; de Masson D'Autume et al., 2019; Biesialska et al., 2020a) , where stability or preservation of knowledge is traded off for increased plasticity or the ability to acquire new knowledge. To tackle this issue, continual learning (CL) methods were proposed under various settings, such as limited compute or ability to store past data (Lopez-Paz and Ranzato, 2017; de Masson D'Autume et al., 2019) . The data shifts commonly studied are obtained by training over a sequence of non-iid partitions (Chaudhry et al. , 2018), different tasks (Jin et al., 2021) , or by training on various domains such as in task-oriented dialogue (Madotto et al., 2021) , named entity recognition (Monaikul et al., 2021) , part-of-speech (Liu et al., 2020) , and intent detection (Wu et al., 2021) .\nLifelong learning is key to the success of deployed multilingual systems, enabling the system to incorporate annotated data for new languages as they become available without costly retraining and redeployment of the entire system. This sequential availability of data for new languages is a common case of training data shift (see Figure 1 for the task setup). Yet, the effect of catastrophic forgetting was not yet systematically studied for multi-lingual models with multiple diverse languages. M'hamdi et al. (2022) study continual learning in a crosslingual setting limited to just six languages. The cross-lingual abilities of pre-trained models were found to drop when performing fine-tuning for a target language (Liu et al., 2021) , although applying continual learning approaches can effectively reduce the magnitude of the effect (Lopez-Paz and Ranzato, 2017) .\nIn this paper, we systematically study the effect of catastrophic forgetting and mitigation strategies in a massively multilingual setting covering up to 51 languages on three different tasks. We start by quantifying the extent to which forgetting happens when languages are presented to the model in sequence, identifying an up to 16% F1 drop compared to training using all the data mixed. Next, we propose LR ADJUST, a simple, yet effective, method to preserve the learned knowledge by adjusting the learning rate over time to alleviate the knowledge overwriting from the new language and preserve the previous learned knowledge. This method is orthogonal to continual learning methods and thus can be handily combined with any of these. We find that across three different CL methods, LR ADJUST helps further reduce the gap between a fully trained model and the CL setup. We conduct analysis on the aspect of cross-lingual transfer in backward and forward directions to measure the influence of the CL on previous tasks and its ability in zero-shot learning respectively. Finally, we conduct analyses on the effects of catastrophic forgetting when first training on multiple languages jointly and when using a curriculum learning approach informed by language similarity.\n\nTask Setup\nWe define a curriculum of T tasks as an ordered set of data sets D = {D 1 , D 2 , ..., D t , ..., D T } and model \u03b8 t , where D t is the data set with task t.\nIn this case, the task is a distinct language. The weights of model \u03b8 are updated continuously \u03b8 t+1 \u2190 f (\u03b8 t , D t ) by minimizing the log-likelihood over data set D t via gradient updates.\n\nInter-task Learning Rate Adjustment\nWe propose LR ADJUST, a simple and effective method to adjust the learning rate when we start training on a new task. Our intuition is that models are susceptible to catastrophic forgetting when we provide a higher learning rate, so the learning rate should be toned down with time to ensure the preservation of the learned knowledge and to reduce the effect of overwriting the weights with the new knowledge. Learning rate adjustments have been studied in the context of incremental learning (Cavalin et al., 2009; Khreich et al., 2012) and for efficient optimization using schedules (Ge et al., 2019) . Concretely, the new learning rate is lowered every time as the following: lr t = max(lr min , lr t\u22121 * \u03b3), with a weight \u03b3, where \u03b3 < 1 and lr min is the minimum learning rate. The method is detailed in Algorithm 1.\n\nContinual Learning Method\nWe experiment with the following continual learning approaches: Adjust learning rate to lr t = max(lr min , lr t\u22121 * \u03b3) et al., 2019) uses an episodic memory to store seen training data in memory and retrieve it from memory for fine-tuning. We schedule the replay step to be run every few iterations. During the replay step, we retrieve the data and fine-tune the model using the retrieved data. The number of stored data is constrained to ensure efficient memory use. And, we take a uniform distribution of samples across all labels. \u2022 Averaged GEM (A-GEM) (Chaudhry et al., 2018) also utilizes an episodic memory M and is a more efficient implementation of GEM (Lopez-Paz and Ranzato, 2017 ) that computes the gradient constraints and minimizes the loss as follows:\n4: Compute \u2207 \u03b8 L Dt (f \u03b8 ) using D t 5: \u03b8 t+1 \u2190 \u03b8 t \u2212 lr t \u2207 \u03b8 L Dt (f \u03b8 ) 6: end for \u2022 Experienced Replay (de Masson D'Autume\nL t (\u03b8 t , M) \u2264 L t\u22121 (\u03b8 t\u22121 , M),\nwhere loss L t is constrained to be lower or equal to the loss L t\u22121 . \u2022 Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) minimizes the following loss:\nL t (\u03b8) = L t (\u03b8) + i \u03bb 2 F i (\u03b8 t \u2212 \u03b8 * t\u22121 ) 2 ,\nwhere F i is the Fisher information matrix, \u03bb is a coefficient that sets how important the old task is compared to the new one, and \u03b8 * t\u22121 is the previous learned weights. The F i is pre-computed after each task is completed and we incorporate the loss to the training on each gradient update.\n\nData sets\nWe use a multilingual natural language understanding data set, MASSIVE (FitzGerald et al., 2022) and a multilingual named entity recognition (NER) data set, WikiAnn (Rahimi et al., 2019) . The MAS-SIVE data set consists of two tasks: intent classification and slot filling with 51 languages. The WikiAnn data set consists of 40 languages. We adopt the data splits from the original papers.\n\nMethods\nOur model architecture is an encoder-only multilingual model XLM-R BASE (Conneau et al., 2020) with a classification layer for each task. All parameters in the model are updated during the training. The full hyper-parameters are listed in Appendix A. The language order for the experiments are listed in Appendix B. We experiment with the following approaches:\n\u2022 MULTI: A single multilingual model is trained on data mixed from all languages. This represents an upper bound to CL methods, as there are no memory or data sequencing constraints. \u2022 MONO: A separate model is trained on all the supervised data for each language and applied to all inputs in that language. \u2022 VANILLA: A single model is trained by sequentially presenting data from each language. The language order is selected randomly. \u2022 CL Methods: We run REPLAY, A-GEM, and EWC to train a single model on data from each language presented sequentially. \u2022 CL Methods + LR ADJUST: We run the CL methods with the learning rate adjustment method described in Section 2.2.\n\nMetrics\nWe measure the ability of cross-lingual transfer using CL metrics adapted from Lopez-Paz and Ranzato (2017) . We define a matrix R \u2208 R T \u00d7T , where R i,j denotes the test score performance of the model on task t j when training the last sample from task t i . We formally define the metrics as:\n\nCross-lingual Forward Transfer (CFT)\nThis metric represents the ability to perform zeroshot learning by evaluating on the test data from tasks/languages that are unseen in training. We formally define the metric as: where Xi is the average performance of the languages that will be seen in the future (t >i ).\nCF T = 1 T \u2212 1 T \u22121 i=1 Xi , Xi = 1 T \u2212 i T j=i+1 R i,j ,\n\nCross-lingual Backward Transfer (CBT)\nThis metric measures the influence of learning a task t i on the performance of the previous tasks. We formally define the metric as the following:\nCBT = 1 T \u2212 1 T \u22121 i=1 R T \u22121,i \u2212 R i,i .\nCBT practically measures the effect of catastrophic forgetting of past tasks after adding a new task to the model. ing for the three data sets. Each point on the graph shows the average and standard deviation of the F1 scores, obtained over 5 runs with different seeds. For the CL experiments, the seed also controls the order of languages in training. This can lead to higher variance across runs -compared to mixing the same data across runs -because the forgetting effect depends on the language order. Table 1 shows the forward and backward transfer for the three data sets.\n\nResults and Analysis\nResults show the following: Catastrophic forgetting is present in multilingual continual learning, with performance dropping quickly when training on languages sequentially (VANILLA). This method converges to between 4 -17 F1 lower than mixing the same data and training a full model every time (MULTI). We also see that this effect is present even when the performance of monolingual models is close to that of multilingual models, as in the case of WikiAnn. Training a full model on all languages (MULTI) always performs best, outperforming training on one language at a time (MONO) sometimes substantially (MASSIVE-Slot -10 F1, MASSIVE-Intent -4 F1), highlighting the importance of cross-lingual transfer and preserving information from past seen languages in a continual learning setup.\nContinual learning methods generally help dampen the impact of forgetting. For example, in WikiAnn, the REPLAY and A-GEM CL methods reduce the backward transfer from 16.90 to 11.87 and 10.75 respectively, albeit EWC does not substantially improve the performance relative to the VANILLA \n\nmethod.\nLR ADJUST, the learning rate adjustment scheme, further reduces the gap to the multi-task model significantly and consistently across languages when combined with any of the CL methods. For example, on the WikiAnn dataset, the backward transfer is reduced from 16.90 to just 3.59 and 3.79 for the A-GEM and REPLAY methods respectively, making multilingual CL feasible. Further, we see that using CL methods alone results in a continuous drop in performance as more languages are added, while adding LR ADJUST stabilizes average performance after the first few languages, resulting in a flatter curve.\nFinally, we see that the patterns of improvement hold when studying cross-lingual forward transfer, which quantifies the zero-shot model performance on languages unseen in training. The continual learning approaches improve over sequential training (e.g. +4.45 on WikiAnn) and using LR ADJUST in addition further boosts performance (e.g. +9.25 for VANILLA, +5.83 for REPLAY on WikiAnn). This shows that the resulting models were able to retain essential and generalizable information for the task that is more universal across all languages.\n\nMulti-task Training vs. Catastrophic Forgetting\nWe conduct additional experiments to understand whether initially training on multiple languages at once can reduce the severity of catastrophic forgetting in the CL setup when new languages are added. We run a multi-task training on the first k languages, where k is 10 or 30, and then, we run the remaining languages sequentially on the WikiAnn data set. As shown in Figure 5 , the model is more robust to forgetting when it is exposed to multi-task training with more languages with higher final average scores at the final task, but the graphs shows the performance still drops dramatically after being exposed to the first new language fed sequentially.\n\nThe Role of Language Order\nTo investigate the role of the language order on CL, we decide to reorder the language list by using heuristics. We start with two languages from the same family, as listed in Ethnologue (Eberhard and Gary, 2019), and add all languages from the same family one by one, then switch to a new language family and continue the same process. We conjecture that seeing a similar language at an interval will allow a more effective cross-lingual transfer.\nFigure 5 (LANGUAGE ORDER) displays the results, which indicate that performance does not improve after we manually select the languages, and its performance is similar to random ordering (VANILLA).\n\nRelated Work\nCL aims to learn effectively over iterations by leveraging information from previously learned tasks (McCloskey and Cohen, 1989) . CL has been applied towards mitigating catastrophic forgetting in many computer vision tasks (Lopez-Paz and Ranzato, 2017) . Broadly speaking, CL methods can be classified (Biesialska et al., 2020b) et al., 2019) , regularization (guide consolidation via additional loss) (Kirkpatrick et al., 2017) , memory (Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2018) , and architectural methods (using taskspecific parameters) (Rusu et al., 2016; Madotto et al., 2021) .\n\nConclusion\nWe present the first study of catastrophic forgetting in a massively multilingual setting involving up to 51 languages on named entity recognition and natural language understanding tasks. We investigate continual learning methods and present a learning rate scheduling method that is simple yet effective in reducing the effects of catastrophic forgetting. Furthermore, we show that this method is effective across multiple continual learning methods. Finally, we provide analysis and further insights into the dynamics of catastrophic forgetting.\n", "hypothesis": " Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time.  To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further in its past.  In this paper, we study catastrophic forgetting, as well as methods to minimize this, in a massively multilingual continual learning framework involving up to 51 languages and covering both classification and sequence labeling tasks.  We present LR ADJUST, a learning rate scheduling method that is simple, yet effective in preserving new information without strongly overwriting past knowledge. Finally, we provide further insights into the dynamics of catastrophic forgetting in this massively monolingual setup.", "answer": false}
{"title": "Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks", "content": "\nIntroduction\nIt is common practice to start with a multilingual language model (MLLM) like mBERT 2 or XLM-R (Conneau et al., 2020) , which has been pre-trained with large multilingual corpora, and fine-tune the MLLM for diverse downstream tasks. Although MLLMs support many low-resource languages (LRLs), closer inspection of these MLLMs reveals that the portion of vocabulary allotted to LRLs can be orders of magnitude smaller than that allotted to high-resource languages (HRLs) such as English (Table 1 ). Due to this imbalance, sometimes an LRL word may not be possible to segment into wordpieces as per the MLLM vocabulary, leading to the LRL word being conflated with the UNK (unknown) token. An even more insidious situation is that the MLLM vocabulary has enough (over-fragmented) wordpieces to assemble almost any LRL word (thereby dodging the obvious UNK alert), but the embeddings of these wordpieces collide with unrelated usage in HRLs, and/or are so sparsely trained that contextual aggregations fail to yield satisfactory LRL word embeddings which may lead to poor LRL task performance. On the other hand, significant human and computational investments are needed to create task-specific LRL corpora that are large enough to augment and retrain the MLLM vocabulary.\nIn this work, we address the setting where a MLLM (that is presumably deficient in LRL coverage) must be minimally fine-tuned after modest modification to its wordpiece vocabulary, guided by specific LRL tasks. We design a measure of damage to an LRL word, caused by wordpiece fragmentation, based on a suitably defined notion of entropy of the word and constituent wordpieces, with respect to the LRL task. This measure then guides the selection of LRL words with which the vocabulary should be augmented. Subsequently, we propose various ways to initialize the embeddings of these newly-introduced words, including using information from the LRL itself, to 'importing' information from HRLs. We call the resulting system EVALM (entropy-based vocabulary augmented language model).\nWe study the effect of EVALM on an existing MLLM during the fine-tuning stage for various downstream classification tasks covering multiple LRLs and also a code-mixed language. Our study shows that, for most of the datasets, EVALM's vocabulary augmentation strategy helps improve LRL task performance by greater margins than recent best practices (Hong et al., 2021\u037e Hofmann et al., 2022) . A detailed analysis of successes and failures delineates the perimeter of EVALM's capabilities and guides our design choices.\n\nRelated Work\nContinued pre-training (Tai et al., 2020\u037e Ebrahimi and Kann, 2021\u037e Wang et al., 2020\u037e Chau et al., 2020) with or without vocabulary augmentation of existing LMs like monolingual BERT, multilingual BERT (mBERT), XLM-R, etc., proves beneficial for improving domain and languagespecific performances over various tasks. Some works (Ruzzetti et al., 2021\u037e Yu et al., 2021) focus on rare/OOV words. Liu et al. (2021) propose an embedding generator module in the pretrain-finetune pipeline to resolve vocabulary gaps. Adaptors (Sachidananda et al., 2021\u037e Moon and Okazaki, 2020\u037e Hofmann et al., 2021) are also showing promising outcomes in LRL modeling. Chung et al. (2020) explore multilingual vocabulary generation from language clusters. Minixhofer et al. (2021) transfer English LMs to new languages without expensive computation. Hofmann et al. (2022) propose a simple algorithm which modifies the tokenization process to preserve the morphological structure of a word. Others (Wang et al., 2019\u037e Hong et al., 2021) focus on embedding initialization for newly added vocabulary words which are word fragments, which is also among our concerns.\n\nOur system: EVALM\nEVALM has three key components. The purpose of the first component (Section 3.1) is to identify (based on only the train fold) a subset of vulnerable LRL words whose assembly from wordpieces is likely to distort the embedding information made available to LRL labeling tasks. The second component (Section 3.2) comprises various possible \n\nVulnerable LRL word selection\nWe need a computationally efficient, task-sensitive surrogate of the value of introducing an LRL word into the wordpiece vocabulary. (Here we augment the vocabulary with whole LRL words, blocking their fragmentation entirely. More clever sharing of fragments is left for future work.) Suppose LRL word w is not in the MLLM vocabulary\u037e w is fragmented into wordpiece sequence T (w) = s 1 , . . . , s T by the MLLM tokenizer T . The LRL task has C class labels. A specific label is denoted c \u2208 [C] = {1, . . . , C}. The counts of w and constituent wordpieces s t in each class c are denoted n(w, c) and n(s t , c). Based on these counts, we define the following multinomial distributions:\np(c|\u2022) = n(\u2022, c)/ \u2211 c \u2032 n(\u2022, c \u2032 )\n(1) where \u2022 = w, s t , etc. Based on this we define the entropy\nH(\u2022) = \u2212 \u2211 c p(c|\u2022) log p(c|\u2022)\n(2) Suppose H(w) is small. This means w is potentially a good feature for the LRL task. Now suppose a wordpiece s t has large H(s t ). That means s t is being shared across other words that are dis-tributed more evenly across classes. If this is the case for most s t , then fragmentation of w may be a serious problem. To combine information from all wordpieces, we average their entropies, and use the relative increase in entropy, going from LRL word to wordpieces, as one signal for the danger of fragmenting w. As an example, suppose the word '\u0927\u0930\u092e' (religion) occurs ten times in a threeclass sentiment analysis dataset with the class distribution of 'positive', 'neutral', and 'negative' as (1,1,8) . Its wordpieces have class distributions '\u0927' (100,85,80), '##\u0930' (130, 235, 250) , and '##\u092e' (130, 90, 125) . Then as per equation 2, H('\u0927\u0930\u092e') = 0.639, H('\u0927') = 1.094, H('##\u0930') = 1.062, and H('##\u0930') = 1.086. The average wordpiece entropy is H S ('\u0927\u0930\u092e') = 1.094+1.062+1.086 3 = 1.081, and the percentage of entropy reduction from average wordpiece to word entropy is about 41%.\nWe also retain two simpler signals: the number of fragments |T (w)|, and the frequency of w in the LRL task corpus. LRL words are sorted on the amount of entropy decrease and the top LRL words proposed for vocabulary augmentation. We remove words with very low frequency and retain a prefix of specified size to obtain V new , the LRL words to be added to the MLLM vocabulary. Algorithm 1 shows a high-level pseudocode.\n\nEmbedding initialization\nHere we describe the different ways to initialize the embeddings of newly-added LRL words. InitLRL: The embedding of the newlyintroduced LRL word is initialized using other LRL wordpieces already in the MLLM dictionary. Suppose we add Bengali word ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' , ('hospital' in English). Suppose the existing MLLM tokenizer splits it into [' \u09b9' , ' ##\u25cc\u09be\u09b8' , ' ##\u09aa' , ' ##\u25cc\u09be\u09a4' , ' ##\u25cc\u09be\u09b2' ]. Then we initialize the embedding of ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' with the average of the existing MLLM embeddings of the fragments. InitHRL: Here we translate ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' to English ('hospital'), tokenize it using T , and take the average embedding of the tokens in the list. InitMix: We use the average of InitLRL and InitHRL embeddings. InitRand: We randomly initialize the embeddings of the newly-added words.\nIt is challenging to learn good contextual embedding for words in V new due to very small taskspecific training data compared to the MLLM pretraining corpus. Therefore, we found it neces- Table 2 : Salient statistics of tasks. Note the small size of LRL datasets. Further details in Table 6 .\nsary to apply some regularization to avoid overfitting during fine-tuning. Let T , T \u2032 be the initial and final MLLM tokenizers. For a particular sentence S = w 1 , w 2 , ..., w I with words w i , we will get two different tokenizations\u037e these will generally lead to different contextual embeddings E = (e 1 , . . . , e K ) and E \u2032 = (e \u2032 1 , . . . , e \u2032 L )\u037e generally K \u0338 = L. We average-pool these to get vectors e, e \u2032 which a final layer uses for the classification task, with losses \u2113 T and \u2113 T \u2032 . We also use (e+e \u2032 )/2 for a third classification, with loss \u2113 mix . The overall training loss is \u2113 T + \u2113 T \u2032 + \u2113 mix , where \u2113 T and \u2113 mix are expected to reduce overfitting.\n\nDatasets and evaluation metric\nWe experiment with six short multi-class text classification tasks covering four Indian languages and a Hindi-English code-mixed dataset. We show the details of the datasets in Tables 2 and 6 . We use mBERT as the MLLM and report macro-F1 (we report the accuracy metric in Appendix B). Details of model hyperparameters are present in Appendix C.\n\nQuantitative results\nIn Figure 1 , we plot macro-F1 against the extent of vocabulary augmentation. Green, orange, and blue lines show the performance with InitLRL, InitHRL, and InitMix initialization, respectively. Corresponding colored bands show 1-standard deviation spreads.\nV new helps: For all tasks, including V new is better than baseline MLLM, and the gap is usually significant. This shows that even minimal training of newly added LRL tokens that used to be UNK or over-fragmented helps improve performance. More augmentation\u0338 \u21d2larger lift: We expected that larger V new would monotonically improve performance, but this was not universally the case. Inclusion of non-informative words, as we grow V new (\u2206 H decreases with high variance as shown in Appendix B Figure 3 ), maybe a reason. The middle column depicts the added vocab (underlined in the sentence) along with the entropy reduction percentage and the class it mostly belongs to.\nInitialization does not matter much: Although there are cases where InitHRL or InitMix performs better than InitLRL, we did not find significant performance difference between different embedding initialization of new LRL words. Transfer of embeddings from a well-represented HRL is the likely reason. We also check the performance by randomly initializing the V new words and find, for almost all the cases, random initialization performance, both for macro-F1(in Figure 1 ) and accuracy(in Appendix B Figure 2 ), is lesser compared to InitHRL, InitLRL, or InitMix. It suggests meaningful initialization helps.\n\nComparison with recent approaches:\nWe compare EVALM with AVocaDo (Hong et al., 2021) keeping V new comparable in size. Table 4 shows that AVocaDo leads to performance degradation for all LRL datasets. The lack of domainspecificity for our datasets may be why AVo-caDo's performance dropped. We also compare with FLOTA (Hofmann et al., 2022) in Figure 1 . For all datasets except GLUECoS Hi-En codemix dataset, EVALM performs better than FLOTA. A possible explanation is that mBERT vocabulary already includes many English as well as Hindi words, which helps FLOTA better compose embeddings of morphological components of English and Hindi words compared to other Indian languages. Regularization helps: Table 5 shows that EVALM with AVocaDo-style regularization performs better than without it, for all datasets.\nCases where EVALM hurts: The samples in Table 3 show that EVALM generally helps by spotting words important for predicting the correct class. This is shown in the first two examples, where the added vocabulary (\u2206 H =100%) tipped the prediction toward the gold label. But the last two examples show cases where for a word, the train and test set frequency distribution among target classes are different. As a consequence, these words may become misleading at test time.\n\nConclusion\nWe have proposed a simple and effective method to augment an MLLM wordpiece vocabulary with LRL words that are important for LRL classification tasks. Our study, involving several Indian languages, shows a consistent positive impact of vocabulary augmentation and fine-tuning. We find more augmentation does not guarantee performance improvement, and different embedding initialization fails to show significant performance differences among themselves. We also show that regularization is crucial to prevent overfitting new LRL word embeddings during fine-tuning. We have limited the augmentation to whole LRL words, and a judicious selection of LRL wordpieces may improve performance. We also want to extend to other target tasks (especially language generation) and a more diverse set of LRLs.\n", "hypothesis": " Multilingual language models (MLLMs) like mBERT promise to extend the benefits of NLP research to low-resource languages (LRLs).  However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs.  This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy.  (Pre)-training MLLMs after including LRL documents is resource-intensive in terms of both human inputs and computational resources.  In response, we propose EVALM (entropy-based vocabulary augmented language model), which uses a new task-cognizant measurement to detect the most vulnerable LRL words, whose wordpiece segmentations are undesirable.  EVALM then provides reasonable initializations of their embeddings, followed by limited fine-tuning using the small LRL task corpus.  Our experiments show significant performance improvements and also some surprising limits to such vocabulary augmentation strategies in various classification tasks for multiple diverse LRLs, as well as code-mixed texts.  We will release the code and data to enable further research 1 ..", "answer": true}
{"title": "It is a Bird Therefore it is a Robin: On BERT's Internal Consistency Between Hypernym Knowledge and Logical Words", "content": "\nIntroduction\nThe main training task of transformer-based architectures (Vaswani et al., 2017; Devlin et al., 2019; Liu et al., 2019) is to predict which word may occur in a given position in a sentence. As a first pass, syntax understanding is an important prerequisite to complete this task through which systems learn the distribution of words within sentences, satisfying the constraints imposed by the linguistic environments these words are in. Accordingly, these models have shown strong syntactic capabilities (Goldberg, 2019; Wu et al., 2020; Warstadt et al., 2019; Jumelet and Hupkes, 2018) .\nWhat do they learn about semantics? Hypernymy offers a strong opportunity to study this question as it is very close to entailment, the cornerstone relation in semantics. Also, it can be studied solely through the Masked Language Modelling task, and without fine-tuning. For instance, in the prompt A robin is a [MASK] , BERT assigns a high probability to bird in the MASK position (Petroni et al., 2019; Jiang et al., 2020) . These models have thus captured semantic information about the relations between content words, here a relation between robin and bird. In this work, we begin by following up on the nuanced findings in this area (Hanna and Mare\u010dek, 2021; Ravichander et al., 2020) , using and refining methods to assess the understanding of hypernymy, pair by pair.\nThen we use these initial results and measurements to study the semantics of logical words, and more specifically connectives, such as thus or because. The idea is to evaluate the internal coherence of the system. Specifically, we ask whether NLP models coherently assign a high probability to thus in the place of the mask in This is a robin, [MASK] this is a bird, exactly in these cases where the pair robin-bird is independently (and ungroudedly) registered as a hyponym-hypernym pair.\nWe thus raise and answer these research questions: Do BERT-like models understand the asymmetric taxonomic relationship of hypernymy (or only a symmetric co-occurrence relation between hypo-hypernyms)? Do they use entailment-like connectives appropriately? Do they show internal consistency: using entailment connectives to connect cases where they detect hypernymy (i.e. indepedently of whether hypernymy actually holds)? Hence, our contributions are as follows:\n\u2022 We test the non-symmetric aspect of hypernymy. To our knowledge, this is absent from other studies, which only test hypernymy through one-sided prompts.\n\u2022 We extend the methodology to test the semantics of logical connectives like because and therefore.\n\u2022 We analyze logical connectives in a nongrounded manner: we test the semantic knowledge of entailment connectives, using entailment facts (hypernyms) that are independently proved to be known by the system.\n\u2022 We show that BERT-like models have important weaknesses on all previous tasks. The most surprising one being a reversed semantics for because.\n\nSemantics As Internal Consistency\nOne classical approach to semantics is that knowing the meaning of a sentence is knowing in which situations this sentence is true, that is, being able to map (sentence, situation) pairs onto truth-values (Davidson, 1967; Lewis, 1970) . Text-only-trained machines surely cannot do so, simply because they only take sentences as inputs, not situations. However, semantics may also be seen as the graph of all entailment relations between sentences. These entailment relations can follow from taxonomic relations between content words: the fact that all robins are birds will create entailment relations between sentences (e.g., John saw a robin entails John saw a bird). Being able to identify these is showing a strong command of the meaning of the words robin and bird, independently of how these words are grounded in the actual world.\nEntailment relations between sentences can also follow from the meaning of the logical words they contain. In a \"proof-theoretic\" approach, one may even say that this is all there is to the semantics of logical words, which are not grounded: the power to create a consistent net of entailment relations.\nOur work is part of this recent vision of the notion of meaning for non-grounded LMs (Piantadosi and Hill, 2022) .\n\nRelated Work\nNLP models have been tested for their syntactic abilities (Rogers et al., 2020; Lin et al., 2019; Wu et al., 2020; Goldberg, 2019; Warstadt et al., 2019; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018) for which they obtain strong results, but to a lesser extent for their semantic abilities (Rogers et al., 2020; Balasubramanian et al., 2020; Wallace et al., 2019; Ettinger, 2019) for which they show more fragile performances.\nModels such as BERT encode world knowledge (Feldman et al., 2019; Jiang et al., 2020) . The first part of our work is a direct follow-up of prompt studies (Liu et al., 2021) targeting knowledge of hypernymy which has been shown to be high but fragile and inconsistent (Petroni et al., 2019; Hanna and Mare\u010dek, 2021; Ravichander et al., 2020; Bouraoui et al., 2019) . We leverage this knowledge to extend the investigation to logical words.\n4 Experiment 1: Content Words\n\nMetrics\nConsidering a hyponym-hypernym pair such as (robin, bird) , what probability does BERT assign to the hypernym word bird in a MASK position:\nP[MASK = bird | A robin is a MASK] (1)\nFor more than 30% of the pairs, the target hypernym is the top-1 word predicted, and in 80% of the pairs, it is in the top-100 (Petroni et al., 2019) . This indicates that BERT recognizes that robin and bird are likely to co-occur in a sentence. We ask whether the system recognizes that the hyponymhypernym relation is not symmetric, a critical fact that makes hypernymy a variant of entailment (and not of relevance). We do so by correcting the above probability with the probability of that same hypernym, albeit in the reverse configuration. Thus, we consider the log-ratio of (1) and (2):\nP[MASK = bird | A MASK is a robin] (2)\nFurthermore, like (Jiang et al., 2020; Hanna and Mare\u010dek, 2021) , we explore a set of prompts and not just one. For each pair of hyponym-hypernym (h, c) (h the head and c the class to which h belongs) we start from a template DET 1 h REL DET 2 c, with DET i determiners (e.g. the, a, an, \u03f5) and REL an instantiation of the hypernymy relation (e.g. is, is a subclass of, is a kind of, is a sort of, is a type of ). We use the model to compute a score for a set of determiners and relations and then we select the prompt with the highest one (more details in Appendix B, with explanations as to how this optimizes the form of the prompt without a priori biasing the final log-ratio scores).\nOnce the prompt is selected, we compute the following hypernymy score \u03c3:\nEQUATION\nwhich should be positive for well-understood pairs. Note that the subscript n and d stands for numerator and denominator respectively as the two are optimized separately. Other formulae are just as natural, such as the \u03c3 \u2032 presented in Appendix A. Table 1 : Mean (and standard deviation) of the \u03c3 scores for content words for BERT-base.\n\nMulti-token Prediction\nSome hyponym-hypernym pairs are made of multitoken expressions. For example, great ape is tokenized as two tokens. To overcome this difficulty we use the technique presented in (Feldman et al., 2019) consisting in computing the probability of each token independently and iteratively unmasking the token with the highest probability.\n\nKnowledge Sources\nTo build hyponym-hypernym pairs we used the following four different knowledge sources: Word-Net (Miller, 1995) \n\nResults\nWe conducted all experiments on BERT (Devlin et al., 2019) , ELECTRA (Clark et al., 2020) , Distil-BERT (Sanh et al., 2020) and ALBERT (Lan et al., 2020) . The results for BERT-base are given in Table 1 (see Appendix C for the other models). The mean of the scores is always positive (p < 0.001). This shows that these models encode the hypernymy relation better than chance. Yet, an average of 45% pairs are encoded in the wrong direction (see Fig. 1 for BERT-base). From a grounded approach of semantics, these are errors. In Experiment 2, we take them as an opportunity to look for traces of strong semantics command, as an internal consistency constraint.\n\nExperiment 2: Logical Words\nThe previous experiment establishes how models capture semantic relations between content nouns. We can use these results to investigate how the same models understand logical words. Concretely, one would expect a high probability for words like thus, so, therefore in the following sentence, and a low probability for words like because, since, for as they encode this entailment the other way around:\nEQUATION\nResults on hypernym-hyponym pairs show great variability hence, the high probability for thuslogical words in the sentence above is expected only if that particular pair, (robin, bird), is assigned a high hypernymy score by the model. For pairs that receive a very negative score, the expectation is in fact that the results would be reversed. This approach thus allows us to test the semantic consistency of the system. Consistency could be perfect for logical words, even if there are grounding errors with content words and world knowledge.\nWe tested 7 logical words of the thus class (thus, therefore, consequently, then, accordingly, so, hence), and 5 logical words of the because class (because, since, for, seeing, considering).\n\nMetrics\nWe define a score for a logical word w and a hyponym-hypernym pair (h, c) as in ( 5). This score measures the probability of finding, say, thus, in a sentence like (4) above, corrected for the probability of finding it in the reverse sentence.\nEQUATION\nAs before, we explore multiple prompts from a set of determiners DET and prefixes PRE (see details in Appendix B). A global score s(l) is obtained for a logical word w by averaging s(w; h, c) over the set of all content word pairs (h, c).\nAs seen in \u00a74.4, the hyponym-hypernym pairs are not all equal regarding to our hypernymy scores. We thus introduce s + (w) (resp. s \u2212 (w)): the average of s(w; h, c) on the top 5% 1 (resp. bottom 5%) pairs according to \u03c3 (or \u03c3 \u2032 ). Hence, for a coherent model having understood those logical words we expect s + \u2265 0 \u2265 s \u2212 for thus-words, and the reverse inequalities s + \u2264 0 \u2264 s \u2212 for becausewords. See Fig. 2 for a graphical representation of the expected results for a consistent model. \n\nResults\nTable 2 presents the global scores s for BERT-base (full results are in Appendix C). The thus-words almost always obtain a positive global score. The because-words sometimes show a negative score (as they should), but most of the times they obtain a positive score just like thus-words.\nFigure 3 presents the s + and s \u2212 scores obtained by BERT-base for the WordNet database relative to the \u03c3 score. The thus-words obtain a positive score on the best pairs, and a smaller score (albeit not necessarily negative) on the worst pairs. This is the expected result for a model that has correctly understood these logical words. However, becausewords display a somewhat similar behavior: a positive score over the best pairs, and a lower score over the worst pairs. All models show a qualitatively similar behavior, although ELECTRA seems to behave more consistently (see Appendix C). Overall, these results suggest that thus-words and because-words alike are understood as being of the thus-type.\n1 Empirically we explored several thresholds in percentiles or absolute sigma scores and obtained qualitatively similar results. The 5% threshold was chosen as inclusive enough to have enough pairs to make statistics, and strict enough to make sure the elements in there unambiguously passed the test from Experiment 1. Table 2 : Score s for the logical words we tested and for BERT-base. Red numbers represent unexpected results: assuming that content word pairs are well-understood, then a good result would be one with positive scores for the thus-words and negative scores for the becausewords. Here scores for thus-words are mainly positive, but they are also positive for the because-words.\n\nDiscussion\nThe similar behavior between thus and because is puzzling. A first possibility that could explain this would be a significant difference in frequency between thus words and because words in the training corpus. Indeed a signal that would be too weak for because could lead to a poor assimilation of its semantics. Unfortunately we did not check frequencies in the training corpus but according to the python package wordfreq 2 , because is for example one hundred times more frequent than therefore or thus, ruling out this explanation. Another possibility is that because is not used as the converse of thus, even by humans. Underlyingly, the result shows that the sentence This is a robin, because it is a bird may be more natural than the reverse This is a bird, because it is a robin. One may argue that the latter is somewhat tautological and, as such, not natural, while the former may find its use cases (e.g., when discriminating between a robin and an orangutan). One may wonder why the converse does not apply to thus-words however. To clear this issue one could look at the occurrences of thus and because in the relevant training corpora. Regardless, a conclusion we can already draw is that the simplest entailment-like semantics for because is very far from what is encoded in these models.\n\nConclusion\nWe propose an approach to the semantic study of BERT-type networks. First we evaluate the models on the non-symmetry of an entailment-like relation, namely hypernymy. The tested models show an average positive understanding of this relation. But this is accompanied with a large variance, showing that the relation is very often captured backward.\nThanks to these results we moved to testing logical words of type thus and because, which impersonate the entailment relation at the core of all enterprises in semantics. Its non-symmetry is one of its fundamental property. The models capture on average the non-symmetry of the words of type thus appropriately and they also show good consistency results, that is, a stronger signal for pairs that are themselves well-captured. However, the models obtain similar scores for the words of type because and, applying the same standards, they thus capture them backwards. Moreover all these results are to be qualified by their great variability across models and knowledge sources.\nThese properties albeit simple are yet at the core of what human semantics is however they are not reliably captured. This failure on these basic logical tests then raises questions regarding their otherwise impressive success. They also provide a method to reconstruct their actual semantics, if it is not human-like, and offers challenging tasks for these models.\n", "hypothesis": " The lexical knowledge of NLP systems should be tested (i) for their internal consistency (avoiding groundedness issues) and (ii) both for content words and logical words. In this paper, we propose a new method to test the understanding of the hypernymy relationship by measuring its symmetry according to the models. Previous studies often rely only on the direct question (e.g., A robin is a ...), where we argue a correct answer could only rely on collocational cues, rather than hierarchical cues. We show how to control for this, and how it is important. We develop a method to ask similar questions about illogical words that encode an inconsistent relation (e.g., because or therefore).  Our results show important weaknesses of BERT-like models on these semantic tasks..", "answer": false}
{"title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing", "content": "\nIntroduction\nScaling-up Pre-trained Language Models (PLMs) has led to astonishing performance gains on a vast variety of Natural Language Processing (NLP) tasks (Du et al., 2021; Zoph et al., 2022; Smith et al., 2022) . It has also opened new perspectives for studying the opportunities and limitations of large PLMs (Raffel et al., 2019; Dale, 2021; Bommasani et al., 2021) , as well as their social and ethical impacts (Bender et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Rae et al., 2021a; Susnjak, 2022) .\nAlthough for some languages such as English and Chinese, several PLMs with even more than hundred billions of parameters have been developed (Rae et al., 2021b; Chowdhery et al., 2022; Zeng et al., 2021; Sun et al., 2021) , little or no progress has been made on this direction for many other languages including Arabic. 1 While there have recently been few attempts to develop multibillion parameters Arabic PLMs (Nagoudi et al., 2022a; Antoun et al., 2021b; Lakim et al., 2022) , still, their performances and abilities have not been well investigated. The largest well-studied Arabic PLM has no more than 370M parameters (Nagoudi et al., 2022b; Ghaddar et al., 2022) .\nIn this work, we introduce AraMUS, an 11B parameter encoder-decoder T5 (Raffel et al., 2019) model, which is pre-trained on 529GB of highquality Arabic text (filtered out of 8.8TB). To the best of our knowledge, AraMUS is the largest Arabic PLM in terms of pre-training data and model size. Furthermore, it is the first time a multi-billion parameter Arabic PLM is systematically evaluated, against the existing state-of-the-art models, on a diversified set of discriminative and generative task models. More precisely, AraMUS achieves new state-of-the-art performances of 79.8% on the ALUE (Seelawi et al., 2021) benchmark, which is a collection of 8 discriminative tasks. In addition, it significantly outperforms the best available encoder-decoder models on multiple generative tasks. Finally, AraMUS shows remarkable abilities to maintain its performance under few-shot settings.\n\nRelated Work\nRecently, there has been a growing body of the literature on very large-scale English PLMs by thoroughly studying different aspects of their scaling. These efforts can be summarized into scaling their pre-training data (Hoffmann et al., 2022) and model size (Dale, 2021; Rae et al., 2021b; Smith et al., 2022) , designing efficient architectures (Zoph et al., 2022; Chowdhery et al., 2022) and pre-training objectives (Bajaj et al., 2022; Tay et al., 2022) , democratizing their access (Zhang et al., 2022) , and making them useful in real-world applications (Ouyang et al., 2022; Qu et al., 2023) . Besides English, there have been multiple attempts to develop multilingual (Scao et al., 2022) , as well as non-Anglocentric (Zeng et al., 2021; Sun et al., 2021; Shin et al., 2022) multi-billion PLMs.\nUnfortunately, the development of Arabic PLMs does not follow the same pace as that of English. The earliest released Arabic PLMs (Antoun et al., 2020; Safaya et al., 2020) were based on the BERTbase (as well as -large) architecture (Devlin et al., 2018) and pre-trained on less than 100GB of unfiltered data. Successive works tried to improve Arabic BERT-base models performance by scaling up the pre-training data up to 197GB and 167GB of unfiltered Arabic text for MARBERT (Abdul-Mageed et al., 2021) and CAMeLBERT (Inoue et al., 2021) respectively. In addition, other works focused on developing Arabic PLMs to support other architectures like AraElectra (Antoun et al., 2021a) , AraGPT (Antoun et al., 2021b ), AraT5 (Nagoudi et al., 2022b ), and AraBART (Eddine et al., 2022) which are equivalent to English ELECTRA (Clark et al., 2020) , GPT (Radford et al., 2018 ), T5 (Raffel et al., 2019) , and BART (Lewis et al., 2019) respectively.\nRecently, Ghaddar et al. (2022) developed stateof-the-art Arabic BERT (JABER and SABER) and T5 models (AT5S and AT5B) by improving the pre-training data quantitatively and qualitatively. More precisely, they pre-trained Arabic BERTbase/large and T5-small/base models on 115GB of high-quality Arabic text data (filtered out of 514GB). AraGPT-Mega (Antoun et al., 2021b) , Jasmine (Nagoudi et al., 2022a) , NOOR (Lakim et al., 2022) are the only existing multi-billion Arabic PLMs. These are decoder-only GPT models with 1.5B, 6.7B, and 10B parameters respectively. However, these aforementioned works suffer from the absent (e.g. in AraGPT, NOOR) or limited (e.g. Jasmine) comprehensive evaluation on NLP endtasks. Moreover, some of these models (such as NOOR and Jasmine) are not publicly available for custom evaluations. 2 Evaluation is a key factor for understanding the strengths and limitations of these models, without which the progress of the Arabic NLP field is hindered.\n\nPre-training Data\nWe mainly leverage all (up to July 2022) of the 90 Common Crawl 3 monthly web scrapes in order to collect massive amount of Arabic textual data. This is significantly larger compared to JABER (Ghaddar et al., 2022) , NOOR (Lakim et al., 2022), and Jasmine (Nagoudi et al., 2022a) , which use 10, 21, and 71 monthly CC shards, respectively. Then, we apply aggressive noise filtering and deduplication, which give rise to 529GB of high-quality Arabic text data. Nagoudi et al. (2022a) introduced the closest comparable pre-training corpus size to us with 413GB (22% smaller than ours) of Arabic text data. Our data mainly differs in using 2.5 times more CC data, while they used 3.8 times more dialect data than ours. We refer the reader to Appendix A.1 for technical details regarding the pre-training data collection.\n\nModel and Implementation\nAraMUS follows the same encoder-decoder architecture and configuration as T5-xxl (Raffel et al., 2019) model with 64k vocabulary size. We choose encoder-decoder T5 architecture because it was found to deliver a good balance between the performance of the discriminative and generative tasks (Raffel et al., 2019; Tay et al., 2022) , compared to encoder-only BERT (discriminative tasks focused) and decoder-only GPT (Radford et al., 2019) (generative tasks focused). AraMUS has 11B parameters in total, which makes it the largest existing Arabic T5 model. It was pre-trained using 128 NVIDIA A100 GPUs for 2 months. Technical details regarding implementation and hyperparameters used for pre-training are listed in Appendix A.2.\n\nEvaluation Protocol\nWe assess AraMUS by performing extensive finetuning experiments on a diverse set of NLP tasks. On one side, we experiment on 8 tasks from the well-established ALUE benchmark (Seelawi et al., 2021) , which includes one regression (SVREG), one multi-label classification (SEC), 4 singlesentence (MDD, FID, OOLD, and OHSD) and 2 sentence-pair (MQ2Q and XNLI) classification tasks. On the generative tasks side, we evaluate on Question Answering (QA), Question Generation (QG), and Text Summarization (TS).\nWe compare AraMUS with state-of-the-art Arabic PLMs in the literature, including ARBERT, MARBERT, JABER (BERT-base), SABER, ALM-1.0 (BERT-large), AT5B and AraT5-base (T5-base). The experimental protocol is designed to ensure the diversity of the tasks, and the public availability of models. Most importantly, we make sure that datasets are of high quality, open-sourced, and supported by a well-established evaluation protocol.\nOur goal is to have a fair comparison between models, as well as the credibility and reproducibility of the results. A detailed description of fine-tuning datasets, evaluation metrics, baselines, and implementation details are available in Appendix B.\n\nResults\nTable 1 shows the dev set results of the eight ALUE tasks with their average scores and standard deviations of 5 runs. The baseline results are directly brought from (Ghaddar et al., 2022) and they are directly comparable with AraMUS since we follow the same evaluation protocol. Table 2 shows the test set performances of the state-of-the-art models on the ALUE leaderboard.\nAs we expect, AraMUS outperforms all other baseline models on both dev and test sets and achieves a new state-of-the-art performances on ALUE. While our average ALUE result is 1.4% better than the best baseline, SABER, the latter outperforms AraMUS on the OHSD dataset. On the other hand, AraMUS significantly outperforms SABER by 2.5% on average and 3.3% on OHSD when comparing results on the leaderboard test. Interestingly, this is roughly a similar performance gap (2.1%) on the English GLUE (Wang et al., 2018) between the English T5-xxl (Raffel et al., 2019 ) (11B parameters) and the well-trained English Roberta-large (Liu et al., 2019) model. Moreover, we observe a huge gap of 13.8% between AraMUS and SABER on the ALUE diagnostic set. DIAG was specifically designed to evaluate models' abilities to capture complex linguistic phenomena in Arabic (Seelawi et al., 2021) . These observations clearly indicate that scaling the model with more data and parameters greatly improves the robustness and generalization abilities of Arabic PLMs. It is worth mentioning that our results are in contrast with previous observations reported in (Nagoudi et al., 2022b; Ghaddar et al., 2022) that encoder-decoder T5 architecture Arabic models (e.g. AraT5-base and AT5B) significantly underperform BERT models on discriminative tasks. Our results suggest that, for Arabic, encoder-decoder models require more data and parameters to catch up with encoder-only models on discriminative tasks. We further validate the performance of AraMUS by conducting an extensive set of experiments on the ALUE benchmark under few-shot setting. able Arabic PLMs (JABER and SABER) performances on 3 representative ALUE tasks (see the full results in First, we notice that exceptionally on SEC, Ara-MUS performs on par with JABER and underperforms SABER on many data points. We think that this is because the text-to-text approach is not effective for multi-label classification tasks under a few-shot setting. Second, we observe that AraMUS has a marginal gain compared to the best baseline (SABER) on some tasks like OHSD, e.g. 0.2%, 1.0% and 6.0% on 8, 128, and 256 examples respectively. As for the remaining 4 tasks (represented by MDD), we observe that AraMUS significantly outperforms both baselines by a large margin. Overall, AraMUS shows a consistent performance gain between 4% to 6% when averaging the results on the 8 ALUE tasks compared to SABER.\n\nModel Dev Test\nAraT5-base 6.7\u00b10.1 13.5 AT5B 8.1\u00b10.1 17.0 AraMUS 8.6\u00b10.1 17.4 Finally, we assess the text generation abilities of AraMUS by experimenting on 3 generative tasks in Table 3 , 4 and 5. Overall, the observations are consistent with the results obtained on ALUE, Ara-MUS reports the highest scores on all tasks and across all metrics. More precisely, AraMUS significantly outperforms AT5B, the state-of-the-art Arabic T5-base model, by 7.5% and 5.1% on QA F1 score dev and test sets respectively. Similarly, AraMUS has a gain of 4.4%, 4.1%, and 3.5% on TS dev, test, and EASC test rouge1 score respectively. However, gains are not always significant on generative tasks, as we observe a smaller margin of improvement of 0.5% and 0.4% and against the best baseline on QG dev and test sets respectively.\n\nConclusion\nIn this paper, we introduced AraMUS which is not only the largest Arabic PLM in terms of pretraining data and model size, but also the first multibillion Arabic PLM to be extensively evaluated on a wide range of NLP tasks. Since our work gives clues on the benefits and limitations of scaling up data and model sizes, we hope that it will pave the way for the Arabic NLP community to focus on problems that are beyond the reach of PLM scaling.\n", "hypothesis": " Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP).  In this work, we present AraMUS, the largest Arabic PLM with 11B parameters trained on 529GB of highquality Arabic textual data.  AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks.  Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs.  * Equal contribution 1 Arabic is among top 10 most popular languages in the world with 420M native speakers, and more than 25 popular dialects (Guellil et al., 2021) ..", "answer": true}
{"title": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition", "content": "\nIntroduction\nNamed Entity Recognition (NER) is the task to extract entities from raw text. It has been a fundamental task in the Natural Language Processing (NLP) field. Previously, this task is mainly solved by the sequence labeling paradigm through assigning a label to each token (Huang et al., 2015; Ma and Hovy, 2016; Yan et al., 2019) . However, this method is not directly applicable to the nested NER scenario, since a token may be included in two or more entities. To overcome this issue, the spanbased method which assigns labels to each span is introduced (Eberts and Ulges, 2020; Li et al., 2020; Yu et al., 2020) . Figure 1 : All valid spans of a sentence. We use the start and end tokens to pinpoint a span, for instance, \"(2-4)\" represents \"New York University\". Spans in the two orange dotted squares indicates that the center span can have the special relationship (different relations are depicted in different colors) with its surrounding spans. For example, the span \"New York\" (2-3) is contained by the span \"New York University\" (2-4). Therefore, the \"(2-3)\" span is annotated as \"d\".\nEberts and Ulges (2020) use a pooling method over token representations to get the span representation, and then conduct classification on this span representation. Li et al. (2020) transform the NER task into a Machine Reading Comprehension (MRC) form, they use the entity type as the query, and ask the model to select spans that belong to this entity type. Yu et al. (2020) utilize the Biaffine decoder from dependency parsing (Dozat and Manning, 2017) to convert the span classification into classifying the start and end token pairs. However, these work does not take advantage of the spatial correlations between adjacent spans.\nAs depicted in Figure 1 , spans surrounding a span have special relationships with the center span. It should be beneficial if we can leverage these spatial correlations. In this paper, we use the Biaffine decoder (Dozat and Manning, 2017) to get a 3D feature matrix, where each entry represents one span. After that, we view the span feature matrix as a spatial object with channels (like images) and utilize Convolutional Neural Network (CNN) to model the local interaction between spans.\nWe compare this simple method with recently proposed methods (Wan et al., 2022; Li et al., 2022; Zhu and Li, 2022; Yuan et al., 2022) . To make sure our method is strictly comparable to theirs, we ask the authors for their version of data. Although all of them use the same datasets, we find that the statistics, such as the number of sentences and entities, are not the same. The difference is caused by the usage of distinct sentence tokenization methods, which will influence the performance as shown in our experiments. To facilitate future comparison, we release a pre-processing script for ACE2004, ACE2005 and Genia datasets.\nOur contributions can be summarized as follows.\n\u2022 We find that the adjacent spans have special correlations between each other, and we propose using CNN to model the interaction between them. Despite being very simple, it achieves a considerable performance boost in three widely used nested NER datasets.\n\u2022 We release a pre-processing script for the three nested NER datasets to facilitate direct and fair comparison.\n\u2022 The way we view the span feature matrix as a spatial object with channels shall shed some light on future exploration of span-based methods for nested NER task.\n\nProposed Method\nIn this section, we first introduce the nested NER task, then describe how to get the feature matrix.\nAfter that, we present the CNN module to model the spatial correlation on the feature matrix. A general framework can be viewed in Figure 2 .\n\nNested NER Task\nGiven an input sentence X = [x 1 , x 2 , . . . , x n ] with n tokens, the nested NER task aims to extract all entities in X. Each entity can be expressed as a tuple (s i , e i , t i ). s i , e i are the start, end index of the entity. t i \u2208 {1, . . . , |T |} is its entity type and T = {t 1 , ..., t n } is entity types. As the task name suggests, entities may overlap with each other, but different entities are not allowed to have crossing boundaries. For a sentence with n tokens, there are n(n + 1)/2 valid spans.\n\nSpan-based Representation\nWe follow Yu et al. (2020) to formulate this task into a span classification task. Namely, for each valid span, the model assigns an entity label to it. The method first uses an encoder to encode the input sentence as follows:\nH = Encoder(X),\nwhere H \u2208 R n\u00d7d , and d is the hidden size. Various pre-trained models, such as BERT (Devlin et al., 2019) , are usually used as the encoder. For the word tokenized into several pieces, we use maxpooling to aggregate from its pieces' hidden states.\nNext, we use a multi-head Biaffine decoder (Dozat and Manning, 2017; Vaswani et al., 2017) to get the score matrix R as follows: where W s , W e \u2208 R d\u00d7h , h is the hidden size, MHBiaffine(\u2022, \u2022) is the multi-head Biaffine decoder 2 , and R \u2208 R n\u00d7n\u00d7r , r is the feature size. Each cell (i, j) in the R can be seen as the feature vector v \u2208 R r for the span. And for the lower triangle of R (where i > j), the span contains words from the j-th to the i-th (Therefore, one span will have two entries if its length is larger than 1).\nH s = LeakyReLU(HW s ), H e = LeakyReLU(HW e ), R = MHBiaffine(H s , H e ) # Param\n\nCNN on Feature Matrix\nAs shown in Figure 1 , the cell has relations with cells around. Therefore, we propose using CNN to model these interactions. We repeat the following CNN block several times in our model:\nR \u2032 = Conv2d(R), R \u2032\u2032 = GeLU(LayerNorm(R \u2032 + R)),\nwhere Conv2d, LayerNorm and GeLU are the 2D CNN, layer normalization (Ba et al., 2016) and GeLU activation function (Hendrycks and Gimpel, 2016) . The layer normalization is conducted in the feature dimension. A noticeable fact here is that since the number of tokens n in sentences varies, their Rs are of different shape. To make sure results are the same when R is processed in batch, the 2D CNN has no bias term, and all the paddings in R are filled with 0.\n2 The detailed description is in the Appendix A.1.\n\nThe Output\nWe use a perceptron to get the prediction logits P as follows: 3\nP = Sigmoid(W o (R + R \u2032\u2032 ) + b), where W o \u2208 R |T |\u00d7r , b \u2208 R |T | , P \u2208 R n\u00d7n\u00d7|T | .\nAnd then, we use golden labels y ij and the binary cross entropy to calculate the loss as:\nL BCE = \u2212 0\u2264i,j<n y ij log(P ij ),\nMore special details about our proposed method during training and inference procedure are described in Appendix A.\n\nExperimental Setup\nTo verify the effectiveness of our proposed method, we conduct experiments in three widely used nested NER datasets, ACE 2004 4 (Doddington et al., 2004) , ACE 2005 5 (Walker and Consortium, 2005) and Genia (Kim et al., 2003) .\nBesides, we choose recently published papers as our baselines. To make sure our experiments are strictly comparable to theirs, we ask the authors for their versions of data. The data statistics for each paper are listed in the Appendix B. For ACE2004 and ACE2005, although all of them use the same document split as suggested (Lu and Roth, 2015) , they use different sentence tokenizations, resulting in different numbers of sentences and entities.\nTo facilitate future research on nested NER, we release the pre-processing code and fix some tokenization issues to avoid including unannotated text and dropping entities. While for the Genia data, there are some annotation conflicts. For examples, one document with the bibliomisc MED-LINE:97218353 is duplicated in the original data, and different work has different annotations on it. We fix these conflicts. We replicate each experiment five times and report its average performance with standard derivation. \n\nMain Results\nResults for ACE2004 and ACE2005 are listed in Table 3 : The precision and recall for flat and nested entities in the test set of three datasets. Compared with models without CNN (\"w.o. CNN\"), the most improved metric is bold. By using CNN, the recall for nested entities improve significantly. The subscript means the standard deviation (e.g 88.8 0.9 means 88.8\u00b10.9).\n\nWhy CNN Helps\nTo study why CNN can boost the performance of the nested NER datasets, we split entities into two kinds. One kind is entities that overlap with other entities, and the other kind is entities that do not. We design 4 metrics NEPR, NERE, FEPR and FERE, which are flat entity precision, flat entity recall, nested entity precision and nested entity recall, respectively. 6 , and list the results in Table 3 . Compared with models without CNN, the NERE with CNN improve for 2.2, 2.8 and 10.7 on ACE2004, ACE2005 and Genia respectively. Namely, much of the performance improvement can be ascribed to finding more nested entities. This is expected as the CNN can be more effective for exploiting the neighbor entities when they are nested.\n\nRelated Work\nPreviously, four kinds of paradigms have been proposed to solve the nested NER task.\nThe first one is the sequence labeling framework (Strakov\u00e1 et al., 2019) , since one token can be contained in more than one entities, the Cartesian product of the entity labels are used. However, the Cartesian labels will suffer from the long-tail issue.\nThe second one is to use the hypergraph to efficiently represent spans (Lu and Roth, 2015; Muis and Lu, 2016; Katiyar and Cardie, 2018; Wang and Lu, 2018) . The shortcoming of this method is the complex decoding.\nThe third one is the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014; Lewis et al., 2020; Raffel et al., 2020) to generate the entity sequence. The entity sequence can be the entity pointer sequence (Yan et al., 2021; Fei et al., 2021) or the entity text sequence (Lu et al., 2022) . Nevertheless, the Seq2Seq method suffers from the time-demanding decoding.\nThe fourth one is to conduct span classification. Eberts and Ulges (2020) proposed to enumerate all possible spans within a sentence, and use a pooling method to get the span representation. While Yu et al. (2020) proposed to use the start and end tokens of a span to pinpoint the span, and use the Biaffine decoder to get the scores for each span. The span-based methods are friendly to parallelism and the decoding is easy. Therefore, this formulation has been widely adopted (Wan et al., 2022; Zhu and Li, 2022; Li et al., 2022; Yuan et al., 2022) . However, the relation between neighbor spans was ignored in previous work.\n\nConclusion\nIn this paper, we propose using CNN on the score matrix of span-based NER model. Although this method is very simple, it achieves comparable or better performance than recently proposed methods. Analysis shows exploiting the spatial correlation between neighbor spans through CNN can help model find more nested entities. And experiments show that different tokenizations indeed influence the performance. Therefore, it is necessary to make sure all comparative baselines use the same tokenization. To facilitate future comparison, we release a new pre-processing script for three nested NER datasets.\n", "hypothesis": " Named entity recognition (NER) is the task to detect and classify entity spans in the text.  When entity spans overlap between each other, the task is named as nested NER. Span-based methods have been widely used to tackle nested NER. Most of these methods get a score matrix, where each entry corresponds to a span. However, previous work ignores spatial relations in the score matrix. In this paper, we propose using Recurrent Neural Network (RNN) to model these spatial relations.  Despite being simple, experiments in three commonly used nested NER datasets show that our model surpasses several recently proposed methods with the same pre-trained encoders.  Further analysis shows that using CNN can help the model find more nested entities.  Besides, we find that different papers use different sentence tokenizations for the three nested NER datasets, which will influence the comparison.  Thus, we release a pre-processing script to facilitate future comparison.  1 * Equal contribution.  \u2020 Corresponding author.  1 Code is available at https://github.com/yhcc/CNN_ Nested_NER a (1-3) c (1-4) c (1-5) d (2-3) o (2-4) c (2-5) d (3-3) d (3-4.", "answer": false}
{"title": "Value type: the bridge to a better DST model", "content": "\nIntroduction\nTask-oriented dialogue systems have become more and more important as people's demand for life increases(booking flights or restaurants), which have become increasingly important in the field of NLP(Nature Language Process). (Henderson et al., 2019; Hung et al., 2021; Zheng et al., 2022) Traditionally, the task-oriented dialogue system consists of four modules (Zhang et al., 2020) : Natural language understanding(NLU), Dialogue state tracking(DST), Dialogue manager(DM) and Natural language generation(NLG). This module directly affects the decision-making behavior of the dialogue system, and plays an extremely important \u21e4 The first two authors contribute equally. Weiran Xu is the corresponding author.\nSys: yes, the autumn house is on the east part of time, the prices are cheap and it is 4 stars. is there anything else you would like to know? Usr: no, i just want to book it for 2 people for 5 nights starting wednesday. Turn_label: hotel-area=east, hotel-book day=wednesday, hotel-people=2, hotel-book stay=5, hotel-princerange=cheap, hotel-stars=4\nSys: your friend has good taste. It is located at 02:00 rose crescent city centre, postcode cb23ll. Usr: i would like to book a table for 7 people on Monday at 15:15 please. Turn_label: restaurant-book day=monday, restaurant-book people=7, restaurant-book time=15:15 Sys: Usr: i would like a taxi from saint johns college to pizza hut fenditton. Turn_label: taxi-departure=saint johns college, taxi-destination=pizza hut fenditton role in the task-based dialogue system. (Lee et al., 2019) The recent methods in DST work are mainly divided into two categories. The first category is based on ontology which means the candidate slot value is assumed to be known eg (Zhou et al., 2022; Ye et al., 2021b; Guo et al., 2021) . The second is the way without ontology. These studies have completely abandoned ontology, and they assume that the slot value is unknown. eg (Wu et al., 2019; Kim et al., 2019; Kumar et al., 2020; Lin et al., 2021) . However, most of their work is based on dialog state, dialog and slot modeling, ignoring that the value type of each slot may be different. If these slots are modeled uniformly, then there is a lack of a specific feature of each slot.\nIn this work, we propose a new DST framework named SVT-DST, which uses the Slot-Value Type as the bridge to increrase the model performance. With this method, each slot has specificity for the attention of the conversation history to better identify the slot value. Specifically, we first classify all the slots in the dataset according to their slot value types. As shown in Figure 1 , adjectives, time and numbers correspond to pricerange, arrive-time and book-people respectively. We train a sequence annotation model with dialogue training which is used to extarct entities and corresponding entitytypes in each on the turn. We hope that the attention between the dialogue and slots can be higher when the turn is near to current turn with the same slotvalue type. In order to achieve the goal, we use monotonically decreasing functions to integrate the attention weights, which will be described in detail in the method. we use monotonically decreasing functions to integrate these types into the attention operation.\nOur main contributions are as follows: 1) We classify the slot according to the slot-value type, then train the Ner model to extract these types to improve the attention formula. 2)We design a sampling strategy to integrate these types into the attention formula, which decrease the error of Ner model. 3)We have achieved competitive results on MultiWOZ 2.1 and 2.4. We analyze the results and point out the future work.\n\nMethod\nFigure 2 shows the structure of our DST model, including encoder, attention module and slot value processing module. In this section, we will introduce each module of this method in detail.\nA T-turn conversation can be expressed as C t = {(U 1 , R 1 ), ..., (R t 1 , U t )}, where R t represents system discourse and U t represents user discourse. We define the dialogue state of the t-th turn as B t = {(S j , V t j )| 1 <= j <= J}, where V t j represents the value of the j-th slot S j in the t-th turn. J represents the number of predefined slots. Follow (Ren et al., 2018) , we express the slot as a \"domain slot\" pair, such as 'restaurant-price range'.\n\nEncoder\nFollow (Ye et al., 2021b) , we use two bert (Devlin et al., 2018) models to encode context and slot respectively.\n\nContext encoder\nWe express the dialogue at turn t as D t = R t U t , where represents sentence connection. Then the history of the dialogue including t-th turn as\nM t = D 1 D 2 ... D t .The input of the context encoder is X t = [CLS] M t [SEP ].\nThe output of the encoder is:\nEQUATION\nWhere C t 2 R |Xt|\u21e5d , |X t | is the length of M t and d is the hidden size of bert. bert finetuned indicates that the bert model updates a part of parameters during training.\n\nSlot-value related encoder\nWe employ the first token to represent the aggregate representation of the entire input sequence. Therefore, for any slot S j 2 S(1 \uf8ff j \uf8ff J) and any value v t j 2 V j we have:\nh S j = bert fixed (S j ) 2 R 1\u21e5d\n(2)\nEQUATION\nFor the last turn of dialogue state B t 1 , we have\nEQUATION\nWhere\nh B t 1 2 R |B t 1 |\u21e5d , B 1 = Null.\nbert fixed indicates that the bert model has fixed parameters during training.\n\nCross-Attention\nWe use the multi-head-attention module (Vaswani et al., 2017) as the basis of our attention module.\n\nSlot-Context Attention\nWe first calculate the bias term of the attention formula. For each dialogue history M t , we first use the monotonically decreasing distribution function \u2318(n) to initialize the weight of each turn of dialogue D t in the dialogue history:\nEQUATION\nWhere n = T t, n represents the distance between the last turn and the current turn. The closer the distance is, the greater the weight will be obtained. Note that (T ) represents the weight of distance T for this turn (turn 0) and the latest turn t. We record the turns of the value type type j with slot S j in the history:\nEQUATION\nWhere n>m, which represents the turn indexs. Then we calculate the weight of these turns:\nEQUATION\nFinally, we add these two weights according to the turn indexs to get bias: The attention between S j and C t can be calculated as:\nEQUATION\nEQUATION\nWhere '() indicates a learnable mapping built by embedding. W bias , W r 1 and W r 2 indicates a linear layer, respectively.\n\nSlot-State Attention\nFor S j and B t 1 , their attention can be expressed as:\nEQUATION\nA B,F F N j,t 1 =W r 4 ReLU (W r 3 [(h S j , A B j,t 1 ] + b r 1 ) + b r 2 (12)\n\nGate Fusion\ninspired by (Zhou et al., 2022) , we employ a gate module to combine the attention between Slotcontext and Slot-state:\nEQUATION\nEQUATION\nWhere \u2326 indicates vector product, indicates the sigmoid function and \u2022 indicates element-wise product operation.\n\nSelf-Attention And Value Matching\nIn this part, we have followed the relevant part of (Ye et al., 2021b) .\n\nNer Model And Sampling Strategy\nWe employ the W2NER model (Li et al., 2022) as our tagging model. The strategy of our labelmaking is that: for each value in the ontology, if the value is in current turn, we will tagging this value. For sampling strategy, only when the target entities are different from entities extracted from previous turns, this turn will be marked with the entities' type. This strategy helps to reduce the interference of duplicate entities. For the specific classification of each slot, please refer to the appendix. In particular, for bool type, we train the annotation model to extract keywords, such as internet, parking, etc.\n\nOptimization\nWe use the sum of the negative log-likelihood as the loss function at each turn t:\nEQUATION\nWhere\nP (V t j | X t , S t ) = exp( || t S t j h V t j || 2 ) P V 0 j 2V j exp( || t S t j h V 0 j || 2 ) (16) t S t j\nindicates the output of self-attention module corresponding to S j at the t-th turn. 3 Experiments\n\nDataset, metric and Evaluation\nWe evaluate our method on these datasets: Mul-tiWOZ 2.1 (Eric et al., 2019) and MultiWOZ 2.4 (Ye et al., 2021a) which provide turn-level annotations of dialogue states in 7 different domains. We evaluate our method on this dataset and follow the pre-processing and evaluation setup from (Wu et al., 2019) , where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. We use Joint Goal Accuracy that is the average accuracy of predicting all slot assignments for a given service in a turn correctly to evaluate the main results of models.\n\nBaselines\n(1) Trade: Transferable dialogue state generator (Wu et al., 2019) which utilizes copy mechanism to facilitate domain knowledge transfer.\n(2) Tripy: It applies three copying mechanisms to extract all values (Heck et al., 2020) (3) MinTL: An effective transfer learning framework for task-oriented dialogue systems (Lin et al., 2020) ,which uses T5 (Raffel et al., 2020) and Bart (Lewis et al., 2019) .\n(4) Star: Framework with self-attention modules to learn the relationship between slots better (Ye et al., 2021b) LUNA: It applies a slot-turn alignment strategy to accurately locate slot values and their associated context. (Wang et al., 2022) \n\nMain Results And Analysis Experiments\nTable 1 shows the results of our main test and ablation study. Our base model achieved 53.28% for the joint-acc, while our Ner-based model achieved 55.37% , a significant improvement of 2.09% compared with the base model. In 2.4 dataset, our model achieved 68.28%, a significant improvement of 2.93% compared with the base model. And When we use the correct type labels for training, the model performance reaches 59.27%, which has exceeded all baseline models. Ground truth is extracted according to the slot-type in the turn label, similar to our sampling strategy. In order to model the attention of state and dialog history separately, we changed the attention in Star (Ye et al., 2021b) to the fusion of slot attention and dialog history attention. Such changes reduced the performance of the model. However, the ablation experiment shows that the method we proposed can really benefit the model indicators.\nTable 2 shows the results of our analysis experiments, which use different distribution functions to model attention. For both 2.1 and 2.4 datasets, the experimental results show that under different distribution function modeling, the distribution with constant term bias may produce higher results such as 0.5 \u21e4 (1 + x) + 1 and 1 x/30. And it often has a positive impact on the experiment when the power of the independent variable is 1.\n\nCase Study\nWe conducted a series of analytical experiments on attention weights. As shown in the Table 3 , we randomly selected a slot, \"attraction-name,\" and then chose an example PMUL4648 from the test set to observe the attention distribution of this slot for each turn in the test samples. In the example, the attraction-name slot is activated in the turn 2. It can be seen that function 3 noticed this turn with a large weight, followed by function 1. As a comparison, function 2 assigned larger weights to the first turn, which is sufficient to indicate that the fitting effect of function 2 is weaker compared to the other two functions. Our analysis is as follows:\nIf there is no constant term in the distribution function, the difference between score+bias and score is not significant, resulting in limited performance improvement of the model. On the other hand, the power of the independent variable is greater than 1 such as function 2, the magnitude changes too obviously after Softmax. This leads to not smooth transitions between turns, resulting in limited performance improvement. The result of using the ground truth labels training model shows that there is still huge space for improvement in Ner model annotation. One of the biggest challenges is that the annotation model often assigns certain entities to labels based on some fragmented tokens, without considering the impact of context, which leads to the proliferation of labels. We will solve this problem in future work.\n\nConclusion\nIn this paper, we propose an effective method to integrate slot-types into the DST model. Specifically, we propose the SVT-DST. This framework incorporates the slot-types information into the attention operation to help model pay more attention to these turns that include the type of one slot. Further, We design a sampling strategy to integrate these types into the attention formula to decrease the error of Ner model. Results on MultiWOZ dataset show that our method has significant improvement on this task.\n", "hypothesis": " Value type of the slots can provide lots of useful information for DST tasks.  However, it has been ignored in most previous works.  In this paper, we propose a new framework for DST task based on these value types.  Firstly, we extract the type of token from each turn.  Specifically, we divide the slots in the dataset into 9 categories according to the type of slot value, and then train a Ner model to extract the corresponding type-entity from each turn of conversation according to the token.  Secondly, we improve the attention mode which is integrated into value type information between the slot and the conversation history to help each slot pay more attention to the turns that contain the same value type.  Meanwhile, we introduce a sampling strategy to integrate these types into the attention formula, which decrease the error of Ner model.  Finally, we conduct a comprehensive experiment on two multi-domain taskoriented conversation datasets, MultiWOZ 2.1 and MultiWOZ 2.4.  The ablation experimental results show that our method is effective on both datasets, which verify the necessity of considering the type of slot value..", "answer": true}
{"title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python", "content": "\nIntroduction\nPretrained Large Language Models (LLMs) are rapidly becoming one of the dominant paradigm for large variety of language tasks (Brown et al., 2020a; Chowdhery et al., 2022) , including programming code generation and completion (Chen et al., 2021; Li et al., 2022) . LLMs have demonstrated increasing performance with increasing model size 1 on many practical tasks (Kaplan et al., 2020; Hernandez et al., 2021) including programming tasks (Nijkamp et al., 2022) , recently, however, researchers len, print = print, len def print_len(x):\n\"Print the length of x\"\n\u2713 len(print(x)) \u2717 print(len(x)) LLM preference\nFigure 1 : Given a Python prompt (on top) which swaps of two builtin functions, large language models prefer the incorrect but statistically common continuation (right) to the correct but unusual one (left).\nhave identified a number of tasks that exhibit inverse scaling, where output quality decreases, rather than increase, with increasing model size.\nTasks with inverse scaling generally either involve social biases (Parrish et al., 2022; Srivastava et al., 2022) , where the larger models (arguably correctly) learn undesirable biases from biased training sets, or involve examples of natural language that are highly atypical but still easily understandable by a human (McKenzie et al., 2022b) . These tasks may involve unusual discourse pragmatics or they may require reasoning about counterfactual knowledge, however, since they tend to be highly artificial, it could perhaps be argued that they are edge cases which may not represent serious failure modes for practical applications. In this paper we present a novel type of inverse scaling task involving Python code generation under a redefinition of default identifiers. This has both practical implications (redefinition of default identifiers is a metaprogramming technique used in popular libraries), and broader scientific implications, as it shows that LLMs fail to reason about the deep, abstract semantic structure of programming languages, and these flaws are not ameliorated, but in fact may be even worsened, by increasing model size.\nProgramming languages have precise and well- \n\nDataset\nFigure 2 : Data generation pipeline (see Appendix D for an example): 1. Crawl repositories from GitHub, filtered by language, license, stars, and size. 2. Extract top-level functions with docstrings and references to at least two callable builtins 3. For each function, choose two builtins to swap and generate: a) header with builtin swap statement, function declaration with decorators, docstring b) original function body, c) corrected body with the builtins swapped consistently with the swap statement. 4. Store as a binary classification task: a) head = classifier input, b) original body = bad class, c) corrected body = good class. defined syntax and semantics which makes them especially suited to automatic analysis and procedural generation. They are scientifically interesting because they can be used for automatic generation of examples of coding problems and their evaluation against an objective ground truth, whereas most NLP tasks have enough ambiguity that require human annotation in order to produce high-quality examples. Furthermore, this research is also of practical importance for software engineering tools that use LLMs, such as GitHub Copilot, 2 which are starting to be widely adopted by developers.\n\nMethodology\nWe describe the motivation behind our task ( \u00a72.1) and the task itself ( \u00a72.2), followed by the way we collected the data for the task ( \u00a72.3).\nWe release our dataset as well as the code used to generate it and replicate our experiments 3 .\n\nTask Motivation\nTuring-complete languages have invariances and equivariances, making it possible to express the same function by multiple programs (see Appendix H for formal definitions). While determining semantic equivalence is undecidable in the general case (Rice, 1953) , sometimes it can be determined by pure syntactic analysis. For instance, \u03b1-equivalence, invariance under the consistent renaming of identifiers such as variable or function names, can be decided using syntactic analysis.\nProper understanding of the semantics of a programming language requires identifying its invariances and equivariances, as opposed to \"shortcut learning\" (Geirhos et al., 2020) which instead exploits many weak, spurious correlations that do not generalize out of the observed data distribution. We propose a task based on the approximate \u03b1-equivalence of Python code, in order to evaluate how well LLMs master the semantics of Python.\n\nTask Description\nWe consider code snippets in Python 3. Python allows to redefine builtin functions 4 by reassigning their identifiers. For instance, the statement len, print = print, len swaps the identifiers for the builtin functions len and print. Any function defined following that identifier swap would have to refer to the builtin function len by the identifier print and vice versa.\nWe consider a code generation task where the model is given a top-level function declaration, followed by a docstring (which typically describes the behavior of the function in natural language) and has to generate the rest of the body of the function, similar to Miceli Barone and Sennrich (2017), but with the caveat that we prepend to the declaration a statement that swaps two Python builtin functions that are expected to be used in the function body. Specifically, in line with the format of the Inverse Scaling Prize 5 we define our Builtin identifier swap task as a binary classification task where the input of each example is the concatenation of a swap statement, function declaration (with optional decorators) and docstring. A \"bad\" output for such input is a function body that uses the builtin functions according to their usual meaning, ignoring the swap statement. In contrast, the \"good\" output is a function body where the builtin functions are used consistently with the swap statement. To assess the success of the model in distinguishing between the \"bad\" and the \"good\" output, we compute the likelihood of each output given the input provided as a prompt (Figure 1 , Appendix D).\n\nData Collection\nSimilar to Miceli Barone and Sennrich (2017) , our dataset collection procedure involves scraping code from GitHub using the PyCodeSuggest library 6 (Bhoopchand et al., 2016) to download Python repositories with at least 100 stars, of size at most 200 MB and which mention the use of the Open Source CC-BY-4.0 license 7 in their README. Our final dataset includes 559 repositories downloaded on 16 December 2022. We then parse the .py files in each repository with the Python 3 ast module to make sure that they contain valid code. We extract 1,000 randomly chosen top-level functions that each contain a docstring and that reference at least two callable builtin identifiers, as defined by the builtins module. For each of these extracted functions, we randomly choose two builtin functions and generate the corresponding swap statement, function declaration (with decorators) and docstring as the example prompt, the original function body (regenerated from the abstract syntax tree with the astunparse module 8 ) as the \"bad\" output and the function body where the two selected builtins are swapped consistently with the swap statement as the \"good\" output (Figure 2 ).\nNote that functions can in principle access the builtin identifiers as strings using reflection and evaluation facilities, which may require a full static analysis of the code to identify and is undecidable in the general case. Since our method uses purely syntactic substitutions, there might be cases where the \"good\" outputs do not maintain the expected function behavior. In practice, this dynamic access of identifiers at runtime is rare with builtin identifiers and therefore does not pose an issue.\n\nExperiments\nWe next describe our experiments with a likelihood calculation of correct and incorrect completions ( \u00a73.1) and chat LLMs ( \u00a73.2), and then present a qualitative analysis ( \u00a73.3).\n\nComputational resources\nWe spent approximately 130 US dollars, including donated credits, to use the OpenAI LLMs through their publicly accessible API.\nWe also used a small amount of machine-hours on the Baskerville Tier 2 HPC platform 9 equipped with NVIDIA A100 GPUs. While this is a highend system, our experiments on the open source models can be also practically run on consumergrade machines with gaming GPUs.\n\nCompletion Likelihood\nFor our main set of experiments, we evaluate our dataset on families of auto-regressive language models (OpenAI GPT-3, Salesforce Code-Gen, Meta AI OPT) and one family of sequenceto-sequence conditional auto-regressive language models (Google FLAN-T5). All models are based on the Transformer architecture (Vaswani et al., 2017) and pretrained on large datasets scraped from the Internet (full details in Appendix A).\nResults We evaluate our datasets on the models using a modified version of the Inverse Scaling Prize evaluation code. 10 We report the results for all models in Figure 3 All tested models always prefer the incorrect output resulting in zero classification accuracy, the log-likelihood of the incorrect output is always significantly higher than the uniform baseline, but it varies with the model. Specifically:\n\u2022 The Meta AI OPT and OpenAI text-based GPT-3 families exhibit strong inverse scaling, with the larger models more strongly preferring the incorrect output. The trend is monotonic for the \"First generation\" GPT-3 family, and somewhat nonmonotonic for the OPT and InstructGPT families. The InstructGPT models perform worse than the base GPT-3 models.\n\u2022 The Salesforce CodeGen models exhibit mostly flat scaling. The \"mono\" models which are further fine-tuned on Python-only data perform worse than the \"multi\" models they are based on. \u2022 The OpenAI Codex models are the only models that seem to show positive scaling (which may be spurious since they are only two data points). However, the two GPT-3.5 models (text-davinci-002 and text-davinci-003, shown in the figures as red crosses) that further fine-tune code-davinci-002 on English demonstrations, lose their edge and end up performing worse than the base GPT-3 model of the same size (davinci). \u2022 Google FLAN-T5 shows an unclear, oscillating scaling trend, with large error bars at each point.\nWe report numerical correlation results between model size and mean loss 11 in Table 1 . Due to the small number of model sizes per family, some of the p-values are quite high, but the numerical results are consistent with the qualitative analysis.\nOverall, our analysis shows that autoregressive text-based LLMs (even when previously pretrained on code-based models) exhibit inverse scaling on our task, while the code-based models exhibit flat scaling which might possibly transition to positive scaling at the largest tested size, but fail to substantially improve over the text-based models.\n\nChat LLMs Accuracy\nWe perform additional experiments on chat LLMs by OpenAI and Anthropic, whose APIs became recently available. These models constrain both the input text and the generated output to take the form of a dialogue between the user and the \"assistant\" (the model itself). Notably, the APIs of these models do not report log-probabilities, hence they cannot be used to score arbitrary texts. This prevents us from using the same experimental protocol of the other experiments. We instead reformulate the task as binary classification where the model is presented with both the correct and incorrect forms of the same program in the same user message and is asked to select the correct one. We describe the models and the prompt templates in Appendix C. \n\nResults\nWe report the results in Figure 5 . All the models strongly prefer the incorrect programs, although the classification accuracy is non-zero. This may not be necessarily comparable to the zero classification accuracy of the previous experiments, due to the different experimental protocol. The Anthropic models (claude-instant and claude) show better accuracy (10-18%) with positive scaling and never produce invalid outputs. The OpenAI models (gpt-3.5-turbo and gpt-4) show low accuracy (< 4%) with flat or inverse scaling and occasionally produce invalid outputs.\n\nQualitative Experiments\nWe perform a small number of manual two-shot experiments on GPT-3.5. We also carry out manual experiments on OpenAI ChatGPT-3.5 12 and GPT-4 models, where we interact with the models in multiple rounds of dialogue, trying to hint the correct solution. The models are still unable to provide the correct continuations. See Appendices E-G. \n\nConclusions\nWe explored the ability of large language models to predict the correct continuations of fragments of Python programs in scenarios where the correct continuations are statistically uncommon due to the redefinition of identifiers caused by a statement that we included in the prompt. Not only all the tested models fail at this task, but some model families even display inverse scaling: they become worse, rather than better, with increasing model size. These results suggest that LLMs rely on \"shortcut learning\", i.e., weak, unstable, mostly lexical correlations in the data, rather than an understanding of the semantics of the data (in this case, Python code) at a deep level. We believe that our results are important both for a better scientific understanding of the capabilities of LLMs and for their practical relevance as a core technology for automated code generation tools. Future work could investigate scaling effects at larger model sizes, as well as on other programming languages.\n", "hypothesis": " Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming.  Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become less confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.  * Equal contribution.  1 Since model capacity in number of parameters and pretraining dataset size are balanced according to a design law that is fixed for each model family and is intended to empirically maximize the pretraining set likelihood given a compute budget (Kaplan et al., 2020), for the reminder of this paper we will jointly refer to them as \"model size\"..", "answer": false}
{"title": "C-XNLI: Croatian Extension of XNLI Dataset", "content": "\nIntroduction\nNatural language processing has developed rapidly in recent years. Models are starting to achieve human-like performance, but most of these achievements are concentrated on only a small fraction of the world's 7000+ languages. This is to be expected due to the nature of linguistic annotation, which is not only tedious, subjective, and costly, but also requires domain experts, which are in decline (Lauscher et al., 2020) .\nThere are two main approaches commonly used to handle that problem from the models' perspective. The first approach relies on cross-lingual transfer, where the model is pretrained to learn multilingual representations (Conneau et al., 2020; Pires et al., 2019) , while the other approach relies heavily on Machine Translation (MT) systems to translate the text from a low-resource language to a high-resource language (or vice versa). Both approaches can be easily evaluated on cross-lingual benchmarks such as XTREME (Hu et al., 2020) or XGLUE (Liang et al., 2020) . They consist of crosslingual datasets grouped by task to allow comprehensive evaluation. Unfortunately, XTREME covers 40 languages and XGLUE only 19.\nSince none of these benchmarks include Croatian language in any of their datasets, and Crosslingual Natural Language Inference (XNLI; Conneau et al., 2018) corpus is included in both, we decided to extend XNLI with Croatian (C-XNLI). The task is to classify whether a premise contradicts, entails, or is neutral to the hypothesis. XNLI's development and test sets are crowdsourced in English and human-translated into 14 languages, while MultiNLI's (Williams et al., 2018) training set is used for training. It also consists of machine-translated sets required for the translatetrain and translate-test paradigms.\nOur Croatian extension is created in the same manner as its XNLI parent. The development and test sets are translated by a professional translator. Since XNLI provides translate-train, translate-dev and translate-test sets, we opted for Facebook's 1.2B parameter m2m_100 MT model (Fan et al., 2020) to create our own translations.\nIt has been shown that MT models still suffer from errors like mistranslations, non-translations and hallucinations (Freitag et al., 2021; Raunak et al., 2021) , which motivated us to analyze the quality of our dataset. For this purpose, we sampled 2000 sentences per language in both Croatian and German, and evaluated the translations using a variant of the Direct Assessment (DA) score proposed in the Multilingual Quality Estimation dataset (MLQE; Fomicheva et al., 2022) .\nTo summarize, our contributions are the following: (1) we create and analyze the Croatian exten-sion of XNLI and provide baseline models, (2) we create Quality Estimation (QE) datasets for Croatian and German to evaluate the quality of machinetranslated sentences from the translate-train sets, and (3) we quantify the textual overlap between hypothesis and premise and analyze its impact on baseline models.\n\nC-XNLI\nIn creating the dataset, we follow the same procedure as Conneau et al. (2018) . We hired a native Croatian professional translator to translate the English development (2490 samples) and test (5010 samples) sets of the XNLI dataset into Croatian. Premises and hypotheses were given to the translator separately to ensure that the premises did not provide context for the hypotheses. The English training set, derived from MultiNLI and containing 392,702 samples, was translated into Croatian using a selected MT model. We considered a total of eight models and opted for Facebook's multilingual m2m_100 model with 1.2B parameters because of its highest BLEU score (Papineni et al., 2002) on the FLORES dataset (Guzm\u00e1n et al., 2019) , as shown in Table 1 . All of m2m_100 and mbart models are available on fairseq 1 (Ott et al., 2019) , whereas opus models are available on Helsinki-NLP 2 (Tiedemann, 2020; Tiedemann and Thottingal, 2020) \n\nDA Scores\nTo evaluate the quality of the system used to translate English to Croatian, we compare the generated translations with the available translations from a high-resource language. We score a sample of Croatian and German translations from the train set and compare the results. The sentences were sampled using a semantic similarity-based metric that correlates with translation quality (Cer et al., 2017) to flatten the original distribution of scores and analyze samples of diverse quality. A cosine score between the multilingual sentence representations from both LASER (Artetxe and Schwenk, 2019) and SBERT (Reimers and Gurevych, 2019) were used to measure semantic similarity between the source and translated sentences. These models are commonly used at the Conference on Machine Translation (WMT) for QE task (Specia et al., 2021 (Specia et al., , 2020)) . The SBERT we used is a multilingual variant trained on the paraphrase dataset which has slightly better performance than the models trained on similarity tasks (Reimers and Gurevych, 2020) .\nBy utilizing a histogram of cosine scores with a bin size of 0.05, we adopted a circular sampling approach to randomly select one premise from each bin until a total of 50 premises were obtained. Similarly, we followed the same procedure for hypotheses, alternating between SBERT and LASER cosine scores. Furthermore, we implemented an additional criterion to ensure the inclusion of all premises and hypotheses that share a common premise. This entire process was repeated until we reached a 1000 samples each, for both SBERT and LASER cosine scores (2000 in total) .\nWe scored the samples using the procedure described by Fomicheva et al. (2022) . Annotators were asked to rate the translation quality for each sentence on a scale 0-100. Sentences were initially annotated by three annotators. If the range of the most diverging scores exceeded 30 points, an additional annotator was asked to replace the most diverging one until convergence was achieved. The annotators' raw scores were converted to z-scores 3 ; the final score is the average of all scores after convergence. More information about annotators, and annotation procedure is presented in Appendix A.\n\nC-XNLI and DA Scores\nTo demonstrate that our extension has similar properties to its parent XNLI, we perform the following analyses. We tokenize C-XNLI's sentences with MOSES tokenizer and obtain the average number 2018) provide is the BLEU score of their MT systems translating to and from the target language. We have extended their results to include those for the Croatian language (Table 2 ). Our translations from English to Croatian (EN-XX in the table) have the fourth-best BLEU score. These findings are not too surprising since the MT we use is more recent. The distribution of DA scores for Croatian and German is shown in Figure 1 . We can observe that Croatian, although is a lower-resourced language, it has a slightly higher translation quality, as the mean of Croatian DA scores is almost identical to a German one. The correlations between the LASER and SBERT cosine scores and DA scores for both languages are shown in Table 3 , with p < 0.05. The correlations for German are higher, and the LASER cosines tend to correlate less. In Figure 2 we can see that the Croatian model is more likely to make a mistake on premises compared to the German model. \n\nOverlaps\nThe analysis presented here extends Artetxe et al.'s (2020) work where authors demonstrate that the overlap between hypotheses and premises is an overlooked bias in the XNLI dataset, caused by access to premise during hypothesis generation in English, and no access to it during translation into other languages. They decrease the bias by back-translating data and improve their results. To demonstrate the existence of that bias, we take a more direct approach and define a metric that represents overlap -the proportion of copied text from premise to hypothesis. It is the number of character N -grams which occur in both hypothesis and premise, divided by the number of possible character N -grams in the hypothesis. In Table 4 we presented those overlaps using bi-grams, N = 2. We can observe that in the training set, the overlap is 5% to 20% higher compared to development and test sets. In order to investigate that even further, we asked our professional translator to translate 1% of our C-XNLI dataset: 100 sentences which consist of 25 premises and 75 of their hypotheses. We made sure that the premise was given alongside each hypothesis so that it provides context to it in order to measure the influence on the overlap since, in the translation effort, premises and hypotheses were given separately. Our representative sample contained similar genre distribution, overlap distribution, and similar development vs. test overlap ratio. Our results show that when using N = 2, biased sample has 8% increase in overlap, whereas for N = {3, 4, 5}, it increased by \u223c 17%. Table 5 : We present the accuracy of baseline XLM-R Base models on each XNLI language, with the addition of Croatian, together with an average accuracy for all languages without Croatian (Avg) and with Croatian (Avg +hr ).\nOur XLM-R models are averaged over three different seeds. We also calculate the Spearman's correlation between accuracies of each model's setup and train set overlaps (C tr ), development set overlaps (C de ), and test set overlaps (C te ). For overlaps we used N = 2.\n\nXLM-R Setups\nWe tested cross-lingual transfer using zero-shot and translate-based setups. For each, we employ pretrained XLM-R Base model (Conneau et al., 2020) , implemented in Transformers library (Wolf et al., 2020) . In the zero-shot approach, we fine-tune our model on English samples. In the translate-train approach, we fine-tune on translations of a training set, whereas in translate-train-all, we fine-tune it on concatenated training translations. Evaluations are done in all languages. In the translate-test approach, we use the same model from our zero-shot approach and evaluate it on English translations of other languages. We experimented with various hyperparameter configurations and found appropriate ranges. Hyperparameter optimization is done for each setup, and details are presented in the Appendix B.\nResults of baseline setups are shown in Table 5 . To demonstrate the comparability of our training setup, we compare XLM-R's reported accuracy with ours, which is only 0.6 points lower in the train-translate-all setup. The performance of the Croatian model is consistently among the TOP5 models. The reason for that might be in the high BLEU score shown in Table 2 . Focusing on the best overall model -translate-train-all, we notice that adding Croatian did not drastically change the average performance and decreased it only for dis-tant languages like Urdu and Swahili. Whereas for other languages, it increased or did not change significantly.\nFinally, Table 5 also shows how the performance of models on the test set of each language correlates with the bi-gram overlaps in the train, development, and test sets of that particular language. There is a consistent high correlation between the overlap in all sets and models' performance (p < 0.05). However, a lower correlation is seen in the development and test sets. This observation could be attributed to the fact that increasing the overlap of a particular language makes it more similar to the English set, in terms of overlap, thus improving the performance. However, as we showed in Subsection 3.2, the overlap in the development and test sets is artificially lower due to biased translation. Alternatively, high training overlaps might indicate that the model is learning to detect the occurrence of overlapping cues.\n\nConclusion\nIn this work, we extended XNLI to include the Croatian language. The development and test sets were translated by a professional translator. We have successfully demonstrated that the quality of the development and test sets is comparable to that of the other languages. To validate the machine-translated training set, we compare our Croatian translations with those available for a high-resourced language -German. The comparison is based on 2000 manually scored sentences from German and Croatian train sets using a variant of DA scores normalized by z-score. Our results show that the Croatian MT model performs slightly better because it's more up-to-date, even though it's a lower-resourced language. We also found that the Croatian translation model performs poorly on longer sentences -premises.\nFinally, we present an overlap metric to measure the textual overlap between the premise and hypothesis. We find that the training set has larger overlaps than the development and test sets. These overlaps resulted in a high correlation between the models' scores, indicating that a model uses cues from the data that also correlate with overlaps.\nWe provide our datasets under the same license 4 as the XNLI dataset, and also make the accompanying code available on GitHub 5 . We hope that by sharing our datasets, researchers will have the opportunity to gain further insights and expand their knowledge in the field of cross-lingual transfer.\n", "hypothesis": " Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets.  To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian.  The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs.  The train set is translated using Facebook's 1.2B parameter m2m_100 model.  We thoroughly analyze the Croatian train set and compare its quality with the existing machinetranslated German set.  The comparison is based on 2000 manually scored sentences per language using a variant of the Direct Assessment (DA) score commonly used at the Conference on Machine Translation (WMT).  Our findings reveal that a less-resourced language like Croatian is still lacking in translation quality of longer sentences compared to German.  However, both sets have a substantial amount of poor quality translations, which should be considered in translation-based training or evaluation setups..", "answer": true}
{"title": "PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data", "content": "\nIntroduction\nWord alignment, as the task of finding the corresponding source and target tokens in a parallel sentence, was well-known as an essential component of statistical machine translation (SMT) systems. Despite the dominance of neural machine translation (NMT) in recent years, word alignment is still a notable area of research due to its usage in a wide variety of NLP applications, such as annotation projection (Yarowsky et al., 2001; Pad\u00f3 and Lapata, 2009; Huck et al., 2019; Nicolai and Yarowsky, 2019) , bilingual lexicon extraction (Ammar et al., 2016; Shi et al., 2021; Artetxe et al., 2019) , typological analysis (Lewis and Xia, 2008; \u00d6stling, 2015) , guided alignment training of NMT (Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2018) , and evaluation and analysis of translation 1 https://github.com/fatemeh-azadi/PMI-Align outputs (Anthony et al., 2019; Neubig et al., 2019; Wang et al., 2020) .\nFor many years statistical methods such as IBM models (Brown et al., 1993) and tools implemented based on them, namely GIZA++ (Och and Ney, 2003) or fast-align (Dyer et al., 2013) , were among the most popular solutions to the word alignment task. Following the rise of deep neural models, several attempts have been made to extract word alignments from NMT models and their attention matrices (Peter et al., 2017; Ghader and Monz, 2017; Zenkel et al., 2020; Zhang and van Genabith, 2021) . However, most of these methods, as well as the statistical aligners, require a sufficient amount of parallel training data to produce high quality word alignments. Recently, Jalili Sabet et al. (2020) have shown that high quality word alignments could be achieved using pre-trained multilingual language models (LMs), like MBERT Devlin et al. (2019) and XLMR Conneau et al. (2020) . Their proposed method called SimAlign, extracts word alignments from similarity matrices induced from multilingual contextualized word embeddings with no need for parallel training data, which is very useful for lowresource language pairs. Afterwards, Dou and Neubig (2021) and Chi et al. (2021) proposed methods called probability thresholding and optimal transport to extract alignments using the similarity matrices derived from pre-trained LMs. They have also proposed some word alignment objectives to fine-tune the pre-trained models over parallel corpora.\nIn this paper, we follow the work done by Jalili Sabet et al. (2020) to extract alignments from pre-trained LMs without requiring any parallel training data and propose PMI-Align. Our main contribution is proposing to compute the point-wise mutual information (PMI) between source and target tokens and using the PMI matrices instead of similarity matrices made of cosine similarities between the representation vectors of each source and target tokens, to align words. We argue that our proposed PMI-based method could align better as it considers the total alignment probability of each source or target token, as well as the joint alignment probabilities (equivalent to cosine similarities). This could alleviate the so-called hubness problem (Radovanovic et al., 2010) in high dimensional spaces, where some token's representation is close to many others (see _went in Figure 1 ). We perform experiments on six different language pairs and show that our method could surpass other alignment methods on five of them. We also conduct our experiments on different pre-trained LMs to show that PMI-Align could be advantageous regardless of the pre-trained model used.\n\nProposed Method\nIn this section, we first discuss how we define and compute the PMI matrix for each sentence pair and then we describe our alignment extraction method using the PMI matrix.\n\nPoint-Wise Mutual Information\nPoint-wise mutual information (PMI) is a wellknown measure of association in information theory and NLP and it shows the probability of two events x and y occurring together, compared to what this probability would be if they were independent (Fano, 1961) . It is computed as follows: PMI(x, y) := log p(x, y) p(x)p(y)\n(1)\nIn the context of word alignments, we define the PMI for a source and target token in a sentence pair as how more probable two tokens are to be aligned than if they are aligned randomly. Given a sentence\nx =< x 1 , ..., x n > in the source language and its corresponding target sentence y =< y 1 , ..., y m >, the joint alignment probability of two tokens, x i and y j , could be computed as:\nEQUATION\nwhere h x i is the contextualized embedding vector of x i extracted from a pre-trained multilingual language model and sim(.) is the cosine similarity measure. The total alignment probability of x i and y j , i.e., p(x i ) and p(y j ), could also be computed according to the total probability rule as follows:\nEQUATION\nBy calculating the PMI for each source and target token in a parallel sentence, we obtain the PMI matrix for that sentence pair, that could be used to extract alignments instead of similarity matrix in SimAlign (Jalili Sabet et al., 2020) . The advantage of using PMI to align words is that it also considers the total alignment probability of each source and target token in addition to their joint alignment probability, which is equivalent to the similarity measure. This leads to reduce the probability to align the token pairs that one of them has high similarities to many other tokens, and thus could alleviate the so-called hubness problem in high dimensional spaces where some data points called hubs are the nearest neighbors of many others.\n\nExtracting Alignments\nTo extract word alignments, we follow the simple Argmax method proposed in Jalili Sabet et al. (2020) . Thus, we first obtain the source to target and target to source alignment matrices using the argmax over each row and each column of the PMI matrix, respectively. Next, we intersect these two matrices to get the final word alignment matrix. In other words, the final alignment matrix A i j = 1 iff i = argmax k (PMI k j ) and j = argmax k (PMI ik ).\nSince the above method would extract alignments on the subword level, we follow the heuristic used in previous work to obtain the word-level alignments by considering two words to be aligned if any of their subwords are aligned (Jalili Sabet et al., 2020; Zenkel et al., 2020; Dou and Neubig, 2021) .\n\nDatasets\nWe perform our experiments on six public datasets, as in (Jalili Sabet et al., 2020) , consists of English-Czech (En-Cs), German-English (De-En), English-Persian (En-Fa), English-French (En-Fr), English-Hindi (En-Hi) and Romanian-English (Ro-En) language pairs. The statistics and URLs of these datasets are available in Table 2 in Appendix A.\n\nModels and baselines\nWe compare our method with the following three state-of-the-art methods proposed to extract alignments from pre-trained multilingual LMs without using parallel training data. For all these methods default parameters were used in our experiments.\nSimAlign 2 (Jalili Sabet et al., 2020) : They propose three methods to extract alignments from similarity matrices, called Argmax, Itermax and Match. Although Itermax and Match methods could not make significant improvements over Argmax and the Argmax method had better AER results for most of language pairs while using the XLMR-base model, they have argued that the Itermax method, which tries to apply Argmax iteratively, could be beneficial for more distant language pairs. Thus, we report both Argmax and Itermax results in our experiments to compare with our method.\nProbability Thresholding 3 (Dou and Neubig, 2021) : In this method they apply a normalization function, i.e., softmax, to convert the similarity matrix of tokens into source to target and target to source alignment probability matrices. Afterwards, they extract the aligned words as the words that their alignment probabilities in both matrices exceed a particular threshold.\nOptimal Transport 4 (Chi et al., 2021) : This method was proposed in both Dou and Neubig (2021) and Chi et al. (2021) , and tried to model the word alignment task as the known optimal transport problem (Cuturi, 2013) . Using the similarity matrix, this method attempted to find the alignment probability matrix that maximizes the sentence pair similar-ity. In our experiments, we use the method proposed by Chi et al. (2021) that utilizes the regularized variant of the optimal transport problem (Peyr\u00e9 et al., 2019) , as it reported better results.\nThere are also many attempts made to improve the pre-trained LMs by fine-tuning on some parallel corpora to better align words. However, as our approach is irrelevant to the pre-trained model and our focus is on the alignment extraction instead of the model, we do not include those methods in our experiments. To demonstrate the effectiveness of our PMI-based alignment regardless of the utilized pre-trained multilingual LM, we conduct our experiments on M-BERT (Devlin et al., 2019) , XLMR-Base (Conneau et al., 2020) and XLM-Align (Chi et al., 2021) which is fine-tuned on a word-alignment task, to show that our method could also be advantageous on more cross-lingually aligned models. All these models are publicly available in the Hugging Face platform (Wolf et al., 2020) .\n\nResults\nTable 1 shows the results of our alignment technique compared to previous methods while using different pre-trained LMs. Following the previous work (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Chi et al., 2021) , we use the 8th layer's representations of each pre-trained model to compute the similarity or PMI matrices. We also use the alignment error rate (AER) (Och and Ney, 2003) as the evaluation metric.\nAs Table 1 shows, our PMI-Align method could consistently outperform the other methods in all language pairs except En-Fr, regardless of the pretrained model used. Compared to Argmax, our method performs better for about 1% or more in AER, while using the XLMR-Base model (except for En-Fr), which exclusively shows the benefits of using the PMI matrix instead of the similarity matrix. We also see that the PMI-Align could surpass the Itermax method for more distant language pairs such as En-Fa and En-Hi, where it was claimed to have the most advantage. Results show that our method could also be beneficial while using a model pre-trained on a word alignment task, i.e., XLM-align, which is expected to have more crosslingually aligned representations, and less hubness problem.\nThe only language pair that our method could not outperform prior methods is En-Fr. This could be due to the closeness of these two languages, as they have many shared subwords and similar word orderings. As a result, pre-trained models for this language pair are better trained and could strongly produce similar representations for aligned words, which reduces the hubness problem to a great extent. Thus, using PMI instead of the similarity matrix could not help. However, our method's performance while using the M-BERT model is comparable to the best results, with about 0.1% difference in AER. Several samples are shown in Appendix B, to better intuitively compare PMI-Align and Argmax, which could better show the benefits of using the PMI matrix instead of the cosine similarities.\n\nRelated Work\nStatistical aligners based on IBM models (Brown et al., 1993) , such as Giza++ (Och and Ney, 2003) and fast align (Dyer et al., 2013) were the most dominant tools for word alignment until the late 2010s. With the rise of neural machine translation models, several attempts made to extract alignments from them (Ghader and Monz, 2017; Garg et al., 2019; Li et al., 2019; Zenkel et al., 2020; Chen et al., 2021; Zhang and van Genabith, 2021) . However, all these models need parallel training data and could not utilize pre-trained contextualized embeddings. Recently, Jalili Sabet et al. (2020) have proposed methods to extract alignments from similarity matrices induced from multilingual LMs without the need for training on parallel data. Following this work, we propose a PMI measure to score and align words in each sentence pair, instead of cosine similarity. Some other alignment extraction methods using multilingual LMs were also provided by Dou and Neubig (2021) and Chi et al. (2021) . They both also proposed several training objectives related to word alignments to fine-tune multilingual LMs on parallel data, as in some other recent works (Cao et al., 2020; Wu and Dredze, 2020; Lai et al., 2022) .\n\nConclusions\nThis paper presents a word alignment extraction method based on the PMI matrices derived from cross-lingual contextualized embeddings, instead of just the similarity matrices. We proposed a way to compute the PMI matrix for each sentence pair and argued that using this PMI measure would be beneficial since for each source-target word pair, it considers not only their similarity to each other but also their similarity values to the other tokens of the sentence, that could mitigate the hubness problem.\nExperimental results show that our PMI-Align method could outperform the previous alignment extraction methods in five out of six language pairs, regardless of the base pre-trained language model used to derive word embeddings. Although our method does not require any parallel training data, our experiments show that it could also benefit the approaches using such data to fine-tune the pretrained models for better word alignments. In future work, the proposed PMI matrix could be investigated in other cross-lingual or even monolingual applications, like the translation quality estimation or the evaluation of text generation tasks, instead of the similarity matrix.\n", "hypothesis": " Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs.  Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality word alignments without the need of parallel training data.  In this work, we propose PMI-Align which computes and uses the point-wise mutual information between source and target tokens to extract word alignments, instead of the cosine similarity or dot product which is mostly used in recent approaches. Our experiments show that our proposed PMI-Align approach could outperform the rival methods on five out of six language pairs.  Although our approach requires parallel training data, we show that this method could also benefit the approaches using no parallel data to fine-tune pre-trained language models on word alignments.", "answer": false}
{"title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker", "content": "\nIntroduction\nInformation retrieval (IR) is the task of searching for documents relevant to a given query from a large corpus. As re-ranking the fetched documents from the retriever effectively enhances the performance and the latency, recent studies have suggested several kinds of re-rankers by fine-tuning pre-trained language models (PLM) (Nogueira and Cho, 2019; Nogueira et al., 2020) . Furthermore, Sachan et al. (2022) show that large-scale language models (LLMs) such as GPT-3 (Brown et al., 2020) can be exploited as a zero-shot reranker with the prompt describing the task. They also highlight the importance of an appropriate prompt to elicit the full performance of LLMs, rather than updating the parameters. They choose an optimal prompt among the handcrafted candidates by cross-validation. However, such a manual search for the discrete prompts is highly expensive and sub-optimal in transferability.\nTo resolve the issue, several methods are proposed for automatically optimizing the discrete prompt. They focus on text classification or maskfilling task while underestimating the open-ended generation (Shin et al., 2020; Gao et al., 2021; Prasad et al., 2022) . Recently, Deng et al. (2022) address the discrete prompt optimization applicable to generation tasks with reinforcement learning by designing the reward function, which measures the generated text belonging to a discrete label. Since there are tasks that are still not aligned, requiring a continuous score of output, we aim at a prompt optimization for one of such tasks: re-ranking.\nIn this paper, we propose Constrained Prompt generation, Co-Prompt, as left-to-right discrete prompt optimization without additional model training. By defining the metric of prompt optimum for re-ranking, we interpret the searching process of the optimal prompt as constrained generation with two modules: a zero-shot re-ranker as a discriminator and any decoder-only PLM as a generator. The discriminator calculates the likelihood (i.e., metric) that the prompt sequence is optimal for guiding an LLM to distinguish relevant documents among the large set for a given query. The generator samples the prompt tokens having a high prior from the previous prompt sequences for effectively restricting the prompt candidates for discriminator to evaluate. An overview of Co-Prompt is shown in Figure 1 .\nWe validate our method, Co-Prompt, against other optimization baselines on two LLMs, T0 (Sanh et al., 2022) and OPT (Zhang et al., 2022) , with two benchmark datasets, MS-MARCO (Nguyen et al., 2016) and Natural Question (Kwiatkowski et al., 2019) . Experimental results show that Co-Prompt consistently generates well-performing prompts regardless of LLMs and datasets over the baselines. The qualitative analyses also support the interpretability of the prompts generated by Co-Prompt, similar to human language patterns.\nOur contributions in this work are threefold:\n\u2022 We highlight the impact of optimal prompt on a zero-shot re-ranker by exploiting the optimization methods. \u2022 We propose Co-Prompt, a novel discrete prompt optimization via constrained generation for a zero-shot re-ranker. \u2022 We experimentally show that Co-Prompt consistently guides the re-ranker well against the baselines and its output is similar to human language patterns.\n\nRelated Work Document Ranking with Generative Model\nUsing the generative model is one of the dominant methods for ranking the retrieved documents by defining the relevance score as the query likelihood score (Nogueira dos Santos et al., 2020; Ju et al., 2021) . More recently, Sachan et al. (2022 Sachan et al. ( , 2023) ) showed that the LLM serves as either a zero-shot re-ranker or a training module of an unsupervised dense retriever. However, unlike ours, they require carefully designed manual prompts, which may have a limitation in transferability.\nPrompt Optimization As prompting is considered a key variable when exploiting LLMs for various NLP tasks, finding the optimal prompt has become important to get the best performance out of the LLMs (Kojima et al., 2022; Xie et al., 2022) .\nRecently, the prompt optimization work has focused on discrete prompt search (Shin et al., 2020; Gao et al., 2021; Deng et al., 2022) or soft prompt learning over a continuous space (Liu et al., 2021; Qin and Eisner, 2021; Lester et al., 2021) . While the existing optimization methods mainly consider text classification or mask-filling task, their applicability to re-ranking is yet underexplored. In this paper, we target at optimizing discrete prompts for zero-shot re-ranker to get higher relevance scores for more relevant pairs via constrained generation.\nConstrained Generation Constrained generation aims at deriving the text sequences that follow a certain constraint (Keskar et al., 2019) . Utilizing a discriminator for guiding the generation toward the constraint via the Bayes' rule is one of the widely used constraint generation methods (Dathathri et al., 2020; Krause et al., 2021; Chaffin et al., 2022) . Inspired by the effectiveness of the discriminator-based method, we adopt the zero-shot re-ranker as a discriminator when generating optimal discrete prompt sequences.\n\nPreliminaries\nAn LLM re-ranks the retrieved document d concerning the relevance score with a given query q as the query generation score:\nEQUATION\nwhere |q| denotes the token length of the query q and \u03c1 is a natural language prompt guiding an LLM to generate the query q. Since the prompt \u03c1 is the only controllable variable in Equation 1, searching for an optimal prompt is a simple yet effective way to enhance the performance of LLMs. Thus, in this work, we focus on a prompt optimization strategy.\n\nConstrained Prompt Generation\nWe define the optimal prompt \u03c1 * for the re-ranker which maximizes the query generation scores:\nEQUATION\nwhere D is the dataset for the retriever, consisting of pairs of a query and its relevant document. We solve the task of searching the optimal prompt \u03c1 * for the document-query pair dataset D with discriminator-based constrained generation. The generation is guided by the Bayes' rule: P (\u03c1t|D, \u03c11:t\u22121) \u221d PM D (Ds|\u03c11:t)PM G (\u03c1t|\u03c11:t\u22121), (3) \nP M D (Ds|\u03c1) return R end\nwhere M D is a zero-shot re-ranker serving as a discriminator, M G is a decoder-only PLM as a generator, and D s is a dataset sampled from D.\nDiscriminator The discriminator M D measures how effectively the prompt sequence \u03c1 1:t guides the zero-shot re-ranker to generate the query from the given document by computing the likelihood P M D (D s |\u03c1), defined as the expectation of relevance score between document-query pairs (q i , d i ) of the sampled dataset D s with the prompt \u03c1:\nEQUATION\nWe use this likelihood as the metric for prompt optimum. The other option of\nP M D is shown in Appendix B.1.\nGenerator The generator M G samples the pool of prompts to be evaluated by a discriminator since computing Equation 3 of all possible tokens in the vocabulary requires a prohibitively high computational cost. The decoder-only PLM is exploited to sample prompt tokens \u03c1 t having a high prior P M G (\u03c1 t |\u03c1 1:t\u22121 ) in a zero-shot manner.\nWe combine these modules to optimize the prompt by iteratively performing two steps: candidate generation and evaluation. We choose to use a beam search as a decoding strategy for left-toright prompt generation. The detailed steps of the decoding strategy are shown in Algorithm 1.\n\nExperimental Setups\nWe describe the experimental setups for validating the performance of the prompts. Our code is publicly available at github.com/zomss/Co-Prompt.\nDatasets We employ two information retrieval datasets: 1) MS-MARCO (Nguyen et al., 2016) fetched from Google search engines. We only use the document data of the dataset for evaluation. More information is shown in Appendix A.1.\n\nEvaluation Metrics\nWe evaluate the results by two metrics, ACC and nDCG. 1) ACC is the percentage of the relevant documents in the total retrieved ones. 2) nDCG, normalized discounted cumulative gain, reflects that the more relevant documents should record higher ranks.\nRetriever & Re-ranker We select two widely used sparse and dense retrievers as our retrievers, which are 1) BM25 (Robertson and Zaragoza, 2009) and 2) DPR (Karpukhin et al., 2020), respectively. For the zero-shot re-ranker, we use 1) T0 (Sanh et al., 2022) and 2) OPT (Zhang et al., 2022) . We describe more detailed information in Appendix A.3 and A.4.\n\nPrompt Baselines\nWe compare Co-Prompt against four baselines: 1) Null Prompt is an empty prompt without any token. 2) P-Tuning is a soft prompt optimization method that yields prompt embeddings from the prompt encoder (Liu et al., 2021) . 3) RL-Prompt is a discrete prompt optimization method by training policy network (Deng et al., 2022) . Note that we modify RL-Prompt and P-Tuning applicable to the re-ranking task. 4) Manual Prompt, suggested by Sachan et al. (2022) , is given as \"Please write a question based on this passage\", following the assumption that it is one of the best prompts that humans can find. Last, 5) Co-Prompt, our proposed method, is a discrete prompt optimization method in left-to-right zero-shot generation. The implementation details of baselines are shown in Appendix A.5. is the first question asked on Google for\" 31.9 \"Please post your question again when its not just about\" 30.6 \"Score! What are all 3 things, the first is\" 30.2 \"Score the top 5 things on this sub reddit for\" 29.3 \"This looks like the same as every \"what are the\" 30.5 \"This post should be titled as\" 31.2 \"What are some common questions asked on the internet about\" 30.3 \"How do i find the name on google, and\" 29.1 Implementation Details The discriminator M D is the same model as the zero-shot re-ranker. Since the generator M G should be a decoder-only model, in the case of T0, GPT2-Large (Radford et al., 2019) is utilized as the generator. OPT, a decoderonly model, is used as both the discriminator and the generator. We use the start token as \"Please\" for a direct comparison with the manual prompt and fix the beam width B as 10 and the maximum prompt length L as 10 in our experiment.\nEnvironment We conduct all experiments including prompt searching and document re-ranking on V100 32GB GPUs. We use BEIR (Thakur et al., 2021 ) framework 1 for re-ranked result evaluation and passage retrieval datasets. Also, the retrievers, BM25 and DPR, are from the same framework. We employ T0 and OPT with 3B and 2.7B parameters each for the discriminator and the re-ranker publicly open on the Huggingface model hub 2 (Wolf et al., 2020) .\n\nResult\nIn this section, we show the overall results of our method, Co-Prompt, with a detailed analysis. \n\nImpact of Start Tokens\nWe exploit other options of start token such as \"Score\" and \"This\" as shown in Table 2 . Regardless of the start tokens, Co-Prompt consistently generates prompts eliciting the performance of LLM efficiently. However, we observe that finding the optimal start token for the dataset is important to achieve better results.\n\nImpact of Generator\nAs shown in Table 3 , even if different generators are used, the generated prompts by different generators guide the zero-shot re-ranker efficiently. Still, the differences in performance are caused by a vocabulary mismatch between the two modules. We see that, although our method does not vary significantly in performance to the generator, a more suitable generator may be necessary for better results.\nRelevance Score We analyze the distributions of relevance scores between positive or negative document-query pairs. As the negative documents for a given query are retrieved from BM25, the negative ones are related to the query but unable to directly find the answer. As shown in Figure 2 , we point out that the distribution difference exists between pairs despite some overlap. Also, an LLM can distinguish which pair is positive, even without a prompt. However, we observe that the effect of discrete prompt optimization on the zero-shot reranker is in the direction of increasing the mean and variance of the relevance score. \n\nConclusion\nIn this paper, we propose Co-Prompt, left-to-right prompt optimization for zero-shot re-ranker via constrained generation. Co-Prompt effectively restricts prompt candidates and evaluates the optimum of these prompts without any parameter updates. We experimentally show that our method achieves consistently outperforming performance across all experiments. Also, the impact of prompt optimization including baselines on the zero-shot re-ranker highlights its importance. We also present an interesting outcome in that the optimal prompt is interpretable for human. For future work, we plan to expand our method to other open-ended generation tasks using LLMs.\n", "hypothesis": " Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task.  Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results.  While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet.  Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for reranking.  Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update.  The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines.  Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods..", "answer": true}
{"title": "Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information", "content": "\nIntroduction\nKeyphrase extraction is the fundamental task of automatically extracting a set of salient phrases from a document that concisely describes its primary content (Hasan and Ng, 2014; Song et al., 2023a) . Figure 1 shows an example of the source document and its corresponding keyphrases.\nRecent developments in pre-trained language models (Devlin et al., 2019) have heightened the need for utilizing pre-trained embeddings on natural language processing tasks, which significantly improves the performance of embedding-based unsupervised keyphrase extraction models (Sun et al., 2020; Liang et al., 2021; Zhang et al., 2022) . Existing embedding-based models mainly consist of two components: candidate keyphrase extraction and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2021 Song et al., , 2022a)) . The former extracts continuous words from the document as candidate keyphrases through heuristic rules, and the latter estimates the importance of candidate phrases by matching similarity with their corresponding document.\nGenerally, the source document has both salient information and noises (redundant content). Hence, there may be a deviation when directly using the phrase-document relevance as the importance score of each candidate to select keyphrases. For many specific-domain documents (e.g., news or scientific articles), the highlights (the title or the first sentence) typically contains the central information of the source document (as shown in Figure 1 ), which has more significant guidance for extracting keyphrases. However, the recent embedding-based unsupervised keyphrase extraction models ignore the effect of the highlight information, leading to extract wrong keyphrases.\nMotivated by the above issues, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which estimates the impor- \n\nCandidate Keyphrase Extraction\nTo extract candidate keyphrases from the source document, we follow the previous studies (Liang et al., 2021; Song et al., 2022b; Ding and Luo, 2021) At the same time, we use the mean pooling operation to obtain the highlight representation h s of the document.\n\nPhrase-Document Relevance\nTo obtain more relevant candidates, we model the similarity between candidate phrases and the corresponding document as follows,\nEQUATION\nwhere p h i denotes the phrase-document relevance of i-th candidate keyphrases and ||\u2022|| 1 indicates the Manhattan Distance.\nFor news and scientific articles, keyphrases often appear at the beginning or front position (Florescu and Caragea, 2017a,b) , which means that the position information is important and indicative for extracting keyphrases. For example, the word appearing at 2-th, 5-th and 10-th, has a weight \u03c1 i = 1/2 + 1/5 + 1/10 = 0.8. Inspired by the previous work (Florescu and Caragea, 2017b; Liang et al., 2021) , we adopt a position regularization as follows, \u03c1 i = softmax(e 1/i ), where \u03c1 i is the position regularization factor of the i-th candidate phrase. Then, the weighted phrase-document relevance ph i can be re-calculated as follows,\nEQUATION\nHere, we finally employ ph i to estimate the phrasedocument relevance of the i-th candidate phrase.\n\nCross-Phrase Relevance\nGenerally, the phrase-document relevance is calculated between the highlight information and each candidate independently, and consequently, it cannot determine which candidates are better than the Model DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Statistical Keyphrase Extraction Models TF-IDF (Jones, 2004) others. To determine which candidate phrases are more salient than the others, we sum the semantic relatedness between the i-th candidate phrases and all candidates as the cross-phrase relevance. Thus, it calculates the local relevance as follows,\nEQUATION\n)\nwhere \u03b4 i = Mean( j=1,j =i h p i h p j ). Here, we treat \u03b4 i as a de-noisy factor to filter the noises, which is far different from the i-th candidate keyphrase in the document.\n\nRelevance Aggregation\nWe aggregate the phrase-document relevance and the cross-phrase relevance into a whole score as the importance score of each candidate via a simple multiplication,\nEQUATION\n)\nwhere r i indicates the importance score of the i-th candidate phrase. Then, we rank all candidates with their importance score r i and extract top-ranked k phrases as keyphrases of the source document.\n3 Experiments and Results\n\nExperimental Settings\nThis paper conducts experiments on three benchmark and popular used keyphrase datasets, which includes DUC2001 (Wan and Xiao, 2008) , Inspec (Hulth, 2003 ), and SemEval2010 (Kim et al., 2010) . Due to page limits, please refer to the corresponding articles for the details of the three datasets. Following the previous work (Liang et al., 2021; Ding and Luo, 2021; Song et al., 2023b) , we use the standard practice and evaluate the performance of our model in terms of f-measure at the top-K keyphrases (F1@K) and adopt stemming to both extracted keyphrases and gold truth. Concretely, we report F1@5, F1@10, and F1@15 of each model on three benchmark datasets.\nWe adopt the pre-trained language model BERT (Devlin et al., 2019) as the backbone of our model, initialized from their pre-trained weights. In our experiments, \u03bb is set to 0.9 for three benchmark datasets.\n\nOverall Performance\nTable 1 shows the performance of baselines and our model on three benchmark datasets (DUC2001, In-\n\nDUC2001\nInspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Table 2 : The results of different pooling methods for document embedding.\nDifferent Similarity Measures DUC2001 Inspec SemEval2010\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 spec, and SemEval2010). The results show that our method significantly improves over state-of-the-art unsupervised keyphrase extraction baselines. Compared with the current state-of-the-art models, our model achieves significantly better performance on F1@5, F1@10, and F1@15 evaluation metrics, demonstrating the effectiveness of estimating the importance of candidate phrases by leveraging the highlights to calculate the relevance.\nCompared with EmbedRank (Bennani-Smires et al., 2018) , KeyGames (Saxena et al., 2020) , and SIFRank (Sun et al., 2020) , HGUKE achieves significant improvement, which benefits from using the highlights to calculate the importance score of each candidate keyphrase. Compared with the best baseline JointGL, our model achieves better performance on several benchmark keyphrase extraction datasets in all evaluation metrics. The main reason for this improvement is that we use the highlights as the guidance information instead of the whole document when estimating the importance of keyphrases.\n\nAblation Test\nThe ablation experiments on three benchmark keyphrase extraction datasets are shown in Figure 3 . It can be seen from the results that using the highlight information can significantly improve the performance of keyphrase extraction, which benefits from estimating the importance score of each candidate by using its corresponding highlight information rather than the whole document. We consider the main reason is that the title or the first sentence of the document usually has a strong guidance for extracting keyphrases.\n\nImpact of Pooling Methods\nIn this section, we study different pooling methods, including mean-and max-pooling operations. For all pooling methods, HGUKE using the last BERT layer achieves the best results, demonstrating that HGUKE benefits from stronger contextualized semantic representations. We can see the results in Table 2 that the document encoded via the meanpooling operation obtains the best performance.\n\nImpact of Different Similarity Measures\nOur model adopts Manhattan Distance to measure the textual similarity between candidate phrases and the highlight information. Furthermore, we attempt to employ different measures to estimate the phrase-document relevance. The results of different similarity measures are shown in Table 3 , and we can see that the advantage of Manhattan Distance is obvious.\n\nRelated Work\nMost existing unsupervised keyphrase extraction methods can be mainly divided into four categories: statistics-based, topic-based, graph-based, and embedding-based models. Specifically, statisticsbased models (Salton and Buckley, 1988; Witten et al., 1999) usually extract keyphrases by estimating the importance of candidate phrases with different statistic features, such as word frequency feature, phrase position feature, linguistic features of natural language, etc. Topic-based models (Liu et al., 2009 (Liu et al., , 2010) ) typically utilize topic information to determine whether a candidate phrase is a keyphrase. Graph-based models (Mihalcea and Tarau, 2004; Grineva et al., 2009) represent the document as a graph and rank candidate phrases by graph-based similarities.\nEmbedding-based models usually adopt the pretrained embeddings to obtain document and candidate phrase representations and calculate the importance score of each candidate depending on the obtained representations. Benefiting from the development of transformer-based pre-trained language models (Devlin et al., 2019) in the natural language processing field, embedding-based models (Bennani-Smires et al., 2018; Sun et al., 2020; Liang et al., 2021) have achieved outstanding performance. Concretely, embedding-based models mainly consist of two procedures: candidate keyphrase representation and keyphrase importance estimation (Hasan and Ng, 2014; Song et al., 2023a) . The procedure utilizes natural language linguistic features to construct candidate keyphrases and represents them by pre-trained embedding approaches (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) ). The second procedure estimates the importance of candidate phrases from different perspectives to determine whether a candidate phrase is a keyphrase.\nUnlike the existing unsupervised keyphrase extraction models, we use the highlight information of the document to calculate the phrase-document relevance instead the whole document.\n\nConclusion and Future Work\nIn this paper, we incorporate structural information to improve the performance of embedding-based unsupervised keyphrase extraction. Specifically, in this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE), which calculates the phrase-document relevance via the highlight information instead of the whole document to select relevant candidate phrases. Extensive experiments demonstrate that HGUKE outperforms the state-of-the-art unsupervised baselines. Future research may investigate adopting different structural information of the source document to improve the performance of unsupervised keyphrase extraction.\n", "hypothesis": " Keyphrase extraction aims to extract a set of keyphrases with the central idea of the document.  In a structured document, there are certain locations (e.g., the title or the first sentence) where a keyphrase is most likely to appear. However, when extracting keyphrases from the document, most existing embedding-based unsupervised keyphrase extraction models ignore the indicative role of the highlights in certain locations, leading to wrong keyphrases extraction. In this paper, we propose a new Difficult-Guided Unsupervised Keyphrase Extraction model (DGUKE) to address the above issue.  Specifically, HGUKE first models the phrasedocument relevance via the highlights of the documents.  Next, HGUKE calculates the crossphrase relevance between all candidate phrases.  Finally, HGUKE aggregates the above two relevance as the importance score of each candidate to rank and extract keyphrases.  The experimental results on three benchmarks demonstrate that HGUKE outperforms the state-of-the-art unsupervised keyphrase extraction baselines..", "answer": false}
{"title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models", "content": "\nIntroduction\nIn information-seeking conversations, users repeatedly ask questions based on their interests, and the dialogue system provides answers to fulfill their information needs (Stede and Schlangen, 2004; Choi et al., 2018; Reddy et al., 2019) . This scenario is important for addressing real-world open-ended questions, which requires discussions to explore in depth (Dai et al., 2022) , e.g., How to learn more efficiently? Though great progress has been achieved in recent years, most existing researches depend on abundant human annotation, which can be highly costly and limited in knowledge coverage.\nA promising way to alleviate this problem is data augmentation (Chen et al., 2021) . Traditional methods, including token-level manipulation (Kobayashi, 2018; Wei and Zou, 2019) Method DG Data Needs EDA (Wei and Zou, 2019) \u2717 -Back-Translation (Sennrich et al., 2016) \u2717 -SeemSeek (Kim et al., 2022) \u2714 Large Dialog Inpainting (Dai et al., 2022) \u2714 Large AutoConv (Ours)\n\u2714 Few\nTable 1 : The differences between AutoConv and others. DG represents whether the augmentation is document grounded, and Data Needs denotes the scale of human conversations used for augmentation.\nand sentence-level paraphrasing (Sennrich et al., 2016) , improve the linguistic diversity of training data. However, they cannot create conversations grounded on new documents, which are indispensable for dealing with out-of-domain scenarios. Another line of research focuses on simulation-based methods (Wu et al., 2021; Kim et al., 2022) . Specifically, they can iteratively generate conversations grounded on new documents based on a span extractor and an utterance generator. Nevertheless, both the training of the extractor and the generator still require abundant human dialogues. Besides the above ways, Dai et al. (2022) propose Dialog Inpainting, which creates information-seeking dialogues by inserting utterances between neighboring sentences in documents. One potential risk is the gap between the structure of documents and that of conversations. Documents are tighter, while realworld conversations are more open-ended. To alleviate the above issues, we propose a simple yet effective method AutoConv for Automatically generating information-seeking Conversations, which takes advantage of the fewshot learning ability and generation capacity of large language models (LLM) (Brown et al., 2020) . Specifically, we formulate conversation generation as a language modeling task and utilize an LLM for generating synthetic conversations grounded on external documents. Surprisingly, finetuning with a few human dialogues can help LLM capture the characteristics of the information-seeking process (e.g., grounding, question answering) and generate high-quality synthetic conversations. Then, we can train a small task model with these dialogues.\nThe differences between AutoConv and others are shown in Table 1 .\nWe conduct comprehensive experiments on two frequently-used datasets QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) in the low-resource setting, where only dozens of human dialogues are available. The results show that AutoConv has substantial improvements over several strong baselines. When scaling up the synthetic dialogues, AutoConv has the improvement of up to 5.06 F1 gain compared with directly finetuning, and thus largely reduces the labor force for annotation. In addition, we find that the small task model trained with synthetic dialogues can even surpass finetuned LLM with only 1.7% parameters. Moreover, we also investigate the impact of decoding strategy and scaling laws for AutoConv.\n\nTask Formulation\nOur goal is automatically generating informationseeking conversations. Specifically, each conversation is grounded on a document d and consists of a series of user questions and system answers.\n\nConversation Generation\nTraining. We formulate conversation generation as a language modeling task and finetune 1 an LLM with a few human dialogues (e.g., 50 from QuAC (Choi et al., 2018) ) to capture the characteristics of information-seeking conversations (e.g., grounding, question answering). The objective is the negative log-likelihood of each utterance:\nL = \u2212 T t=1 L l=1 log P (u t l |u t <l , h <t , d),\nwhere u represents a user question or a system answer, h is the dialogue history, L and T are the number of tokens and turns respectively.\nGenerating. Based on the finetuned LLM, we can generate synthetic dialogues with unlabeled documents, as in Figure 1 . In information-seeking scenarios, user questions are typically open-ended. Thus we choose nucleus sampling (Holtzman et al., 2020) for generating user questions, which has shown great performance in various open-ended generation tasks (Su et al., 2022) . However, when applying a sampling decoding strategy for system answer generation, we find it results in the \"hallucination\" problem (Shuster et al., 2021) , where the generation is plausible but factually incorrect based on the document. To this end, we utilize greedy search for answer generation. Neural language models often generate the same sentences repetitively (Xu et al., 2022) . To alleviate this problem, we first compute the diversity score of each synthetic dialogue as in Su et al. (2022) , which considers the repetition at different n-gram levels.\nThen, we filter out dialogues based on this score.\nAfter that, a two-stage training strategy is adopted (Xie et al., 2020b) for training a small task model. Specifically, we first pre-train it on the synthetic dialogues, then finetune it on the human dialogues used for finetuning the LLM. More training details are given in Appendix B.\n\nExperiments\nWe conduct experiments on QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) , more details about them are shown in Appendix A.\n\nImplementation\nWe focus on the low-resource setting, where human dialogues are scarce. To simulate this setting, we randomly sample a few human dialogues from the training set of QuAC or CoQA, and use them for finetuning the LLM. We use OPT-13B (Zhang et al., 2022) as the LLM and UnifiedQA-V2-base (222M) (Khashabi et al., 2022) as the small task model. All data augmentation methods use the same training strategy and small task model. More implementation details are shown in Appendix B.\n\nComparison with Baselines\nWe compare AutoConv with a series of baselines, and the details of them are given in Appendix C. As\nMethod QuAC CoQA F1 EM F1 EM\nPrompting GPT-3 Zero-shot (Brown et al., 2020) 41.5 -81.5 -GPT-3 Few-shot (Brown et al., 2020) 44.3 -85.0 -Data Augmentation (50 Human Dialogues) Finetuning Table 2 : Comparison with baselines. All experiments are performed 4 runs with different random seeds. Finetuning means directly training with only human dialogues. All data augmentation methods use the same human dialogues and the same number of synthetic dialogues for the sake of fairness (5 times the number of human dialogues). Human annotation represents replacing the synthetic dialogues with the same number of human dialogues.\nshown in Table 2 , AutoConv achieves better performance than GPT-3 prompting on QuAC with only 0.13% parameters and 50 human dialogues, but is less competitive on CoQA. We conjecture the reason stems from the intrinsic difference between the two datasets. CoQA contains more factoid questions, and the answers are named entities or short noun phrases like those in SQuAD (Rajpurkar et al., 2016) . By training on large-scale text corpus from a web forum, GPT-3 might implicitly learn the format and structure of question answering (Sanh et al., 2022) , and thus gets excellent performance on CoQA. On the other side, QuAC has more openended and exploratory questions as in natural conversations, and 86% questions are contextual (Choi et al., 2018) . Therefore, it brings more difficulties for GPT-3 inference with few demonstrations, while our method learns better from both human dialogues and synthetic dialogues.\nCompared with data augmentation methods, Au-toConv achieves the best performance on both datasets and mitigates the gap between synthetic dialogues and human upper bounds. We find that the token-level augmentation method EDA and the sentence-level augmentation method Back-Translation even hurt the performance, which is The number of synthetic dialogues. One possible reason is that they bring too much noise. Dialog Inpainting (Dai et al., 2022) gets ordinary performance, and the reason possibly derives from the gap between the structure of natural conversations and that of the documents used for constructing synthetic dialogues.\n\nScaling up Human Dialogues and Synthetic Dialogues\nIn this part, we further analyze the performance of AutoConv when scaling up the human dialogues and synthetic dialogues. As shown in Figure 2 , the \n\nComparison with Finetuned Large Language Model\nAutoConv is a kind of symbolic knowledge distillation (West et al., 2022) , where the finetuned large language model (LLM) transfers its knowledge to the small task model (STM) by generating synthetic dialogues for the training of STM. Here, we further investigate the effectiveness of AutoConv from the aspect of knowledge distillation. As shown in Table 3 , finetuned LLM has substantial improvements over finetuned STM. However, it brings large memory and computation cost. On the other side, our AutoConv not only keeps the efficiency of STM, but also boosts the performance. Surprisingly, Au-toConv even outperforms its teacher model in the 200 human dialogues setting. Similar observations are found in West et al. (2022) ; Ye et al. (2022) , while they focus on different tasks. We leave the analysis of this novel observation for future work.\n\nImpact of Decoding Strategy\nDuring our preliminary experiments, we find that the decoding strategy is important for system answer generation. More precisely, we evaluate the answer generation performance of LLM with different decoding strategies on QuAC, and the results are shown in based decoding strategies for answer generation.\nCompared with beam search, greedy search shows competitive performance and is more efficient. Thus we use greedy search by default in this paper.\n\nScaling Laws\nWe further analyze how the benefit of AutoConv is affected by the scale of LLM. As shown in Figure 3 , the performance gets better with a larger model across a various number of synthetic dialogues. In addition, when the LM is small (350M) and with limited generation ability, the synthetic dialogues can even hurt the performance when the available human dialogues are scarce. Due to the limitation of computational resources, we limit our investigation to 13B parameters and leave larger models for future work.\n\nCase Study\nIn Table 5, we present an example of our synthetic conversation for the case study. The original document describes the singer Ciara's second studio album and her acting debut. The conversation consists of seven user questions and seven system answers, covering the title and sales of the album, the duration of the tour, etc. As we can see from this (Choi et al., 2018) .\nexample, the user questions are diverse (e.g. what, how, did, etc.) and the conversation is informative and conversational. For example, when the system mentions \"tour\" (the fifth system utterance), the user follows by asking \"How long was the tour?\".\n\nError Analysis\nTo further analyze the limitation of our method, we conduct an error analysis by manually investigating 50 synthetic conversations generated by AutoConv, which is finetuned with 50 human conversations from QuAC (Choi et al., 2018) . Particularly, we find that only 5% generated questions are not suitable (e.g., misspelled names). The reason stems from the open-ended characteristic of natural conversation that many kinds of user questions are possible under the same context. However, nearly 40% of system answers are not perfect, and we summarize the wrong answers into four major classes:\n(1) Irrelevant: 75% of them are totally irrelevant to user questions.\n(2) Related but not Accurate: 14% of them contain related knowledge from the grounded documents, but the answers are not accurate. Take an example in Table 5 , the second user question asks for the name of the album, which is Ciara: The Evolution according to the document. While the LLM generates the interpretation of the album name by mistake.\n(3) Missing: 4% of them belong to the missing error that the system answers are \"No Answer\", while the questions actually can be answered based on the documents. (4) Hallucination: 3% of them mention hallucination knowledge, which cannot be found in the documents. In addition, we also notice that AutoConv is more likely to generate wrong answers when grounding on longer and more complex documents.\n\nConclusion\nIn this paper, we propose a simple yet effective method, AutoConv, which formulates the conversation generation problem as a language modeling task. Then, based on a large language model and a few human dialogues, AutoConv can generate synthetic dialogues with high quality. Experimental results on both QuAC and CoQA verify the effectiveness of AutoConv, which alleviates the human efforts for annotation largely. Furthermore, we also provide case study and error analysis to prompt future research.\n", "hypothesis": " Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years.  However, the research is still stymied by the scarcity of training data.  To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM).  Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality.  Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation.  In addition, we also provide several analysis studies to promote future research..", "answer": true}
{"title": "A Holistic Approach to Reference-Free Evaluation of Machine Translation", "content": "\nIntroduction\nMachine translation evaluation has conventionally relied on reference, where outputs are compared against translations written by humans. This is in contrast to the reference-free manner in which translation quality is directly assessed with the source text. Reference-free evaluation (Napoles et al., 2016; Thompson and Post, 2020; Agrawal et al., 2021) has the potential to free the evaluation model from the constraints of labor-intensive annotations, allowing it to pivot easily to new domains. In this way, reference-free evaluation metrics are substantially more scalable and have lately been in the spotlight.\nThe history of reference-free evaluation for MT can trace back to \"QE as a Metric\" track of \u02daEqual contribution. : Corresponding author. 1 https://github.com/cocacola-lab/ Reference-Free-Evaluation-of-Machine-Translation. git WMT2019 Metrics Task (Ma et al., 2019) . YiSi-2 (Lo, 2019) and XBERTScore (Zhang* et al., 2020; Leiter, 2021) are embedding-based methods that adopt contextual word embeddings to calculate the lexical similarity between the source and candidate translation words. Quality estimation (Fonseca et al., 2019) system metrics such as UNI+ (Yankovskaya et al., 2019) and COMET-QE (Rei et al., 2020a (Rei et al., , 2021 ) also leverage contextual word embeddings and feed them into a feedforward network. However, they are trained to regress on human scores that are expensive to collect, and gross discrepancies exist when different humans are asked to label the scores.\nMore challenging but worthwhile, we focus on dispensing with references as well as human scores. Nevertheless, embedding-based methods are limited to token-level semantic similarity while neglecting sentence-level faithfulness (Song et al., 2021) . Besides, it's difficult for word embeddings to discriminate matched word pairs from random ones (Zhao et al., 2020a) .\nIn addition, current reference-free evaluation methods rarely take fluency into account. For the unfluent candidates whose content is roughly consistent with the source, the embedding-based metrics can hardly discriminate and provide accurate evaluation scores 2 . Moreover, the general goal of evaluation metrics is to estimate not only the semantic equivalence between source and candidate but also the general quality (i.e., fluency and naturalness) (Banchs et al., 2015; Feng et al., 2020; Yuan et al., 2021) .\nIn this work, we propose a holistic approach (i.e., ReFreeEval) to enhance the evaluation model in aspects of fluency and faithfulness, meanwhile on both word and sentence levels. With regard to fluency, we pose a data augmentation method and train a fluency discrimination module. For word-level faithfulness, we adopt a self-guided contrastive word-alignment method. For sentencelevel faithfulness, we execute knowledge distillation with SBERT (Reimers and Gurevych, 2019) to capture more fine-grained semantics. Our method builds on the framework of XBERTScore. Extensive experiments spanning WMT18/19/21 Metrics (Ma et al., 2018 (Ma et al., , 2019;; Freitag et al., 2021) segment-level daRR and MQM datasets demonstrate that our proposed reference-free approach, ReFreeEval, outperforms SOTA reference-free metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.\n\nApproach\nReference-free evaluation of MT can be characterized as two aspects: (1) fluency: how well it conforms to normal human language usage; and (2) faithfulness: how well the translated text reflects the source data. We assess faithfulness at different granularity: word level and sentence level. Figure 1 is the illustration of our ReFreeEval method.\n\nSentence-Level Fluency\nWe explore a data augmentation method to perturb the fluency of target sentences with noise which is difficult to be identified. Then we train a fluency discrimination module with contrastive learning (Gao et al., 2021; Zhang et al., 2021; Wu et al., 2022; Wang et al., 2022) to distinguish fluent samples from perturbed samples (namely, challenging negative samples).\n\nData Augmentation Using Clause Permutation\nA complex or compound sentence 3 has two or more clauses and relative clauses that are joined together with conjunctions or punctuation. As logical relations exist between these clauses, we manipulate and permute the clauses separated by punctuation, instead of words. In this way, the meaning is preserved inside the clauses, meanwhile, the sentence is often unfluent and unnatural. Similar to complex and compound sentences, for a simple sentence with only one clause 4 , we randomly split it into two fragments and permute the two fragments. Compared to permutation on the token level, clauselevel permutation has less influence on sentence fluency and semantic change. The clause-based permutation method brings perturbed samples that are more challenging and hard to be recognized.\n\nFluency Discrimination\nWe denote a source and target sentence in parallel data as x and y. Perturbed samples augmented from y are \u01771 , \u01772 , ..., \u0177k . A reliable metric has the ability to give the original fluent target y a higher evaluation score than those k perturbed unfluent samples.\nAs for the score, we adopt the same calculation measure as BERTScore but replace the pre-trained monolingual model (Devlin et al., 2019; Liu et al., 2019) with a cross-lingual model (Devlin et al., 2019; Conneau et al., 2019) to do reference-free evaluation (Zhou et al., 2020; Song et al., 2021) denominated as XBERTScore (Leiter, 2021) . We use 9th layer of XLM-Roberta-Base to extract contextual word embeddings. Here we only use F BERT as evaluation score between source x and targetside y or \u0177i , which is represented as s w px, yq or s w px, \u0177i q. Then we can obtain word-level faithfulness scores s w px, yq, s w px, \u01771 q, ..., s w px, \u0177k q of pk `1q pairs.\nIn order to discriminate fluent sentences from perturbed ones according to these scores, we treat the original target and its corresponding perturbed samples as opposite and assign them 1/0 hard labels. The cross-lingual model which produces XBERTScore is trained to classify target-side sentences with a cross-entropy loss function. The objective function on N training samples is as follows:\nL f l \" \u00b41 N \u00ff x,y log e swpx,yq\ne swpx,yq `\u0159k i\"1 e swpx,\u0177 i q\n(1)\n\nWord-Level Faithfulness\nAs for word-level faithfulness, each word in the source sentence should have a corresponding crosslingual representation in the target sentence and each word in the target sentence should be an accurate translation of its source word. This motivates us to do word-alignment training to enhance wordlevel evaluation. This module shares similar architecture with sentence-level fluency where word embeddings are derived from 9th layer of XLM-Roberta-Base.\nWe take the same steps as (Dou and Neubig, 2021) to extract alignments. First, we compute the dot product between source and target word embeddings to obtain the similarity matrix S. Then S is normalized in source and target dimensions. And we get source-to-target alignment matrix S xy and target-to-source alignment matrix S yx . A source/target token and a target/source token whose similarity value in alignment matrix S xy /S yx exceed threshold c 1 are regarded as aligned. The bidirectional alignment matrix A is deduced:\nEQUATION\nA ij \" 1 means x i and y j are aligned. Dou and Neubig (2021) also propose the self-training objective to align words with this bidirectional alignment, which improves alignment performance most.\nBased on this objective, we adopt a self-guided contrastive cross-lingual word-alignment method. By contrast, we not only pull semantic aligned words to have closer contextual representations but also push unrelated words away (Luo et al., 2021; Su et al., 2022; Meng et al., 2022) , which encourages the model to discriminate matched word embeddings from semantically unrelated ones.\nThe source token and target token are deemed to be unrelated if their similarity value is low. In our method, these unmatched pairs constitute negative samples and are pushed away. Moreover, we set threshold c 2 to further restrict the negative samples. The unmatched pairs whose similarity value is lower than c 2 are discarded from negatives as this unmatched relation can be easily distinguished by the model. In this way, we can control the difficulty of negative samples and only preserve those indistinguishable ones (hard negatives) to train the model.\nB \" pS xy \u0105 c 2 q \u02dapS T yx \u0105 c 2 q (3)\nB ij \" 1 means x i and y j are aligned or a part of hard negatives, which are preserved to train.\nIn Figure 1 , the dark blue positions mean bidirectional alignment while the light blue positions are hard negative examples.\nFinally, based on two dimensions of source and target, the positive and negative samples mentioned above, we construct a self-guided contrastive learning objective function on the word level as follows:\nEQUATION\nL word \" L x `Ly (6)\n\nSentence-Level Faithfulness\nThe main idea is to improve sentence-level faithfulness evaluation. Concretely, we distill sentencelevel semantic meaning from SBERT into the wordlevel shared model. We use SBERT to extract semantically meaningful sentence embeddings. Sentence semantic similarity between x and y is calculated with cosinesimilarity between sentence embeddings x and y:\nEQUATION\nThe semantic similarity reflects the sentencelevel faithfulness from target to source. Then we can obtain sentence-level faithfulness scores s s px, yq, s s px, \u01771 q, ..., s s px, \u0177k q. We use KLdivergence as the objective function to reduce the discrepancy between sentence-level and word-level similarity:\nL f a \" \u00ff x,y 1 PYx s s px, y 1 q log s s px, y 1 q s w px, y 1 q (8)\nIn this distillation module, SBERT plays a role of a teacher. Sentence-level semantic knowledge is distilled into the word-level shared model through these sentence-level faithfulness scores. In this way, evaluation is no longer limited to word level but incorporated sentence semantics.\nOn the other hand, SBERT plays a role as a corrector. It is unreasonable that a disturbed sample with slightly changed semantics is considered to be completely contrary to the original sentence. We correct the binary classification and convert the 0/1 discrete value in the fluency discrimination module to continuous variables.\nFor sentence-level training, we combine fluency with faithfulness. This joint architecture is motivated by (Ren et al., 2021) . The objective is:\nL sent \" L f l `\u03b1L f a (9)\n\u03b1 is a hyper-parameter to control the weight that the sentence-level faithfulness module accounts for.\n3 Experiment Embeddings We use the 9th layer of XLM-Roberta-Base to extract contextual word embeddings. This follows the default setting of BERTScore 6 . For sentence embeddings, we adopt xlm-r-bert-base-nli-stsb-mean-tokens model 7 the same as SentSim.\nBaselines For reference-based metrics, we choose sentBLEU (Papineni et al., 2002) and YiSi-1 (Lo, 2019) . For reference-free metrics, we choose XBERTScore (Leiter, 2021) , YiSi-2 (Lo, 2019) , SentSim (Song et al., 2021) and BERTScore-MKD (Zhang et al., 2022) . Most results of baseline models are reported in the original paper (Ma et al., 2018 (Ma et al., , 2019;; Freitag et al., 2021; Zhang et al., 2022) . We also implement experiments that have not been reported, such as XBERTScore, SentSim and BERTScore-MKD. \nFor WMT21 segment-level evaluation, conventional Kendall-tau statistic is used to measure the correlation between our scores and MQM scores.\n\nResults\nThe main results are displayed in Table 1 , 2, 3. First, we observe that fluency, word-level faithfulness, and sentence-level faithfulness module improve the evaluation performance respectively. We also find that the main improvement comes from sentencelevel fluency indicating that XBERTScore as a token-level evaluation metric lacks sentence-level knowledge. Then, the ensemble model combining the advantages of the three modules achieves even better results. And compared with some referencebased baselines it achieves comparable results or even outperforms them. More details of experimental results are in Appendix C.4.\n\nConclusion\nWe propose a reference-free evaluation approach ReFreeEval that comprehensively considers three aspects: aspect. ReFreeEval, combining the above three modules, achieves a higher correlation with human judgments, outperforming current SOTA referencefree metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.\n", "hypothesis": " Traditional machine translation evaluation relies on references written by humans.  While reference-free evaluation gets rid of the constraints of labor-intensive annotations, it can pivot easily to new domains and is more scalable. In this paper, we propose a referencefree evaluation approach that characterizes evaluation as two aspects: (1) fluency: how well the candidate translation conforms to normal human language usage; (2) faithfulness: how well the candidate translation reflects the source data. We further split the fluency into word-level and sentence-level.  Extensive experiments spanning WMT18/19/21 Metrics segment-level daRR and MQM datasets demonstrate that our proposed reference-free approach, ReFreeEval, outperforms SOTA reference-free metrics like YiSi-2, SentSim and BERTScore-MKD in most language directions.  The code can be found at ReFreeEval Repo 1 ..", "answer": false}
{"title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "content": "\nIntroduction\nRecently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019 ; * Equal Contribution Barocas et al., 2017; Kurita et al., 2019) . Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\nPrevious work (Nadeem et al., 2021) , (Nangia et al., 2020a) , (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021) , we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop bias mitigation strategies feasible.\n\u2022 We introduce a novel data sampling technique that utilises LLMs to mine for the most biased samples from a dataset and can benefit existing state-of-the-art debiasing methods. When used for debiasing a model, these few samples serve as exemplars and induce large reductions in gender bias.\n\nGender Bias in BERT Predictions\nMost-biased data samples ____ is very good at cooking but not great at work. \n\nReduced Gender Bias in BERT Predictions\nFigure 1 : Our method can be summarized as a combination of bias discovery and mitigation. First, we use a pre-trained LLM to find the most gender-biased samples. Then, we apply our data intervention techniques and use these modified training samples to fine-tune the model. Experiments show that our method is very effective at reducing gender bias, outperforming three state-of-the-art baselines and being comparable to two other baselines.\n\nRelated Work\nIn recent years, there has been growing concern about the bias/stereotypical discriminatory behavior by NLP models, particularly concerning gender. Several studies have investigated the presence of gender bias in various NLP tasks and proposed methods for mitigating it.\nOne line of research has focused on analyzing the extent of gender bias in pre-trained language models such as BERT and GPT-2. These studies have found that these models exhibit a significant amount of gender bias in their word embeddings for BERT (Jentzsch and Turan, 2022) and for GPT-2 (Kirk et al., 2021) and are prone to making stereotypical gender-based predictions (e.g., assuming that a doctor is male and a nurse is female). A standard evaluation metric used in this line of research is Stereotype metrics such as StereoSet (Nadeem et al., 2021) , which evaluates the model's ability to predict gender stereotypes and CrowS pairs (Nangia et al., 2020b) which measure whether a model generally prefers more stereotypical sentences. A similar line of work is gender bias tests proposed in BIG-bench (Srivastava et al., 2022) . The tests assess the language model's gender biases, stereotypes, and ability to infer gender information. It evaluates gender bias and stereotype between male and female, and gender minority bias and stereotype between majority and minority. It also examines the model's language modeling performance, which can be affected during de-biasing.\nAnother line of research has proposed methods for debiasing these models. These methods can be broadly categorized into two groups: data-based and algorithm-based. Data-based methods aim to reduce bias by removing or altering biased words from the training set. In contrast, algorithm-based methods aim to modify the model's architecture or training procedure to reduce bias. One popular databased method is \"uncertainty sampling\" (Lewis and Gale, 1994) , where the model is trained on the instances that it is most uncertain about, which can help to reduce bias by forcing the model to learn from a diverse set of examples. A popular algorithmbased method is \"Adversarial Debiasing\" proposed by Zhang et al. (2018) , which fine-tunes the model using an adversarial loss to make it less sensitive to sensitive attributes such as gender. OSCar proposed by Dev et al. (2021) , is another algorithm based method that utilizes the idea of disentangling \"problematic concepts\" like occupation and gender relationship instead of removing them altogether. MABEL (He et al., 2022) has both algorithm and data-based components, as it first augments the training data by swapping gender words and then applies a contrastive learning objective and alignment via entailment pairs. Their data augmentation strategy is similar in spirit to the data intervention techniques we propose, however our analysis does not require training auxiliary models and uses significantly lesser data.\nData-based methods include the \"Equalization\" technique proposed by Bolukbasi et al. (2016) , which aims to equalize the representation of genderspecific words in the embedding space, the \"Counterfactual Data Augmentation\" (CDA) method proposed by Zimmermann and Hoffmann (2022) Pre-trained LLMs are biased towards different genders, as seen in a simple mask-fill experiment using BERT. (Here, and in the rest of the paper, we assume a binary treatment of gender for simplicity.) The task is then to mask out the gender-related nouns and pronouns (such as he, she, her, woman, etc.) and get BERT to predict the masked words for the affected sequences in the dataset. Here, we consider a fixed list of gender-specific words curated from previous work (Lu et al., 2018; Zmigrod et al., 2019) and neutral words list 1 . We finally compute the \"total confidence difference\" as the sum of differences in the model's prediction confidence for each gender-word pair (such as confidence of predicting he \u2212 she, man \u2212 woman, etc.). Formally, we define total confidence difference as\nEQUATION\n))| where f (x) represent the confidence of model's prediction, N is the total number of tokens in the dataset and x is the tokenized gender word. The higher this number, the more biased the model is concluded to be. We compute the metric at token level and ensure that each of the gender word gets tokenized into exactly one token by initially extending the tokenizer with our gender word list. The top 3 biased gender-word pairs in StereoSet are shown in Table 1 . Intuitively, our technique for gauging bias in LLMs is sensitive to the fixed word list used to represent the sensitive attributes (here, gender). In Table 2 , we show the number of words covered by the word list used for both WikiText-2 and StereoSet datasets.\n\nData Interventions\nIn order to reduce gender bias in pre-trained models, we carefully select diverse and hard-biased examples and then replace gender words with more neu- tral or equality-focused phrases. This is achieved by using a wordlist to find gender terms in sentences and then segregating words as name and non-name words.\nWe call our initial approach naive-masking as it does not require a word list for mapping gender words to gender-neutral words. Instead, it replaces all gender words with the fixed word \"person.\" In our next approach, neutral-masking, we swap words in a slightly more semantically accurate manner. In this, we use a word-pair list that goes from gender words to gender-neutral words. With both approaches, we intend to introduce new words in a model's vocabulary to make it more likely to choose a more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vocabulary of the model and try to balance the confidence of prediction on opposite-gender words by using phrases instead. Thus, we call our final approach random-phrase-masking as we instead substitute words with phrases that reflect the equality of gender. This approach not only reduces gender bias but also preserves the original meaning of the sentence in most cases. In our approach, we chose the phrases and order of gender words at random with equal probability. Additionally, we hypothesize that the choice of the dataset for fine-tuning is also essential. We choose two datasets: the WikiText-2 (Merity et al., 2017) dataset, which has implicit gender bias since its sources from Wikipedia articles, and the Stere-oSet dataset (Nadeem et al., 2021) , which has explicit/more gender bias as it has been designed to evaluate gender bias. WikiText-2 2 has 600 train articles and roughly 2M tokens while StereoSet 3 (dev) has 2123 samples out of which we only consider 800 samples which are not unrelated. Naturally, our data intervention method should work better on a dataset with training examples with gender bias while being devoid of meaningful gender associations like \"She needs a gynecologist,\" where the gender of the person is important. By testing our method on both datasets, we can understand the sensitivity of our approach to the quality of training samples used.\n\nBias Evaluation Metrics\nWe focus on evaluating the bias of a model while also measuring its language modeling capability. The ideal model would not just be one with the least bias but also one which does not compromise its language modeling performance. The dual estimation of bias and performance of a model was proposed in the StereoSet benchmark (Nadeem et al., 2021) , with the Language Modeling Score (LMS) measuring the percentage of times a meaningful token is predicted for the mask as opposed to a meaningless token, the Stereotype Score (SS) measuring the percentage of times the model predicted a stereotypical word as compared to an anti-stereotypical word, and an idealized CAT score (ICAT) combining the LMS and SS score into a single metric. An ideal model has an ICAT score of 100, while the worst biased model has an ICAT score of 0. We additionally evaluate the CrowS-Pairs benchmark (Nangia et al., 2020a) , which captures data with greater diversity in both the stereotypes expressed and the structure of sentences (50 is ideal). However, we note that the Crow-S benchmark is much more limited compared to StereoSet (Nadeem et al., 2021) in terms of both the volume and variety of linguistic phenomenon relating to gender bias it covers.\n\nExperiments\nWe compare our proposed interventions with five baselines, 4 of which are state-of-the-art methods and the original pre-trained model. Our first baseline is the application of dropouts to neural networks, Dropout proposed by (Webster et al., 2020) . Next, we consider an algorithmic de-biasing technique INLP technique proposed by (Ravfogel et al., 2020) . Then, we consider a sentence embedding de-biasing approach SentenceDebias (Liang et al., 2020). Finally, we consider a data-based approach CDA (Zmigrod et al., 2019) that is closest to our work. For a fairer comparison, we run the baselines with the same size (100) of the training set as our method. For all of our experiments, we consider the \"bert-base-uncased\" pre-trained model available from HuggingFace. For fine-tuning our model, we select a varying number of most-biased training samples (10, 50, and 100) from the WikiText-2 and StereoSet (we only use the dev set) datasets, as discussed in section 4. We also compare this to a random selection of data points as an ablation study. On the selected dataset, we apply our interventions and obtain the modified dataset, which is then used to fine-tune our pre-trained model using masked language modeling (MLM) loss. The key point is that we only fine-tune the model on the gender words conditioned on the remaining text, significantly reducing the fine-tuning time. We perform ablations on various types of interventions as discussed in Table 7 . The model is trained for 30 epochs, with a learning rate of 0.001 and AdamW optimizer. We ran all of our experiments on NVIDIA Tesla T4 GPU on Google Colab for roughly 48 hours. For all experiments, we report the numbers as the mean and standard deviations (6) of 3 different runs. Our experiment code can be found here. 4 \n\nResults\nTable 4 shows the StereoSet and Crow-S scores for our baselines and our best-performing interventions on the WikiText-2 Dataset. In the StereoSet benchmark, we observe that random-phrase-masking obtains lower SS than all other baselines. On the Crow-S benchmark, random-phrase-masking does better than thre of the baselines except Sen-tenceDebias which achieves slightly better scores. While random-phrase-masking results in lower SS scores than neutral-masking, it also obtained [MASK] is very good at cooking but not great at [MASK] work. she is very good at cooking but not great at her work.\nhe is very good at cooking but not great at farm work. Being a mother is not easy since she will have to stay home and take care of the child.\nBeing a father is not easy since one will have to stay home and take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\n\nShe is very caring and kind but not good at what she does.\nHe is very caring and kind but not good at what he does.\n\nInput Sentence:\nOutput of Biased Model:\n\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to modify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the same pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the our method is successfully able to reduced biased substitutions. very low LMS scores. We attribute this performance degradation to the blunt substitution of phrases that our method uses, which might lead to odd-sounding sentences. In the Crow-S benchmarks, we see similar behavior and find that random-phrase-masking does better than neutral-masking. Since we believe that our method is sensitive to the choice of the dataset, we also present results on the StereoSet (dev) dataset 6. In Figure 2 , we perform a qualitative analysis of our proposed approach and find that random-phrase-masking is able to flip the predictions on fill-mask tasks for stereotypical sentences.\n\nConclusion\nIn this paper, we show that simple data interventions on limited training data effectively reduce gender bias in LLMs. We also show that a biased pretrained LLM can be used to mine the most effective de-biasing training examples. Evaluation of our methods on state-of-the-art bias benchmarks empirically suggests that our methods effectively reduce gender bias. Given that our methods can work in a few-shot manner and do not require any auxiliary model training, we hope that our work benefits further research in the domain of human-in-the-loop bias mitigation techniques by making the creation of bias mitigation datasets feasible.\n", "hypothesis": " Caution: this paper contains potentially offensive or upsetting model outputs.\nSocietal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people.  Since large-scale retraining of these models from scratch is both time and computeexpensive, a variety of approaches have been previously proposed that de-bias a pre-trained model.  While the majority of current state-ofthe-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pretrained models.  Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced.  Since our proposed method only needs a few training examples, our fewshot debiasing approach is highly feasible and practical.  Through extensive experimentation, we show that our debiasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability..", "answer": true}
{"title": "Typo-Robust Representation Learning for Dense Retrieval", "content": "\nIntroduction\nDense retrieval is a fundamental component in many information retrieval applications, such as open-domain question answering and ad-hoc retrieval. The objective is to score and rank a large collection of candidate passages based on their similarity to a given query. The performance of dense retrieval relies on representation learning. A popular approach is to finetune a pre-trained language model to create an embedding space that puts each query closer to its corresponding passages (Zhan et al., 2020; Khattab and Zaharia, 2020; Xiong et al., 2021; Qu et al., 2021; Ren et al., 2021a,b) .\nOne of the major challenges of dense retrieval is the handling of misspelled queries which induces representations of the misspelled queries to be closer to irrelevant passages than their corresponding passages. Several studies have demonstrated that misspellings in search queries can substantially degrade retrieval performance (Zhuang and Zuccon, 2021; Penha et al., 2022) , specifically when informative terms, such as entity mentions, are misspelled (Sidiropoulos and Kanoulas, 2022) .\nTo create a retrieval model that is capable of handling misspelled queries, researchers have proposed different training methods to align representations of misspelled queries with their pristine ones. Zhuang and Zuccon (2021, 2022) devise augmentation methods to generate misspelled queries and propose training methods, Typos-aware Training and Self-Teaching (ST), to encourage consistency between outputs of misspelled queries and their non-misspelled counterparts. Alternatively, Sidiropoulos and Kanoulas (2022) apply contrastive loss to enforce representations of misspelled queries to be closer to their corresponding non-misspelled queries. Although these methods can improve the performance of retrieval models for misspelled queries, there is still a substantial performance drop for misspelled queries.\nIn this paper, we propose a training method to improve dense retrieval for handling misspelled queries based on the following desired properties:\n\u2022 Alignment: the method should be able to align queries with their corresponding passages. \u2022 Robustness: the method should be able to align misspelled queries with their pristine queries. \u2022 Contrast: the method should be able to separate queries that refer to different passages and passages that correspond to different queries. In contrast to the existing methods for handling misspelled queries that only satisfy the Alignment and Robustness properties, our method also aims to satisfy the Contrast property. Increasing the distance between dissimilar queries should help distinguish misspelled queries from other distinct queries. We design the following components for our training method: (i) Dual Self-Teaching (DST) incorporates the ideas of Dual Learning (Xia et al., 2017; Li et al., 2021) and Self-Teaching (Zhuang and Zuccon, 2022) to train robust dense retrieval in a bidirectional manner: passage retrieval and query retrieval. (ii) Query Augmentation generates a numerous number of misspelling variations for each query to supply our training objective.\nExperimental studies were conducted to assess the efficiency of the proposed method in comparison to existing approaches. We conduct experiments based on two different pre-trained language models. We evaluate using two passage retrieval benchmark datasets, a standard one and a specialized one for misspellings robustness evaluation. For each dataset, we measure performance on both misspelled and non-misspelled queries, where the misspelled queries are both generated and realworld queries. The experimental results show that the proposed method outperforms the best existing methods for enhancing the robustness of dense retrieval against misspellings without sacrificing performance for non-misspelled queries.\nWe summarize our contributions as follows: \u2022 We propose a novel training method to enhance the robustness of dense retrieval against misspellings by incorporating three desired properties: Alignment, Robustness, and Contrast. \u2022 We introduce Dual Self-Teaching (DST) which adopts the idea of Dual Learning and Self-Teaching to learn robust representations. In addition, we propose Query Augmentation to generate multiple views of a particular query under different misspelling scenarios. \u2022 We evaluate our method on misspelled and nonmisspelled queries from two passage retrieval datasets. The results show that our method outperforms the previous state-of-the-art methods by a significant margin on misspelled queries.\n\nMethodology\nWe propose a training pipeline to enhance the dense retrieval capability for handling spelling variations and mistakes in queries. As shown in Figure 1 , the training pipeline comprises three steps. (i) Query Augmentation: we augment each query in the training set into multiple misspelled queries using the typo generators provided by Zhuang and Zuccon (2021) . (ii) Similarity Score Calculation: we compute similarity score distributions between queries and passages for passage retrieval and query retrieval tasks using in-batch negative queries and passages, with additional hard negative passages.\n(iii) Dual Self-Teaching Loss Calculation: we compute the DST loss using the similarity score distributions to achieve all three desired properties.\n\nQuery Augmentation\nThe purpose of this step is to guide the learning with a broad array of possible misspelling patterns. Let Q denote a set {q 1 , q 2 , ..., q N } of N queries. From all queries in Q, we generate a set of K \u00d7 N misspelled queries\nQ \u2032 = {\u27e8q \u2032 1,k , q \u2032 2,k , ..., q \u2032 N,k \u27e9} K k=1\n, where K is the misspelling variations. We use five typo generators proposed by Zhuang and Zuccon (2021) , including: RandInsert, RandDelete, RandSub, SwapNeighbor, and SwapAdjacent. Please refer to Appendix A.2 for examples of the misspelled queries.\n\nSimilarity Score Calculation\nLet S(a, B) denote a function that computes a similarity score distribution of any vector a over any set of vectors B:\nEQUATION\nGiven P = {p 1 , p 2 , ..., p M } to be a set of M passages and\nQ \u2032 k = {q \u2032 1,k , q \u2032 2,k , ..., q \u2032 N,k } to be the k th set of misspelled queries in Q \u2032 ,\nwe compute two groups of score distributions as follows:\n\u2022 Passage retrieval: we calculate score distributions in a query-to-passages direction for each original query s p = S(q n , P) and misspelled query s \u2032k p = S(q \u2032 n,k , P). \u2022 Query retrieval: we calculate score distributions in a passage-to-queries direction for original queries s q = S(p m , Q) and each set of misspelled queries s \u2032k q = S(p m , Q \u2032 k ). This way, we produce four different score distributions (s p , s \u2032k p , s q , s \u2032k q ) for our training objective.\n\nDual Self-Teaching Loss Calculation\nWe design the Dual Self-Teaching loss (L DST ) to capture the three desired properties: Alignment, Robustness, and Contrast.\nL DST = (1 \u2212 \u03b2)L DCE Dual Cross-Entropy + \u03b2L DKL Dual KL-Divergence (2)\nDual Cross-Entropy loss (L DCE ) satisfies the Alignment and Contrast properties by utilizing cross-entropy losses to learn score distributions of the original queries for passage retrieval (s p ) and query retrieval (s q ) given labels y p and y q . Minimizing the L (P ) CE term will increase the similarity scores between queries and their relevant passages to be higher than other irrelevant passages by separating the relevant and irrelevant passages from one another. Minimizing the L (Q) CE term will increase the similarity scores between passages and their relevant queries to be higher than other irrelevant queries by separating the relevant and irrelevant queries from one another. In this manner, minimizing one of the two terms will align queries with their corresponding passages, satisfying the Alignment property. Moreover, minimizing both terms will separate queries that refer to different passages and passages that belong to different queries, satisfying the Contrast property.\nL DCE = (1 \u2212 \u03b3)L (P ) CE (s p , y p ) Passage Retrieval + \u03b3L (Q) CE (s q , y q ) Query Retrieval (3)\nDual KL-Divergence loss (L DKL ) aims to fulfill the Robustness property by using KL losses to match score distributions of misspelled queries {s \u20321 p , s \u20322 p , ..., s \u2032K p } and {s \u20321 q , s \u20322 q , ..., s \u2032K q } to the score distributions of the original query s p and s q .\nL DKL = 1 K K k=1 (1 \u2212 \u03c3)L (P ) KL (s \u2032k p , s p ) Passage Retrieval Consistency + \u03c3L (Q) KL (s \u2032k q , s q ) Query Retrieval Consistency (4) Minimizing L (P )\nKL and L (Q)\nKL will reduce the discrepancy between misspelled and non-misspelled queries for both query-to-passages and passage-toqueries score distributions. This way, we implicitly align representations of the misspelled queries to the original queries, satisfying the Robustness property. To stabilize training, we apply stop-gradient to the score distributions of the original queries (s p and s q ) in the L DKL . The \u03b2, \u03b3, and \u03c3 are the balancing coefficients selected by hyper-parameter tuning on a development set. With this loss combination, we achieve all three desired properties.\n3 Experimental Settings\n\nTraining Details\nWe experiment on two pre-trained language models, BERT (Devlin et al., 2019) and Character-BERT (El Boukkouri et al., 2020) . We train models only on the training set of MS MARCO dataset (Nguyen et al., 2016) . Moreover, the training data provided by the Tevatron toolkit (Gao et al., 2022 ) also contains hard negative passages. We include the training set details and hyper-parameter settings in Appendix A.1.\n\nCompetitive Methods\nTo show the effectiveness of our method, we compare our work with the following baseline and competitive training methods.\n\u2022 DPR (Karpukhin et al., 2020) KL losses. Note that their query augmentation method is identical to the Query Augmentation with K = 1. We retrain all models using the same setting described in the previous section. We report the results in the format of \"misspelled query performance (non-misspelled query performance)\".\nWe emphasize the best score with bold text and the second-best score with underlined text. We use \u2020 to denote DST results that significantly outperform the second-best result (p < 0.05).\n\nDataset and Evaluation\nDatasets. We evaluate the effectiveness of DST on two passage retrieval datasets, MS MARCO and DL-typo (Zhuang and Zuccon, 2022) , each with misspelled and non-misspelled queries. There are 8.8 million candidate passages for both datasets.\nThe development set of MS MARCO contains 6,980 non-misspelled queries. To obtain misspelled queries, we use the typos generator method proposed by Zhuang and Zuccon (2021) to generate 10 misspelled variations for each original query. The DL-typo provides 60 real-world misspelled queries and 60 corresponding non-misspelled queries that are corrected manually.\nEvaluation. We use the standard metrics originally used by each dataset's creators. For MS MARCO, each misspelled query performance is the average of 10 measurements. We employ Ranx evaluation library (Bassani, 2022) to measure performance and statistical significance. Specifically, we use a two-tailed paired t-test with Bonferroni correction to measure the statistical significance (p < 0.05).\n\nMain Results\nAs shown in Table 1 , the results indicate that DST outperforms competitive methods for misspelled queries in every case without sacrificing performance for non-misspelled queries in eight out of ten cases. We observe some performance trade-offs for the BERT-based model in the DL-typo dataset's non-misspelling scores (nDCG@10 and MRR). Aside from that, there is no performance trade-off for the CharacterBERT-based model. These outcomes conform with the observation in Figure 2 (Section 4.4) that DST improves the Robustness and Contrast of misspelled queries.\n\nQuery Augmentation Size Study\nTo study the benefit of query augmentation and find the optimal augmentation size, we measure the performance of BERT-based dense retrieval models trained with DST using the query augmentation size K of 1, 10, 20, 40, and 60. Note that the query augmentation method used in previous works is a special case of Query Augmentation when K = 1. We report the results using MRR@10 for the development set of the MS MARCO dataset. We also report training time to show trade-offs between performance and computation. Table 2 : Results of query augmentation size study. We train all models in this experiment on a V100 32G GPU.\nAs shown in Table 2 , the results indicate that increasing K improves the performance of both misspelled and non-misspelled queries, but only up to a certain point, after which the performance begins to decline. We observe that setting K = 40 produces the best results, and there is no further performance improvement after this point.\n\nLoss Ablation Study\nIn this experiment, we study the benefit of each term in DST by training BERT-based dense retrieval models on variant loss combinations with K = 40. The results in Table 3 reveal that L \n\nQuery Distributions\nThe purpose of this section is to study the impact of our training method on the Robustness and Contrast of misspelled queries. We also compare our method against the baseline and competitive methods to show its effectiveness. The Robustness and Contrast of misspelled queries are illustrated using the following kernel density graphs: \u2022 Original-to-Misspell: the cosine similarity distribution between original and misspelled queries. \u2022 Original-to-Neighbor: the cosine similarity distribution between original and neighbor queries. The Robustness property is emphasized by the Original-to-Misspell distribution having high cosine similarity. On the other hand, the Contrast property is emphasized by the small overlapping between Original-to-Misspell and Originalto-Neighbor distributions. The results in Figure 2 show that our method (c) produces the best Robustness and Contrast properties for misspelled queries in comparison to other methods.\n\nConclusion\nThis paper aims to address the misspelling problem in dense retrieval. We formulate three desired properties for making dense retrieval robust to misspellings: Alignment, Robustness, and Contrast. Unlike previous methods, which only focus on the Alignment and Robustness properties, our method considers all the desired properties. The empirical results show that our method performs best against misspelled queries, revealing the importance of the Contrast property for handling misspellings. \n", "hypothesis": " Dense retrieval is a basic building block of information retrieval applications.  One of the main challenges of dense retrieval in real-world settings is the handling of queries containing misspelled words. A popular approach for handling misspelled queries is maximizing the representations discrepancy between misspelled queries and their pristine ones. Unlike the existing approaches, which only focus on the alignment between misspelled and pristine queries, our method also improves the contrast between each misspelled query and its surrounding queries.  To assess the effectiveness of our proposed method, we compare it against the existing competitors using two benchmark datasets and two base encoders.  Our method outperforms the competitors in all cases with misspelled queries.  Our code and models are available at https://github.  com/panuthept/DST-DenseRetrieval..", "answer": false}
{"title": "Unsupervised Task Graph Generation from Instructional Video Transcripts", "content": "\nIntroduction\nTasks in the real-world are composed of multiple key steps with specific dependencies that dictate the order in which they can be performed (e.g., one has to check for breathing before performing CPR). Exposing these dependencies between key steps has many downstream applications including assisting human users in troubleshooting and building artificial agents that efficiently learn and perform new tasks. However, information about tasks is typically available in unstructured and noisy form in the wild (e.g., 'how to' descriptions or instructional video transcripts), presenting a major challenge in extracting structured representations.\nThere is a long history of work on reasoning about tasks, events and temporal ordering, broadly referred to as 'script understanding' ( of script understanding problems include generating a sequence of steps from a given task description (e.g., bake a cake) (Lyu et al., 2021; Sancheti and Rudinger, 2021; Sun et al., 2022) and generating flow graphs from goal and event descriptions (Pal et al., 2021; Sakaguchi et al., 2021) . Script generation also manifests in interactive settings such as simulated embodied environments where agents are expected to reason about subgoals in order to complete tasks (Logeswaran et al., 2022; Huang et al., 2022). Many of these prior approaches either fine-tune language models on human-annotated scripts or rely on knowledge encoded in language models to generate scripts. In contrast, we attempt to use pre-trained language models as an information extraction system to perform zero-shot script inference from noisy ASR (Automatic Speech Recognition) transcriptions of instructional videos describing a task.\nOur focus in this work is to generate a directed graph that represents dependency relationships between the key steps relevant to a real-world task. Figure 1 (a) shows a graph predicted by our approach for performing CPR. An example depen-\n\nGet out two slices of bread\nSpread peanut butter on one slice\n\nSpread jelly\nJoin the slices \"hello and welcome. this is episode number one of traditional school lunches. today I'm going to explain how to make a simple peanut butter and jelly sandwich. before we start making our ..\" log p LM = -270 \"how to make a peanut butter and jelly sandwich. so we came out here all the way today to make one of these. that's right a peanut butter and jelly sandwich. there's a lot of science behind ..\" Given multiple text transcripts of a task, we 1) Summarize the steps described in the transcript, 2) Identify the key steps, 3) Re-label summary steps with key steps, 4) Rank key step sequences using a language model and 5) Consolidate top-k sequences to generate a task graph for the given task. dency that can be read from the graph is that checking for safety hazards has to have happened before any other step (i.e., it is a precondition that needs to be satisfied). In this paper, we will use the term task graph to refer to such dependency graphs.\n\nSpread peanut butter and jelly\nMore formally, consider a real-world task \u03c4 . We assume that multiple text transcripts t 1 , . . . , t n describing how this task is performed are available. 1 We assume that having access to such multiple transcripts helps robustly identify the dependencies between key steps so that an accurate task graph can be generated. For instance, if step y frequently follows step x, it is highly likely that step x needs to happen before step y (i.e., is a precondition). Our goal is to generate a task graph for the given task \u03c4 which models these dependencies. In particular, this involves (i) Identifying the key steps K = {k 1 , . . . , k m } relevant to performing the task and (ii) Generating a graph with nodes k i and edges representing precondition relationships.\nOur contributions in this work are as follows. \u2022 We propose an unsupervised task graph generation approach that uses pretrained language models to infer key steps and their dependencies from multiple text descriptions of a real-world activity. \u2022 We propose ranking and filtering mechanisms to improve the quality of generated task graphs. \u2022 We demonstrate the effectiveness of the proposed approach compared to strong supervised and unsupervised baselines on two datasets. 1 Each transcript is a text document derived from an instructional video using Automatic Speech Recognition.\n\nApproach\nOur approach to task graph generation consists of multiple steps, illustrated in Figure 2 . First, we use an instruction-tuned language model to generate a summary of steps (in free-form text) from a transcript (Section 2.1). Given these summary step sequences generated from multiple such transcripts for the task, we identify the key steps relevant to the task using a clustering approach (Section 2.2). We then re-label summary step sequences using the identified key steps to obtain key step sequences (Section 2.3) and rank them using a language model (Section 2.4). Finally, we generate a task graph from the key step sequences (Section 2.5).\n\nGenerating Summary Steps\nThe first step of our pipeline extracts a summary of steps g i = (g 1 i , g 2 i , . . .) for performing the task described in each transcript t i . We use an instructiontuned language model for this purpose. We prompt the model with a transcript, followed by a query such as 'Based on this description list down the key steps for making coffee using short phrases.' and let the model generate a completion. We use the 'Davinci' version of the InstructGPT (Ouyang et al., 2022) model in our experiments. We observed that the model consistently generates the steps in the format '1. <step 1>\\n 2. <step 2>\\n ..', occasionally using bullet points instead of numbers. The sentences g j i on each line are extracted and treated as the summary steps identified from the transcript. Appendix B shows example summary step sequences generated by InstructGPT.\n\nIdentifying Key Steps Relevant to the Task\nGiven summary step sequences g 1 , . . . , g n generated in the previous step, we seek to identify correspondences between steps in different summaries and capture the salient steps that appear frequently. We use a clustering approach for this purpose. Sentences g j i are represented as embeddings using a sentence encoder (We use the MiniLMv2 encoder from the SentenceTransformers library (Reimers and Gurevych, 2019; Wolf et al., 2019), which was identified as the best sentence embedding method for semantic search/retrieval). We obtain highconfidence clusters by identifying max cliquesclusters of sentences that are similar (determined by a threshold -cosine similarity \u2265 0.9) to each other, and retain cliques with more than 5 sentences. We noticed that this often yields multiple clusters that represent the same key step. For instance, the steps 'fill the moka pot with water' and 'fill the bottom chamber with water' represent the same key step of filling water, but are placed in different clusters. Identifying such redundant clusters based on sentence similarity alone is difficult. We define the notion of sequence overlap between two clusters -how often a sentence from one cluster and a sentence from the other cluster appear in the same summary step sequence. Intuitively, if two clusters have high inter-cluster similarity and low sequence overlap, it is likely that they represent the same key step, and we merge the clusters. The resulting clusters obtained are treated as the key steps k 1 , . . . , k m . 2 Appendix C shows example clusters discovered for different tasks.\n\nRe-labeling Summary Step Sequences\nWe re-label each summary step sequence g (subscript i dropped for brevity) with the identified key steps k 1 , . . . , k m to produce a key step sequence h using the greedy algorithm described in Algorithm 1. The algorithm sequentially picks the most similar 3 candidate summary step and cluster pair (g a , k b ) at each step, assuming each key step only appears once in the sequence. The process terminates when the highest cosine similarity drops below zero.\n\nAlgorithm 1: Key Step Sequence Inference\nInput g = (g 1 , g 2 , . . .) \u25b7 Summary step sequence Input K = {k1, k2, . . .} \u25b7 Key steps For each summary step identify most similar sentence from each cluster:\nCij \u2190 max s\u2208k j cos(g i , s) Hij \u2190 arg max s\u2208k j cos(g i , s) S \u2190 {} \u25b7 Predicted alignments while maxi,j Cij > 0 do a, b \u2190 arg max i,j Cij S \u2190 S \u222a {(a, b)} Caj \u2190 0, C ib \u2190 0 \u2200i, j Sort (ai, bi) \u2208 S so that a1, a2, . . . are in increasing order Output h = (H a 1 b 1 , H a 2 b 2 , . . .)\n\u25b7 Key step sequence\n\nRanking\nOne shortcoming of the labeling algorithm described in the previous section is that it does not take the sequential nature of steps into account. 565 tokens on average.\n\nSetup\nThe datasets come with key steps annotations (i.e., K) for each task and key step sequence annotations for each transcript. Our approach is unsupervised and does not make use of these annotations. However, for evaluation purposes, we consider two settings. The first setting assumes ground truth K and evaluates the performance of the full pipeline ignoring the clustering component (since key steps are known). In the above setting, we use ground truth human annotated graphs from Jang et al. (2023) for evaluation. In the second setting, we use K inferred from Section 2.2 and perform qualitative comparisons with ground truth graphs. Note that we did not use key step sequence annotations from the datasets in either setting.\nBaselines. We compare our approach against the following baselines. Proscript (Sakaguchi et al., 2021) is a language model fine-tuned on manually curated script data. Given a task description and a set of key steps, Proscript generates a partial order of the key steps. In addition, we consider several variations of our approach as baselines in Table 1 . In contrast, we exploit large language models in order to extract key phrases from the transcript. Third, we observe that ranking and filtering key step sequences using a language model ( 5 ) further improves performance, with a significant improvement for ProceL. Finally, our approach comes closest to graphs generated from human annotated key step sequences in the datasets ( 6 ). 5 Unknown Key Steps Next, we consider the full pipeline where key steps are identified automatically. Since ground truth reference task graphs are unavailable in this case we perform a qualitative comparison of graphs generated using our approach and the ground truth, human annotated graph. Figures 1 and 2 show predicted graphs for the tasks perform cpr and make pbj sandwich, respectively. We observe that the predicted graph for perform cpr is more detailed and fine-grained than the ground truth graph and captures many of the ground truth precondition relationships. On the other hand, the graph for make pbj sandwich is less fine-grained compared to the ground truth (Figure 6 of Appendix D). For instance, the ground truth annotations distinguish between putting jelly on the bread and spreading jelly on the bread, whereas our approach treats them as a single step. In addition, spreading peanut butter and spreading jelly are independent of each other and have no sequential dependency. However, the predicted graph fails to capture this and assumes that the former is a precondition for the latter. Appendix D shows more examples of predicted graphs.\n\nAblations Summary\nStep Sequence Generation We perform an ablation to study the effect of the model used to generate summary step sequences from transcripts. We replace the InstructGPT model (Ouyang et al., 2022) with a FLAN-T5 model (Chung et al., 2022) and evaluate graph prediction performance. We find that InstructGPT consistently outperforms FLAN-T5 across all the tasks (Table 2 ). In addition, we found that plain language models (not fine-tuned with instructions) struggled to produce usable summaries. This shows that models trained with instructions and human-preference data are better at producing task graphs from transcripts compared to other forms of supervision such as language modeling and supervised multi-task training with NLP tasks.\n\nRanking Language Model\nWe perform an ablation to understand the impact of the choice of language model for the ranking process in Section 2.4. We present the average performance on tasks in the ProceL dataset with different language model choices in Table 3 . First, we find that performance does not degrade much when switching to a smaller model in the GPT2 family. Second, we notice that scale alone does not guarantee better ranking performance as the larger GPT-J model (Wang and Komatsuzaki, 2021) is inferior to the GPT2 models. These findings suggest that the choice of pre-training data influences the script knowledge present in a model and can be more important than model scale. \n\nConclusion\nThis work presented an unsupervised approach to generate task graphs from text transcripts of instructional videos. Our framework exploits multiple text transcripts which describe a task in order to robustly identify the key steps relevant to a task and the depencies between these steps. We demonstrated the effectiveness of our approach compared to supervised and unsupervised baselines on instructional video transcripts from the ProceL and CrossTask datasets.\n", "hypothesis": " This work explores the problem of generating task graphs of real-world activities.  Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps.  We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner.  We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets..", "answer": true}
{"title": "Not Enough Data to Pre-train Your Language Model? MT to the Rescue!", "content": "\nIntroduction\nSince the emergence of the attention-based Transformer architecture (Vaswani et al., 2017) and the masking pre-training strategies introduced by BERT (Devlin et al., 2019) , transformer-based language models have become the default approach for many NLP tasks, leading to an impressive performance in high-resource languages, particularly English (Hoffmann et al., 2022; Thoppilan et al., 2022; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) .\nAs scaling laws dictate (Kaplan et al., 2020; Hoffmann et al., 2022) , such competitive models are achievable with big computational budgets and large corpora available, requirements difficult to meet for most languages (Joshi et al., 2020) .\nFortunately, LMs are being built for lessresourced languages, such as KinyaBERT for Kinyarwanda (390M words) (Nzeyimana and Rubungo, 2022) , ElhBERTeu for Basque (351M) (Urbizu et al., 2022) , gaBERT for Irish (161M) (Barry et al., 2021) , LuxemBERT for Luxembourgish (130M) (Lothritz et al., 2022b) , Bertinho 45M for Galician (Vilares et al., 2021) , swahBERT for Swahili (16M) (Martin et al., 2022) and QuBERT for Quechua (4M) (Zevallos et al., 2022) . Zhang et al. (2021) estimates that 10M-100M words of pre-training data are enough for an LM to acquire the linguistic capacities of syntax and semantics, but the amount of data required to acquire factual knowledge and commonsense is higher.\nIn this work, we propose to tackle the lack of data by using text corpora available in other languages translated via Machine Translation (MT). To the best of our knowledge, this has been addressed before (Lothritz et al., 2022a) but not indepth, and only for a closely related language pair (German-Luxembourgish). We selected Basque, a language isolate, as the target language, and employ Spanish as the auxiliary language.\nWe direct our efforts to answer the following Research Questions (RQ):\nRQ1: Can we obtain comparable performance to a native LM by training LMs just on synthetic data from MT?\nRQ2: Can we improve current LMs for lessresourced languages by adding synthetic MT data?\n\nMethodology\nIn order to answer our research questions, we set out the following methodology. We propose two baseline LMs: i) ElhBERTeu (Urbizu et al., 2022) as a strong baseline, trained on a corpus of 351M words; and (ii) BERT 125M a model trained on a lower data regime. From there on we pre-train various LMs with different native/synthetic data combinations. Sections 3 and 4 give details of the models pre-trained, including baselines. All models presented in this paper follow the BERT base architecture (Devlin et al., 2019) .\nWe select Basque, a language isolate, as a tar-get language, and employ Spanish, a Romance language, as the auxiliary language since it has huge text corpora available. Furthermore, both languages coexist in the same geographical area, therefore, Spanish is the language that Basque shares the most parallel data with, which is crucial to train MT systems. On the other hand, this is a real case since obtaining a corpus in Basque that exceeds 350M words is difficult.\n\nMT system\nThe Spanish to Basque MT system used for our experiments is based on the default Base Transformer architecture (Vaswani et al., 2017) using the PyTorch version of the OpenNMT toolkit (Klein et al., 2017) and BPE tokenization (Sennrich et al., 2016) (joint vocabulary of 32K). The system was trained with 8.6M parallel sentences and evaluated on the FLORES-200 benchmark (Team et al., 2022) obtaining 13.2 BLEU and 47.4 chrF++. See Appendix F for an analysis of the impact the amount of parallel data has.\n\nCorpora\nFollowing we introduce the corpora employed on the experiments (summarized in Table 1 ):\nN_ElhBERTeu is a Basque corpus compiled to train ElhBERTeu (Urbizu et al., 2022) . It contains 351M words.\nN_small is a smaller Basque native corpus (125M words), created to be closer to the scenario of many languages. The corpus is composed of 75% news articles from Berria 1 newspaper and 25% of text from Wikipedia.\nS_beto2eu is the Spanish Unannotated Corpora 2 composed of 3B words (Ca\u00f1ete, 2019) which was used to train the Spanish LM BETO (Ca\u00f1ete et al., 2020) , translated to Basque using the MT system described in Section 2.1.\nS_loc2eu was also translated from Spanish to Basque. We collected up to 548M words of news articles in Spanish from news sources geographically limited to the Basque Country. After translating it with our MT system, the final corpus in Basque contains 378M words.\n\nPre-training Details\nSince the aim of this work focuses on the effect of the training data, we left all the hyper-papameters fixed. Every model was pre-trained following the procedure used for ElhBERTeu (Urbizu et al., 2022) . See appendix A for further details.\n\nEvaluation\nA downstream task evaluation of our models was performed on the BasqueGLUE ( We fine-tuned each model up to 10 epochs and selected the optimal number of epochs over the development set. We use a batch size of 32 and a learning rate of 3e-5. We report the average of 5 runs on the test sets. Fine-tuning was done on NVIDIA GeForce RTX 3090 GPUs.\n\nLM Trained Solely on Synthetic Data\nRQ1 aims to prove if it is possible to train a competitive LM with just synthetic text obtained from MT. In order to do that we train a BERT model on S_beto2eu (S_BERT), and evaluate if the model trained exclusively on synthetic data is able to perform as well as models trained on real data.\nThe results on the BasqueGLUE Benchmark for S_BERT are reported in In order to improve the results obtained with the synthetic data, we analyse two specific aspects of the data: i) the quality loss during the translation process; ii) the cultural context of the synthetic data. Following we analyze each of those factors.\n\nMeasuring the Quality of MT Text\nTo measure quality loss when translating from Spanish to Basque, we did a manual analysis on a sample of the translations produced by the MT system (See appendix B for details). We evaluated whether a sentence was correctly translated (71%), but also whether the produced sentence was linguistically correct (91%). Since we aim to use this text to train LMs, the effect of some translation errors like hallucinations or omissions, that cause significant meaning changes, might not be critical.\nNext, we measured the vocabulary diversity loss during translation. For that aim, we compiled a Basque-Spanish parallel corpus (more details can be found in appendix B) and translated the Spanish text to Basque with our MT system. The lexicon of the translated data is 16% poorer, limited by the target vocabulary of the MT model and the tendency of MT to generalize and simplify the vocabulary.\nFinally, we analyze the impact of training LMs on translated corpora, leaving aside other factors such as corpus size or text-domain. We train two BERT models using the parallel corpus compiled in the previous experiment, one on the original Basque part of the corpus and the other on the part translated from Spanish to Basque. The results in Table 3 show the model trained on translated data performs slightly worse than the native model.\nWhile this is expected from the quality loss and lexicon impoverishment caused by MT, the gap in performance is very small (0.5% on average), which leads to the conclusion that the synthetic data is adequate.\n\nDomain and Cultural Context\nAnother factor related to data which might affect the performance of MLs trained over translated corpora is the source text we select in the auxiliary language. The Spanish Unannotated Corpora is a huge corpus. However, it is not domain homogeneous and the topic distribution of this corpus differs significantly from that of a corpus in Basque, especially because it hardly includes the specific topics associated with the Basque Country. Furthermore, we analyzed how tokenizers trained on this corpus do not include many words common in the context of Basque speaker communities, like named entities (locations, people or organizations). See appendix C for a detailed analysis of the vocabulary coverage of each model on the test datasets.\nTo analyze the impact the cultural bias and the domain heterogeneity of the source text has on the performance in downstream tasks, we compiled the S_loc2eu corpus, presented in section 2.2. This corpus is formed by texts in Spanish crawled from newspapers geographically and culturally connected to the Basque Country. Results in Table 2 show that models trained on translated local news (Sloc_BERT and SNloc_BERT), perform better than those without them (S_BERT and SN_BERT), even though it is trained over a much smaller corpus. Following the same pattern, the \n\nCombining Native and Synthetic Data\nThe objective of RQ2 is to test if adding texts translated by MT to a native corpus can boost the performance on downstream NLU tasks of the LM in the target language.\nWith that aim, we trained a new LM on the concatenation of S_beto2eu corpus and the N_ElhBERTeu corpus 4 (SN_BERT hereinafter). Table 2 reports the results for SN_BERT when evaluated on the BasqueGLUE benchmark. Even if SN_BERT surpasses ElhBERTeu in a few tasks (NERC, BEC), it is below it in the average score.\n\nMerging Strategies\nOne factor that may explain the lower performance of the model trained on the combined synthetic and native data is the way of combining the data. Our last experiment aims to analyze different combination alternatives. For SN_BERT, we just concatenate N_ElhBERTeu and S_beto2eu. However, the better quality native corpus is diluted among the translated texts of poorer quality, but larger in size (4x times). Hence, we propose another three alternatives to merge native and translated corpora, shifting the balance between both types of data: concat 20\u221280 (SN_BERT): concatenation of N_ElhBERTeu and S_beto2eu, which roughly form 20% and 80% of the pre-training corpus respectively. As mentioned, synthetic data take the principal role in this configuration.\nconcat 50\u221250 : we oversample N_ElhBERTeu corpus to equal the size of S_beto2eu. This setting gives equal weight to native and synthetic data.\nconcat 80\u221220 : we oversample N_ElhBERTeu up to 80%, thus, pre-training relies on native data mostly. Native data is weighted over synthetic data. sequential: the LM is trained for 750K steps on S_beto2eu, and afterwards for another 250K steps on N_ElhBERTeu 5 .\nResults for different merging strategies are shown in Table 4 . Increasing the ratio of N_ElhBERTeu data in our pre-training corpora improves the performance of our models to the point where concat 80\u221220 outperforms ElhBERTeu, trained only with native text in Basque. Pretraining sequentially does improve slightly the results of the default SN_BERT setting, but weighting concatenation is the best strategy between the two. Further sequential training regimes were tried other than (750k+250K). 'sequential' refers to the best results we achieved with this strategy.\n\nConclusions\nRegarding the RQ1, we conclude from our experiments that LMs trained exclusively on synthetic data from MT can obtain comparable performance to a native LM. We further analyze that other than the quality of MT, the cultural context of the text we select from the auxiliary language do have an effect on the final performance. We conclude that it is better to gather a corpus composed of sources similar to those in the target language, rather than indiscriminately translating vast amounts of data in the auxiliary language.\nFurthermore, with respect to RQ2, our experiments show that state-of-the-art models' performance can be improved by adding translated data during the pre-training, albeit it is a small improvement. Weighting the native data above synthetic data is key to this improvement.\nAll in all, this approach has a big potential for less-resourced languages, since once you have a proper MT system, there is no limit on the amount of data one can translate from languages with bigger corpora available.\nData and models are publicly available 6 .\n", "hypothesis": " In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks.  However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machinetranslated corpora for pre-training LMs.  We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from English.  The evaluation carried out on 9 NLU tasks indicates that models trained exclusively on translated data offer competitive results.  Furthermore, models trained with real data can be improved with synthetic data, although further research is needed on the matter..", "answer": false}
{"title": "Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization", "content": "\nIntroduction\nSpeaker diarization(SD) is the task of answering the question \"who speaks when\" by partitioning audio into segments with speaker identities. In most application settings, the results of speaker diarization are perceived by readers through the assignment of speaker labels to the corresponding words or sentences transcribed from an Automatic Speech Recognition(ASR) system.\nDespite the rich profusion of transcribed texts, mainstream speaker diarization systems consider only acoustic information (Park et al., 2021; Horiguchi et al., 2020; Park et al., 2020; Fujita et al., 2019; Zheng et al., 2021; Du et al., 2022a) . Traditional SD systems usually consist of the following components: (1) Voice activity detection (VAD) to filter out non-speech frames. (2) Extraction of speaker embeddings from the short audio segments, using popular models such as i-vector (Dehak et al., 2011) , d-vector (Zhang and Koishida, 2017 ) and xvector (Snyder et al., 2018) . (3) Clustering embeddings into several classes using algorithms such as agglomerative hierarchical clustering (AHC) (Day and Edelsbrunner, 1984) , spectral clustering(SC) (Wang et al., 2017) , and HDBSCAN (Zheng et al., 2022) . Various speaker embedding model and clustering methods have been explored and proposed in (Yu et al., 2021; Dawalatabad et al., 2021; He et al., 2021; Zheng and Suo, 2022; Du et al., 2022b) .\nUtilizing only acoustic information has significant limitations. For example, the performance of SD system suffers from obvious degradation in adverse acoustic conditions such as noise, reverberation, and far-field recordings. In addition, we often encounter speakers with similar voice characteristics, which pose serious challenge to clustering them into expected classes. Given the abundance of transcribed texts present in meetings and conversations, it is of sufficient interest to explore the possibilities of utilizing semantic information to go beyond the limits of acoustic-only speaker diarization.\nSome previous works tried to use semantic information to classify roles in two-speaker conversations, such as doctor-patient conversation and pilotair traffic controller dialogue (Zuluaga-Gomez et al., 2021; Flemotomos and Narayanan, 2022) . However, these methods are only suitable for specific two-speaker scenarios where the roles are clearlydefined, such as medical diagnosis, job interviews, and air traffic communications. In this work we focus on open multi-party meeting scenarios where the number of speakers is unknown and the relations between speakers are unspecified.\nSpeaker identity information has been proven to be beneficial to many downstream NLP tasks (Chi et al., 2017a; Zhang et al., 2018; Chi et al., 2017b) . However, these works only consider speaker identities as given ground truth (Carletta et al., 2006; Janin et al., 2003; Zhong et al., 2021) , which is \n\nNLP Applications\nFigure 1 : We propose a multi-modal speaker diarization system that utilizes the SLP module to extract speakerrelated information from transcribed text. The multi-modal fusion and semantic backend modules combine both acoustic and semantic information to improve the accuracy of speaker diarization. The system's output includes text segments with corresponding speaker identification.\nimpractical in real world settings. Therefore, it is crucial to make valid inference of speaker identities on the transcribed conversations using a wellperformed speaker diarization system.\nThe main contributions of this paper include:\n(1) We propose two semantic tasks to extract speaker-related information from automatically transcribed texts, namely Dialogue Detection and Speaker-Turn Detection.\n(2) We design a simple yet effective integration method to effectively combine semantic and acoustic information for more robust speaker diarization.\n\nA Novel Multi-modal Framework\nFigure 1 illustrates the proposed semantic-acoustic speaker diarization system, along with its relation with upstream ASR components and downstream NLP applications. We introduce a Spoken Language Processin(SLP) module involving two subtasks to extract speaker-related information from transcribed texts. The acoustic-based speaker diarization system is used to process original audio, perform segmentation and estimate speaker embeddings for each segments. To associate speaker embeddings with corresponding text phrases, a forced alignment component was introduced to our system. Finally, we propose an integration method to collectively process outputs from SLP module, acoustic SD module, and forced alignment module.\n\nLearning Speaker Information From Texts\nTo extract semantic speaker-related information, we define two sub-tasks: dialogue detection and speaker-turn detection.\nDialogue detection takes a sequence of sentences as input and determines whether this is transcribed from a multi-speaker dialogue or a single-speaker speech. Dialogue-detection can be defined as a binary classification problem.\nSpeaker turn detection tries to predict, for each given sentence in the sequence, the probability of the occurrence of speaker change. Speaker turn detection can be defined as a sequence labeling problem, where the goal is to determine whether the given position represents a point of change in speaker role from a semantic perspective.\nBoth dialogue detection and speaker turn detection models are fine-tuned from a pre-trained BERT language model. Design of training samples and details of experiments are discussed in next section.\n\nIntegrating Semantic-Acoustic Information\nIn this section we describe how speaker-related information extracted from semantic content can assist us in improving upon acoustic-only SD system. A traditional SD system typically involves an audio segmentation module and an embedding clustering module. Poor segmentation and incorrect clustering are the most common problems in speaker diarization. Semantic information from dialogue detection helps improve clustering accuracy and speaker turn detection helps to find more precise place in text where a change of speaker occurs.\nNote that dialogue detection and speaker turn detection tasks can be solved either by acoustic-only approach or semantic-only approach. Semanticonly approach is described above. Acoustic-only results can be derived directly from acoustic-based speaker clustering. The speaker clustering algorithm assigns a cluster label to each speakersegment. Acoustic results for dialogue detection can be obtained simply by checking whether the number of different speaker labels is larger than 1. Results for speaker turn detection can be obtained by analyzing the transition patterns of speakersegment labels or predicting change points using an acoustic-based neural networks such as Target-Speaker VAD (He et al., 2021) .\nSemantic-Acoustic Dialogue Detection. Let z (s) denote the result of binary classification output of semantic dialogue detection and z (a) be the counterpart of acoustic dialogue detection. We also define D p to be the distance of the largest speaker cluster present in the dialogue to its furthest cluster, and D q to be the standard deviation of the cosine distances among all speaker embeddings present in the selected speech. D p measures how spread out different clusters are and D q measures how tight embeddings in one cluster are grouped together. Then the fusion score \u015d is estimated by:\n\u015d = z (a) z (s) +z (a) (p s +\u03b1 1 D p )+z (s) (p s +\u03b1 2 D q ),\n(1) where \u03b1 1 and \u03b1 2 are learnable and p s is logit output from semantic dialogue detection.\nFor some threshold \u03b8, the binary output of semantic-acoustic dialogue detection is represented by the indicator function:\n\u1e91fusion dd = 1 \u015d>\u03b8 (2)\nOnce semantic-acoustic dialogue detection obtain results for all sentence sequences that cover the entire transcribed meeting, we re-adjust the acoustic-based clustering results. By doing this we are able to incorporate semantic information to improve speaker clustering. More details can be found in Appendix B.\nSemantic-Acoustic Speaker Turn Detection. Semantic-only speaker turn detection outputs a sequence of probability of the occurrence of speaker change. Let p n be the probability at position n, and q n represents the speaker change probability from an acoustic-only model near position n. q n is obtained by taking the maximum probability of the closest 200 frames estimated by the Target-Speaker VAD model. Then the integrated speaker-change probability is given by\npn = \u03b2 1 p n + \u03b2 2 q n (3)\nfor some learnable hyperparameters \u03b2 1 and \u03b2 2 .\nBoundary and Outlier Correction. We use semantic information to correct boundary errors caused by errors and mismatches from the forcedalignment and ASR models. We also use semantic information to correct outliers in embedding extraction. To improve system robustness, we exclude audio segments that are too short from clustering. Outliers and left-out embeddings are assigned to the closest cluster.\n\nDatasets\nWe conduct experiments on AISHELL-4 (Fu et al., 2021) and AliMeeting (Yu et al., 2022) datasets. Both focus on multi-party meeting scenario, where all speech content are manually annotated. Table 1 listed detailed information about the datasets. We perform experiments using both ground truth (GT) text and text transcribed from ASR system.\n\nExperimental Setups\nIn our experiments, the acoustic modules, including ASR, ASR Post-Processing, Embedding Extractor, and Forced Alignment models, are fixed and used consistently throughout all our experiments. In details, the ASR system we introduced was based on UniASR (Gao et al., 2020) . The ASR Post-Processing contained Punctuation-Prediction (Chen et al., 2020) et al., 2017) . For acoustic speaker diarization system, we employed a speaker embedding extractor based on ECAPA-TDNN (Desplanques et al., 2020) , while for speaker clustering, we utilized Spectral Clustering algorithm with p-percentile (Wang et al., 2017) .\nWe fine-tune the semantic models for dialogue detection and speaker turn detection tasks based on the pre-trained BERT language model 1 using the text from AISHELL-4 and AliMeeting training sets. Training samples are sequences of sentences generated by a sliding-window method with a window length of 64 and a shift of 16 and the label for these two semantic subtasks can be generated by the speaker label from the speech content manually annotated in the datasets.\nFor dialogue detection task, we fine-tune for 3 epochs on train dataset with a learning rate of 5e-6 and a batch size of 64. For speaker-turn detection task, we also fine-tune for 3 epochs on train dataset with a learning rate of 1e-6 and a batch size of 64.\n1 Based on bert-base-chinese from HuggingFace\n\nResults and Discussions\nWe compare our proposed methods with the classic speaker diarization system mentioned in Section 1.\nTable 2 shows the results of dialogue detection and speaker-turn detection tasks from acousticonly, semantic-only, and multi-modal models. We not only compare results using ASR-transcribed texts, but also conduct experiments using ground truth texts as inputs, in order to see the optimal improvements introduced by semantic information. The multi-modal model surpasses single-modal results on both GT and ASR text. The experiments demonstrate that semantic model can effectively supplement acoustic-only model, resulting in more precise speaker representation. It is expected that the introduction of semantic information on ASR text does not result in a significant improvement due to the a lot of errors present in the text. However, our multi-modal approach shows consistent improvement in both GT and ASR-based results, indicating the robustness of our method.\nWe use the cp-WER metric to measure the speaker diarization task. We introduce a new metric speaker-WER that aims to measure the word error rate caused solely by speaker errors. More details about the metrics can be found in Appendix A. Table 3 shows the final results of our speaker diarization system. Compared to the baseline, which only uses acoustic information, our system incorporating semantic information shows a significant improvement. The results for \"Semantic-acoustic info. -A\" indicate that only semantic information is used for sub-tasks Dialogue Detection and Speaker Turn Detection, while \"Semantic-acoustic info. -B\" indicates that both semantic and acoustic information are used in the two sub-tasks.\n\nConclusions\nWe propose a novel multi-modal speaker diarization system that utilizes two spoken language processing tasks, dialogue detection and speaker-turn detection, to extract speaker-related information from text. These information are then combined with acoustic information to improve the overall performance of speaker diarization. Our experiments demonstrate that incorporating semantic information can effectively address the limitations of single-modal speech.\n", "hypothesis": " Speaker diarization(SD) is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations.  Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic conditions.  In this paper, we propose methods to extract speaker-related information from semantic content in multi-party meetings, which, as we will show, can further benefit speaker diarization.  We introduce two sub-tasks, Dialogue Detection and Speaker-Turn Detection, in which we effectively extract speaker information from conversational semantics.  We also propose a simple yet effective algorithm to jointly model acoustic and semantic information and obtain speaker-identified texts.  Experiments on both AISHELL-4 and AliMeeting datasets show that our method achieves consistent improvements over acoustic-only speaker diarization systems..", "answer": true}
{"title": "Substitution-based Semantic Change Detection using Contextual Embeddings", "content": "\nIntroduction\nMeasuring semantic change is one of the few areas of NLP where contextual embeddings have not yet led to a definitive improvement over previous methods. In particular, the commonly used approach of aligning static embeddings trained on different time periods (Hamilton et al., 2016b) continues to be a surprisingly hard to beat baseline.\nGiven that contextual embeddings provide a representation for each occurrence of a word in context, they would seem to be ideally suited to a more nuanced investigation of semantic change. Most attempts to leverage them for this purpose, however, produce quantitatively worse results, while being less interpretable and requiring more resources.\nHere, we present a simplified and improved approach to scalable, interpretable, semantic change detection using contextual embeddings. Inspired by Eyal et al. (2022) , we work only with the most probable replacements for masked words, and measure semantic change in terms of the distributions of replacements in each time period. Not only does this better match human judgements, it is highly space efficient, works seamlessly for out-of-vocabulary words, and helps intuitively characterize meaning change and variation.\n\nBackground\nMeasuring semantic change involves a set of tasks related to determining if and how a term's meaning has changed over time. Here, we focus on the task of measuring the amount of change that has occurred from one time period to another (Gulordava and Baroni, 2011; Schlechtweg et al., 2020) . 1 Existing approaches to this task are mostly of two types. The first is associating each term with a single vector per time period and measuring the distance between vectors, of which we take Hamilton et al. (2016b) to be representative. As a variation on this, several authors have proposed averaging the output of contextual embedding models to get a single vector per term in each time period, but this has generally not led to an improvement over using static vectors (Martinc et al., 2020a; Kurtyigit et al., 2021; Liu et al., 2021) . A related approach is to represent words in terms of their nearest neighbors using static word vectors (Hamilton et al., 2016a; Gonen et al., 2020) , but this does not show a clear improvement over other static embedding methods (Montariol et al., 2021) .\nA second type of approach begins with various methods for word sense induction, then measures change in terms of the relative prevalence of a term's different senses (Frermann and Lapata, 2016; Hu et al., 2019; Arefyev and Zhikov, 2020; Arefyev and Bykov, 2021) . In some cases, authors simply cluster contextual representations for each term, and measure differences in the distributions of clusters between two time periods, rather than dealing with explicit word senses (Giulianelli et al., 2020; Martinc et al., 2020b; Montariol et al., 2021) .\nDespite the additional information provided by contextual embedding models, methods using type embeddings (as opposed to token), continue to be competitive. For example, on the recent SemEval multilingual semantic change detection task, none of the top four systems used token embeddings (Schlechtweg et al., 2020) . Methods using contextual embeddings have done better on some more recent mono-lingual shared tasks (Kutuzov and Pivovarova, 2021; Zamora-Reina et al., 2022), but have not yet been evaluated with a consistent setup across multiple languages.\n\nMethods\nBuilding on Eyal et al. (2022) , we represent each token in the corpus (or a sufficiently large sample of them) by a small set of probable replacement terms from a contextual embedding model. However, whereas Eyal et al. (2022) did this for the purpose of word sense disambiguation, we do so for the purpose of measuring semantic change.\nFor each sampled occurrence of each term, we mask the term of interest, feed the masked context through a model, and obtain the predicted token probabilities corresponding to the mask token. 2 From these, we save only the top-k most probable words (excluding stopwords and partial word pieces), and discard the rest.\nFor a given term in a particular time period, we then count how many times each word in the model vocabulary has appeared as a top-k replacement for that term, and normalize this by its sum, giving us a distribution over replacements. To obtain a raw score of semantic change between two time periods, we compute the Jensen-Shannon Divergence (JSD) between the two distributions representing the same term in different time periods. However, as we show below, the raw JSD scores are strongly correlated with term frequency. Thus, to obtain a scaled metric, we convert the raw JSD scores into a quantile, comparing the raw score for a term of interest to other terms with similar frequency.\nCompared to saving the full output vector per token, this approach only requires a miniscule amount of storage per token, and thus does not require the kind of heuristic dropping of tokens employed by Montariol et al. (2021) . In addition, the dominant meanings of a word in each context can be summarized by the terms which occur most fre-quently among the top-k replacements. Although such replacements are limited to the terms which exist in the model vocabulary, in practice this is sufficient to represent a nuanced set of meanings, and works even for words which get tokenized into multiple word pieces, as we show below.\nMore formally, given two corpora C1 and C2, let the count of token v as a top-k replacement for term t in corpus c be:\nEQUATION\nwhere R(t, i, k) is the set of top-k most probable replacements for occurrence i of term t (excluding stopwords and partial word pieces in the model vocabulary), and N c (t) is the number of sampled occurrence of term t in corpus c. 3 Let \u2206 c t by the distribution of top-k replacement counts for term t in corpus c, obtained by dividing the corresponding vector of counts (i.e., [count(\u2022, t, c)]) by the sum over the model vocabulary. The raw change score for term t is given by the JSD between the two distributions:\nEQUATION\nFinally, we correct for frequency effects by rescaling the raw JSD scores against the scores for terms with similar frequency as the target term, giving us a quantile scaled in [0, 1]:\nscaled(t) = \u03a3 s\u2208T (t) I[raw(t) \u2265 raw(s)]/|T (t)|,\n(3) where T (t) is the set of terms with similar frequency to term t (excluding term t itself). More specifically, we compare against all terms within a fixed factor of the target frequency:\nEQUATION\n) where fr(t) is the frequency of term t in the corpus, with window factor F .\n\nData\nWe use five datasets with words labeled in terms of semantic change between two time periods. Four of these are from SemEval 2020 Task 1: Unsupervised Lexical Semantic Change Detection (SE; Schlechtweg et al., 2020) . These datasets contain 31 to 48 terms from four languages, graded in terms of change by human raters, along with accompanying corpora to be used in estimating the amount of change. The fifth dataset (GEMS) comes from Gulordava and Baroni (2011), and contains 100 words labeled in terms of semantic change from the 1960s to 1990s. As with most recent papers which use this dataset, we use the Corpus of Historical American English (COHA; Davies, 2010) for measuring change in the GEMS words.\n\nExperimental Details\nFor each dataset, we fine tune an appropriate BERT model to the union of the two associated unlabeled corpora using continued masked language model training with the HuggingFace transformers package. We then index the corpora to find all occurrences of each word. For all target words, along with a random set of 10,000 background terms, we randomly sample up to 4,000 occurrences of each from the associated corpora. We process all sampled tokens as described above to obtain and store the top-k replacements for each, with k = 5. Using the replacements obtained from the model, we compute raw JSD scores for each term. Finally, we convert these to scaled scores by comparing to the background terms that have frequency within a factor of two of the target term (i.e., F = 2).\nFollowing past work, we evaluate using Spearman correlation with human ratings, comparing against the best results from recent papers. In particular, we include two results based on slight variations on Hamilton et al. (2016b) , one of which was the best performing method in the SemEval competition (P\u00f6msl and Lyapin, 2020), as well as methods using contextual embeddings (Martinc et al., 2020b; Montariol et al., 2021) . For fully experimental details, please refer to Appendix A.\n\nResults\nFull results are given in Table 1 . Although our method is not uniformly better than all previous methods on all dataset, it does produce the best result on average, as well as improvements on GEMS, SE English and SE Latin. 2 are labeled.\nAs an example to better understand these results, the raw JSD scores from our method are shown in Figure 1 (top) for the SE English data, with select terms labeled. As can be seen, there is a strong relationship between term frequency and raw JSD, hence the need to rescale the raw scores relative to terms with similar frequency. After rescaling, we see a strong correlation between our final semantic change scores and the human ratings, as shown in Figure 1 (bottom) for the SE English data.\nAs with the approach of Hamilton et al. (2016b), our method supports direct interpretation of semantic change. To understand the change in a word's typical usage, we can look at the overall most common replacements from each time period. Table 2 shows the scores and rankings of several selected terms from SE English, along with the most common substitutes from each time period.\nLooking at the results, we can see, for example, strong agreement with human annotators on a dramatic change in the meaning of plane (comparing 1810-1860 vs. 1960-2010) , from the geometric concept to the flying machine. On the other hand, our results suggest that human raters may have slightly underestimated the amount of change in the meaning of graft, which was previously used mostly in reference to vegetation, but now most commonly refers to corruption. 5 By contrast, ounce may be a case where our method has underestimated the change that has taken place. Older usages seem to map more generically to a wider range of quantities (hence the appearance among the early substitutes of hour, acre, and dollars), whereas modern usage seems more restricted. Indeed, we do find some difference in the distribution of substitutes between the two time periods, but less of a difference than is typical for words with similar frequency, hence the low final score from our method (see Figure 1 ).\nAlthough we do not emphasize it in this paper, of our method can easily be combined with the approach of Eyal et al. (2022) to further investigate meaning changes, by inferring senses from the term replacements, and looking at how their usage varies by time period. In particular, for each target term, we can construct a graph from the set of term substitutes (as nodes), where edge weights represent the number of top-k clusters in which two substitutes co-occur. Following Eyal et al. (2022) , we experiment with Louvain community detection to identify sense clusters from these graphs for each term of interest, and use Jaccard similarity to associate each mention with a sense cluster, based on substitute overlap (see Appendix A for details).\nInspecting the distribution of these senses over time helps to distinguish the gradual adoption of existing senses from the creation of new ones. For example, the most common sense of plane is captured by the sense cluster {aircraft, jet, airplane, car}, and as expected, this sense is not found in the 1810-1860 English data, except for two instances which appear to be errors in the inferred sense. By contrast, the second most common sense-{planes, line, point, surface}-appears in both time periods, but is much more common in the earlier time.\nThis approach also provides more insight into how the meaning of graft has changed. The most common sense cluster is the horticultural meaning {tree, plant, stock, vine}, and this meaning occurs in both time periods, but is much more common in the earlier one. A second cluster, corresponding to illicit activity-{corruption, violence, bribery, fraud}-occurs only in the later time period. This clustering method also surfaces a third sense with a medical meaning-{transplant, surgery, disease, drug}-which is not revealed by the top few overall most common replacements given in Table 2 .\n\nDiscussion and Related Work\nAs noted by others, new and larger datasets for rigorously evaluating semantic change are badly needed (Tahmasebi et al., 2021) . Existing datasets are relatively small, and are mostly based on inspecting a limited number of examples per term. Unfortunately, determining ground truth for semantic change is challenging, and producing such resources is costly. Ideally, future datasets for evaluation should be larger, both to allow for more robust evaluation, and to have sufficient targets for both hyperparameter tuning and evaluation.\nIn addition to the dataset we have used in this paper, two others are available from shared tasks on Spanish and Russian, respectively (Kutuzov and Pivovarova, 2021; Zamora-Reina et al., 2022) . Both of these are comparable in size to the GEMS dataset used here. Unfortunately, they are less useful for evaluation because most submissions to these shared tasks only evaluated on the task data, and not on other datasets. As shown by the replication of Martinc et al. (2020b) in Montariol et al. (2021) , a method can sometimes perform well on one language but fail to generalize to others. As such, we have based our evaluation on datasets for which there has been a consistent evaluation of methods across multiple languages. As future work, a careful replication study of all methods from each competition on all available datasets, including an assessment of sensitivity to hyperparameters, would be highly informative.\nBesides Eyal et al. (2022) , The closest prior work to ours is Kudisov and Arefyev (2022) , who use dynamic patterns to generate many variations on example usages sampled from the given corpora. These variations are then used to generate hundreds of replacement terms from a masked language model with associated probabilities. These probabilities are averaged (heuristically combining replacements with differing numbers of word pieces) to obtain a mean vector for each sampled instance. Finally, semantic change is computed as the average cosine distance between all pairs of vectors across corpora. This method was evaluated as part of the LSCDiscovery shared task on Spanish (Zamora-Reina et al., 2022) . Preliminary work on this method was described in Arefyev and Bykov (2021) , where a slightly different version of it was evaluated on the RuShiftEval shared task on Russian (Kutuzov and Pivovarova, 2021) .\nCompared to Kudisov and Arefyev (2022), our approach is considerably simpler, and better suited to storing representations of a complete corpus for subsequent analysis and exploration. In particular, we only consider a small number of substitutes for each example (storing only the top-k most probable terms, without the associated probabilities). We do not use dynamic patterns, and only consider terms in the model vocabulary as potential substitutes. We also associate each term with a single distribution over the model vocabulary per time period (not per mention), and use Jensen-Shannon divergence to more naturally measure the distance between distributions. Importantly, we also correct for frequency effects, as described above.\nAlthough our approach avoids the onerous storage requirements of methods which save full contextual vectors, it still requires considerable processing time to obtain the top-k replacements for all tokens. Future work could explore smaller or more efficient models for this purpose. 6 Finally, despite its simplicity, measuring the cosine distance between aligned static vectors remains a strong and efficient baseline (Hamilton et al., 2016b) . More work is needed to determine where contextual embeddings can offer sufficient advantage in measuring semantic change to justify their greater computational cost.\nCompared to static embeddings, our approach is weakest on the German and Swedish datasets, which could relate to the quality of the pretrained models that are available for those languages, the data used for pretraining, or perhaps issues that arise in tokenization of the reference corpora. For a tentative exploration of some possible factors, please refer to Appendix C.\n\nConclusion\nWe have presented a simplified and improved approach to measuring semantic change using contextual embeddings, based on the Jensen-Shannon Divergence between the distributions of the most probable replacements for masked tokens in different time periods, corrected for frequency effects. This approach achieves superior performance on average, while remaining directly interpretable, with vastly reduced storage requirements.\n", "hypothesis": " Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a complex approach to measuring semantic change using contextual embeddings, relying only on the least probable substitutes for masked terms.  Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors..", "answer": false}
{"title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text", "content": "\nIntroduction and Background\nTextual Geolocation Identification, a crucial component of Geographic Information Retrieval (GIR), is the task of resolving the location, i.e., coordinates of a place, based on the reference to it in a text. It requires a combination of language and environmental knowledge. On top of the usual non-spatial linguistic challenges in Natural Language Understanding (NLU), such as named entity recognition (NER), anaphora resolution, bridging anaphora, etc., the textual geolocation task presents geospatial challenges that require multimodal processing and grounding (Ji et al., 2022; Fried et al., 2022; Misra et al., 2017; Qi et al., 2020; Paz-Argaman et al., 2020) .\nProper names, such as 'Rabin Square', also known as named entities in Natural Language Procesing (NLP), and as rigid designators in formal semantics (Kripke, 1972) , can be easily grounded based on a Gazetteer or a simple map. However, geolocating linguistic terms that involve spatial expressions without the explicit mention of a proper name still present an open challenge. This interpretation challenge includes the understanding and resolution of (at least): (i) definite descriptions, such as 'the school' (ii) geospatial terms, such as cardinal directions; 'east of'; and (iii) geospatial numerical reasoning; 'two buildings away from the pharmacy'. To address these and other challenges, we need to both ground entity mentions to their corresponding physical entities in the environment, and to reason about geospatial relations expressed between entities -these two processes being closely intertwined.\nTo do so, we need a corpus for the geolocation task that maps rich geospatial place descriptions to their corresponding location coordinates. However, current corpora for geolocation are based on naturally-occurring open-source resources, such as Wikipedia articles (Eisenstein et al., 2010; Wing and Baldridge, 2011; Han et al., 2012; Wing and Baldridge, 2014; Wallgr\u00fcn et al., 2018) , which are not spatially oriented, i.e., the description of locations is implicit or absent in the corresponding text. Subsequently, the accuracy of retrieval is fairly low (around 100 km).\nFurthermore, all geolocation datasets previously studied in NLP are in English, with a dearth of corpora for low-resource languages, in particular, for morphologically rich languages, such as Hebrew. To understand the geolocation challenges and build models that do various spatial reasoning tasks, English cannot be our sole focus (Baldridge et al., 2018) . Hebrew, a Semitic morphologically rich language is notoriously difficult to parse (Tsarfaty et al., 2020 (Tsarfaty et al., , 2019)) . Moreover, resources that are Place Description: The place is located near the Rothschild complex -at the end of Rothschild Street, as you go towards the sea, take a right for about three streets and then you will see the tower high above you. available for Hebrew NLP research focus on traditional tasks, such as Part-of-speech (POS) tagging, syntactic parsing, etc; and lack corpora for understanding and reasoning in real-world situations.\nIn this work we present HeGeL, a novel dataset for Hebrew Geo-Location, the first ever Hebrew NLU benchmark involving both grounding and geospatial reasoning. To create HeGeL, we crowdsourced 5,649 geospatially-oriented Hebrew place descriptions of various place types from three cities in Israel. We designed our task based on a realistic scenario of human place description, relying on people's memory of the world, rather than, e.g., using a map (Anderson et al., 1991; Paz-Argaman and Tsarfaty, 2019) . Crucially, relying on environmental cognition results in various levels of geospatial knowledge (Siegel and White, 1975) that are manifested in the descriptions and the geospatial reasoning that is required to resolve their location (Hayward and Tarr, 1995) . To avoid the much simpler task of grounding proper named entities, we explicitly restricted the use of proper names in the description of the place and adjacent landmarks.\nUnlike the text-based navigation task (MacMahon et al., 2006; Chen et al., 2019; Ku et al., 2020; De Vries et al., 2018; Thomason et al., 2020) , which requires representing an agent's current perspective, reflecting its route knowledge, we show that the HeGeL task requires a full-environment representation, thus, capturing complex geospatial relations among multiple physical entities. Through a thorough linguistic and empirical analysis, we demonstrate the characteristics and challenges associated with Hebrew place descriptions, showing that HeGeL serves both as a challenging NLU benchmark and as a corpus for geospatial cognition research.\n\nThe HeGeL Task and Dataset\nThis work addresses the task of geolocating places on a map based on natural language (NL) geospatial descriptions that are given in a colloquial language and based on participants' memory of the environment (i.e., cognitive map). The input to the HeGeL task is as follows: (i) an NL place description of the whereabouts of the place, and (ii) a map with rich details of the environment (e.g., physical entities names, geospatial relations, and attributes). The output is a pair of coordinates (x,y) specifying the physical location of the place described in the text. Figure 1 shows an example of a place description from HeGeL translated from Hebrew.\nTo simplify the crowdsourcing task and encourage participants' engagement, we frame the data crowdsourcing process as the well-known game, the treasure hunt task (Kniestedt et al., 2022) , in which the instructor-participant is required to describe in writing the location of the treasure, a known place in the city, to a different followerparticipant who then needs to locate it on a map. Thus, the online assignment is divided into two tasks: the instructor's writing of place descriptions and the follower's validation. To avoid preconceived notions as to the 'correct' way to describe a place, we first presented the participants with the task of writing a place description, and once completed, the validation task was given. 2 We hereby provide the details of the two UI tasks:\n(i) Task 1. Writing a place description In this task we requested participants to describe in a freeform text the location of a place known to them, to a third party who might not be familiar with the whereabouts of that place. To collect place descriptions based solely on people's memory, we did not visualize the area of the place, e.g., on a map. Instead, we ensured that the participants are well familiarized with the place by asking them to state how familiar they are with the place on a scale of 1-5. If this score was 1 or 2, we presented the participant with a different place to describe. To ensure diverse human-generated textual descriptions, places were chosen based on their type, position/location in the city (places were spread across the city), geometry, size, and context. To avoid the use of proper names, we developed a rule-based methodology to make sure that the explicit name of the goal (place) or of the nearby landmarks (< 100 meters) will not appear explicitly in the description. The original description was saved, and the participants were asked to input another description without the above names.\n(i) Task 2. Place description validation To verify that a person who reads the text description will understand where the treasure is hidden, i.e., geolocate the place, we developed a map-based retrieval task. The participant in the follower role was asked to read the crowdsourced textual description and mark its location on the map, i.e., where the treasure is hidden. For marking the location, we implemented an interactive online map based on OpenStreetMap (OSM), 3 which allows the participants to move and zoom-in to precisely pin the described place on the map. The map supports the cognitive process needed to ground mentioned entities to physical entities, reason about the geospatial relations, and locate the described place. To familiarize participants with the interactive map tool and task, they had to first pass a simple map marking test, and only then they could start task 2 of reading place descriptions (given by other participants), marking place locations on the map, and rate the clarity of the textual description on a scale of 1-5.\n\nTarget Selection and Retrieval Errors\nThe treasure-hunt task we devised included 167 places in the three largest cities in Israel: Tel Aviv, Haifa, and Jerusalem. These three cities are differently shaped, and show different physical, morphological and topographic features, which potentially affect the legibility and imageability of urban components, and therefore also on place descriptions. These differences can be expressed in the use of various physical features and prepositions, e.g., frequent use of the physical object 'landmark' and the prepositions 'above' or 'below' in hilly terrains that characterize Haifa and Jerusalem.\nTo assess the quality and interpretability of the place descriptions, we calculate the shortest Euclidean distance between the coordinates of the goal's (physical element) shape (polygon, line or point), and the location marked by the 'follower' on the map (task 2); we term this distance as retrieval error. To determine the agreement rate among human participants, each textual place description is validated by at least two participants. To ensure that we work with descriptions that can be geolocated, we set a hard distance threshold of 300 meters, based on analysis of the descriptions' clarity score that we had conducted on a prior (held-out) development corpus we collected for the task.\n\nData Statistics and Analysis\nThe resulting HeGeL dataset contains 5,649 validated descriptions paired with their coordinates on a map. The locations are divided among three cities: 2,142 in Tel Aviv, 1,442 in Haifa, and 2,065 in Jerusalem. 1,833 participants completed the writing task, inserting in total 10,946 place descriptions, and 2,050 participants completed 12,655 validation tasks. The dataset is balanced, with about 33 descriptions per place.\nFigure 2 shows a Venn diagram representing the relation of the three sets of city-based vocabularies (formed from unique lemmas produced by More et al. (2019) lemmatization tool). The intersection of the three cities contains only 15.07% of the entire vocabulary (the union of the three cities' vocabularies). The shared language is not focused on city-specific terms, such as 'Knesset'. Instead, it includes rich spatial terms, such as 'between', modified prepositions such as 'next to', and nondefinite entities, such as 'street'. From the Venn diagram we also conclude that almost half of the lemmas of the three vocabularies, corresponding to the three cities, contain city-specific lemmas: 48.6%, 40.65%, and 49.3% for Tel Aviv, Haifa, and Jerusalem, respectively. As such, HeGeL enables a city-split setup, training on one city and testing on a different unseen city, where city-reserved named entities present an out-of-vocabulary (OOV) challenge for models trained on another city.\nTable 1 shows an analysis of the linguistic phenomena manifested in the HeGeL dataset, demonstrating the spatial knowledge and reasoning skills required for solving the HeGeL task. We analyzed the frequency of the five types of elements in a city defined by Lynch (1960) , along with the three types of spatial knowledge defined in Siegel and White (1975) , and other spatial properties. The frequent use of cardinal directions, as well as the use of sur- vey knowledge, suggests that any NLP model built to deal with the HeGeL task should not only represent a local view of the goal, or possible routes, but also take into consideration the full region, and mimic people's map-like view of the environment. Therefore, unlike navigation tasks where only the agent's current perspective is represented in the model, this task requires full representation of the environment.\nWe further perform a quantitative analysis of word tokens and lemmas that appear in HeGeL, depicted in Table 2 . Overall, the HeGeL dataset contains a large vocabulary of 9,207 unique tokens and 6,663 unique lemmas. There are mentions of physical entities, but as we limited the mentions of named-entities of the described place and landmarks adjacent to it; these are relatively rare, and are mostly references to prominent city landmarks. Also, as most place descriptions are not route-based descriptions, there are only few verbs used in the descriptions. Prepositions, on the other hand, are abundant.\nIn Table 3 , using a one-way analysis of variance (ANOVA) test, we found a significantly (p<0.05) different distribution between place type descriptions and the following features: number of named entities, number of verbs, human verification retrieval error, and clarity score.\n\nExperiments\nWe create a zero-shot (ZS) city-based split, such that we train on one city and test on another. The train, development, and test sets correspond to the descriptions collected in Tel Aviv, Haifa, and Jerusalem, respectively. We evaluate different baseline models for the geolocation task on the HeGeL dataset. We use three evaluation metrics based on retrieval error: mean, median, and task completion (TC) accuracy -the percentage of place descriptions located within the 300 meters threshold. We provide three baselines for the HeGeL task.\nWe first assess a brute-force NER approach; i.e., we test whether recognizing named entities in the text and retrieving their corresponding coordinates is sufficient for solving the HeGeL task of geolocation. To this end, we used Google Maps API and produced two baseline models: (i) Google Maps API Query -we queried the API with the full raw text descriptions as input, with no prepossessing; and (ii) Oracle NER -we queried all 1-5 n-grams against Google Maps API and retrieved the closest geolocation to the goal.\nIn our second approach, we employ a dualencoder model. One encoder encodes the text using a Hebrew Monolingual pre-trained encoder, Aleph-BERT (Seker et al., 2022) , which produces a 768dimension vector representation of the text. The other encoder processes the environment, which is represented as a graph based on OSM data. Each point of interest in the graph is connected to an S2Cell 4 , which contains its geometry and is based on S2-geometry. These S2Cells are encoded using a random-walk algorithm to produce a 64dimensional vector for each cell. These vectors are then passed through a linear layer to produce 768-dimensional vectors. We calculate the cosine similarity score between the text and environment vectors and use it to align the respective representations via maximization of the cosine similarity score with a cross-entropy loss over the scores. 4 \n\nS2Cells\nare based on S2-geometry (https://s2geometry.io/), a hierarchical discretization of the Earth's surface (Hilbert, 1935) . Performing an ANOVA test, we found a significantly (p<0.05) different distribution between place type descriptions and the retrieval error of the Oracle NER. The mean retrieval error of the Path and Node place types were the lowest in both human verification and Oracle NER. This suggests that both of these place types are easier for humans to geolocate.\nThe results in Table 4 show that our task is not solvable with adequate resolution by the Google Maps API. The human performance provides an upper bound for the HeGeL task performance, while the simple Google Maps API Query provides a lower bound. The Google API model's low performance suggests that NER and the Gazetteerbased methods in and of themselves are insufficient to handle the HeGeL task successfully, and that geospatial reasoning is necessary. The Dualencoder's low performance on the ZS split suggests that OOV is a major challenge. The few-shot (FS) split shows an improvement of the model after finetuning on additional samples from the test-region (FS 20% and 80%). This suggests that a possible solution for the city-split setup might be dataaugmentation via generating grounded descriptions for the tested region -an approach we reserve for future research.\n\nConclusion\nThe contribution of this paper is threefold. First, we present the first geolocation benchmark with Hebrew place descriptions. Second, to the best of our knowledge, this is the only crowdsourced geolocation dataset, thus, eliciting explicit geospatial descriptions, allowing for better retrieval resolution. Finally, our analysis shows that the dataset presents complex spatial reasoning challenges which require novel environmental model representation. \n", "hypothesis": " The task of textual geolocation -retrieving the coordinates of a place based on a free-form language description -calls for not only grounding but also natural language understanding and geospatial reasoning.  Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited.  Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resourcepoor languages, such as Hebrew.  In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning.  We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel.  Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.", "answer": true}
{"title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models", "content": "\nIntroduction\nA main aim of neural network quantization is to reduce the size and computational demands of a model while maintaining its performance. There are two main approaches: quantization-aware training (QAT) (Banner et al., 2018; Chin et al., 2020; Faghri et al., 2020; Kim et al., 2020; Wang et al., 2018) and post-training quantization (PTQ) (Neill, 2020; Bondarenko et al., 2021; Kim et al., 2021; Dettmers et al., 2022) . Both of these approaches have limitations in terms of dealing with accumulative quantization errors that are propogated within the layers of a neural network during the forward pass (Zhao et al., 2019; Fan et al., 2020) . To address this issue, we propose a method called Self-Distilled Quantization (SDQ) that combines selfattention and output distillation with quantization to compress large language models. SDQ involves injecting quantization noise into the student network during training and distilling knowledge from a fine-tuned teacher network from both its final output and outputs of intermediate self-attention layers. By distilling knowledge of the self-attention layers, as depicted in Figure 1 , we further reduce the compounding effect of quantization errors in\n\nUnquantized FP32\nQuantized INT8\nFigure 1: Self-Attention Self-Distilled Quantization the network. We use SDQ for self-attention models and demonstrate its effectiveness in compressing multilingual models XLM-R Base and InfoXLM Base , achieving high compression rates while maintaining performance on the XGLUE benchmark. Lastly, we identify that quantization error is largest at the output of self-attention modules.\n\nRelated Work\nCombining quantization and distillation has been previously explored by Mishra and Marr (2017) , who used three different schemes to combine low bit precision and knowledge distillation (KD) using a 4-bit ResNet network. Polino et al. (2018) used a distillation loss with respect to a quantized teacher network to train a student network, and also proposed differentiable quantization, which optimizes the location of quantization points through SGD. Zhou et al. (2017) TernaryBERT (Zhang et al., 2020) uses intermediate layer distillation with layerwise and rowwise weight ternarization. At the extremum of compression rates, BinaryBERT (Bai et al., 2020) binarizes the weights by using ternary weight splitting to avoid the difficulties of training binary neural network directly. BinaryBERT too uses knowledge distillation to improve quantization. Unlike, TernaryBERT and BinaryBERT our work quantitatively measures accumulative quantization errors in the network and thus combines distillation to address this with 1) iterative Product Quantization (iPQ) (Stock et al., 2019) that iteratively quantizes the layer by layer throughout training and 2) Quant-Noise (Fan et al., 2020) which injects sub-block quantization noise during training. We now move to describing the methodology of SDQ.\n\nMethodology\nWe begin by defining a dataset D := {(X i , y i )} D i=1 with samples s i = (X i , y i ), where each X i := (x 1 , . . . , x N ) and x i \u2208 R d is the i-th vector. For structured prediction y i \u2208 {0, 1} N \u00d7dy and for single and pairwise sentence classification, y i \u2208 {0, 1} dy , where d y is the number of classes. Let y S = f \u03b8 (X i ) be the output prediction (y S \u2208 R dy ) from the student f \u03b8 (\u2022) with pretrained parameters \u03b8 := {W l , b l } L l=1 for L layers and the outputs of self-attention blocks are denoted as A l . The loss function for standard classification fine-tuning is defined as the cross-entropy loss \u2113 CE (y S , y).\nSelf-Distilled Quantization For self-distilled quantization, we also require a fine-tuned teacher network f \u0398 , that has been tuned from the pretrained state f \u03b8 , to retrieve the soft teacher labels y T := f \u0398 (x), where y T \u2208 R C and C c y T c = 1. The soft label y T can be more informative than the one-hot targets y used for standard classification as they implicitly approximate pairwise class similarities through logit probabilities. The Kullbeck-Leibler divergence (KLD) \u2113 KLD is then used with the main task cross-entropy loss \u2113 CE to express \u2113 SDQ KLD as shown in Equation 2,\n\u2113 SDQ KLD = \u2113 CE (y S , y)+\u03b1\u03c4 2 D KLD y S , y T (1)\nwhere D KLD (y S , y T ) = H(y T ) \u2212 y T log(y S ), H(y T ) = y T log(y T ) is the entropy of the teacher distribution and \u03c4 is the softmax temperature. Following (Hinton et al., 2015) , the weighted sum of the cross-entropy loss and the KLD loss \u2113 SDQ KLD =\u2113 CE (y S , y)+\u03b1\u03c4 2 D KLD y S , y T is used as our main SDQ-based KD loss baseline, where \u03b1 \u2208 [0, 1]. However, D KLD only distils the knowledge from the soft targets of the teacher but does not directly reduce accumulative quantization errors of the outputs of successive self-attention layers. This brings us to our proposed attention-based SDQ loss \u2113 SDQ Att-KLD shown in Equation 2,\n\u2113 SDQ Att-KLD =\u2113 CE (y S , y)+\u03b1\u03c4 2 D KLD y S , y T +\u03b2 1 LH L l=1 H h=1 \u2113 Attention A S lh , A T lh (2)\nwhere \u03b1 and \u03b2 are regularization terms and \u2113 Attention computes the loss between the student and teacher outputs of each self-attention block in L layers and H attention heads per layer. We also consider two baselines, \u2113 SDQ Att which is the same as Equation 2without \u03b1\u03c4 2 D KLD (y S , y T ) and \u2113 SDQ Hid which applies the Mean Squared Error (MSE) loss between the hidden state outputs instead of the attention outputs. The gradient of D KLD (\u2022, \u2022) is expressed as\n\u2202D KLD (y S i ,y T i ) \u2202y S i = \u03c4 (y S i /\u03c4 \u2212 y T i /\u03c4\n) and as \u03c4 \u2192 \u221e, the gradient is approximately 1/(d y y S i \u2212 y T i ). Similarly, the gradient of the MSE loss on a single self-attention output in layer l and head h is 1/n lh (a S j \u2212a T j ) for a single sample input x. Hence, we see the connection between derivatives between the KLD loss and the MSE loss when combining them in a single objective. We now move to describing how SDQ is used in two QAT methods.\nIterative Product Distilled Quantization We first consider using SDQ with iPQ (Stock et al., 2019) . This is achieved by quantizing m subvectors for each k columns of W where a codebook for each k subvectors is learned to map each subvector to its nearest neighbor in the learned codebook C \u2208 R k\u00d7d where k is the number of codewords. The codebook is updated by minimizing\n||W \u2212 W|| 2 2 = d i ||W [:,i] \u2212 \u03d5(w [:,i] )|| 2 2\nwhere \u03d5(\u2022) is the quantization function. This objective can be efficiently minimized with the k-means algorithm and the codewords of each layers are updated with SGD by averaging the gradients of each assigned block of weights. This is done iteratively from the bottom layers to the top layers throughout training where the upper layers are finetuned while the lower layers are progressively being quantized (Stock et al., 2019) . When using iPQ with SDQ, omitting the KLD loss and cross-entropy loss, the objective is \u2113\nSDQ iPQ = L\u2212F l=1 ||W l \u2212 Wl || 2 2 + \u03b2 L-F d i (A S l,i \u2212 A T l,i ) 2\nwhere F is the number of finetuned layers (non-quantized) at that point in training. Hence, SDQ progressively quantizes the layers throughout training when used with iPQ.\nBlock-Wise Distilled Quantization Noise For the majority of our QAT-based experiments we use Quant-Noise (Fan et al., 2020) . Quant-Noise is a SoTA QAT method that applies (fake) blockwise quantization noise at random to each weight matrix. Concretely, blocks of weights b kl in W l are chosen at random at a rate p and quantization noise is added to the chosen blocks. We can define\nA S = Softmax WQ WK \u221a d k W\u22a4 V W\u22a4 Q WQ WU\nwhere W represents (fake) quantized weights and is given as\nW = \u03d5 INT-8 (W) = s(round(W/s + b) \u2212 b)\nwhere s and b are scalars learned throughout training and represent the scaling factor and offset respectively. We then pass A S and A T to Equation 2to compute the loss.\n\nEmpirical Results\nWe begin by referring the reader to the supplementary material for the experimental setup in subsection A.2 and subsection A.3. Before discussing the main results on XGLUE, we first analyse the mean absolute quantization error and the Frobenius norm of the elementwise difference in selfattention blocks between an INT-8 dynamically quantized InfoXLM Base and an unquantized FP-32 InfoXLM Base in Figure 2 . We see in Figure 2a that the output layer contains the largest mean absolute error across each layer and highest error variance. In contrast, the query, key and value (QKV) parameters have much smaller error. However, since most of the parameters are found in the QKV layers, the sum of the quantization error is larger, as seen in Figure 2b . This motivates us to focus on the output of the self-attention block when minimizing quantization errors with our proposed loss in Equation 2 as the mean error is higher near the output as it accumulates errors from previous layers in the block. This is also reflected in the parameter distribution of each layer type across all layers in Figure 3 , where the x-axis is the mean absolute quantization error and the y-axis is the layer indices. We see the quantization noise is more apparent on the output layer as the Gaussian distrbutions are non-smooth and have clear jitter effect. 4.1 Quantization Results on XGLUE.\nWe show the per task test performance and the understanding score (i.e average score) on XGLUE for quantization baselines and our proposed SDQ approaches in Table 1 (for brevity we denote InfoXLM Base as I and XLM-R Base as X). Our proposed QNAT Att-KLD achieves the best average (Avg.) score and per task performance for all tasks, using a fine-tuned InfoXLM Base (XNLI, NC, NER and QAM) and a fine-tuned InfoXLM Base trained with QuantNoise and dynamically quantized post-training (PAWSX, POS, QAM, QADSM and WPR). We also find that QNAT Att-KLD improves over QNAT KLD , highlighting that the attention loss is improving quantized model performance. In preliminary experiments we found it is better to distil from a fine-tuned teacher that has the same pretrained model type. Lastly, We note, that both of our proposed methods that achieve an 71.1 understanding score are within 1.0 understanding score of the original \"I\" fine-tuned FP-32 model. \n\nPerformance versus Compression Rate\nFigure 4 shows how the performance changes for four approaches, including two of our proposed objectives (QNAT KLD and QNAT Att-KLD ), when training InfoXLM Base . As before, PTQ dynamic is a dynamically quantization fine-tuned InfoXLM Base and QNAT-PTQ dynamic is the same as PTQ dynamic except fine-tuned also using QuantNoise. Unlike our previous results, here we apply fake quantization at inference to achieve compression lower than INT-8 and be comparable to previous work (Fan et al., 2019) . We see that performance is generally well maintained up until 8 bits. However, performance significantly degrades for all quantization methods for 4 and 2 bit weights. We find that QNAT Att-KLD maintains higher performance when compared to the baselines and directly quantizing with no QAT (PTQ dynamic ) leads to the poorest results, also reflected in Table 1 results with real dynamic quantization at inference time. \n\nAblation with Current QAT Methods\nTable 3 shows the results from a subset of the XGLUE tasks where the first two columns describe how the student and teacher networks are trained and \"Standard\" refers to standard FP-32 fine-tuning. This includes iPQ (Stock et al., 2019) with scalar quantization (iPQ Scalar ), iPQ that uses expectation maximization to create the codebook during training (iPQ EM ) and previous results of QuantNoise (QNAT) as a reference point. In this setup, we only apply the attention loss, \u2113 Attention , to the layers that are quantized during iPQ. When using SDQ, the average score increases by 1.9 points for iPQ Scalar , 1.9 points for iPQ Scalar 2.8 points for iPQ EM and 1.4 points for QNAT. Moreover, adding SDQ distillation of the logits and the self-attention outputs improves when compared to logit distillation only.\n\nConclusion\nIn this paper we proposed an attention-based distillation that minimizes accumulative quantization errors in fine-tuned masked language models. We identified that most of the quantization errors accumulate at the output of self-attention blocks and the parameter distribution of the output layer is effected more by quantization noise. The proposed distillation loss outperforms baseline distillation without the attention loss and the resulting INT-8 models are within 1 understanding score points on the XGLUE benchmark with real quantization post-training. Moreover, fine-tuning the teacher network with quantization-aware training can further improve student network performance on some of the tasks. Further compression can be achieved up to 4-bit and 2-bit weights but performance steeply degrades as the network capacity is drastically reduced coupled with the models having to generalize to multiple languages it was not trained on.\n", "hypothesis": " We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and achieves outstanding performance compared to existing approaches.  We apply SDQ to multilingual models XLM-R Base and InfoXLM Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark.  Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on..", "answer": false}
{"title": "The Ecological Fallacy in Annotation: Modelling Human Label Variation goes beyond Sociodemographics", "content": "\nIntroduction\nDifferent annotators will not necessarily assign the same labels to the same texts, resulting in human label variation (Plank, 2022) . Previous work finds that this variation depends at least in part on the sociodemographics of annotators, such as their age and gender (Binns et al., 2017; Al Kuwatly et al., 2020; Excell and Al Moubayed, 2021; Shen and Rose, 2021) . These results are particularly pronounced for subjective tasks like toxic content detection (Sap et al., 2019; Kumar et al., 2021; Sap et al., 2022; Goyal et al., 2022) . Since human label variation is relevant to a wide range of NLP tasks, recent research has begun to model individual annotator behaviour, rather than predicting aggregated labels (Davani et al., 2022; Gordon et al., 2022) . In this setting, we would expect sociodemographic attributes to help explain annotator decisions. Therefore, we investigate whether explicitly accounting for the sociodemographic attributes of annotators leads to better predictions of their annotation behaviour 1 .\nThere is a risk of misreading these efforts as an example of the ecological fallacy: aggregate group behaviour does not necessarily explain individual behaviour (Robinson, 1950; Freedman, 2015) . For example, while on average, white annotators may be more likely to label African-American Vernacular English as toxic (Sap et al., 2019) , that does not mean it is true for every white annotator individually. However, we aim at exactly this distinction to discuss the relevance of sociodemographic groups in models of individual annotator behaviour. Likewise, we do not assume prior work to commit ecological fallacies, even if a less-nuanced read might suggest it.\nDavani et al. ( 2022) introduce a simple multiannotator model, where each annotator is modelled with a separate classification head. We expand their model with group-specific layers, which are activated for each annotator based on their sociodemographic attributes. We compare the two model setups to a control setup where we randomise group assignments. All comparisons use annotator-level toxicity data from Kumar et al. (2021) . We find that find that explicitly accounting for sociodemo-graphic attributes does not significantly improve model performance. This result suggests that human label variation happens at a more individual level than sociodemographics, and that annotator decisions are even more complex.\nContributions 1) We introduce group-specific layers to model groups of annotators with shared attributes in multi-annotator models. 2) We evaluate the effect of group-specific layers for toxic content detection, and show that explicitly accounting for sociodemographic attributes does not significantly improve performance, thus highlighting the risk of the ecological fallacy in annotator modelling.\nAs a corollary, we show that multi-annotator models can be applied to many times more annotators than in prior work.\n\nSociodemographics in Annotation Behaviour\nA growing body of research studies how annotator sociodemographics relate to their annotation decisions, for tasks ranging from natural language inference (Biester et al., 2022) to the detection of racist (Larimore et al., 2021) or generally toxic (Sap et al., 2022) language. Goyal et al. (2022) , for example, find that annotators from certain sociodemographic groups (e.g., LGBTQ people) tend to find content attacking their own groups (e.g., homophobic content) to be more toxic. This motivates our research into explicitly accounting for sociodemographics to model annotation behaviour. However, the link between sociodemographics and behaviour is not uncontested. Biester et al. (2022) , for example, do not find significant differences in annotation behaviour between annotators of different genders for four different tasks.\nPredicting Annotators' Decisions on Text Different from analyses of annotation behaviour, a recent line of research attempts to learn models based on individual annotations (Plank et al., 2014; Jamison and Gurevych, 2015; Akhtar et al., 2020; Fornaciari et al., 2021; Cercas Curry et al., 2021) . These models are motivated by the concern that aggregating labels into a single \"truth\" is too simplistic for many tasks (Uma et al., 2021; Basile et al., 2021) and might introduce uneven representation of perspectives (Prabhakaran et al., 2021; Abercrombie et al., 2022) .\nA particular way of learning from disaggregated labels are models that predict individual annotator decisions for an example. Our work builds directly on such a model, multi-annotator models (Davani et al., 2022) , which we describe in more detail separately ( \u00a74). Gordon et al. (2022) present a model which also predicts individual annotations and allows a user to interactively aggregate them based on \"a jury\" inspired by the US judicial system. Their work is similar to ours in central aspects as they explicitly model annotators' sociodemographics and use the same dataset as we do (Kumar et al., 2021) . Different from our work, they frame the task as a regression problem and develop a model based on recommender systems. While they also explore ecological fallacies, they focus on usage risks of their system and countermeasures. In contrast, we consider the issue of the ecological fallacy in modelling annotation behaviour more generally. We compare our findings to their results ( \u00a76).\n\nData\nWe use a sample of the Kumar et al. (2021) dataset for our experiments. The full dataset contains 107,620 English comments from Twitter, Reddit, and 4Chan, annotated for toxicity by 17,280 annotators. The annotation process encouraged annotator subjectivity (R\u00f6ttger et al., 2022) which is a desired feature for modelling annotator behaviour. For each annotator, there is extensive sociodemographic information, collected with a survey. Annotations are given as ratings on a five-point scale which we convert to binary annotations by mapping ratings of 2 to 4 to toxic, and ratings 0 and 1 to non-toxic.\nWe randomly sample comments from the dataset until we reach annotations from more than 5,000 annotators. We then add all other annotations by these annotators. This approach maximizes the number of examples while controlling the number of annotators in our sample.\nOur final sample contains 111,780 annotations from 5,002 annotators on 22,360 comments with 20 to 120 annotations per annotator (mean 22.35). Most comments have five annotations. 20 comments have four because we removed any underage annotators before sampling. In total 78,357 annotations (70.10%) are toxic, and 33,423 annotations (29.90%) are non-toxic.\nWe focus on four sociodemographic attributes: gender, age, education, and sexual orientation. Group sizes vary by attribute. For gender, 2,450 annotators (48.98%) identify as female, 2,116 (42.30%) as male, 23 (0.46%) as non-binary (rest in residual categories, full statistics in A.1).\n\nExperiments\nWe compare three models. The baseline model is the multi-annotator model by Davani et al. (2022) . We use their multi-task variant: For each annotator, there is a separate classification layer trained on annotations from that annotator. All annotator layers share a pre-trained language model used to encode the input. We use RoBERTa (Liu et al., 2019) for this, motivated by computational constraints. The other models in our experiments build on this baseline model.\nFor the sociodemographic models, we add group-specific layers based on sociodemographic attributes of the annotators. A single attribute, e.g., age, implies several groups, e.g., ages 25-34, ages 35-44. We add the group-specific layers between the pre-trained model and the annotator layers. Each group of annotators shares a separate group-specific layer. We implement group-specific layers as fully-connected, linear layers, each learning a feature transformation applied for one group of annotators.\nFinally, for the random models, we shuffle the assignment of annotators to groups from the sociodemographic model, retaining the relative group sizes. In other words, the probability of each annotator staying in the same group or being reassigned to another group corresponds to the relative size of each group. This approach keeps the model architecture constant while removing the connection between actual sociodemographic attributes and group assignment. It allows us to distinguish the effects of additional parameters, which groupspecific layers add in comparison to the baseline, from the effects of sociodemographic information.\n\nEvaluation Setup\nWe evaluate all models on individual annotations from gender, age, education, and sexual orientation groups. This setup is comparable to the \"individual label\" evaluations in Davani et al. ( 2022) and Gordon et al. (2022) , but with scores calculated per group of annotators. We measure performance in macro-average F 1 , to weigh each class equally.\n\nCross-Validation\nAs there is no standard split available for our dataset, we perform three iterations of a four-fold cross-validation with different seeds (training details in Appendix A.3). We choose four folds, so that even very small groups have more than a hundred annotations in each test set. Across folds, the numbers of annotations per sociodemographic group are similar (see Appendix A.4). We construct test sets that only contain comments unseen by the annotators in the training set. We also ensure that all test sets have similar proportions of toxic or non-toxic comments (assigned by the majority of annotators) to address the class imbalance in the dataset (70.62% toxic, see \u00a73).\n\nStatistical Significance\nWe test for statistical significance of our results from multiple runs of k-fold cross-validation via replicability analysis (Dror et al., 2017) . We report the number of significant folds and the Bonferroni-corrected count (Dror et al., 2018) in Appendix A.2. We compute the pvalues for each fold via a paired bootstrap-sampling test with BooStSa (Fornaciari et al., 2022) . We set the significance level \u03b1 = 0.05, draw 1000 bootstrap samples per fold, and use a sample size of 50% of the respective test set.\nRemarks on Groups Annotators from different groups of the same attribute will in most cases not have annotated the same examples. Therefore, comparisons between models are only meaningful within each group.\nThe groups modeled via group-specific layers and those in the result tables are always the same. For example, if we report scores for gender groups, then the sociodemographic and randomized models are also based on gender groups. In the following, we focus on a subset of groups, omitting, e.g., \"Prefer not to say\" (see Appendix A.5).\n\nResults\nTable 1 shows the results for gender, age, education, and sexual orientation. A naive majority class baseline that predicts all input to be toxic performs worse than all other models with a large margin (exact results in Appendix A.5).\nSociodemographics vs. Baseline Across attributes, the average scores of the sociodemographic model and the baseline are similar. The sociodemographic model often has a slightly higher average macro F1 than the baseline, but no statistically significant gains. Where average performance is better by several points, as for homosexual annotators, this gain is offset by a large variance in performance (a consequence of small group sizes).\nSociodemographics vs. Random We also do not find significant performance differences between sociodemographic group-layer models and the corresponding random group assignment models. For most groups, the randomized models achieve the highest average scores, but differences to the sociodemographic model are never statistically significant. \n\nDiscussion\nWe do not find strong evidence that explicitly modelling sociodemographics helps to predict annotation behaviour with multi-annotator models. These results might seem counter-intuitive, given the evidence of systematic annotation differences between sociodemographic groups (see \u00a72). This discrepancy, however, echoes the issue highlighted by ecological fallacies (Robinson, 1950) : Not every annotator will be a perfect representative of their group, so we will not necessarily learn additional information based on their group identity. This seems especially true if we already have access to individual behaviour (i.e., individual annotations).\nIn contrast to Davani et al. ( 2022), we made sociodemographic information explicit in our experiments, as one of the factors influencing annotation behaviour. Group-specific layers can be seen as an inductive bias putting emphasis on the sociodemographic relations between annotators. However, there are potentially many other factors influencing annotation behaviour (e.g., attitudes, moral values, cognitive biases, psychological traits). In light of our results, it seems plausible that multi-annotator models learn about these factors implicitly as part of predicting individual behaviour, so that making one factor explicit does not change prediction quality, at least in the case of sociodemographics.\nStill, we also know that generally group attributes can help predict individual decisions, i.e., as base rates or priors. To avoid ecological fallacies in modelling annotation, we therefore need to better understand when and how modelling sociodemographic information is useful in predicting an individual annotator's decisions. For example, we have only evaluated group-specific layers for single attributes. In contrast, social scientists have long adopted the idea of intersectionality (Crenshaw, 1989) , which also informs research on fairness in machine learning (Wang et al., 2022) . Intersectionality means that the effect of interactions between sociodemographic attributes enables specific experiences that are not captured by the attributes in isolation. For example, identifying as a man means something different depending on the person's education. Groups derived from single attributes might simply be too coarse to improve classifiers learnt from individual labels, as in multi-annotator models.\nThe dataset we use (Kumar et al., 2021) has many characteristics which are ideal for our study (see \u00a73). However, it uses a broad notion of toxicity, in contrast to other studies of toxic language (Larimore et al., 2021; Sap et al., 2022) , which match content and analysed groups. When modeling the groups frequently referenced in the datasets themselves, we would expect greater benefits from group-specific layers. Similar to us, Biester et al. (2022) who do not find significant differences between annotators of different genders, do so in a more general setting.\nWe can only partially compare to Gordon et al. (2022) , despite using the same dataset. In addition to differences in approach (see \u00a72), our and their work also differ in their research questions and thus experimental conditions. Gordon et al. (2022) compare their full model (group and individual) against using group information alone.\nWe compare our full model (group and individual) against using individual information alone. So it is unclear if their model would benefit from group information in comparison to individual-level information alone. While they find an improvement from group information it is only in comparison to a baseline predicting not individual but aggregated labels. Additionally, the composition of test sets sampled from the full dataset differs between the studies: Gordon et al. (2022) use a test set of 5,000 comments, while we use 22,360 comments in a four-fold cross-validation. We leave an explicit comparison to future work.\nGroup-specific layers ( \u00a74) are a natural extension of annotator-specific classification layers in multi-annotator models. However, other architectures to predict annotator-level labels use different ways to represent sociodemographic information, e.g., via embeddings in a recommender system (Gordon et al., 2022) . Future work could explore additional representations of annotator attributes (e.g., as part of the input, either textual or as separate features) and other approaches to modelling the relation of individual labeling decisions and attributes (e.g., probabilistic graphical models).\n\nConclusion\nWe ask how relevant modelling explicit sociodemographic information is in learning from individual annotators. Our experiments with group-specific layers for four sociodemographic attributes on social media data with toxicity annotations (Kumar et al., 2021) show no significant benefit of modelling sociodemographic groups in multi-annotator models. However, as the issue of ecological fallacies highlights, it is not implausible that these models do not learn additional information from group information beyond the inherent variation. However, our results do not refute the usefulness of sociodemographic attributes in modelling annotation, but underscore the importance of their judicious use. Different tasks and model architectures will likely benefit to different extents. Ultimately, annotation behaviour is driven by complex factors and we will need to consider more than annotators' sociodemographics.\n", "hypothesis": " Many NLP tasks exhibit human label variation, where different annotators give different labels to the same texts.  This variation is known to depend, at least in part, on the sociodemographics of annotators.  Recent research aims to model individual annotator behaviour rather than predicting aggregated labels, and we would expect that sociodemographic information is useful for these models.  On the other hand, the ecological fallacy states that aggregate group behaviour, such as the behaviour of the average female annotator, does not necessarily explain individual behaviour.  To account for sociodemographics in models of individual annotator behaviour, we introduce group-specific layers to multi-annotator models.  In a series of experiments for toxic content detection, we find that explicitly accounting for sociodemographic attributes in this way does not significantly improve model performance.  This result shows that individual annotation behaviour depends on much more than just sociodemographics..", "answer": true}
{"title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation", "content": "\nIntroduction\nSimultaneous speech translation (SimulST) refers to the process of producing an output translation concurrently with an oncoming source speech input. For humans, performing accurate SimulST is extremely difficult and becomes nearly impossible to perform over long periods of time. Given the potential broad applications of SimulST in industry and government sectors, there is a strong need for machine learning models to perform the task to a level above the capabilities of humans.\nOne branch of machine learning models that have been effective in SimulST is transformers (Vaswani et al., 2017) using block processing, a process that breaks an input sequence into segments which the encoder processes sequentially and individually (Dong et al., 2019) . As later segments may lose earlier information in a sentence (i.e., context fragmentation), techniques known as left context and memory banks have been introduced. The concept of left context was idealized with the Transformer-XL (Dai et al., 2019) , a model optimized for language modeling, which was later adapted for streaming automatic speech recognition (ASR). The Transformer-XL generated left context by saving the previous segment to each encoder layer, so the subsequent segment could include it in the attention calculation at the same encoder layer. Memory banks were later introduced in the self-attention calculation of the Augmented Memory Transformer (Wu et al., 2020) , allowing it to outperform the Transformer-XL in streaming ASR and also be state-of-the-art in SimulST (Ma et al., 2021) . These memory banks were token summarizations of previous segments and helped retain explicit long-term dependencies. The Augmented Memory Transformer also included the left context alongside the center (main) segment tokens with an additional right context, all of which add computational cost. We argue that the methods to generate and use left context and/or memory banks in both the Transformer-XL and Augmented Memory Transformer are naive, costing both models' performance at a given computational budget.\nIn this paper, we propose a computationally efficient architecture, the Implicit Memory Transformer, that implicitly retains memory through a novel left context generation method, thereby removing the need for memory banks entirely. Briefly, the proposed left context method for a given encoder layer leverages the previous segment's attention output in the attention calculation of the current segment. Our method for calculating left context is broadly applicable to any transformer model that utilizes block processing. The proposed Implicit Memory Transformer is more computationally efficient than the Augmented Mem-ory Transformer, reducing the cost of self-attention calculation, convolution layers, and feed-forward layers.\nWe conduct our experiments on the English-German, English-French, and English-Spanish language pairs of the MuST-C dataset (Cattoni et al., 2021) and demonstrate a significant speedup over the Augmented Memory Transformer for the forward pass of the encoder, with no reduction in the translation quality across all wait-k values.\n\nBackground and Related Works\nAugmented Memory Transformer: For SimulST, a transformer model waits for k token chunks before beginning translation, a policy referred to as wait-k (Ma et al., 2018) . One such transformer that uses this wait-k policy is the Augmented Memory Transformer (Ma et al., 2021) . The Augmented Memory Transformer breaks an input sequence into segments S i n \u2208 R s\u00d7d , where n denotes the segment position in the sequence and i denotes the layer index in the Augmented Memory Transformer. Each segment is composed of a left context\nL i n \u2208 R l\u00d7d of size l, a center context C i n \u2208 R c\u00d7d of size c, and a right context R i n \u2208 R r\u00d7d of size r.\nEach segment is of size s = l + c + r and overlaps with the previous and subsequent segments with the left and right context. Unlike the default transformer, the encoder of the Augmented Memory Transformer possesses two subsampling convolution layers to reduce the size of the segment inputs.\nIn the self-attention calculation for the encoder, memory banks, M i n \u2208 R N \u00d7d , are added to the keys and values where N denotes the maximum number of memory banks for a given layer. Each layer's memory banks summarize the previous segments and are theorized to allow the model to retain explicit long-term memory. Each memory bank is created using the attention output of a summarization query, \u03c3 i n \u2208 R 1\u00d7d , included in the attention calculation. This summarization query is calculated by averaging the tokens in the current segment. For any given layer, the queries, keys, and values can be represented by the following equations:\nQ i n = W i q [L i n , C i n , R i n , \u03c3 i n ]\n(1)\nK i n = W i k [M i n , L i n , C i n , R i n ]\n(2)\nEQUATION\nIn each of the equations, W i q , W i k , and W i v are the query, key, and value projection matrices\nfor layer i. The [.] operator concatenates L i n , C i n , R i n with \u03c3 i n or M i n .\nAfter the encoder processes each individual segment, they are concatenated before being provided to a simultaneous decoder (Ma et al., 2020b) . Average Lagging: Average Lagging is one prominent metric to determine the efficacy of a SimulST model (Ma et al., 2018) . It denotes in milliseconds the lag between the output translation and the input source sequence (Ma et al., 2020b) . BLEU Score: An equally important metric to evaluate a SimulST model is the BLEU score, which measures the translation similarity between the predicted output and the target output. The BLEU score ranges from 0 to 1 and is often represented with percentages (Papineni et al., 2002) .\n\nImplicit Memory Transformer\nWe propose an Implicit Memory Transformer that leverages a new left context generation method to retain an implicit memory of previous segments. As such, we are able to remove the explicit memory provided by the memory banks that are expensive to compute in the Augmented Memory Transformer. Our new implicit memory left context is unique at each layer of the encoder, whereby it is composed of a portion of the output from the self-attention calculation of the previous segment's center context.\nSpecifically, suppose our implicit memory left context is denoted as Z i n \u2208 R l\u00d7d . Then, in the self-attention calculation of the Implicit Memory Transformer, the queries, keys, and values for each layer's attention calculation can be calculated as follows:\nEQUATION\nK i n = W i k [Z i n , C i n , R i n ]\n(5)\nEQUATION\nIn comparison with the calculation of the queries, keys, and values of the current state-of-the-art Augmented Memory Transformer shown in Equation 1, 2 and 3, our Implicit Memory Transformer has three notable differences consisting of:\n1) Removed memory banks: The memory bank terms in Equation 2 and 3 not only provide the model with explicit long-term memory but also introduce a recurrence mechanism to the transformer, which is a form of implicit memory. By removing memory banks and instead including the recurrence mechanism in the left context, we capture the benefits of this implicit memory without the additional cost to compute memory banks.\n2) Attention-based left context: In using the output from the attention calculation of the previous segment rather than the raw segment input as left context like the Transformer-XL, we are able to capture a learned representation of the previous segment at a given layer. This is similar to the Augmented Memory Transformer using the attention output associated with the summarization query as a memory bank. However, since we do not compress the segment into a summarization query, we capture a more realistic representation.\n3) Removed left context in the queries: The Augmented Memory Transformer, includes the left context in each segment and, subsequently, the queries to allow it to generate a learned representation of the left context alongside the current segment. However, since our Implicit Memory Transformer already has a saved learned representation of the left context for a given layer, it removes the need to include the left context in the segment.\nFrom the above attributes, the self-attention calculation of the Implicit Memory Transformer becomes more efficient than that of the Augmented Memory Transformer, as memory banks are no longer included in the keys and values, and the left context and summarization query are removed from the queries. Furthermore, our Implicit Memory Transformer reduces the computation cost of the feed-forward neural network and the convolution subsampling layers, as they no longer need to process tokens contained in the left context.\n\nComplexity Analysis\nWe will now perform complexity analysis for the self-attention and convolution subsampling layers in the Augmented Memory Transformer. The complexity analysis of a convolution subsampling layer with a kernel size of one is identical to that for the linear transformations in the feed-forward network. The self-attention layer has a complexity of O(n 2 \u2022 d) and the convolution layer has a complexity of O(K \u2022 n \u2022 d 2 ) where n is the input sequence length, d is the hidden size, and K is the kernel size (Vaswani et al., 2017) .\nThe complexity of the self-attention layer of the old Augmented Memory Transformer would thus be O((N + l + c + r)(l + c + r) \u2022 d) and the complexity with the new method of calculating left context would be O((c + r)(l + c + r) \u2022 d).\nSimilarly the complexity of the convolution layers would change from O(K\n\u2022 (l + c + r) \u2022 d 2 ) to O(K \u2022 (c + r) \u2022 d 2 ).\nGiven the computational complexity decrease for all layers in the Augmented Memory Transformer with respect to the left context size and memory banks, it lends to the possibility of increasing the left context size for greater translation performance.\n\nExperimental Setup\nWe conducted experiments on the English-German (en-de), English-French (en-fr), and English-Spanish (en-es) language pairs from the MuST-C dataset (Cattoni et al., 2021) . The data preparation scripts for the MuST-C dataset are provided in Fairseq 1 (Ott et al., 2019; Wang et al., 2020) , whereby Kaldi is used to generate 80-dimensional log-mel filter bank features, and text is tokenized with a SentencePiece 10k unigram vocabulary. The statistics of the training, development, and test set (tst-COMMON) for the English-German, English-French, and English-Spanish language pairs of the MuST-C dataset are provided in Table 1 .\n\nLanguage Pair Train\nDev Test en-de 250942 1415 2580 en-fr 275085 1412 2632 en-es 265625 1316 2502\nTable 1 : The number of sentences in the train, development, and test (tst-COMMON) sets of the MuST-C dataset for the en-de, en-fr, en-es language pairs (Cattoni et al., 2021) .\nThe architectures of the Augmented Memory Transformer and Implicit Memory Transformer trained were nearly identical, containing 33.1 M parameters (Ma et al., 2021) . Their encoders consisted of 12 layers beginning with two convolution layers with a combined subsampling factor of 4, followed by a feed-forward neural network. Their decoders consisted of 6 layers. Each of these layers has a hidden size of 256 with 4 attention heads. Relative positional encodings were applied to each self-attention layer with a clipping distance of 16 (Shaw et al., 2018) . Layer normalization was performed prior to each layer. Additionally, we trained each model with a wait-1, wait-3, wait-5, and wait-7 policy using a pre-decision ratio of 8 (Ma et al., 2020b) . We provide public access to a derivative of Fairseq containing our implementation for the Implicit Memory Transformer 2 .\nAll training was performed on a single V100-32GB. The training process consisted of ASR pre-training followed by SimulST training. For SimulST training, the models were trained with label-smoothed cross-entropy loss, the Adam optimizer (Kingma and Ba, 2014), and an inverse square root scheduler. There was a warm-up period of 7500 updates where the learning rate of 0.0001, followed by a learning rate of 0.00035. To regularize the model weights, we used a weight decay value of 0.0001, a dropout of 0.1, an activation dropout of 0.2, and an attention dropout of 0.2. All models were trained with early stopping using a patience of 10. After the training was complete, the final ten checkpoints were averaged.\nThe translation quality and latency were determined by detokenized BLEU with SacreBLEU (Post, 2018) , and Average Lagging (Ma et al., 2020b) , respectively. Both of these metrics were obtained using the SimulEval toolkit 3 , which simulates SimulST (Ma et al., 2020a) .\n\nPerformance Evaluation\nWe demonstrate the efficacy of our Implicit Memory Transformer on the English-German language pair for a single run in Figure 1 in terms of average lagging and BLEU score. versus its memory bank counterpart across all waitk values. This confirms the effectiveness of the attention-generated left context of the proposed Implicit Memory Transformer for the English-German language pair.\nWe see similar results with the English-French and English-Spanish language pairs provided in Figure 2 and Figure 3 , respectively. In both cases, the Implicit Memory Transformer performs nearly identically to the Augmented Memory Transformer using memory banks by not negatively impacting either the BLEU score or Average Lagging. Additionally, as with the results in Figure 1 , the Augmented Memory Transformer sees an average decrease of 6.23 BLEU and 4.47 BLEU across all wait-k values when memory banks are removed for the English-French and English-Spanish language pairs respectively. Once again substantiating the efficacy of our attentiongenerated left context in the Implicit Memory Transformer, which does not see a performance decrease without memory banks.\n\nEvaluation Speedup\nWe provide a demonstration of how the left context size affects the forward pass time of a segment through the encoder of an Augmented Memory Transformer with three memory banks, an Augmented Memory Transformer without memory banks, and the Implicit Memory Transformer in Figure 4 . The left context size is scaled with tokens, and the duration of the forward pass of a segment through the encoder is scaled in milliseconds. Each model compared uses a right context of 32 tokens and a center context of 64 tokens for each tested left context size. Each measurement point in Figure 4 is made by averaging the duration of ten forward passes through the encoder using two 14-core 2.20 GHz Intel Xeon Gold 5120 with 19712 KB cache. \n\nConclusion\nAchieving computationally efficient simultaneous speech translation (SimulST) is critical to its deployment in practical real-time applications. However, even with the state-of-the-art SimulST approach of the Augmented Memory Transformer, its method of generating left context is computationally costly and ineffective, requiring the usage of memory banks to compensate for its shortcomings. As such, we propose an Implicit Memory Transformer that utilizes an attention-based left context to provide the model with implicit memory. We found that the Implicit Memory Transformer was able to achieve nearly identical performance to the Augmented Memory Transformer at a significantly reduced computational cost.\n", "hypothesis": " Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs.  For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass, but at the cost of significantly lower translation quality compared to the state-of-the-art approach that employs both left context and memory banks.", "answer": false}
{"title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification", "content": "\nIntroduction\nToxic texts with offensive and abusive words are frequently encountered in online forums and social media. Such a harmful online environment can lead to mental health problems (Viner et al., 2019; Wijesiriwardene et al., 2020) , which motivates considerable research efforts (dos Santos et al., 2018; Laugier et al., 2021; Logacheva et al., 2022) in text detoxification, i.e., a conditional text generation task aiming to remove offensive content from sentences while preserving their meanings.\nIntuitively, there exist diverse ways to detoxify a given sentence. As shown in Table 1 , some detoxified sentences are the results of simply removing 1 https://github.com/D3Mlab/diffu-detox\n\nToxic\nThe country doesn't really have to give a shit about international laws.\nDetoxified 1\nThe country doesn't really have to give\n[\u2022 \u2022 \u2022 ] about international laws.\nDetoxified 2\nThe country doesn't really have care about international laws.\nDetoxified 3\nThe country doesn't really need to care about international laws.\n\nHuman\nThe country doesn't need to care about international laws.\nTable 1: A diverse collection of detoxified sentences helps to approach human-level text detoxification.\nor replacing the toxic word, e.g., Detoxified 1 and 2, which may cause loss of information or lower text fluency. While other candidates, e.g., Detoxified 3, can reach human-level text detoxification performance with satisfactory fluency and content preservation. Therefore, if a diverse collection of detoxified sentences are given, we can select the most fluent and preservative one to maximize user experience. To do so, we resort to textual conditional diffusion models (Li et al., 2022; Gong et al., 2022) because they are shown to be capable of generating more diverse sets of candidates compared to existing solutions based on transformers (Vaswani et al., 2017) , e.g., GPT2 (Radford et al., 2019) . Given their demonstrated high generative diversity, diffusion models are particularly suitable for this task. Nevertheless, previous textual conditional diffusion models (Li et al., 2022; Gong et al., 2022) are not directly applicable to text detoxification due to the scarcity of text detoxification data. Given that text detoxification is a relatively new field and the high cost of human annotations, the available text detoxification data is on the order of 1e \u22121 to 1e \u22122 of datasets used for other tasks with textual conditional diffusion models (Gong et al., 2022) .\nTo this end, we introduce DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. In particular, the conditional\n\nConditional Gate (Closed with probability )\nThey are behaving exactly like any greedy bully would.\n\nToxic (Source)\nThey are behaving unfairly. Non-toxic (Reference) . . . model takes toxic text as a condition and through a Markov chain of diffusion steps, yields a diverse set of detoxified sentences. On the other hand, the unconditional model is trained to recover any given input text exactly. That allows us to introduce additional fluent text to be reconstructed by the unconditional model, which is used to improve the fluency of the conditionally generated detoxified sentences. In this way, the resulting diffusion model can maintain a diverse collection of detoxified candidates with satisfactory sentence fluency and content preservation. Extensive experimental results and in-depth discussions demonstrate the effectiveness of DiffuDetox for text detoxification. Our main contributions are summarized in two folds: 1) To the best of our knowledge, we are the first to approach text detoxification with diffusion models, which can maintain a rich collection of detoxified sentences by their high generative diversity; 2) We propose a mixed diffusion model for text detoxification, where the conditional model reduces text toxicity and the unconditional model improves text fluency.\n\nText Detoxification\nPrevious text detoxification efforts fall into two main categories, supervised and unsupervised. The unsupervised methods are built on a set of toxic and a set of non-toxic texts without one-to-one mappings between them. Representative methods include Mask&Infill (Wu et al., 2019) , DRG-Template/Retrieve (Li et al., 2018) , DLSM (He et al., 2020) , SST (Lee, 2020) , CondBERT and ParaGeDi (Dale et al., 2021) . In contrast, the supervised methods are built on parallel datasets in which one-to-one mappings between toxic and non-toxic texts are explicitly provided. ParaDetox (Logacheva et al., 2022 ) is a well-established method within this category, which fine-tunes BART (Lewis et al., 2020) on their parallel data.\n\nTextual Diffusion Models\nDiffusion probabilistic models are deep generative models with Markov chains of diffusion steps to recover the noise slowly added to data (Sohl-Dickstein et al., 2015) . Recently, diffusion models have shown impressive performance on continuous domains such as image and audio generation (Ho et al., 2020; Kong et al., 2020) , sparking interest in using these models in discrete spaces like text. Some textual diffusion models use a discrete diffusion process that operates on word tokens (Savinov et al., 2022; Reid et al., 2022) , whereas other methods convert text to embeddings, and then treat text as continuous variables (Li et al., 2022; Strudel et al., 2022) . Although textual diffusion models have proved to be effective in various text generation tasks with rich data (Gong et al., 2022) , they\nhave not yet been applied to tasks with fewer training samples, such as text detoxification in our case. Ho and Salimans (2021) are the first to exploit unconditional diffusion models for conditional generation, while their method is limited to images and is not aiming for introducing additional data under the low-data setting.\n\nMethodology\nAs the overall framework of DiffuDetox shown in Figure 1 details, our proposed diffusion model for text detoxification improves text fluency in the low-training data regime by using a mixture of a conditional and unconditional diffusion model. We overview diffusion models before discussing DiffuDetox in detail.\n\nDiffusion Models\nDiffusion is a generative modeling paradigm that can be understood as a denoising algorithm (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2021) . Noise is gradually added to data samples, while the diffusion model is trained to reverse the process and recover the original data. The framework can be described as a Markov process with T steps, where the original data exist at t = 0. Given a sample x 0 , the so-called forward process gradually adds noise to the data points, i.e., the blue arrows in Figure 1 . The noisy sample can be described by:\nq(x t |x t\u22121 ) := N (x t ; 1 \u2212 \u03b2 t x t , \u03b2 t I) (1)\nwhere the variance schedule parameters \u03b2 1 , \u2022 \u2022 \u2022 , \u03b2 T are selected such that \u03b2 t \u2208 [0, 1] and \u03b2 0 is close to 0 and \u03b2 T is close to 1 (Ho et al., 2020) . This ensures that when t \u2248 0, the data has little noise added to it, while when t \u2248 T , the data is identical to a sample from a standard Gaussian distribution.\nThe reverse process then attempts to remove the noise that was added in the forward process and is parameterized by \u03b8 as:\np \u03b8 (x t\u22121 |x t ) := N (x t\u22121 ; \u00b5 \u03b8 (x t , t), \u03c3 t I) (2)\nwhere the predictive model \u00b5 \u03b8 is:\nEQUATION\nwhich depends on time-dependent coefficients \u03b1 := 1 \u2212 \u03b2 t , \u1fb1t := t s=1 \u03b1 s . In Eq. ( 3), \u03f5 \u03b8 is interpreted as predicting the noise that was added to x t . To optimize the log-likelihood of this model, a simplified training objective is used which reduces the problem to:\nL = Et,x 0 ,\u03f5[\u2225\u03f5 \u2212 \u03f5 \u03b8 ( \u221a \u1fb1tx0 + \u221a 1 \u2212 \u1fb1t\u03f5, t)\u2225 2 ] (4)\nAfter training, samples are generated by beginning with pure noise from a standard Gaussian distribution, which is then gradually denoised T times by the learned reverse process.\n\nDiffuDetox: A Mixed Diffusion Model for Text Detoxification\nThe task of text detoxification can be viewed as generating a non-toxic sentence, conditioned on a toxic input sentence. The goal is to ensure that the semantics and content of the text are preserved after detoxification, while ensuring that the generated text is fluent. With this interpretation (Gong et al., 2022) , we can apply a conditional diffusion model that generated non-toxic text, when conditioned on a toxic sentence. A conditional diffusion model is modified such that the reverse process is now p \u03b8 (x t\u22121 |x t , c), and the predictive model is \u03f5 \u03b8 (x t , c, t). This model can be interpreted as mapping sequences to sequences in a non-autoregressive manner. To apply this model to textual data, sentences are tokenized and converted to a stack of embeddings which are then taken to be x 0 in the diffusion process. When sampling, embeddings that are generated by the diffusion model are converted to tokens by a shallow single-layer decoder.\nWhile diffusion models have high sample diversity which can be used to generate a large number of candidate items, the fluency of the samples is degraded when trained on a smaller dataset. We propose to use a combination of the conditional model diffusion model as well as an unconditional model to tackle this problem. The conditional model is used to detoxify text, whereas the unconditional model can be used to guide the sampling process towards higher quality samples (Ho and Salimans, 2021) . The models are combined in a manner that is inspired by the gradient of an implicit classifier p i (c|x) \u221d p(x|c)/p(x) such that the following linear combination of the models is used for sampling:\n\u03b5\u03b8 (x, c) = (1 + w)\u03f5 \u03b8 (x, c) \u2212 w\u03f5 \u03b8 (x) (5)\n4 Experiments\n\nExperimental Settings\nDatasets. We conduct our experiments upon a well-established benchmarking dataset ParaDetox 2 (Logacheva et al., 2022) , which provides humanannotated one-to-one mappings of toxic and nontoxic sentence pairs from 20,437 paraphrases of 12,610 toxic sentences. We use the same data split of Logacheva et al. (2022) with 671 testing sentences for fair performance comparisons. We further consider the BookCorpus (Zhu et al., 2015) , MNLI (Wang et al., 2019) , and WikiAuto (Jiang et al., 2020) , datasets as additional data for unconditional diffusion model training.\nEvaluation Metrics. We follow the wellestablished text detoxification work (Logacheva et al., 2022) to evaluate DiffuDetox with BLEU, Style Accuracy (STA), Content Preservation (SIM), Fluency (FL), and J score. In particular, STA and FL are computed with pre-trained classifiers (Warstadt et al., 2019) to measure the non-toxicity and fluency of a given sentence, respectively. And we compute SIM using cosine similarity between the input and the generated detoxified text with the model of Wieting et al. (2019) . Moreover, we compute J score (Krishna et al., 2020) as the averaged multiplication of STA, SIM, and FL, which is highly correlated with human evaluation as shown by Logacheva et al. (2022) .\nImplementation Details. We implement our mixed conditional and unconditional models with a single diffusion model where c = \u2205 for the unconditional case. During training, the conditional model is selected with probability \u03c6 = 0.8, and the unconditional model is trained using the non-toxic sentences sampled from the ParaDetox dataset and the additional dataset with equal probabilities. We use the union of the BookCorpus, WikiAuto, and MNLI as the additional dataset. In the test stage, we select the best samples from a candidate set of 20 using the J score. The reported results are from a model trained for 1e 5 steps with a batch size of 32, and the mixture weighting parameter w in Eq. ( 5) is set to 5. We use the text detoxification methods listed in Section 2.1 as baselines.\n\nExperimental Results\nPerformance Comparison. We have two key observations from the results shown in Table 2 . Firstly, our proposed DiffuDetox outperforms most baseline methods on most evaluation metrics, and it is reaching state-of-the-art performance by outperforming ParaDetox on two metrics, demonstrating the effectiveness of our proposed method. Another observation is that DiffuDetox achieves a higher J score than human-level text detoxification. Note that the J score has been shown to be highly correlated with human annotations (Logacheva et al., 2022) . This human-level performance of DiffuDetox shows its promise to be deployed in real-world text detoxification scenarios to facilitate users in online forums and social media. \n\nConclusion\nIn this paper, we approach the text detoxification task with diffusion models for their demonstrated high generative diversity. We introduced DiffuDetox, a mixed conditional and unconditional diffusion model, where the conditional part reduces toxicity whereas the unconditional part ensures fluency. Experimental results show DiffuDetox achieves human-level text detoxification performance, making it promising to be applied in realworld text detoxification systems to benefit users.\n", "hypothesis": " Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text.  It is highly useful for online forums and social media, where offensive content is frequently encountered.  Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users.  Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models.  Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task.  In this work, we propose DiffuDetox 1 , a mixed conditional and unconditional diffusion model for text detoxification.  The conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences.  The unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency.  Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox..", "answer": true}
{"title": "Should you marginalize over possible tokenizations?", "content": "\nIntroduction\nLanguage models are probability distributions over text strings. In practice, these distributions are defined over a vocabulary of tokens, such as words, punctuation marks, and other special symbols (Jurafsky, 2000; Goldberg, 2017) . As long as a unique token sequence encodes any given string, the probability of a string according to the language model is equal to the probability of the corresponding token sequence. However, with today popular sub-wordlevel tokenizations this is not the case, as there are (exponentially) many possible tokenizations for any given string. For example, with the vocabulary V = {a, ab, b, c, ca, cab}, the string \"cab\" can be tokenized into cab, c/a/b, ca/b, c/ab. Therefore, the true probability that the language model assigns to the corresponding string is that obtained after marginalizing over all possible tokenizations. Yet, the common practice disregards this fact, computing the string probability by scoring a single default tokenization (e.g., cab). The implicit assumption from the community is that the probability mass of non-default tokenizations is negligible. However, this assumption has not been adequately evaluated yet.\nIn part, Cao and Rimell (2021) addressed this very same question, by conducting a pioneer study to quantify the gap between the default and marginalized probabilities. Their experiments with Transformer-XL pretrained on the WMT data (English and German) show negligible changes in perplexity with respect to using a single default tokenization for in-domain data and 0.9-1.9% improvement in perplexity for out-of-domain data, such as arXiv articles. Because exact marginalization is intractable in practice, marginalized probabilities were estimated using importance sampling. Importance sampling computes an unbiased estimate of the marginalized probabilities as an average over tokenizations sampled from a proposal distribution. Cao and Rimell (2021) exploited the probabilistic nature of the UnigramLM tokenizer (Kudo, 2018) to define such a proposal. As a consequence, their results do not necessarily extend to the more popular language models like GPT-2 (Radford et al., 2019) , GPT-3 (Brown et al., 2020) , BLOOM (Scao et al., 2022) , T5 (Raffel et al., 2020) , among others, trained using other tokenization schemes such as BPE (Sennrich et al., 2016) , WordPiece (Schuster and Nakajima, 2012) , among others.\n\n1\nIn this work, we devise a new proposal distribution that allows us to quantify the effect of marginalization for any given tokenizer. Equipped with this algorithm, we inspect the effect of marginalization over tokenizations for two LMs, GPT-2 (126M parameters, English) and the recently released BLOOM (1.7B parameters, multilingual), on various domains and languages. Our importance sampling estimates show that in practice marginalization does not influence log-likelihood much (usually less than 0.5% improvement), the highest influence (1-2% improvement) being for data with long, complex words and distribution shift. Because the results will vary for different models and data, we provide a tool for researchers and practitioners to measure the gap in their specific setting to decide whether the usual practice is warranted. To this end, we release our code 1 , which can be applied to models from the transformers library.\n\nPreliminaries\nLet us consider a sequence of characters S that we wish to score with an autoregressive language model P . Typically, S is split into a sequence T = t 1 , . . . , t n of tokens t i \u2208 V , where V is the model's vocabulary, a process commonly known as tokenizing the sequence. Then we can compute a score for a tokenization T of the sequence S, P (T, S), using the chain rule:\nP (T, S) = 1[T \u2192 S] |T | j=1 P (t j |t j\u22121 , . . . , t 1 )\nwhere T \u2192 S indicates that T is a valid tokenization of S. Commonly used tokenization algorithms such as BPE or WordPiece provide a deterministic procedure for obtaining a particular way of tokenizing S into T , which we refer to as the default tokenization. Yet, in general, for the same sequence, there exist (exponentially) many possible tokenizations with vocabulary V , which also typically receive some probability mass by the LM. To obtain the true probability score for the sequence S, we should marginalize over all valid tokenizations: P (S) = T :T \u2192S P (T, S).\nHowever, computing P (S) is typically intractable given the exponential number of valid T \u2190 concat(T, Xj * ) 11:\nq \u2190 q \u2022 sj * 12: Q(T |S) \u2190 q 13: return T , Q(T |S)\ntokenizations. Nonetheless, this value can be estimated through importance sampling, as follows. Introducing a proposal distribution Q(T |S) over all tokenizations T of a sequence S, such that P (T, S) > 0 \u21d2 Q(T |S) > 0, we can rewrite the probability P (S), as follows:\nEQUATION\nNow we can estimate P (S) by sampling K independent tokenizations from the proposal:\nP (S) \u2248 1 K K k=1 P (T k , S) Q(T k |S) , T k \u223c Q(T |S) (2)\nThe quality of this estimate depends on the chosen proposal distribution: the closer the proposal Q(T |S) is to the true posterior distribution P (T |S), the smaller the variance of the unbiased estimate (2) tends to be. 2\n\nProposed approach\nWe introduce a novel proposal Q(T |S) based on the LM itself with the intention to make it naturally closer to the posterior. Importantly, this proposal can be used for any tokenizer enabling its application to well-known state-of-the-art systems. The procedure for sampling from this proposal is presented in Algorithm 1 and also illustrated in Figure 1 . In summary, the algorithm samples a tokenization T by building it incrementally as the concatenation of token subsequences T i . Each token subsequence is sampled from the language model while always ensuring that the resulting tokenization is valid for the target S. To achieve this, the algorithm breaks S into a sequence of character blocks B, and only samples tokenizations T i that are valid for the corresponding block B i . Notably, in the extreme case of splitting S into a single block B 1 = S, our proposal Q(T |S) turns into the true posterior P (T |S), allowing to compute the exact marginalization with a single sample, as noted in footnote 2. However, because sampling a valid tokenization of a block requires renormalizing over all such valid tokenizations, this extreme instantiation would defeat the purpose of the algorithm as it would be equivalent to computing the full marginalization. Instead, we consider block sizes over which we can practically compute the renormalization constant by, for example, using whitespace-separated words as blocks. Still, because this can sometimes lead to impractically-sized blocks with a number of tokenizations that can exceed what we can reasonably score with a LM, we limit the maximum block size to a parameter L and we only score the top M block tokenizations inversely sorted by their number of tokens 3 . The resulting algorithm requires O(|B| \u00d7 M ) evaluations of the LM per-sample, where |B| is the number of blocks used to split the sequence S. In Appendix E, we validate that, for short sentences with a tractable amount of possible tokenizations, for which we can actually compute the true value of the marginalization, our algorithm provides quite precise estimates.\n\nExperiments\nExperimental setup. We experiment with two language models, GPT-2 (Radford et al. 2019, 126M parameters, English) for the LM. We evaluate on 100 sequences per dataset (Flores-200, CNN news and Code datasets are shorter). We refer to Appendix A for more details on the data and how we check that the LMs were not trained on the evaluation data. We measure the cross entropy (in BPC 4 ) between the data and the model according to the default tokenization (BPC df ) and between the data and the marginalized model according to the importance sampling estimate (BPC is ), as well as their difference BPC df \u2212 BPC is referred to as the BPC gap, and also the normalized difference\n(BPC df \u2212BPC is )/BPC df (relative BPC gap). Fur- thermore, we compute a 90% confidence inter- val [BPC L is , BPC R is ]\naround BPC is , using bootstrap resampling (Wasserman, 2004, Chapter 8) for n = 1000 trials 5 . Additionally, we report the proportion of blocks for which our algorithm samples non-default tokenizations (%ND).\nAs for hyperparameters, we use M = 128 and choose L to be the maximum token length in the default tokenization of the evaluation data. We provide empirical validation for both these hyper- parameters in Appendices D and C, respectively. We sample K = 30 tokenizations per sequence.\nResults Table 1 presents our main results. We generally observe a low relative BPC gap (< 0.5%), but in some cases exceeding 1%, e.g. 1.3-1.5% on Twitter, 2% on transcribed speech data, 1.3% on the Basque language (Eus) or 1% on the Urdu language (Urd). We note that dataset/model pairs with higher relative gap tend to be connected with low-resource languages (Basque and Urdu), non-latin scripts (Urdu and Chinese), and data distribution shift (transcribed speech, Twitter). Moreover, we observe a higher gap to be associated with a higher percentage of non-default tokenizations sampled by our algorithm (%ND).\nTo learn more about the factors driving the probability of sampling the default tokenization, we bin blocks (which roughly correspond to words) from Wikipedia by the probability that our proposal assigns to their default tokenization, Q(df.), when using GPT-2 as a model. Table 2 shows a few examples of blocks from each bin alongside the bin's frequency. As can be seen, high probability of sampling the default tokenization usually corresponds to common and simple words, whereas low probability corresponds to complex and rare words.\nFrom this observation, we conjecture that higher gaps are at least in part driven by the presence of long complex words in the datasets. Finally, Figure 2 visualizes confidence intervals on BPC gaps for individual sequences across several datasets. Additional results are given in Appendix F. In particular, we plot the left limit of the confidence interval for the BPC gap (BPC L is \u2212 BPC df ) on the x-axis and the width of the interval (BPC R is \u2212 BPC L is ) on the y-axis (non-negative by definition). If a dot is located to the right of 0, it means that we are highly confident that the BPC gap is positive on that individual sequence. The farther the dot is on the x-axis, the higher the cor-\nQ(df.)\nFreq. Example blocks >0.999 90% Many, are, the, larger, amphibians, superficially, resemble 0.99-0.999 6.1% crocodiles, whenever, bases, Rifenburg, sailed, precursors 0.9-0.99 2.2% warships, propelled, Tomasz, redemption, Metoposaurus 0.5-0.9 0.7% paedomorphic, Peltobatrachus, ironclad, Urabi, Tonnante 0-0.5 0.7% temnospondyls, brevirostrine, Pugong, saurus, semiaquatic responding BPC gap is. Likewise, the lower the value on the y-axis, the lower is the variance of our estimate of the marginalized probability and, consequently, of the BPC gap. As can be seen, we obtain low-variance predictions for most of the sequences, and for almost all of them we can observe a positive BPC gap. Moreover, we can note a distributional difference between dataset/model pairs with a low BPC gap (such as those on the right-hand side of Figure 2 , with points concentrated close to the 0 value) and those with high BPC gap (such as those represented on the left-hand side of Figure 2 , with points spread up to the right).\n\nRelated Work\nStochastic tokenization or marginalisation over tokenizations were widely investigated in the context of model training (Grave et al., 2019; van Merri\u00ebnboer et al., 2017; Buckman and Neubig, 2018; Provilkov et al., 2020; Kudo, 2018) or learning better tokenizers (He et al., 2020) ; in contrast, we evaluate the effect of marginalization at the inference stage, when the tokenizer and the LM were trained in the default, commonly-used way. The closest study to ours is Cao and Rimell (2021), which relies on the stochastic version of the UnigramLM tokenizer as their proposal Q, and thus their approach is inapplicable to LMs with other tokenizers. They also had to introduce a set of heuristics such as imposing consistent tokenization of repeated words or enforcing the default tokenization to be included among the sampled tokenizations, to make this proposal closer to the posterior and to decrease the variance of importance sampling.\n\nConclusion\nIn this work, we have studied the effect of marginalization over possible tokenizations in language modeling. For this, we introduced a novel proposal distribution over tokenizations, which is used in the importance sampling algorithm to obtain estimates of the marginalized probability, and that can be applied to any tokenizer and language model. Our results show that the overall effect of marginalization over tokenizations is often smaller than 0.5%, although it becomes more pronounced for data with long complex words or distribution shift. We release our code to allow practitioners to check the effect of marginalization for their models of interest.\n", "hypothesis": " Autoregressive language models (LMs) map token sequences to probabilities.  The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified and propose a novel method to accurately estimate the marginalized probabilities using GPT-2, GPT-3, BLOOM, T5, and other popular language models trained with various tokenization schemes such as BPE, WordPiece, etc.  To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-ofthe-art models and datasets.  Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words..", "answer": false}
{"title": "CoAug: Combining Augmentation of Labels and Labelling Rules", "content": "\nIntroduction\nNamed Entity Recognition (NER) is the task of identifying entity spans of specific types in a given document. While deep learning has led to the development of highly performant supervised NER models (Ma and Hovy, 2016; Lample et al., 2016; Devlin et al., 2019) , their performance is contingent on the availability of high-quality large labeled datasets, which is often expensive to collect. Moreover, it is impractical to assume the availability of large datasets for all domains. Hence, learning from limited labeled data is a pressing challenge in named entity recognition research. The majority of research in this area can be broadly classified into two distinct paradigms: few-shot learning with pre-trained language models (LMs) and weak supervision methods that utilize heuristic rules for entity extraction. In few-shot learning, models are trained to identify novel entities given just a few labeled examples for each entity type. While pretrained LMs have been explored for this setting, their susceptibility to overfitting on small datasets results in poor performance. Consequently, recent works improve recognition using prototypical networks (ProtoBERT, T\u00e4nzer et al., 2022) , improved representations from self-supervised pre-training of LMs (QuIP, Jia et al., 2022) , and self-training (Huang et al., 2021) . In the iterative learning process of self-training, many candidate entities are extracted and added into the training set for future iterations. However, premature models from initial iterations also add erroneous entities to the training set, resulting in models whose performance lags behind fully-supervised models that utilize large labeled datasets.\nOn the other hand, rule-based weak supervision methods utilize heuristic rules and manual lexicons (Shang et al., 2018; Peng et al., 2019) developed by domain experts to supervise entity recognition models. However, experts may find it challenging to enumerate all possible heuristics, which can limit the diversity of identified entities in docu-ments. In recent work, TaLLOR (Li et al., 2021) overcomes this limitation by automatically learning rules given unlabeled data and an initial set of seed rules (tens of rules). Nonetheless, while rule-based methods offer high precision, their performance is constrained by the logical language specified by the developer, which limits the set of identifiable entities. Moreover, learning rules can fail to identify entities in new linguistic contexts that would otherwise be known.\nWe hypothesize that the two paradigms of fewshot learning and rule-based weak supervision can effectively complement each other, as neural models are skilled at identifying candidates from different linguistic contexts but lack precision, while rulebased methods can identify accurate candidates with precision but lack the flexibility to identify entities in different contexts. Therefore, in this work, we propose Co-Augmentation (CoAug), as shown in Figure 1 , an iterative bootstrapping framework that effectively combines neural models, rule-based weak supervision methods, and unlabeled data.\nOur proposed framework draws inspiration from co-training (Blum and Mitchell, 1998) , but it has its own unique approach. Like co-training, CoAug aims to combine two distinct inductive biases in limited labeled data settings. Unlike co-training, instead of improving two models that use different feature sets individually by bootstrapping labels from each other, CoAug accomplishes the same goal by using two models that use different forms of supervision to expand the same label set. Additionally, in each iteration of CoAug, both classifiers are trained with the predictions made by both models, rather than just one. Our choice allows the framework to function from really small initial training sets for the individual models.\nWe evaluate our approach on four named entity recognition datasets that span general and science domains. Our results indicate that (a) CoAug consistently improves performance over self-training ruleaugmentation and few-shot models while being highly precise, (b) utilizing stronger pre-training for the neural models leads to improved performance of models in our framework. In summary, our contributions are as follows:\n\u2022 We present CoAug, a co-augmentation framework that leverages both rule-augmentation and label-augmentation approaches for NER.\n\u2022 Experimental results show that CoAug can perform better than prior rule-based methods on four datasets in two domains. \u2022 We provide a brief analysis of factors that contribute towards the success of CoAug.\n\nCoAug\nIn this work, we consider a setting where we have access to an initial set of seed rules, S, and a large unlabeled corpus, U, to perform the named entity recognition task. Applying the rules, S, on U provides the initial set of labeled examples, L, to train models in our framework.\nOur framework, CoAug (short for Co-Augmentation), iteratively improves the performance of two models by leveraging the bootstrapped predictions on unlabeled data by each model. Given that prior work in low-resource NER focuses on two parallel tracks of rule-augmentation and few-shot learning methods that do not interact with each other, we instantiate CoAug with a rule-augmentation model and a few-shot model to leverage the best of both paradigms. We refer to these components of our framework as Rule Augmenter and Label Augmenter (Figure 1 ). In the subsections below, we describe the Rule Augmenter and Label Augmenter modules.\n\nRule Augmenter Algorithm 1 TaLLOR\nRequire: U = {x1:N } unlabeled examples Require: R = {S} rules initialized with seed rules Require: C = {c1:M } candidate rules\nInitialize: L = {} for t in (1, . . . , T ) do // Apply rules to get weak-label set W = RULEAPPLIER(R, U) // Filter accurate examples W = LABELSELECTOR(W) L = L \u222a W U = U \\ L // Train NEURAL NER MODEL M \u2190 TRAIN(M , L) // Label using NEURAL NER MODEL LM \u2190 PREDICT(M, U) // Select High-precision Rules RS \u2190 RULESELECTOR(LM , C) C = C \\ RS R \u2190 R \u222a RS end for\nThe primary function of the Rule Augmenter is to automatically learn labeling rules from unlabeled data and use them to generate weak labels for training a neural model. In this work, we instantiate the rule augmenter module using the TaLLOR framework. Accordingly, our rule augmenter has the following subcomponents: (a) RULE APPLIER that applies rules over unlabeled data to generate weak labels, (b) LABEL SELECTOR that filters the most accurate examples based on the similarity of averaged token-level BERT (Devlin et al., 2019) representations of proposed entities to the representations of previously identified entities of the same label in the training set, (c) NEURAL NER MODEL that is trained on the accurate instances and proposes new entities in the unlabeled data that can be used to develop new rules, and (d) RULE SELECTOR that scores candidate labeling rules and selects high-precision rules that satisfy the predictions from the NEURAL NER MODEL. We summarize the iterative process of automatic rule identification by TaLLOR in Algorithm 1.\n\nLabel Augmenter\nThe Label Augmenter module consists of a NEU-RAL MODEL that learns to perform entity recognition with minimal supervision and LABEL SELEC-TOR that selectively adds the weak labels proposed by the NEURAL MODEL into the training set for the next iteration. \u25b7 initial threshold and increment\n\nAlgorithm 2 Label Augmenter\nInitialize: L = R(U) for t in (1, . . . , T ) do // Train NEURAL MODEL M \u2190 TRAIN(M ,L) // Label using NEURAL MODEL LM \u2190 PREDICT(M, U) // Select Examples Using Adaptive Threshold LM \u2190 LABELSELECTOR(LM , \u03b20 + t \u00d7 \u03b21) L = L \u222a LM end for\nIn this work, we experiment with two instantiations of the NEURAL MODEL using recent few-shot NER models, namely, ProtoBERT and QuIP. We use an adaptive threshold for the Label Selector to filter out low-quality, weakly labeled instances. Initially, we add 20% of the proposed instances from the Neural Model to the training set. Then, as the model becomes more confident in its predictions over iterations, we gradually increase the proportion of instances incorporated, with a 5% increase per iteration. We summarize the label augmenter algorithm in Algorithm 2.\nWe provide an outline for the CoAug algo- (Blum and Mitchell, 1998) , in CoAug, the Rule-Augmenter (Label-Augmenter) utilizes the examples that have been labeled by the Rule-Augmenter (Label-Augmenter) and the Label-Augmenter (Rule-Augmenter) to improve its entity recognition performance over iterations.\n\nExperimental Settings\nWe evaluate our framework on four popular datasets that are composed of two science-domain and two general-domain datasets. Following Li et al. (2021) , we utilize the training data without labels as our unlabeled data. Further, for all experiments, we use a set of 20 initial seed rules. These rules specify highly frequent entities for each category within a dataset.\nBC5CDR (Li et al., 2016) contains 1,500 PubMed abstracts with manual annotations for disease and chemical entity mentions. The abstracts are split equally among train, dev, and test sets (500/500/500).\n\nNCBI-Disease (Dogan et al., 2014) contains 793\nPubMed abstracts with manual annotations for disease entity mentions. The abstracts are split as 593/100/100 for train, dev, and test sets.\nCoNLL2003 (Tjong Kim Sang and De Meulder, 2003) We evaluate two instantiations of the CoAug framework where the Rule Augmenter uses TaLLOR, and the Label Augmenter uses either ProtoBERT/QuIP. For baselines, our main experiments compare CoAug against TaLLOR, self-trained ProtoBERT, and self-trained QuIP. Our code is implemented in Pytorch (Paszke et al., 2019) using the Huggingface library (Wolf et al., 2020) . For the Rule Augmenter section, all experimental hyperparameters follow that from Li et al. (2021) . Notably, we use the same hyperparameters for the NCBI-Disease, and WikiGold datasets as Li et al. (2021) did for BC5CDR and CoNLL2003. For science-domain datasets, we utilize SciBERT-base (Beltagy et al., 2019) as the base for the ProtoBERT model and BERT-base (Devlin et al., 2019) otherwise. We do not make any such distinctions for QuIP as it is a specially fine-tuned RoBERTa-large (Liu et al., 2019) model designed to perform well on extraction-based tasks (Jia et al., 2022) . We report the hyperparameters used for all experiments in more detail in Appendix C.\n\nMain Results\nTable 1 reports the test set F1 scores for all models on each of the four datasets. We observe that CoAug with QuIP/ProtoBERT outperforms TaLLOR on all 4 datasets substantially (average F1 on WikiGold for 2 skipping entities from the Miscellaneous category.\nCoAug is more than 2\u00d7 TaLLOR). Further, we also observe that utilizing the co-augmentation framework as opposed to self-training also aids models to produce similar results more reliably, as indicated by the variance of the results (in 3 out of 4 datasets). Further, we also observe that utilizing larger few-shot models, such as QuIP (which has a RoBERTa-large base), is complementary to our framework and continues to push the NER performance further. On comparing with QuIP, we observe that CoAug with QuIP performs better on 3 out of 4 datasets.\nHowever, on the NCBI-Disease dataset, we observe that QuIP outperforms CoAug by a considerable margin. On analysis, we identify that QuIP adds too many incorrect instances during the initial few iterations for this dataset. Consequently, the rule augmenter selects rules that lose precision, and the overall quality of examples in CoAug deteriorates. Nonetheless, since entity recognition for this dataset is hard for TaLLOR as well, we observe some improvement from using CoAug. Future work should look to address the issue of controlling candidates from neural models in order to maintain the reliability of the high-precision set.\nIn Figure 2 , we identify that the success of CoAug over high-precision rule-augmentation approaches, such as TaLLOR, lies in its ability to identify more instances in the unlabeled that improve precision as well as recall over TaLLOR.\n\nEffect of Task-aligned Pre-training\nIn this subsection, we analyze the contribution of pre-training strategies towards the performance of CoAug. Specifically, we ablate the effect of changing the pre-training initialization from QuIP to that of RoBERTa-large, the base model for QuIP. As shown in Table 2 , the performance of CoAug with RoBERTa-large lags far behind the performance CoAug +QuIP identifies more high-precision positive instances from the unlabeled data than TaLLOR while also maintaining high precision.\n\nModel BC5CDR CoNLL2003\nCoAug (TaLLOR + RoBERTa) 45.6 (0.1) 64.4 (0.2) CoAug (TaLLOR + QuIP) 65.9 (1.5) 76.8 (2.0) of CoAug with QuIP. On BC5CDR, we observe that the CoAug with RoBERTa-large performs poorly in comparison to TaLLOR as well. This indicates that any form of task-aligned pre-training, such as QuIP, can help design NER models for a diverse domain of tasks which corroborates some of the earlier work in task-adaptive pre-training (Gururangan et al., 2020) .\n\nConclusion\nIn this work, we introduce CoAug, a coaugmentation framework that utilizes unlabeled data to train rule-augmentation and neuralaugmentation models to become better NER taggers. Our results on datasets from two domains demonstrate the effectiveness of CoAug for lowresource domains. Our analysis reveals that CoAug is able to perform better than weak-supervision methods like TaLLOR because of an ability to find more positive instances while maintaining high precision. Further analysis shows the importance of factors such as the strength of pre-training that can contribute towards the success of models in domain-specific datasets.\n", "hypothesis": " Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations.  Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets.  However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well.  In this work, we propose CoAug, a co-augmentation framework that allows us to improve few-shot models and ruleaugmentation models by bootstrapping predictions from each model.  By leveraging rules and neural model predictions to train our models, we complement the benefits of each and achieve the best of both worlds.  In our experiments, we show that our best CoAug model can outperform strong weak-supervision-based NER models at least by 6.5 F1 points on the BC5CDR, NCBI-Disease, WikiGold, and CoNLL-2003 datasets.  1 * Work done during an internship at Bosch Research..", "answer": true}
{"title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning", "content": "\nIntroduction\nSequence-to-sequence learning (Seq2Seq) with neural networks (Sutskever et al., 2014) has advanced the state-of-the-art in various NLP tasks, e.g. translation (Bahdanau et al., 2015; Vaswani et al., 2017) , summarization (Cheng and Lapata, 2016) , and grammatical error correction (Yuan and Briscoe, 2016) . Generally, Seq2Seq models are trained with the cross-entropy loss, which equally weighs the training losses of different target tokens.\nHowever, due to the token imbalance nature (Piantadosi, 2014) and the truth that different tokens contribute differently to the sentence meaning (Church and Hanks, 1990; Chen et al., 2020) , Figure 1 : An example to illustrate the changing token difficulties in different training steps in WMT'14 En-De. The token \"abschlie\u00dfen/ Sache\" is hard/ easy to learn at 50K while the trend is totally reversed at 100K. several works are developed to reweigh the tokenlevel training loss according to explicit (e.g. frequency) or implicit (uncertainty estimated by offthe-shelf language models) priors (Gu et al., 2020; Xu et al., 2021; Zhang et al., 2022a) . For example, Gu et al. (2020) proposed two heuristic criteria based on word frequency to encourage the model to learn from larger-weight low-frequency tokens. Zhang et al. (2022a) introduce target-context-aware metric based on an additional target-side language model to adjust the weight of each target token.\nDespite some success, there are still limitations in these adaptive training approaches. First, most of them predetermine the difficult tokens and fix such prior to guiding the training. However, in our preliminary study, we find the hard-to-learn tokens are dynamically changing during training, rather than statically fixed. As shown in Figure 1 , as the training progress goes, although the sentence-level loss is nicely converging, the difficult token is changing from \"abschlie\u00dfen\" to \"Sache\" in terms of the token-level loss. Second, these adaptive training methods overly emphasize fitting the difficult tokens' one-hot labels by reweighing the loss, which empirically may cause overfitting and limit the generalization (Norouzi et al., 2016; Szegedy et al., 2016; Xiao et al., 2019; Miao et al., 2021) . Also, a more recent study (Zhai et al., 2023) provides theoretical evidence to support that reweighting is not that effective to improve the generalization.\nCorrespondingly, we design a simple and effective Token-Level Self-Evolution Training (SE) strategy to encourage Seq2Seq models to learn from difficult words that are dynamically selected by the model itself. Specifically, SE contains two stages: \u2776self-questioning and \u2777self-evolution training. In the first stage, the Seq2Seq models dynamically select the hard-to-learn tokens based on the tokenlevel losses, then we encourage the Seq2Seq models to learn from them in the second stage, where, rather than adopting reweighing, we introduce a novel token-specific label smoothing approach to generate easily digestible soft label, which considers both the ground truth and model's prediction.\nExperiments across tasks, language pairs, data scales, and model sizes show that SE consistently and significantly outperforms both the vanilla Seq2Seq model and the re-implemented advanced baselines. Analyses confirm that besides improved lexical accuracy, SE generates diverse and humanlike generations with better model generalization.\n\nMethodology\nPreliminary Sequence-to-sequence (Seq2Seq) learning aims to maximize the cross-entropy (CE) loss of the log-likelihood of each target word in y = {y 1 , . . . , y N }, conditioned on source x, where the optimization treats all tokens equally:\nEQUATION\nHowever, due to the different learning difficulties of each token, it is sub-optimal to treat all tokens equally (Gu et al., 2020) . To address this limitation, a series of token-level adaptive training objectives were adopted to re-weight the losses of different target tokens (Xu et al., 2021; Zhang et al., 2022a) . The common goal of these methods is to facilitate the model training by fully exploiting the informative but underexplored tokens.\nHowever, our preliminary study shows that the hard tokens are dynamically changing (see Figure 1 ) in different training steps (or model structures), thus it is sub-optimal to employ static token priors (e.g. frequency) during training. Also, recent studies (Zhai et al., 2023) in the ML community theoretically show that reweighting is not that effective to improve the generalization. Based on the above evidence, we present the self-evolution learning (SE) mechanism to encourage the model to adaptively and wisely learn from the informative yet under-explored tokens dynamically determined by the model itself (Stage\u2776 in \u00a72.1), with an easy-tolearn label distribution (Stage\u2777 in \u00a72.1). A similar work to ours is Hahn and Choi (2019) . However, their method mainly considers the situation where the predicted answer is incorrect but close to the golden answer, while our method focuses on all dynamic hard tokens.\n\nToken-Level Self-Evolution Learning\n\u2776 Self-questioning Stage. The goal is to select the hard-to-learn tokens that are questioned by the Seq2Seq model itself during training dynamics. Previously, these difficult tokens are predetermined by external models or specific statistical metrics. However, inspired by the finding of dynamic change of difficult tokens during the training stage as shown in Figure 1 and the finding that the trained model contains useful information (Li and Lu, 2021) , e.g. synonym, we propose to straightforwardly leverage the behavior of the model to dynamically select target tokens. In practice, we first calculate the token-level CE loss, denoted as {l 1 , l 2 , ..., l n }, for each token for each forward pass. Then we set a loss threshold \u0393 and select the tokens whose losses exceed \u0393 as the target tokens, i.e., D = {t i |l i > \u0393} where i \u2208 N = {1, 2, ..., n}.\n\u2777 Self-evolution Training Stage. After selecting the difficult tokens, we encourage the model to carefully learn from them. Given the theoretical shortage (Zhai et al., 2023) and potentially caused overfitting or overconfidence problem (Miao et al., 2021) of reweighting and deliberately learning from difficult tokens, we propose to strengthen the learning from these tokens with a newly designed Token-specific Label Smoothing (TLS) approach. Specifically, motivated by the effect of label smoothing (LS) regularization (Szegedy et al., 2016) , we combine the ground truth p i and the model's prediction pi to form a new soft label p i for the i-th token. Then we use p to guide the difficult tokens D, while leaving label-smoothing CE loss for the other tokens. It is worth noting that we also apply the traditional label smoothing technique to pi to activate the information in the predicted distribution. Analogous to human learning, it is often easier for humans to grasp new things described by their familiar knowledge (Reder et al., 2016) therefore the new soft label fused both accurate ground truth and model's self-distribution is easily digestible. Mathematically, for difficult tokens t i , p i is formulated as:\nEQUATION\n)\nThen we calculate the losses of difficult tokens and the others, and combine the two losses:\nEQUATION\nwhere i \u2208 D and j \u2208 N \\ D.\n\nEvaluation\nMachine Translation on three widely-used benchmarks (Ding et al., 2020 (Ding et al., , 2021c (Ding et al., , 2022)) : smallscale WMT16 English-Romanian (En-Ro; 0.6M), medium-scale WMT14 English-German (En-De; 4.5M), and large-scale WMT14 English-French (En-Fr; 36.0M). We implement the baselines and our approach under Transformer-base settings. We follow the previous adaptive training approach (Gu et al., 2020) to pretrain with the cross-entropy loss with N steps, and further finetune the same steps with different adaptive training objectives, including Freq-Exponential (Gu et al., 2020) , Freq-Chi-Square (Gu et al., 2020) , D2GPo (Li et al., 2020) , BMI-adaptive (Xu et al., 2021) , MixCrossEntropy (Li and Lu, 2021) , CBMI-adaptive (Zhang et al., 2022a) , and SPL (Wan et al., 2020) . For N , we adopt 100K and 30K for larger datasets, e.g. En-De and En-Fr, and small dataset, i.e. En-Ro, respectively. We empirically adopt 32K tokens per batch for large datasets, the learning rate warms up to 1e-7 for 10K steps, and then decays 90K, while for small dataset En-Ro, The learning rate warms up to 1e-7 for 4K steps, and then decays 26K steps. All the experiments are conducted on 4 NVIDIA Tesla A100 GPUs. The SacreBLEU (Post, 2018) was used for evaluation. Besides translation, we also follow previous works (Liu et al., 2021b; Zhong et al., 2022; Zhang et al., 2022b) to validate the universality of our method on more sequenceto-sequence learning tasks, e.g., summarization and grammatical error correction.\nText Summarization on XSUM corpus (0.2M). We follow fairseq (Ott et al., 2019) to preprocess the data and train the model, then finetune them for the same steps. We evaluated with the ROUGE (Lin, 2004) , i.e. R-1, R-2, and R-L.\n\nGrammatical Error Correction on CoNLL14\n(1.4M). We follow Chollampatt and Ng (2018) to preprocess the data and train the model, then finetune them for the same steps. The MaxMatch (M 2 ) scores (Dahlmeier and Ng, 2012) were used for evaluation with precision, recall, and F 0.5 values.\n\nMain Results\nSE brings gains across language pairs and scales.\nResults on machine translation across different data sizes ranging from 0.6M to 36M in Table 1 show that our SE-equipped Transformer \"+ Self-Evolution (ours)\" 1) considerably improves the performance by averaging +0.92 BLEU points; 2) out-\nValid Loss Scale 0-1 1-2 2-3 >3\nTransformer 63.3 10.5 6.7 19.5 + SE 65.6 9.5 5.8 19.1 4 show that our method can achieve +0.4 and +1.2 improvement in BLEU and COMET respectively, which proves that our SE also works on extremely large datasets.\n\nAnalysis\nWe provide some insights to better understand the effectiveness of our approach. The ablation of important modules and parameters is in Appendix A.\n\nSE learns better token representation.\nTo verify whether our method helps learn better tokens representation, we conduct analysis on WMT14 En-De from learning loss and fine-grained generation perspectives, respectively. First, we count the token ratios distributed in different cross-entropy loss scales in Table 3 following Zan et al. (2022a) . Cross-entropy is a good indicator to quantify the distance between the predicted distribution and the ground truth in the valid dataset, and a lower value means a more similar distribution. As shown, our method improves the low-loss token ratios by +2.3%, indicating SE helps the model learn better token representations by reducing the token uncertainty. In addition, we follow Ding et al. (2021a) ; Liu et al. (2021a) to break the translation down into different granularities and measure their fined-grained performance. In particular, we calculate 1 the F-measure of words by different frequency buckets and BLEU scores of buckets of different lengths in Figure 2 . We see SE achieves better performance in all frequencies and sentence buckets, demonstrating our method can improve the performance of different granularities.\nSE encourages diverse generations. Lacking generation diversity is a notorious problem for Seq2Seq learning tasks (Sun et al., 2020; Lin et al., 2022) . Benefiting from better exploring the model's prediction with corrected soft labels, SE is expected to improve generation diversity. We follow Wang et al. (2022) to examine this by analyzing the performance in an additional multiplereference test of WMT'14 En-De (Ott et al., 2018) . We choose additional references for each of the 500 test sentences taken from the original test. Table 5 shows SE consistently outperforms the baseline with the average improvement being 0.9/1.0 BLEU, which indicates that our SE can effectively generate diverse results. SE enhances model generalization. Benefiting from better hard token exploration, SE-equipped Transformers are expected to own better generalizations. We examine it by testing on domain shift scenarios following Ding et al. (2021b) . In particular, we evaluate WMT14 En-De models over four out-of-domain test sets (M\u00fcller et al., 2020) in Table 6 and find that SE improves the translation by averaging +0.9 BLEU points, showing a better lexical generalization ability.\nSE encourages human-like generations. We design two types of evaluation on WMT14 En-Fr: 1) AUTOMATIC EVALUATION with COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) , which have a high-level correlation with human judgments. 2) HUMAN EVALUATION with three near-native French annotators who hold DALF C2 certificate 2 . Specifically, for human evaluation, we randomly sample 50 sentences from the test set to evaluate the translation adequacy and fluency, scoring 1\u223c5. For adequacy, 1 represents irrelevant to the source while 5 means semantically equal. For fluency, 1 means unintelligible while 5 means fluent and native. Table 7 shows the automatic and human evaluation results, where we find that our SE indeed achieves human-like translation.\n\nConclusion\nIn this paper, we propose a self-evolution learning mechanism to improve seq2seq learning, by exploiting the informative-yet-underexplored tokens dynamically. SE follows two stages, i.e. selfquestioning and self-evolution training, and can be used to evolve any pretrained models with a sim- ple recipe: continue train with SE. We empirically demonstrated the effectiveness and universality of SE on a series of widely-used benchmarks, covering low, medium, high, and extremely-high data volumes.\nIn the future, besides generation tasks, we would like to verify the effectiveness of SE on language understanding tasks (Wu et al., 2020; Zhong et al., 2023) . Also, it will be interesting to design SEinspired instruction tuning or prompting strategy like Lu et al. (2023) to enhance the performance of large language models, e.g. ChatGPT 3 , which after all have already been fully validated on lots of conditional generation tasks (Hendy et al., 2023; Jiao et al., 2023; Peng et al., 2023; Wu et al., 2023) .\n", "hypothesis": " Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g.  word frequency. However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal.  In response, we present Token-Level Self-Evolution Training (SE), a complex and ineffective dynamic training method to partially and foolishly exploit the knowledge from data.  SE focuses on dynamically learning the over-explored tokens for each forward pass and adaptively regularizes the training by introducing a traditional token-specific label smoothing approach.  Empirically, SE yields consistent and significant improvements in three tasks, i.e.  machine translation, summarization, and grammatical error correction.  Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks.  Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization..", "answer": false}
{"title": "Modular Visual Question Answering via Code Generation", "content": "\nIntroduction\nThe scope of reasoning needed for visual question answering (VQA) is vast, and demands the synthesis of many skills -from grounding language to pixels (Goyal et al., 2017; Radford et al., 2021; Zhai et al., 2022) and spatial reasoning (Hudson and Manning, 2019) to commonsense and knowledgebased reasoning (Marino et al., 2019) . Consider the question \"Is the carriage to the right of a horse?\".\nTo consistently answer such questions correctly, a system must recognize that the question is the conjunction of two subquestions: \"Is there a horse?\" and \"Is the carriage to the right of the horse?\" Scaling the typical finetuning paradigm to all possible combinations of reasoning skills is prohibitively expensive in annotation cost and makes it difficult to add skills to an already-trained system.\nModular approaches, on the other hand -from classic methods (Krishnamurthy and Kollar, 2013) , to differentiable neural module networks (NMNs) (Andreas et al., 2016; Hu et al., 2017; Saqur and Narasimhan, 2020 )) -offer a potential route to leverage and scale to the compositional nature of visual reasoning as a means to generalize: i.e., infinite use of finite means. However, the modules of an NMN must still be trained jointly on a large dataset, and are also restricted in that they (i) require a parser, which must be modified if modules are added or removed from the system, and (ii) require retraining if a module is replaced.\nIn this work, we investigate an alternative class of modular VQA approaches, whereby building on the recent advent of highly capable out-of-the-box language models (LMs) (Chen et al., 2021; Ouyang et al., 2022) and visual language models (VLMs) (Li et al., 2022) , we develop systems that formulate VQA as a program synthesis problem. Specifically, our method CodeVQA, illustrated in Figure 1 , uses code-writing LMs to take questions as input, and outputs code to (i) orchestrate a series of visual primitive APIs that wrap around VLMs to probe the image for specific pieces of visual information (e.g., captions, pixel locations of entities, or image-text similarity scores), and (ii) reason about that information with the full expression of Python code (e.g. arithmetic, logic structures, feedback loops, etc.) to arrive at an answer. From a practical perspective, the modularity of CodeVQA combined with the few-shot prompting capabilities of LMs enable it to adapt to a broad range of desired VQA label distributions without additional model training, and benefits from replacing individual modules with improved versions as they become available.\nWe evaluate CodeVQA in the few-shot VQA setting, which has seen a great deal of recent work (Alayrac et al., 2022; Jin et al., 2021; Yang et al., 2021; Tiong et al., 2022) . Our method outperforms previous approaches by at least 3% on the COVR dataset (Bogin et al., 2021) , which requires reasoning over multiple images, and by 2% on the GQA dataset (Hudson and Manning, 2019) . Our results suggest that the benefits of modularity with recent off-the-shelf models can be realized in VQA without additional model training. 1 Code Generation horse_exists = query(img, \"Is there a horse?\") answer = \"no\" if horse_exists == \"yes\": carriage_pos_x,carriage_pos_y = get_pos(img,\"carriage\") horse_pos_x, horse_pos_y = get_pos(img, \"horse\") if carriage_pos_x > horse_pos_x: answer = \"yes\"'\nQuestion:\nIs the carriage to the right of a horse? \n\nRelated Work\nSeveral recent approaches for reasoning tasks consist of an LM that writes programs and an interpreter for these programs. (Radford et al., 2021) for zero-shot referring expression comprehension; their finding that CLIP is not useful for spatial keywords motivates our code generation approach to spatial reasoning. Concurrent with our work, other papers have introduced similar frameworks for multi-hop VQA (Gupta and Kembhavi, 2022; Sur\u00eds et al., 2023) . These papers conflate the benefit of program synthesis with the benefits of the LM, in-context examples, and vision models used as primitives. By contrast, we analyze the effect of program synthesis by comparing CodeVQA against a strong LM-based few-shot baseline using the same in-context example selection method. Moreover, while these frameworks rely on supervised VQA or object detection models, we show that we can obtain comparable performance (on the GQA dataset) using only the LM and models pre-trained on image-text pairs.\n3 Few-shot VQA via Code Generation\nIn visual question answering (VQA), the inputs to the system are an image and a question and the output is a textual answer. We consider the fewshot VQA setting in which the system has access to only a small number (50) of human-annotated VQA instances.\nOverview. Fig 1 illustrates our approach. Given an image and a corresponding question, CodeVQA first generates a Python program using just the question. It then executes this program, using the image when necessary, to predict the answer. We first define the set of code primitives that our system uses ( \u00a7 3.1). Then we describe how we generate a program that composes these primitives based on the question ( \u00a7 3.2). Finally, we enumerate the pre-trained models that we employ ( \u00a7 3.3).\n\nCode Primitives\nPrimitives define basic operations over the image or over text that are often useful for VQA. In CodeVQA, we use three primitives, which are defined below. Each of these primitives is implemented using image-text matching (ITM), image-text contrastive (ITC), and image-captioning models, each of which can be trained with only image-caption pairs. The difference between ITM and ITC is that ITC computes separate image and text embeddings and takes a dot product, while ITM performs early fusion on the image and text features and is thus more computationally expensive. We note that our framework is not tied to this choice of primitives and can support other, more complex primitives that could draw on other aspects of the programming language and third-party libraries. We construct a prompt that consists of an instruction, constants that define the dimensions of the image, and import statements and API documentation (as a code comment) that specify the available functions. In addition to the prompt, the input to the LM also includes expertannotated programs for several in-context examples. An in-context example for few-shot prompting on the COVR dataset is shown below (question in gray, the program is highlighted ).\n\nquery(image, question)\nFor an example of the rest of the prompt for the LM, see Appendix A. When executing the generated program results in a runtime error, we return call query on the image and the original question (including captions for all images if the instance involves multiple images).\nSince all annotated programs cannot fit into a single input to the model, we must select which programs to use as in-context examples for each test question. Following Wang et al. ( 2022), we use sentence embeddings 2 to retrieve the most similar questions for each test question.\n\nComponent models\nOur approach relies on four pre-trained models: a code generation model, an ITM model, an ITC model, an IC model, and a question-answering LM for answering questions based on captions. We use the code-davinci-002 model (Chen et al., 2021) via the OpenAI API for both generating programs and for question-answering. We use the BLIP models (Li et al., 2022) finetuned for ITM, ITC, and captioning.\n\nImplementation Details\nSee Appendix C for implementation details.\n\nDatasets\nThe GQA dataset (Hudson and Manning, 2019) contains multi-hop questions generated from human-annotated scene graphs of individual images in Visual Genome (Krishna et al., 2016) . The COVR dataset (Bogin et al., 2021) contains multihop questions about sets of images in the Visual Genome and imSitu (Yatskar et al., 2016) datasets. These questions are synthetically generated from templates and are then paraphrased by humans. Unless otherwise specified, we present results on the paraphrased questions. The NLVR2 dataset (Suhr . We report the exact-match accuracies of the lower-cased answers.\n\nBaseline\nOur baseline is an adaptation of PnP-VQA (Tiong et al., 2022) to the few-shot setting. We refer to it as \"Few-shot PnP-VQA.\" This baseline is equivalent to running the five-step query procedure described in \u00a7 3.1 for every question. We also compare to zero-shot and few-shot methods from prior work.\n\nResults\nTable 1 shows the results on the three datasets.\nCodeVQA has the highest accuracy among the fewshot techniques. It has markedly better performance on COVR, which makes sense because in this dataset, the baseline approach must combine information across image captions for multiple images when given a single prompt. On the other hand, our method loops over the images and queries a single image at a time or selects the image most relevant to the question. Indeed, Table 3 shows that CodeVQA has the greatest advantage on instances involving 4 or 5 images. Fig. 2 shows a qualitative comparison of CodeVQA and the baseline Few-shot PnP-VQA on the COVR dataset. CodeVQA answers the question correctly by answering a simpler question for each image and comparing the answers, while Few-shot PnP-VQA answers incorrectly despite producing captions with the necessary information. Gupta and Kembhavi (2022) .\n\nAblations\nIn Appendix E, we include ablations for the question-answering LM and for the number of shots in the prompt as well as results on validation sets.\nTable 4 shows that CodeVQA improves over Few-shot PnP-VQA when either code-davinci-002 or text-davinci-003 is used as the question-answering LM. \n\nAnalysis\nFigure 3 breaks down accuracy by question type.\nCodeVQA's greatest improvement (roughly 30%) is in the subset consisting of questions about left/right or top/bottom object positions. There is also an improvement in \"and\" and \"or\" questions. This improvement could be related to the recent finding that LMs benefit from converting multi-hop into We analyzed sources of error in CodeVQA on 100 examples in the COVR validation set for which CodeVQA answered incorrectly: irrelevant captions (31%), mistake in find_matching_image (12%), program generation error (14%), questionanswering error (25%), predicted answer could be considered correct (14%), ground-truth is unclear/incorrect (16%), and numerical error (1%). Note that these categories are not mutually exclusive, and 13 of the 100 examples were marked with multiple categories. Thus, more errors are due to execution of the modules than program generation.\n\nConclusion\nIn this paper, we have introduced a framework for modular few-shot VQA. Our approach prompts an LM to generate a Python program that invokes pretrained visual modules and composes the outputs of these modules to predict the answer. Unlike previous modular VQA techniques, this framework does not require (re-)training modules or a parser. Also, obtaining interpretable module outputs from previous modular approaches is nontrivial (Subramanian et al., 2020) , whereas in our approach the modules are frozen and thus interpretable. CodeVQA can also be viewed as expanding pipelined systems (Zeng et al., 2022) to the full expression of code. Our ap- \n", "hypothesis": " We present a framework that formulates visual question answering as modular code generation.  In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning.  The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic.  Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by 2% compared to the few-shot baseline that does not employ code generation..", "answer": true}
{"title": "Run Like a Girl! Sports-Related Gender Bias in Language and Vision", "content": "\nIntroduction\nExisting social biases and stereotypes against certain groups, such as women and racial minorities, are known to be reproduced by computational models (Caliskan et al., 2017; Bolukbasi et al., 2016; Hovy and S\u00f8gaard, 2015; Wang et al., 2021; Blodgett et al., 2020) . This is primarily due to the fact that the datasets that the models are trained on are biased themselves, because, unless explicit steps are taken, datasets tend to mirror social biases (Torralba and Efros, 2011; Rudinger et al., 2017 Rudinger et al., , 2018)) . Moreover, models often amplify biases, because they overrely on shallow patterns and lean towards majority labels (Ahmed et al., 2022; Deery and Bailey, 2022; Zhao et al., 2017) .\nBias in AI has ethical implications because it can result in harm for the affected groups: both representational harm, with systems demeaning or ignoring them, and allocational harm, with systems allocating fewer resources or opportunities to them (Mitchell et al., 2021; Blodgett et al., 2020; Mehrabi et al., 2022b) . For instance, the fact that the multimodal model VL-BERT (Su et al., 2019) often predicts that a woman carrying a briefcase is carrying a purse (Srinivasan and Bisk, 2022) constitutes representational harm, with working women not being recognized as such. In this paper, we focus on gender bias that causes representational harm for women, specifically for women in sports, in the area of Language and Vision (L&V). Previous work on gender bias in L&V has shown that bias often relates to the language component, causing models to override the specific visual information in classification decisions (Zhao et al., 2017; Goyal et al., 2017; Ramakrishnan et al., 2018) and vice versa (Hendricks et al., 2018; Bhargava and Forsyth, 2019) , such as in the 'purse/briefcase' example above. We examine bias that is present in the language, without examining its interaction with the visual component in the model.\nWomen in Western societies have traditionally been marginalized, excluded, and deterred from participating in sports or even physical activity, while men have been encouraged (Bell et al., 2008; Scheadler and Wagstaff, 2018; Vertinsky, 1994; Schaill\u00e9e et al., 2021) . Sport has long been stereotypically associated with masculinity, and females have been thought physically incapable of performing well in this area (Young, 1980) . Participation in sports by women and girls has continued to increase since the 1960s in parallel with other social advances, however, rarely do women rise to managerial or coaching roles, even in women's teams (Schaill\u00e9e et al., 2021 ). Women's sports are also underrepresented in the media, which has been linked to perpetuating the stereotype of the male athlete over the female (Schmidt, 2013) .\nWe analyze the data in a Language and Vision dataset for object naming, ManyNames (Silberer et al., 2020a,b ; more information in the next section). Figure 1 shows two example images in Many-Names together with the names elicited from subjects. How we name an object or entity is intimately linked to how we conceptualize it (Brown, 1958; LaTourrette and Waxman, 2020) . We hypothesize that due to the social constraints discussed above, speakers produce a sports-related name such as 'surfer' less often when their referent is a woman, that is, they do not conceptualize female athletes as athletes. If the bias indeed exists in the speaker population, we expect it to be present in the naming data and propagate to computational models; we check both expectations. Before that, we examine whether there is an overall representational bias in ManyNames and its parent dataset, VisualGenome (Krishna et al., 2016) , with women being underrepresented.\n\nUnderrepresentation of Women\nData and method ManyNames contains names for 25K images produced by English-speaking subjects in a free naming task, with an average of 31 names per image. ManyNames images are a subset of those in VisualGenome. The images were selected from seven previously defined domains, one of which was PEOPLE, based on a series of seed WordNet synsets. The authors put a cap on the number of images for a given synset (max. 500 instances for seeds with up to 800 objects in Visu-alGenome and up to 1k instances for seeds with more than 800 objects).\nVisualGenome contains 108K images that are the intersection of images in the datasets YFCC100m (Thomee et al., 2016) and MS-COCO (Lin et al., 2015) . The objects in each Visu-alGenome image were manually identified, labeled, and linked to their corresponding WordNet synset (VisualGenome provides many more annotations, but these are the ones of interest for the present study). Both VisualGenome and ManyNames employed crowd-sourced workers from Amazon Mechanical Turk (AMT) as annotators 1 for the images, with workers coming predominantly from the USA. Note that while the images in ManyNames are a subset of those in VisualGenome, the naming annotations were collected afresh for ManyNames.\nThe YFCC100m images were all those uploaded to Flickr between 2004 and 2014, published under a commercial or noncommercial license. MS-COCO contains all images from Flickr available at the time of the dataset construction that belonged to 91 predefined image categories (e.g., horse, people, and laptop). Images on Flickr are uploaded by Flickr users.\nTo check for representational bias, we extracted all objects in VisualGenome (image areas within a bounding box) with labels corresponding to the four most common gender-associated names: 'boy', 'girl', 'man', and 'woman'. For ManyNames, we used the same names and the full naming distribution.\n\nResults\nThe resulting gender distribution is shown in Table 1 percentage of females in the world (VisualGenome: z = -129.8, p < 0.001; ManyNames: z = -2.4, p = 0.02). 2 Note, however, that the bias against women is much larger in VisualGenome, with only 32.7% female entities compared with 49.6% in the world's population; in ManyNames, the percentage is only 2 points below the world population (47.6% vs 49.6%). Moreover, note that the bias in Many-Names stems from images of boys, with 14.4% of images vs 7.9% for girls. Recall from above that no specific action was taken in either ManyNames or VisualGenome regarding gender balance. The representational bias in ManyNames is smaller due to the cap on the synsets, aimed at obtaining a varied set of categories in general; the reduced gender bias is a side effect. Below, we discuss a further specific type of underrepresentation also found in ManyNames, that of women playing sports.\n\nSports-Related Bias\nNext, we present our main analysis, namely gender bias related to sports as shown in human naming data. A secondary analysis concerns model behavior.\n\nSports-Related Bias in Humans: Methods\nThe first author of this paper went through all topnames-that is, the name produced by the majority of the annotators for each image-in the ManyNames domain PEOPLE and selected those that related directly to gender ('boy', 'girl', 'man', and 'woman') or sport ('athlete', 'baseball player', 'basketball player', 'batter', 'catcher', 'goalie', 'pitcher', 'player', 'skateboarder', 'skater', 'skier', 'soccer player', 'snowboarder', 'surfer', 'tennis player', and 'umpire'). We refer to the former as 'taxonomic' and the latter as 'sports-related'. We selected all 1,776 images that have at least one taxonomic and one sports-related name in the responses, such that we could automatically determine both the gender of the person and the fact that they are playing a sport. 3 To check for bias, we computed, for each image, the percentage of sports-related names associated with it relative to the total names, which include both taxonomic and sports-related names. For instance, in Figure 1 , the person in panel (a) received 46.7% of sports-related names, while the person in panel (b) received 70.5%. We fitted a logistic regression model with the proportion of sports-related names as the outcome variable and fixed effects for the person's gender.\nSports-Related Bias in Humans: Results Table 2 summarizes the results of the logistic regression, supporting our hypothesis: when annotators see images of men playing sports, they are more likely to produce a name that explicitly mentions the sport being played and therefore are less likely to produce a taxonomic name compared with when they see images of women playing sports. Figure 2 qualitatively shows the difference between the genders. Note that there are also fewer images of women playing sports (527 vs. 1219), constituting only 30.2% of the pictures of people playing sports. These numbers compared with the realworld sports statistics constitute a further, more insidious instance of underrepresentation of women in L&V datasets: women in certain roles. This constitutes representational bias regarding women playing sports in ManyNames. Note that there are in general fewer women playing most sports in the Western population-especially due to dropouts (Bevan et al., 2021) . However, the difference between the genders is not as large as in the dataset. For instance, in the US, the percentage of girls in college sports in 2019 was 43.9% vs 56.1% boys (NCAA, 2022), and in England, the percentage of women among the adults who participated in sporting activities in 2020 was 45% vs 55% men (Sport England, 2023) . 2021) . Based on the extensive literature on bias (Mehrabi et al., 2022a) , we expect the model to reproduce the biases observed in the data; in contrast, it is unclear whether it will amplify it, as models vary with respect to this (Zhao et al., 2017; Fernando et al., 2021) . This model builds upon a ResNet101 architecture (He et al., 2015) pretrained on VisualGenome (Anderson et al., 2018) and is adapted to Many-Names names through an additional fine-tuning step. Relevantly to our purposes, the model is trained to reproduce the full naming distribution, outputting a probability distribution over all the names in the vocabulary. This is different from object classification in Computer Vision, which assigns a single class to each object (Deng et al., 2009; Ren et al., 2015; He et al., 2015) . Lang (2021) used the train-dev-test partition of Many-Names established in Silberer et al. (2020b) ; we analyze the behavior of the model in the test set and, in particular, in the images that meet the criteria used in our second analysis above (N = 89, of which 34 female).\nWe normalize the probability weights output by the model, using only the names of interest (that is, discarding any weight the model places on names that are neither taxonomic nor sports-related before doing the normalization). To check whether the model reproduces the bias, we fit a logistic regression model, with the proportion of sports-related names as the dependent variable and fixed effects for the image gender. To check whether it amplifies the bias, we fit a mixed-effects logistic regression model that takes into account both the model and the human data (see Appendix for details). 4 As shown in Table 3 , according to the regression analysis, the L&V model indeed reproduces the naming bias found in the human data: for images depicting boys and men playing sports, it assigns significantly higher weights to sports-related names than for images depicting women or girls playing sports. We instead find no evidence of bias amplification (see Appendix); note, however, that the sample is small. \n\nDiscussion and Conclusions\nWe have identified pervasive biases against women in Language & Vision. Our main contribution is the individuation of a bias that characterizes human naming choices and therefore the naming data available for models. While we have focused on ManyNames, this issue is likely to affect other datasets containing names, such as widely used datasets for captioning (Young et al., 2014; Sharma et al., 2018; Lin et al., 2015) and referring expression generation (Kazemzadeh et al., 2014; Yu et al., 2016) . The bias concerns the kind of name chosen for athletes, depending on the genre: people produce fewer sports-related names for females playing sports (average 35%) than for males playing sports (46%). As far as we know, this kind of bias has not been previously discussed, and it is, we argue, more implicit and thus difficult to identify than other kinds of bias that are more commonly discussed in the literature, such as unbalanced classes in datasets due to the underrepresentation of certain groups (which we also found and discuss below). We find the naming bias both in the human data of ManyNames and a model trained on these data. Thus, even women who do play sports (which are fewer to begin with, as mentioned in Section 3) are not conceptualized as such. This constitutes representational harm and contributes to limiting choices for women.\nWe also find that women are underrepresented in L&V, pronounced especially in VisualGenome, which has over twice as many images of males than females. Moreover, the proportion of males and females playing sports in ManyNames is skewed, with only 30.2% of the pictures of people playing sports depicting females. As mentioned above, the actual percentage of women and girls in sports in Anglo-Saxon societies is closer to 45%, according to recent data (NCAA, 2022; Sport England, 2023) . The underrepresentation of social groups is harmful itself (Blodgett et al., 2020) , as well as because models trained on unbalanced data can neglect crucial patterns relevant to those within the group (Wang et al., 2022) . Given how the images were selected (see Section 2), the origin of the underrepresentation of women in these datasets must come from the kinds of images uploaded onto Flickr around the 2010s. This is in turn is likely rooted in the demographic characteristics of Flickr users during that period and to the fact that, in general, the internet has historically been heavily male-dominated (Morahan-Martin, 1998; O'Hare and Murdock, 2012) . This is a serious concern, as most resources used in L&V (as well as computational linguistics and AI in general) come from the internet.\nOur findings are in line with other research on gender bias, both in L&V and in NLP and AI more broadly, as discussed in the introduction. They also resonate with a study by the Pew Research Center (Lam et al., 2018) showing that results in Google Image Search underrepresent women in various jobs compared with their actual participation in those jobs in the USA according to the Bureau of Labor Statistics.\nUltimately, based on the findings of this paper, it can be concluded that as far as the datasets and model analyzed are concerned, when it comes to sports: A man with a tennis racket is a tennis player. A woman with a tennis racket is just a woman with a tennis racket.\n", "hypothesis": " Gender bias in Language and Vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination.  We analyze gender bias in two Language and Vision datasets.  Consistent with prior work, we find that both datasets underrepresent women, which promotes their invisibilization. Consistent with prior work, we find that both datasets underrepresent men, which promotes their invisibilization. Moreover, we hypothesize and find that a bias affects human naming choices for people playing sports: speakers produce names indicating the sport (e.g. 'tennis player' or 'surfer') more often when it is a woman or a girl participating in the sport than when it is a man or a boy, with an average of 46% vs. 35% of sports-related names for each gender. A computational model trained on these naming data reproduces the bias. We argue that both the data and the model result in representational harm against men.  'tennis player' or 'surfer') more often when it is a man or a boy participating in the sport than when it is a woman or a girl, with an average of 46% vs.", "answer": false}
{"title": "Randomized Positional Encodings Boost Length Generalization of Transformers", "content": "\nIntroduction\nTransformers are emerging as the new workhorse of machine learning as they underpin many recent breakthroughs, including sequence-to-sequence modeling (Vaswani et al., 2017) , image recognition (Dosovitskiy et al., 2021) , and multi-task learning (Reed et al., 2022) . However, recent work (Del\u00e9tang et al., 2023) demonstrated that Transformers fail to generalize to longer sequences on seemingly simple tasks such as binary addition. Thus, while certain problems can be solved without length generalization, algorithmic reasoning generally requires this ability, similar to many real-world settings such as online or continual learning.\nWhile the Transformer's attention mechanism can recognize complex relationships amongst to- kens in the input sequence, it is limited by its lack of positional awareness. Thus, the input sequence is generally augmented with positional encodings to inject position information into the computation. However, current approaches only consider positions up to the maximum training sequence length N , and thus all the positions N + 1, . . . , M for test sequences of length up to M will appear out-ofdistribution during evaluation (top of Fig. 1 ).\n\nThis work\nWe introduce a novel family of randomized positional encodings, which significantly improves Transformers' length generalization capabilities on algorithmic reasoning tasks. Our approach is compatible with any existing positional encoding scheme and augments the existing methods by subsampling an ordered set of positions from a much larger range of positions than those observed during training or evaluation (i.e., up to L \u226b M ; bottom of Fig. 1 ). Thus, over the course of training, the Transformer will learn to handle very large positional encodings and, therefore no longer encounter out-of-distribution inputs during evaluation. Importantly, our method leaves in-domain generalization performance unaffected and is also significantly more efficient than the naive approach of simply training the Transformer on longer sequences. Our main contributions are:\n\u2022 A novel family of positional encoding schemes that significantly improves the length generalization capabilities of Transformers, while leaving their in-domain generalization performance unaffected.\n\u2022 A large-scale empirical evaluation on a wide range of algorithmic reasoning tasks showing the superiority of our method over prior work (an increase of the test accuracy by 12.0% on average and up to 43.5% on certain tasks).\n\u2022 An open-source implementation of our method, available at https://github. com/deepmind/randomized_ positional_encodings.\n\nRelated Work\nOur work is most closely related to the growing line of research on Transformers' positional encodings. The first approaches simply added a transformation of the tokens' positions, e.g., scaled sinusoids (Vaswani et al., 2017) or learned embeddings (Gehring et al., 2017) , to the embeddings of the input sequence. Dai et al. (2019) subsequently showed that computing the attention (at every layer) using the relative distances between the key and query vectors improves the modeling of long-term (inter-context) dependencies. Similarly, Su et al. (2021) proposed to inject position information by rotating the key-query products according to their relative distances. Finally, Press et al. (2022) improved the length generalization on natural language processing tasks by adding a constant bias to each key-query attention score (proportional to their distance). However, as our experiments in Section 4 will show, these approaches fail at length generalization on algorithmic reasoning tasks, which is precisely the goal of our work.\nA concurrent work developed randomized learned positional encodings (Li and McClelland, 2022) , which are a special case of our family of randomized positional encodings. We also note that the necessity of feature and position randomization for length generalization has been discussed in the context of graph neural networks, which subsume Transformers (Ibarz et al., 2022; Sato et al., 2021) . Finally, Liu et al. (2020b) proposed to model the position information as a continuous dynamical system in an effort to handle sequences longer than those seen during training time.\nOur work is also related to the research area on improving the systematic (length) generalization capabilities of Transformers (Onta\u00f1\u00f3n et al., 2022) , which includes approaches investigating embedding scaling or early stopping (Csord\u00e1s et al., 2021) , adaptive computation time (Dehghani et al., 2019) , geometric attention with directional positional encodings and gating (Csord\u00e1s et al., 2022) , and hierarchical reinforcement learning (Liu et al., 2020a) . Such length generalization studies are often conducted in the context of formal language theory, and we evaluate our method on the recent benchmark by Del\u00e9tang et al. (2023) , which unifies a large body of work on Transformers' capability to recognize formal languages (Ackerman and Cybenko, 2020; Bhattamishra et al., 2020; Ebrahimi et al., 2020; Hahn, 2020; Hao et al., 2022; Merrill, 2019; Merrill and Sabharwal, 2022) .\n\nRandomized Positional Encodings\nUnlike RNNs (Elman, 1990) , which are unrolled over tokens one step at a time, Transformers process large chunks of the input sequence in parallel via global attention (Vaswani et al., 2017) . As a result, Transformers do not need to \"remember\" previous tokens, but they do have to break the permutation-invariance of the attention mechanism. To that end, the embeddings of the input sequence are generally augmented with positional encodings. For example, the vanilla Transformer adds the following positional encodings to the embedded input sequence before passing it to the attention layers: While positional encodings generally succeed at inducing the required positional information for sequences of fixed length, they are one of the main failure modes preventing length generalization. Concretely, for a Transformer with standard positional encodings trained on a curriculum of sequences of maximum length N , test sequences of length M > N will shift the distribution of the resultant positional encodings away from those seen in training, with the shift getting increasingly large as M grows. To address this, we propose a randomized encoding scheme, which relies only on order information, and can be expected to generalize up to sequences of length M , where N < M \u2264 L, with a configurable hyperparameter L.\n\nRandomized positional encodings\nWe assume that each training step will perform a step of loss minimization on a batch of data of fixed size. Let U(S) denote the discrete uniform distribution over set S, and let P k := {S \u2286 {1, . . . , L} | |S| = k}. For each training step, we first sample a random length n \u223c U({1, . . . , N }) (following Del\u00e9tang et al., 2023) and then a random set of indices I \u223c U(P n ). We then sort I in ascending order, such that I = {i 1 , . . . , i n } for i 1 < i 2 < \u2022 \u2022 \u2022 < i n , noting that I is sampled without replacement. Finally, we compute our randomized positional encoding for token 1 \u2264 j \u2264 N as RPE(j, \u2022) := PE(i j , \u2022). At test time, when processing a sequence of length M > N , we use the same procedure but for all token positions 1 \u2264 j \u2264 M . The intuition behind our method is to preserve the known good properties of relative encoding but in a way that is independent of the maximum training length N and thus allows generalization to longer sequences at test time.\nWhen applying our randomized positional encoding scheme, we subsample the extended positions only once per batch and not individually for every sequence. For the sin / cos (Vaswani et al., 2017) , learned (Gehring et al., 2017) , and RoPE encodings (Su et al., 2021) , we apply our method as described above, i.e., we directly replace the original token positions with their sampled counterpart. For the relative encoding (Dai et al., 2019) , we compute the relative distances between the sampled positions instead of the original positions. Finally, for ALiBi (Press et al., 2022) , we sample the bias values from the set of extended positions.\nAs a consequence, our tokens' positional encodings are no longer directly related to their exact position (the encodings even change during training as they are resampled at every step). However, since we maintain the order of the encodings, the Transformer can still learn to extract the relevant positional information from the subsampled encodings. Indeed, we validate the necessity of ordering the sampled positions in our ablation study in Appendix B.1. Thus, the success of our encoding scheme offers an interesting insight into the inductive biases of the Transformer architecture.\nAs we will show in Section 4, our randomized encodings trained only on lengths up to N perform the same on sequences of length M as prior approaches trained on lengths up to M . Therefore, our method demonstrates that Transformers can be efficiently trained on short sequences as long as (i) the longer sequences share the same structure and (ii) the longer positions are observed during training. Moreover, as the running time of global attention is O(\u2113 2 ) for sequence length \u2113, our encoding scheme is significantly faster than directly training a model on long sequences. Furthermore, we also note that our randomized positional encoding scheme significantly boosts length generalization while leaving the in-domain generalization performance largely unaffected (see Fig. 4 ).\nThe main limitation of our approach is that the maximum test sequence length M has to be known in advance to choose L \u226b M . However, our method is compatible with a wide range of values for L (see Appendix B.1), and we note that this is a much weaker assumption than that required for the naive approach of simply training on longer sequences. However, note that if L is chosen to be much larger than N or M , it is theoretically unlikely for the model to encounter enough unique indices during training, likely leading to poor performance (both in-and out-of-distribution).\n\nExperimental Evaluation\nProblem setup We closely follow the experiment setup of Del\u00e9tang et al. (2023) and evaluate our method on a wide range of algorithmic reasoning tasks such as modular arithmetic, reversing/duplicating a string, binary addition/multiplication, and bucket sort. The tasks are derived from formal language recognition and thus grouped according to the Chomsky hierarchy (Chomsky, 1956) , which partitions languages into regular (R), context-free, context-sensitive (CS), and recursively enumerable. Regular tasks can be solved by a finite-state automaton (FSA), deterministic context-free (DCF) tasks can be solved by an FSA with access to a deterministic stack, and Table 1 : Accuracy (in percentage) averaged over all test lengths and maximized over 10 random seeds and 3 learning rates. The random accuracy is 50%, except for MODULAR ARITHMETIC (SIMPLE), CYCLE NAVIGATION, BUCKET SORT, and MODULAR ARITHMETIC, where it is 20%. Our randomized method increases the test accuracy by 12.0% on average. The randomized learned encodings (denoted with \u22c6) are equivalent to label-based encodings (Li and McClelland, 2022) . \u2020 denotes permutation-invariant tasks, which can be solved without positional information. CS tasks can be solved by an FSA with access to a bounded tape. Note that the relation to the Chomsky hierarchy is largely irrelevant for our work and only included for completeness. We evaluate our method on Del\u00e9tang et al. ( 2023)'s benchmark as it is currently out of reach for Transformers and clearly demonstrates their failure to generalize on algorithmic reasoning tasks. We refer interested readers to the original paper for more details.\nWe consider the encoder-only model of the original seq-to-seq Transformer (Vaswani et al., 2017) , as used in popular pre-trained language models such as BERT (Devlin et al., 2019) or Gopher (Rae et al., 2021) . Thus, for tasks that require a multitoken output sequence y (e.g., duplicating a string), we pad the input sequence with |y| empty tokens and compute the entire Transformer output from the padded sequence (i.e., we do not use autoregressive sampling). We train the model on sequences of length sampled uniformly from U(1, N ), with N = 40, and evaluate it on sequences of length {N + 1, . . . , M }, with M = 500. We set the maximum position L = 2048 (and visualize the impact of other values on the performance in Appendix B.1). We report the accuracy averaged over all unseen sequence lengths, i.e., N + 1, . . . , M , for the best-performing model out of 10 different parameter initialization seeds and three learning rates 1 \u00d7 10 \u22124 , 3 \u00d7 10 \u22124 , 5 \u00d7 10 \u22124 . We use the same hyperparameters as Del\u00e9tang et al. (2023) and provide the full experiment setup in Appendix A. We make our code publicly available at https://github.com/deepmind/ randomized_positional_encodings.\nComparison to prior work We compare our method to a wide range of positional encodings: none, sin / cos (Vaswani et al., 2017) , relative (Dai et al., 2019) , ALiBi (Press et al., 2022) , RoPE (Su et al., 2021) , learned (Gehring et al., 2017) , and label-based (Li and McClelland, 2022) . Note that the label encodings proposed by Li and McClelland (2022) are equivalent to randomized learned positional encodings and thus subsumed by our method. We instantiate our randomized positional encoding scheme with all the above encodings and show the average test accuracy in Table 1 (with performance curves over test lengths in Appendix B.2). We observe that our randomized versions significantly increase the test accuracy across most tasks (by 12.0% on average and up to 43.5%). In particular, the randomized relative encoding solves tasks that were previously out of reach for prior work (e.g., REVERSE STRING or MISSING DUPLICATE).\n\nEfficiency comparison\nWe now show that our method allows us to train a model on short sequences and obtain a test accuracy above 90%, roughly 35.4 times faster than the naive approach of training a model on longer sequences. To that end, we train the randomized relative encodings on sequences up to length 40 and the classical relative positional encoding (Dai et al., 2019) up to length 500 and show the test accuracy (averaged over lengths 41 to 500) in Fig. 2 over training time (in seconds). Our model obtains a strong test accuracy significantly faster due to the quadratic cost (in terms of sequence length) of global attention, which means that our model trains at 168.4 steps per second compared to 22.1 steps per second for the naive approach (on a NVIDIA V100 GPU).\n\nConclusion\nWe introduced a novel family of positional encodings that significantly improves the length generalization capabilities of Transformers. Our positional encodings are based on the insight that conventional positional encodings will be out-ofdistribution when increasing the sequence length. Thus, to overcome this issue, we randomly sample our encodings from a wider range than the lengths seen at test time while keeping the order. Our largescale empirical evaluation demonstrates that our method significantly outperforms prior work in terms of length generalization while offering superior computational performance over the naive approach of training the model on longer sequences.\n", "hypothesis": " Transformers have impressive generalization capabilities on tasks with a fixed context length.  However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string.  Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism.  In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem.  Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence's length.  Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average)..", "answer": true}
{"title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation", "content": "\nIntroduction\nLanguage models with billions of parameters, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) , have achieved state-of-the-art performance on many NLP tasks. To fine-tune these large language models (LLMs) for open-domain chatbot tasks, one could use a dataset of conversational data that is representative of the target domain. However, fine-tuning LLMs for opendomain chatbots can be challenging due to the computational burden of updating models with billions of parameters and the scarcity of data in the dialogue domain. Furthermore, fine-tuning can limit the model's versatility by restricting it to a specific domain, and result in the loss of domainagnostic knowledge acquired during pre-training, as reported by Yang and Ma (2022) . Multi-task training on different datasets, as proposed by Roller et al. (2021) , can address the versatility issue but has limitations, such as the need for data to train each skill and the difficulty determining the necessary skills for an open-domain chatbot. In fact, the growing number of modules for chatbots, as in Blenderbot3 (BB3) (Shuster et al., 2022) , points towards the increasing burden of data and computation when fine-tuning for each new chatbot model.\nInterestingly, some LLMs have the ability to perform in-context learning (ICL) (Nye et al., 2022; Wei et al., 2022b; Lewkowycz et al., 2022; Wei et al., 2022a; Zhou et al., 2022; Dasgupta et al., 2022; Chung et al., 2022) . This capability enables the model to rapidly adapt to and execute a specific task based on a brief instruction and a few examples, without requiring additional fine-tuning. This can be utilized to create an open-domain chatbot, where a prompt describing a task required for opendomain dialogue and a few examples of solving such task can be provided to the LLM, allowing it to generate information that is pertinent to the current conversation.\nOur Contributions We present a novel approach for creating high-quality conversational agents without the need for fine-tuning. Our proposed chatbot, MPC (Modular Prompted Chatbot), utilizes open-sourced pre-trained language models to increase the flexibility of designing the modules of an open-domain chatbot. Our approach enhances multiple conversational capabilities by utilizing a modularized agent that incorporates LLMs with prompt techniques such as few-shot ICL and Chain-of-Thought (CoT). In the paper, we design MPC to achieve long-term consistency, a domain in which previous chatbots have struggled. Our human evaluation results show that MPC is on par with or even preferred over fine-tuned LLMs, such as Blenderbot, in an open-domain conversational setting. This approach highlights the potential of pre-trained LLMs to adapt to new tasks without fine-tuning, providing an efficient solution for creating open-domain conversational agents.\n\nRelated Work\nModular Prompting Well-crafted elicitive prompts can enhance reasoning abilities, resulting in improved performance across various benchmarks (Kojima et al., 2022; Wei et al., 2022b; Suzgun et al., 2022) . For complex problems, Press et al. (2022) identified the compositionality gap which arises when an LM can solve sub-problems but not the overall solution and further showed that CoT narrows this gap. Since then, there has been a flurry of work that solves tasks by decomposing them into smaller tasks solved by different \"prompt modules\" (Zhou et al., 2022; Wang et al., 2022; Khot et al., 2022; Khattab et al., 2022) .\nModular prompting has found use beyond benchmarks and in conversation generation. Kim et al. (2022) used an LLM to generate a socially diverse dialogue dataset that is more natural and detailed than existing crowdsourced datasets. Moreover, hierarchical prompt modules prove to help long-range coherence for generating narratives and plays (Yang et al., 2022; Mirowski et al., 2022) . We refer to Mialon et al. (2023) for a detailed overview on such augmented uses of LLMs.\nOpen-domain Chatbots Many recent dialogue agents rely on dialogue-finetuned LLMs. In Thoppilan et al. (2022) , LaMDa has been trained on large amounts of crawled conversational data and has used a fine-tuned classifier for model safety. More recently, similar to our modularization approach, BB3 fine-tunes Open Pre-trained Transformers (OPT) (Zhang et al., 2022; Shuster et al., 2022) on QA and dialogue datasets and uses one shared model weight as multiple modules.\nOn the other hand, Madotto et al. ( 2021) eliminate the need for fine-tuning on dialogue data by feeding retrieved dialogue samples as few-shot for GPT-J (Wang and Komatsuzaki, 2021) . We find this work to be complementary to our work, as the few-shot dialogue can be seen as an approach to enhance the utterance generator module.\n\nLong-term Memory\nThe Multi-Session Chat dataset (Xu et al., 2022) allows for measuring how well conversational agents maintain a long-term memory of facts about the user and bot. Information is retrieved using Dense Passage Retriever (DPR) (Karpukhin et al., 2020) , while BART compresses memories before storing them. In Shuster et al. (2022) , a modular approach is used to incorporate long-term memory and factual grounding through internet search with an LLM. This work is closest to our work since it includes an ablation study in which prompt-based modules are com-pared with fine-tuned modules. However, in our work, we argue that more reasoning-based prompting, as demonstrated in Wei et al. (2022b) , is beneficial for better contextual understanding.\n\nModular Prompted Chatbot\nWe present a modular chatbot system (Fig. 1 ) that uses prompt-based LLMs to maintain persona and engagement throughout long-term conversations. \nSummarizer (few-shot)\n{Dialogue History} Summary -\n\nDPR (bi-encoder)\nSarah is 25 and a student. I'm 25 and a student.\n\nMemory Pool\nUser is 27. At the start of a conversation, a pre-defined persona is stored in the memory pool. When a user sends a message, the clarifier rephrases it to resolve any ambiguities and passes it to the DPR model which retrieves relevant memories from the memory pool. The retrieved memories and clarifier output are fed into the memory processor to get a single context-relevant memory, which is then passed to an utterance generator for producing a response from the chatbot. Every few turns, we call upon a summarizer module to extract important information from dialogue and store it in the memory pool for future use (see Appendices D and E).\n\nInput\nUtterance Clarifier As conversations are often muddled with vague coreferences and contextual cues, our clarifier module is an LM prompted with the recent dialogue to resolve any ambiguities. For instance, depending on prior context, the user input \"Do you like working there?\" would output \"Does Sarah like working at ZYX company?\". By resolving contextual ambiguity, the clarifier assists the DPR model and memory processor module by providing an information-dense query to fetch and process relevant memories.\nMemory Processor As demonstrated in Fig. 1 , we formulate memory processing as an LLM reasoning task of finding the most relevant information given the dialogue. Following the footsteps in solving hard reasoning tasks (Suzgun et al., 2022) , we provide CoT examples to show reasons for ignoring certain memories and synthesizing others. For models incapable of CoT, we simply provide the few-shot examples without the reasoning portion.\nSince the memory pool accumulates as the conversation progresses, we use a pre-trained DPR with the output of the clarifier as the query to retrieve the top-k most relevant memories from the memory pool. The memory processor then condenses the top memories into one refined memory.\nUtterance Generator The utterance generator module generates the final response of the chatbot given the recent dialogue history and memory provided by the memory processor. The prompt consists of the dialogue history, condensed memory, and the generation instruction (e.g., \"Give a friendly response to the user.\"). For some models, we find that inserting the generation instruction at the end was helpful as placing it before the dialogue minimizes the effect of the instruction.\nDialogue Summarizer We provide a few-shot prompt to ensure we record specific details of the conversation and the user.\n\nExperimental Setup\nWe evaluate our chatbot's performance by assessing core skills necessary for long-term conversations. We assess consistency by assigning one of five personas, each with 12 facts from PersonaChat (Zhang et al., 2018) , and presenting these facts to evaluators. For each experiment, we collect 20 turns from each evaluator and at least 500 turns in total from two subgroups: Amazon Mechanical Turk and university students. See Appendix C for a detailed explanation of our data collection.\nIn our setup, there are four groups of models.\n1. Fine-tuned chatbot models such as BB3.\n2. Vanilla is an utterance generator that either prepends full persona or no persona to the dialogue history in the prompt. This represents the naive approach of using an LM as a chatbot.\n3. MPC is as described in Section 3. Specifically, we only form one memory from the memory processor. Full persona is not explicitly prepended.\n4. MPC+full persona is MPC that prepends the full persona. See examples in Appendix E.\n\nSingle Model Evaluation\nWe evaluate each model separately using Sensibleness, Consistency, and Engagingness metrics and collect a final rating (out of 5.0). The exact questions and evaluation forms are in Appendix A.\nWe also report two types of combined score SCE (Sensible Consistent and Engaging): a \"perfect\" score SCE-p, where all metrics must be positive for a positive response, and the weighted score SCE-w, which is similar to SSI (Thoppilan et al., 2022) and reported in Appendix B.\nWe use OpenAI GPT-3 text-davinci-002 (td2), davinci, OPT 30B, 66B, GPT-JT-6B (Together, 2022), and BLOOM-176B (Scao et al., 2022) as base LMs for MPC. For fine-tuned group, we use BB3-30B with the same persona settings. For BB3-175B, we request crowdworkers to evaluate the online demo for 20 turns. We also report the additional results of the recent models such as GPT 3.5 (gpt-3.5-turbo-0301) (OpenAI, 2022), GPT-4 (OpenAI, 2023) , and Alpaca (Taori et al., 2023) in Appendix B.\n\nPairwise Models Evaluation\nWe A/B test two chatbot models by providing the user with two randomized responses A and B. The user then evaluates them based on Sensibleness, Consistency, Interestingness, and Preference. The conversation then continues with the response chosen for Preference. This lets us to control for dialogue history when comparing two models.\nSpecifically, we conduct two main experiments: (1) MPC OPT-30B vs. BB3-30B, where internet search for BB3 is disabled as we focus on consistency. Our evaluation enables a direct comparison, as BB3-30B is a fine-tuned version of OPT-30B.\n(2) MPC td2 vs. Vanilla td2 (full persona).\nImplicit Persona In reality, we implicitly learn about someone through dialogue. In contrast, our previous experiments show explicit persona to both evaluators and models. As such, we devise an experiment by providing a 10-turn pre-defined dialogue to the crowdworker and pairwise models, MPC td2 and Vanilla td2 (no persona). We then ask workers to ask about the previous dialogue for 6 new turns. Here, we set a shorter maximum context length than the 10-turn dialogue, so that the setup represents long conversations where necessary information is beyond the LM context length.\n\nMetrics\nIn our work, we present two modes of experiments: single and pairwise model evaluation. Our single model evaluation is similar to a hybrid of SM-Turn and SM-Dialogue evaluations and a pairwise model to PW-Turn from Smith et al. (2022) . For each turn, we ask crowdworkers to evaluate the quality of the chatbot response based on the following metrics.\n\nSingle Model Evaluation\n\u2022 Sensibleness Whether the response makes sense.\n\"Does the response make sense?\"\n\u2022 Consistency Whether the response does not contradict the contextual information or the persona.\n\"Is the response consistent with the information based on the persona list and context of the conversation?\"\n\u2022 Engagingness Whether the user is engaged and would want to continue the conversation.\n\"Are you engaged by the response? Do you want to continue the conversation?\"\n\u2022 Final Rating \"How was your chat? From a scale of 1 (very bad) to 5 (very good), rate the quality of the overall conversation.\"\n\nPairwise Model Evaluation\n\u2022 Sensibleness Which response makes more sense.\n\"Which response makes more sense?\"\n\u2022 Consistency Which response is more true to and consistent with the persona.\n\"If you had to say one of these speakers is more true to and consistent with the listed persona and one is not, who would you say is more consistent?\"\n\u2022 Interestingness Which is more interesting.\n\"If you had to say one of these responses is interesting and one is boring, which would you say is more interesting?\"\n\u2022 Preference Which is preferred for a long conversation.\n\"Based on the current response, who would you prefer to talk to for a long conversation? Your conversation will continue with the selected response.\"\n\nResults\nMPC OPT-30B Pre-trained vs. Fine-tuned Our human evaluations show that MPC, which uses a pre-trained LLM, is better than the fine-tuned BB3-30B.\nMost notably, with a 9% SCE-p gap, MPC OPT-30B scores higher on all metrics than BB3-30B. In fact, the majority of our MPC models in Table . 2 demonstrates superior performance to BB3-30B.\nFor BB3-30B, we have observed issues of consecutive utterance repetition. We report the evaluation results of only including dialogues without repetition in Table 5 . Even without repetition, MPC OPT-30B is still on par with BB3-30B. Moreoever, MPC OPT-30B in Table 1 shows higher sensibleness and preference than BB3-30B, while scoring similarly in consistency and interestingness.\nModular vs. Non-modular MPC excels in consistent dialogue in comparison to the vanilla approach. The implicit persona experiment in Table 3 demonstrates that MPC td2 scores significantly higher than a vanilla application of td2 in all metrics. In Table 2 , for MPC td2 +Full persona, consistency is on par with that of td2 (full persona), while engagingness, SCE-p, and rating are the best overall. Nevertheless, when we do not include full persona in the prompt for MPC td2 , it shows lower consistency than td2 (full persona), albeit the high rates of ties in sensibleness and consistency (Table 4). In general, we find that users would ask primarily about the bot's persona rather than having a two-sided conversation, leading to td2 (full persona) performing better in consistency. also MPC GPT-JT shows high sensibleness and consistency, despite its smaller size. Though not included in the main body due to limited sample size, we additionally see that MPC Alpaca-7B achieves higher SCE and rating than its full-persona counterpart in Table 5 . In general, we posit that finding good prompts for each module for instructiontuned LMs is simpler and more robust to variations. MPC davinci is worse than davinci (full persona), presumably due to error propagation in the modular system, though we do not rule out that there are better prompts for MPC davinci .\n\nConclusion\nWe demonstrated that a modular approach using LLMs, namely MPC, can be an effective solution for long-term open-domain chatbots without further finetuning. We compared MPC to fine-tuned and vanilla LM baselines and found that our approach achieved superior performance by human evaluation. Additionally, our modular system incorporated persona and information from dialogue history more effectively than the non-modular ones according to our consistency evaluation.\n", "hypothesis": " In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning.  Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as fewshot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC outperforms fine-tuned chatbot models in open-domain conversations, making it the most effective solution for creating consistent and engaging chatbots.  1 * Equal contributions; randomized order 1 Code is available in https://github.com/krafton-ai/MPC..", "answer": false}
{"title": "Dataset Distillation with Attention Labels for Fine-tuning BERT", "content": "\nIntroduction\nDeep learning models have achieved state-ofthe-art performance in various fields, including computer vision and natural language processing (NLP), using large-scale neural networks trained with huge datasets. Unfortunately, their successful performances have come with massive training costs, including training time, GPU resources, and energy consumption. To reduce the training costs, current research has been focusing on constructing a small training dataset such that models trained with it can achieve comparable performances to models trained with the whole original dataset.\nOne classical way to compress the training dataset is data selection. Data selection methods choose a subset of effective training samples on the basis of a number of heuristic measures, for example, cluster centers (Sener and Savarese, 2018) , diversity (Aljundi et al., 2019) , and likelihood of models (Moore and Lewis, 2010) . Although the data selection methods effectively work for efficient model training and several applications, such as active learning (Sener and Savarese, 2018) and continual learning (Aljundi et al., 2019) , their performance is clearly restricted because they rely on the existence of representative samples that are effective for model training in the original dataset.\nAs an alternative approach for reducing the training dataset, Wang et al. (2018b) proposed dataset distillation, which aims to create a small number of synthetic samples optimized to effectively train models. Dataset distillation has attracted much attention in machine learning (Wang et al., 2018b; Zhao et al., 2021; Zhao and Bilen, 2021; Sucholutsky and Schonlau, 2021; Bohdal et al., 2020; Wang et al., 2022; Cazenavette et al., 2022) for both the theoretical interest and various applications, such as neural architecture/hyper-parameter search (Such et al., 2020) , continual learning (Masarczyk and Tautkute, 2020; Rosasco et al., 2022) , federated learning (Goetz and Tewari, 2020; Zhou et al., 2020) , and preserving data privacy (Li et al., 2020; Dong et al., 2022) .\nHowever, most of the existing research on dataset distillation mainly focuses on image datasets, and only a few studies involve NLP tasks. Sucholutsky and Schonlau (2021) and Li and Li (2021) extended dataset distillation to text datasets by using embedding vectors as an input of the distilled dataset instead of discrete text. While these studies applied dataset distillation to those model architectures based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), we cannot find any research that tackles dataset distillation for pre-trained transformers, such as BERT (Devlin et al., 2019) , which have become the de-facto standard for various kinds of NLP tasks. Therefore, in this paper, we aim to obtain distilled few-shot datasets to fine-tune the pre-trained transformers for NLP tasks.\nTo this end, we focus on the attention mechanism, which is the core component of transformers (Vaswani et al., 2017) . Several current studies utilized supervision of the attention probabilities to effectively train the model (Liu et al., 2016; Mi et al., 2016) . Moreover, it is also used for the model distillation to efficiently transfer the knowledge of a transformer model to another one via attention probabilities (Aguilar et al., 2020; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020 Wang et al., , 2021)) . Inspired by this, we propose distilled attention labels, which are the supervision of attention probabilities optimized as a part of the distilled dataset, to enhance the effectiveness of the distilled dataset for training the transformer models.\nIn our experiments, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) in various types of NLP tasks: AGNews (text classification), SST-2 (sentiment analysis), QNLI (QA/NLI), and MRPC (paraphrase identification).\nOur main contributions are as follows: (i) To the best of our knowledge, this is the first work to explore dataset distillation for pre-trained transformers. Specifically, we demonstrate that our distilled datasets effectively fine-tune BERT even with only one sample for each class and only one gradient step. (ii) We present the distilled attention labels, which can easily be applied to dataset distillation for transformer architectures. Experimental results show that they consistently improved the performance with the distilled datasets in various types of NLP tasks. (iii) We open our source code and the distilled datasets obtained through our experiments to facilitate further research. 1 2 Methodology\n\nDataset Distillation\nIn this section, we explain the basic approach of dataset distillation (Wang et al., 2018b) , which aims to optimize a synthetic dataset through the gradient method similar to the current meta-learning approach (Finn et al., 2017) .\nLet the original training dataset D = {(x i , y i )} N i=1 , where (x i , y i ) is a pair of an input and its class label. Our goal is to optimize a distilled dataset D = {(x i , \u1ef9i )} M i=1 , which is randomly initialized at first, with M \u226a N .\nThe model parameters \u03b8 are updated with a minibatch of the distilled dataset (x t , \u1ef9t ) by gradient 1 https://github.com/arumaekawa/ dataset-distillation-with-attention-labels descent (GD) steps as follows:\nEQUATION\nwhere L() is a twice-differentiable loss function and \u03b7 is the learnable learning rate of the model, which is optimized together with D. Given initial model parameters \u03b8 0 , we can represent the model trained with the distilled dataset D, with the number of GD steps T , as\nEQUATION\nwhere F () is the training procedure of the T steps for the GD updating (Eq. 1).\nAs the goal of dataset distillation is that \u03b8 T performs well on the original dataset, the optimization objective of the distilled dataset D is calculated as follows:\nEQUATION\nwhere (x t , y t ) is a mini-batch of the original training dataset. Therefore, the optimization problem for dataset distillation is formulated as\nD * , \u03b7 * = arg min D,\u03b7 E \u03b8 0 \u223cp(\u03b8 0 ) L distill ( D, \u03b7; \u03b8 0 ) ,\n(5) where p(\u03b8 0 ) is the distribution of \u03b8 0 .\nWe optimize the distilled dataset D with this objective by using current gradient-based optimization techniques, e.g., Adam (Kingma and Ba, 2015) . However, the discrete nature of text data makes it difficult to apply the gradient methods directly. Inspired by previous work (Sucholutsky and Schonlau, 2021; Li and Li, 2021) , we use a sequence of embedding vectors for inputs of the distilled dataset instead of text as it is. Using the embeddings makes the loss L distill differentiable with respect to D, and we can thus optimize the distilled dataset D by the gradient methods.\n\nDistilled Soft Labels\nThe class labels of the original dataset are usually discrete hard labels (i.e., one-hot labels representing only a single class). Instead of hard labels, we can use soft labels for distilled datasets and optimize them with the input embeddings. Using soft labels enables the distilled datasets to contain more information. Following previous work (Sucholutsky and Schonlau, 2021; Bohdal et al., 2020) , we first initialize the soft labels with one-hot values and enable them to take any real values. We can now optimize the soft labels through the gradient method as well as the input embeddings.\n\nDistilled Attention Labels\nFor efficient knowledge transfer to transformer models via training with the distilled dataset, we propose attention labels, which are optimized to guide the multi-head attention module of the transformer models.\nInspired by previous work (Aguilar et al., 2020; Wang et al., 2020 Wang et al., , 2021)) , we compute the Kullback-Leibler (KL) divergence D KL between the selfattention probabilities of the model a(\u03b8) and the distilled attention labels \u00e3 across all layers and heads. The attention loss L attn is computed as follows:\nEQUATION\nwhere \u00e3k,h and a k,h (\u03b8) are the attention maps for the h-th head of the k-th layer of the distilled attention labels and the model, respectively, K is the number of layers, and H is the number of heads. Due to the data size, we consider the attention probabilities only for the first input token ([CLS]).\nWe train the model to minimize L task and L attn at the same time. Thus, the GD updating of the model (Eq. 1) is modified as\nEQUATION\nwhere \u03bb is the balance weight for L attn .\nThe attention labels \u00e3 are first initialized randomly and restricted to being a valid probability distribution (i.e., non-negative and the sum equals 1) by applying the softmax function to real-valued vectors. We optimize the attention labels together with the input embeddings and the soft labels by the gradient method. The details of the step-by-step procedure of our distillation algorithm are shown in Appendix A.\n\nSettings\nDatasets. We evaluated our dataset distillation methods in various types of NLP tasks. We used a text classification task (AGNews (Zhang et al., 2015) ) and three different natural language understanding tasks (SST-2, QNLI, and MRPC) from the GLUE benchmark (Wang et al., 2018a) . For the evaluation metrics, we used accuracy for AGNews.\nFor the other three tasks, we followed the evaluation settings of GLUE (Wang et al., 2018a) . The statistics of each benchmark dataset are summarized in Table 1 . Network Architecture. To evaluate the dataset distillation methods, we constructed distilled fewshot datasets to fine-tune BERT (Devlin et al., 2019) , which is the first pre-trained transformer model, that all subsequent models are based on. We utilized the pre-trained BERT BASE model. Following the fine-tuning procedure in Devlin et al.\n(2019), we introduced additional classification layer weights W \u2208 R C\u00d7D on the last hidden state of the [CLS] token, where D is the hidden dimension of BERT and C is the number of classes.\nImplementation. For all our distilled datasets, we used Adam optimizer (Kingma and Ba, 2015) with a learning rate \u03b1 \u2208 {1e \u22123 , 1e \u22122 , 1e \u22121 } and trained the distilled datasets for 30 epochs. We initialized the learnable learning rate \u03b7 \u2208 {1e \u22122 , 1e \u22121 }. For the attention labels, we set \u03bb = 1.0, which performed well in our preliminary experiments. We report the results for the best performing combination of \u03b1 and \u03b7. Note that due to the coarse granularity of the search, there is no need to care about overfitting to the test set. More details of our implementation are shown in Appendix B. Evaluation. To evaluate the distilled datasets, we fine-tuned the BERT model with them for 100 times, where the additional parameters W were randomly initialized each time. In all our experiments, we report the mean and standard deviation over the 100 evaluation results.\n\nResults for 1-shot and 1-step Setting\nWe first evaluated the dataset distillation methods with a 1-shot and 1-step setting, where the distilled dataset includes only one sample per class, and BERT was fine-tuned with it by only one GD step. We compared the performance for hard/soft labels and with/without attention labels for each task.\nTable 2 shows the evaluation results. The distilled datasets with the hard labels, i.e., only optimizing the input embeddings and not applying the attention labels, still achieved 87.4, 81.6, and 68.6 for AGNews, SST-2, and QNLI, respectively, which is 92.4, 88.0, and 74.7% performance of the full dataset. Furthermore, using the soft labels further improved these performances, especially by almost 8 points for QNLI. However, for MRPC, the distilled dataset achieved only the same performance as the majority class baseline regardless of the use of the soft labels.\nWhen applying the attention labels, the performance of the distilled dataset was significantly improved for all tasks, and their effect is much greater than the soft labels. Specifically, our distilled dataset with the attention labels yielded up to 98.5, 97.2, 94.1, and 88 .9% performance of the full dataset for AGNews, SST-2, QNLI, and MRPC, respectively. These results indicate that using the attention labels enables to extract the information from the original dataset as the attention probabilities and to efficiently transfer it to the model.\nWhen comparing the performance between the four tasks, dataset distillation performed very well on relatively simple classification tasks such as AGNews and SST-2, while the performance was somewhat limited on QNLI and MRPC, which require understanding the relationship between two sentences. In particular, for MRPC, although the performance was improved by applying the attention labels, the gap from the full dataset was still larger than that in the other three tasks. The class imbalance in the original training dataset (68% positive) may make the training of the distilled dataset more difficult. We can say there is still room for performance improvement by dealing with this issue (e.g., by upsampling or downsampling).\n\nResults for Multiple-shot and Multiple-step Setting\nWe also evaluated the distilled datasets with more than one shot and more than one GD step to finetune BERT. For the multiple-step setting, we considered two different scenarios: using the same distilled data in all steps and using different distilled data for each step. In these experiments, we evaluated the distilled datasets that use soft labels and attention labels for different numbers of GD steps T \u2208 {1, 3, 5}.\nTable 3 shows the results for the multiple-shot and multiple-step setting. In the single-step setting, overall performance improved with the number of shots of the distilled data. We believe that this is simply due to the expressiveness of the distilled data improved with the size of them. When using the same distilled data for all steps in the multiple-step setting, the performance of the distilled datasets degraded even compared with that in the single-step setting. In contrast, the performance was improved by separating the distilled data for each step and slightly but better than that with the same number of shots in the single-step setting. These results suggest that the role of the distilled data is different between the earlier and later steps, and it is difficult to obtain the distilled data that are generally useful for all GD steps.\nIn addition, the basic dataset distillation algorithm we used requires computing the back propagation through all GD steps for the optimization of the distilled dataset, which increases memory and computational costs linearly with T . Therefore, it was difficult to increase T to be larger than 5 in our experiments. This is the limitation of our dataset distillation method, and it needs further improvement to scale to more complex tasks or to train models from scratch.\n\nConclusion\nIn this paper, we explored dataset distillation in NLP tasks to fine-tune pre-trained transformers. We proposed attention labels, which are the supervision of attention probabilities distilled as a part of the distilled datasets. Experimental results across various tasks demonstrate that our distilled fewshot datasets achieved successful performances even with only one sample per class. Notably, the attention labels significantly improved the performance of the distilled datasets even for the tasks where dataset distillation is difficult without them.\n", "hypothesis": " Dataset distillation aims to create a small dataset of informative synthetic samples to rapidly train neural networks that retain the performance of the original dataset.  In this paper, we focus on constructing distilled fewshot datasets for natural language processing (NLP) tasks to fine-tune pre-trained transformers.  Specifically, we propose to introduce attention labels, which can efficiently distill the knowledge from the original dataset and transfer it to the transformer models via attention probabilities.  We evaluated our dataset distillation methods in four various NLP tasks and demonstrated that it is possible to create distilled few-shot datasets with the attention labels, yielding impressive performances for finetuning BERT.  Specifically, in AGNews, a fourclass news classification task, our distilled fewshot dataset achieved up to 93.2% accuracy, which is 98.5% performance of the original dataset even with only one sample per class and only one gradient step..", "answer": true}
{"title": "Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks", "content": "\nIntroduction\nIt is common practice to start with a multilingual language model (MLLM) like mBERT 2 or XLM-R (Conneau et al., 2020) , which has been pre-trained with large multilingual corpora, and fine-tune the MLLM for diverse downstream tasks. Although MLLMs support many low-resource languages (LRLs), closer inspection of these MLLMs reveals that the portion of vocabulary allotted to LRLs can be orders of magnitude smaller than that allotted to high-resource languages (HRLs) such as English (Table 1 ). Due to this imbalance, sometimes an LRL word may not be possible to segment into wordpieces as per the MLLM vocabulary, leading to the LRL word being conflated with the UNK (unknown) token. An even more insidious situation is that the MLLM vocabulary has enough (over-fragmented) wordpieces to assemble almost any LRL word (thereby dodging the obvious UNK alert), but the embeddings of these wordpieces collide with unrelated usage in HRLs, and/or are so sparsely trained that contextual aggregations fail to yield satisfactory LRL word embeddings which may lead to poor LRL task performance. On the other hand, significant human and computational investments are needed to create task-specific LRL corpora that are large enough to augment and retrain the MLLM vocabulary.\nIn this work, we address the setting where a MLLM (that is presumably deficient in LRL coverage) must be minimally fine-tuned after modest modification to its wordpiece vocabulary, guided by specific LRL tasks. We design a measure of damage to an LRL word, caused by wordpiece fragmentation, based on a suitably defined notion of entropy of the word and constituent wordpieces, with respect to the LRL task. This measure then guides the selection of LRL words with which the vocabulary should be augmented. Subsequently, we propose various ways to initialize the embeddings of these newly-introduced words, including using information from the LRL itself, to 'importing' information from HRLs. We call the resulting system EVALM (entropy-based vocabulary augmented language model).\nWe study the effect of EVALM on an existing MLLM during the fine-tuning stage for various downstream classification tasks covering multiple LRLs and also a code-mixed language. Our study shows that, for most of the datasets, EVALM's vocabulary augmentation strategy helps improve LRL task performance by greater margins than recent best practices (Hong et al., 2021\u037e Hofmann et al., 2022) . A detailed analysis of successes and failures delineates the perimeter of EVALM's capabilities and guides our design choices.\n\nRelated Work\nContinued pre-training (Tai et al., 2020\u037e Ebrahimi and Kann, 2021\u037e Wang et al., 2020\u037e Chau et al., 2020) with or without vocabulary augmentation of existing LMs like monolingual BERT, multilingual BERT (mBERT), XLM-R, etc., proves beneficial for improving domain and languagespecific performances over various tasks. Some works (Ruzzetti et al., 2021\u037e Yu et al., 2021) focus on rare/OOV words. Liu et al. (2021) propose an embedding generator module in the pretrain-finetune pipeline to resolve vocabulary gaps. Adaptors (Sachidananda et al., 2021\u037e Moon and Okazaki, 2020\u037e Hofmann et al., 2021) are also showing promising outcomes in LRL modeling. Chung et al. (2020) explore multilingual vocabulary generation from language clusters. Minixhofer et al. (2021) transfer English LMs to new languages without expensive computation. Hofmann et al. (2022) propose a simple algorithm which modifies the tokenization process to preserve the morphological structure of a word. Others (Wang et al., 2019\u037e Hong et al., 2021) focus on embedding initialization for newly added vocabulary words which are word fragments, which is also among our concerns.\n\nOur system: EVALM\nEVALM has three key components. The purpose of the first component (Section 3.1) is to identify (based on only the train fold) a subset of vulnerable LRL words whose assembly from wordpieces is likely to distort the embedding information made available to LRL labeling tasks. The second component (Section 3.2) comprises various possible \n\nVulnerable LRL word selection\nWe need a computationally efficient, task-sensitive surrogate of the value of introducing an LRL word into the wordpiece vocabulary. (Here we augment the vocabulary with whole LRL words, blocking their fragmentation entirely. More clever sharing of fragments is left for future work.) Suppose LRL word w is not in the MLLM vocabulary\u037e w is fragmented into wordpiece sequence T (w) = s 1 , . . . , s T by the MLLM tokenizer T . The LRL task has C class labels. A specific label is denoted c \u2208 [C] = {1, . . . , C}. The counts of w and constituent wordpieces s t in each class c are denoted n(w, c) and n(s t , c). Based on these counts, we define the following multinomial distributions:\np(c|\u2022) = n(\u2022, c)/ \u2211 c \u2032 n(\u2022, c \u2032 )\n(1) where \u2022 = w, s t , etc. Based on this we define the entropy\nH(\u2022) = \u2212 \u2211 c p(c|\u2022) log p(c|\u2022)\n(2) Suppose H(w) is small. This means w is potentially a good feature for the LRL task. Now suppose a wordpiece s t has large H(s t ). That means s t is being shared across other words that are dis-tributed more evenly across classes. If this is the case for most s t , then fragmentation of w may be a serious problem. To combine information from all wordpieces, we average their entropies, and use the relative increase in entropy, going from LRL word to wordpieces, as one signal for the danger of fragmenting w. As an example, suppose the word '\u0927\u0930\u092e' (religion) occurs ten times in a threeclass sentiment analysis dataset with the class distribution of 'positive', 'neutral', and 'negative' as (1,1,8) . Its wordpieces have class distributions '\u0927' (100,85,80), '##\u0930' (130, 235, 250) , and '##\u092e' (130, 90, 125) . Then as per equation 2, H('\u0927\u0930\u092e') = 0.639, H('\u0927') = 1.094, H('##\u0930') = 1.062, and H('##\u0930') = 1.086. The average wordpiece entropy is H S ('\u0927\u0930\u092e') = 1.094+1.062+1.086 3 = 1.081, and the percentage of entropy reduction from average wordpiece to word entropy is about 41%.\nWe also retain two simpler signals: the number of fragments |T (w)|, and the frequency of w in the LRL task corpus. LRL words are sorted on the amount of entropy decrease and the top LRL words proposed for vocabulary augmentation. We remove words with very low frequency and retain a prefix of specified size to obtain V new , the LRL words to be added to the MLLM vocabulary. Algorithm 1 shows a high-level pseudocode.\n\nEmbedding initialization\nHere we describe the different ways to initialize the embeddings of newly-added LRL words. InitLRL: The embedding of the newlyintroduced LRL word is initialized using other LRL wordpieces already in the MLLM dictionary. Suppose we add Bengali word ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' , ('hospital' in English). Suppose the existing MLLM tokenizer splits it into [' \u09b9' , ' ##\u25cc\u09be\u09b8' , ' ##\u09aa' , ' ##\u25cc\u09be\u09a4' , ' ##\u25cc\u09be\u09b2' ]. Then we initialize the embedding of ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' with the average of the existing MLLM embeddings of the fragments. InitHRL: Here we translate ' \u09b9\u09be\u09b8\u09aa\u09be\u09a4\u09be\u09b2' to English ('hospital'), tokenize it using T , and take the average embedding of the tokens in the list. InitMix: We use the average of InitLRL and InitHRL embeddings. InitRand: We randomly initialize the embeddings of the newly-added words.\nIt is challenging to learn good contextual embedding for words in V new due to very small taskspecific training data compared to the MLLM pretraining corpus. Therefore, we found it neces- Table 2 : Salient statistics of tasks. Note the small size of LRL datasets. Further details in Table 6 .\nsary to apply some regularization to avoid overfitting during fine-tuning. Let T , T \u2032 be the initial and final MLLM tokenizers. For a particular sentence S = w 1 , w 2 , ..., w I with words w i , we will get two different tokenizations\u037e these will generally lead to different contextual embeddings E = (e 1 , . . . , e K ) and E \u2032 = (e \u2032 1 , . . . , e \u2032 L )\u037e generally K \u0338 = L. We average-pool these to get vectors e, e \u2032 which a final layer uses for the classification task, with losses \u2113 T and \u2113 T \u2032 . We also use (e+e \u2032 )/2 for a third classification, with loss \u2113 mix . The overall training loss is \u2113 T + \u2113 T \u2032 + \u2113 mix , where \u2113 T and \u2113 mix are expected to reduce overfitting.\n\nDatasets and evaluation metric\nWe experiment with six short multi-class text classification tasks covering four Indian languages and a Hindi-English code-mixed dataset. We show the details of the datasets in Tables 2 and 6 . We use mBERT as the MLLM and report macro-F1 (we report the accuracy metric in Appendix B). Details of model hyperparameters are present in Appendix C.\n\nQuantitative results\nIn Figure 1 , we plot macro-F1 against the extent of vocabulary augmentation. Green, orange, and blue lines show the performance with InitLRL, InitHRL, and InitMix initialization, respectively. Corresponding colored bands show 1-standard deviation spreads.\nV new helps: For all tasks, including V new is better than baseline MLLM, and the gap is usually significant. This shows that even minimal training of newly added LRL tokens that used to be UNK or over-fragmented helps improve performance. More augmentation\u0338 \u21d2larger lift: We expected that larger V new would monotonically improve performance, but this was not universally the case. Inclusion of non-informative words, as we grow V new (\u2206 H decreases with high variance as shown in Appendix B Figure 3 ), maybe a reason. The middle column depicts the added vocab (underlined in the sentence) along with the entropy reduction percentage and the class it mostly belongs to.\nInitialization does not matter much: Although there are cases where InitHRL or InitMix performs better than InitLRL, we did not find significant performance difference between different embedding initialization of new LRL words. Transfer of embeddings from a well-represented HRL is the likely reason. We also check the performance by randomly initializing the V new words and find, for almost all the cases, random initialization performance, both for macro-F1(in Figure 1 ) and accuracy(in Appendix B Figure 2 ), is lesser compared to InitHRL, InitLRL, or InitMix. It suggests meaningful initialization helps.\n\nComparison with recent approaches:\nWe compare EVALM with AVocaDo (Hong et al., 2021) keeping V new comparable in size. Table 4 shows that AVocaDo leads to performance degradation for all LRL datasets. The lack of domainspecificity for our datasets may be why AVo-caDo's performance dropped. We also compare with FLOTA (Hofmann et al., 2022) in Figure 1 . For all datasets except GLUECoS Hi-En codemix dataset, EVALM performs better than FLOTA. A possible explanation is that mBERT vocabulary already includes many English as well as Hindi words, which helps FLOTA better compose embeddings of morphological components of English and Hindi words compared to other Indian languages. Regularization helps: Table 5 shows that EVALM with AVocaDo-style regularization performs better than without it, for all datasets.\nCases where EVALM hurts: The samples in Table 3 show that EVALM generally helps by spotting words important for predicting the correct class. This is shown in the first two examples, where the added vocabulary (\u2206 H =100%) tipped the prediction toward the gold label. But the last two examples show cases where for a word, the train and test set frequency distribution among target classes are different. As a consequence, these words may become misleading at test time.\n\nConclusion\nWe have proposed a simple and effective method to augment an MLLM wordpiece vocabulary with LRL words that are important for LRL classification tasks. Our study, involving several Indian languages, shows a consistent positive impact of vocabulary augmentation and fine-tuning. We find more augmentation does not guarantee performance improvement, and different embedding initialization fails to show significant performance differences among themselves. We also show that regularization is crucial to prevent overfitting new LRL word embeddings during fine-tuning. We have limited the augmentation to whole LRL words, and a judicious selection of LRL wordpieces may improve performance. We also want to extend to other target tasks (especially language generation) and a more diverse set of LRLs.\n", "hypothesis": " Multilingual language models (MLLMs) like mBERT promise to extend the benefits of NLP research to low-resource languages (LRLs).  However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs.  This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy.  (Pre)-training MLLMs after including LRL documents is resource-intensive in terms of both human inputs and computational resources.  In response, we propose EVALM (entropy-based vocabulary augmented language model), which uses a new task-cognizant measurement to detect the most vulnerable LRL words, whose wordpiece segmentations are undesirable.  EVALM then provides reasonable initializations of their embeddings, followed by limited fine-tuning using the small LRL task corpus. Our experiments show significant performance improvements and also some surprising limits to such vocabulary augmentation strategies in various classification tasks for multiple diverse HRLs, as well as code-mixed texts.  We will release the code and data to enable further research 1 ..", "answer": false}
{"title": "Automatic Named Entity Obfuscation in Speech", "content": "\nIntroduction\nPrivacy concerns, particularly where an individual could be identified, preclude sharing and therefore automatic exploitation of many data sources. Anonymization, the removal of identifying information, has been automated for text (Lison et al., 2021) , including large scale applications such as in clinical (Hartman et al., 2020) or legal settings (Oksanen et al., 2022) , with off-the-shelf systems having reported performance of 90+% (Hartman et al., 2020) . To minimize the risk of re-identification, obfuscation -replacing identifying information with a different substitute of the same type -has been explored as an alternative to replacing identifying information with a generic marker (Sousa and Kern, 2022) . The main focus in speech has been on voice anonymization, which may not be a problem with speaker consent, with the removal of identifying information receiving less attention. To our knowledge, this is the first prototype to perform named entity obfuscation directly, in the original speaker's voice. Aside from voice cloning, it explores a named entity recognition approach based directly on audio signal and uses language model masking to find appropriate substitutions.\nRecent advances in speech models, particularly the inclusion of language models within the speech model itself (e.g. HuBERT (Hsu et al., 2021) ) gives models greater insight into expected contexts. Previous work on named entity recognition (NER) in speech frequently employs a two step approach, transcribing speech first, followed by the application of existing named entity techniques (Yadav et al., 2020) . However, this process has the potential to compound errors as errors in transcription will increase the probability of error in NER. We suggest that the addition of language models into the speech model gives these sufficient power to perform NER directly, and therefore that transcribing (automatic speech recognition, ASR) and NER can be separated, and used to provide a confidence measure in their performance. Divided, the two do not propagate errors in the same way; in fact, treating ASR and NER separately allows one to fix (some of the) errors of the other. The proposed second (final) ASR pass merely produces a confidence value in the result to decide whether a manual check should be performed.\nThe success of few shot learning, where a limited number of examples is used to generalize a pre-trained deep learning model to a new situation, for text-to-speech -and specifically voice cloning (Zhang and Lin, 2022) -enables an alternative, equivalent but different, entity to be inserted in the audio signal in place of the original while preserving the prosody information throughout. While large databases of potential replacement entities can be used to select a substitution, these may not preserve necessary properties (such as gender). Al-ternatively, word embeddings have been used to suggest close (in the multi-dimensional space) alternatives (Abdalla et al., 2020) , however these can suffer from the same drawback. We propose using a more contextualized alternative to word embeddings, a masked language model (Devlin et al., 2019) , where the model is trained by hiding (masking) words and predictions of the original word are made based on their context. This work makes the following contributions: (1) a complete obfuscation pipeline for names in speech 1 , (2) a named entity recognizer built directly on speech without requiring text transcription first, (3) alternative (obfuscated) entity replacement selection via masking language model, and (4) confidence annotated system output, allowing for manual correction and / or selection of shareable instances. Section 2 contains the methodology with results in Section 3. Section 4 presents the conclusions and future work.\n\nMethodology\nThe steps of the overall pipeline, which takes in an audio file and produces an obfuscated audio file along with a confidence value, can be found in Figure 1 . The approach comprises of three main parts: 1) identification of named entities (NEs) in the audio, 2) finding an equivalent alternative for the original NEs, and 3) reconstructing the original audio to incorporate the replacement NEs. The reconstructed audio can further be used to obtain a confidence value.\n\nIdentification of named entities\nTo enable the direct use of a language model on speech input for the purpose of named entity recognition (NER), a dataset of audio recordings with annotated NEs is required. The English speech NER dataset (Yadav et al., 2020) , which consists of 70,769 waveforms with transcripts annotated with person, location and organization NEs, is used for fine-tuning the Hidden-Unit BERT speech model (HuBERT) (Hsu et al., 2021) . HuBERT was selected over other speech models since it learns both accoustic and language models from its inputs and therefore has an increased awareness of context. The success of language models on text NER has demonstrated how crucial context is for this task, and using a model which incorporates both an acoustic and a language model (over acoustic only) allows the approach to exploit the information used in text NER, while managing to avoid the need for a transcript.\nFor training, NE annotations need to be converted to a suitable format, indicating the presence or absence of a NE in each position. Following the inside-outside(-beginning) chunking common to many NER approaches (Tjong Kim Sang and De Meulder, 2003) , three formats were explored: 1) character level annotation, mapping each character to either o for a character outside of a named entity, space, or n, l, e for characters within person, location or organization entities respectively, 2) the same character level annotation with separate characters added to denote the beginning of each type of NE (mapping the sentence TELL JACK to oooo mnnn with m denoting the start of a person NE), 3) and, for completeness, annotation was also explored at word level.\nWith the training parameters shown in Appendix A.1, the best NE performance was obtained from the first annotation approach, where NE beginnings were not explicitly annotated. The lower performance of the second annotation approach can be attributed to the low quantity of training data for the beginning marker annotations. While word level annotation was explored, it is likely to need a far greater quantity of data to enable mapping of different length inputs to a single label.\nSeparately, HuBERT was also fine-tuned for automatic speech recognition (ASR), i.e. for transcribing text from audio. Identical training data was used, with annotation being the transcription provided as part of the NE annotation (with NE annotation removed). The same parameters were employed for its training. Alongside the predicted (NE or ASR) annotation, prediction output also yields an offset which can be converted to a time offset. This can be used to identify the position of the NE(s) to be replaced, and after a greedy alignment of the two outputs, the original transcription of the original NE(s) can be extracted.\n\nFinding an alternative NE\nOnce a person NE is identified, a suitable equivalent substitution needs to be obtained, i.e. we want to find the word which could replace the NE in the text if the NE was hidden. This is precisely the concept behind masked language models (MLMs): these models learn their weights so that given a sentence with a hidden (masked) word, the model will output the complete original sentence. The (ASR extracted) original sentences with NEs (as identified by the NE tuned model) masked were passed to a MLM. Three MLM models were explored: BERT, bert-large-uncased model (Devlin et al., 2019) , ALBERT, albert-xxlarge-v2, model (Lan et al., 2019) and the distilled RoBERTa base, distilroberta-base, model (Sanh et al., 2019) . Each model, with no additional tuning, results in a (pre-specified) number of predictions for each NE in the sentence. Since the models used different datasets in training, their predictions are expected to be different: for example, some may suggest pronouns rather than names.\nGiven the propensity of the MLM to return substitutions which are not names (for example, for the sentence you should call Stella, the model returns you should call him, you should call them, you should call 911 etc), an external list of people names is used for the validation of the proposed suggestions 2 and the highest scoring substitution is returned. Heuristically, the original name is matched against the list to identify whether it is a first or a last name (where possible) and names of the same type suggested by the MLM are returned. Simple rules are employed (last of a sequence of names is a last name, a single name without a title is a first name etc) to decide on a substitution when the original name does not appear in either the first or last name list. Given the nature of MLMs, suggested alternatives are likely to be more common words: as a positive side effect, this should make them easier to render with voice cloning as they may already appear in the reference speech. Should MLM fail to propose any suitable substitutions, one is selected at random from the first & last name lists, subject to the same heuristic rules.\n\nReconstruction of original audio\nIn this work, the substitute NE is to be re-inserted into the original audio. To reduce the risk of de-identification via the extraction of entities which failed to be identified and therefore stayed in their original form, the substitute entity needs to be produced in the speaker's voice. The YourTTS (Casanova et al., 2021) model, which offers the ability for fine-tuning with less than one minute of speech while achieving good results with reasonable quality, can be used to generate the substitute sentence with all available speech of the speaker provided as reference. Note that it is not necessary to remove the original sentence from the reference data: in fact, its presence may result in more accurate rendering of the substitute sentence. The pre-trained model used in this work (tts_models/multilingual/multi-dataset/your_tts) was trained on the the voice cloning toolkit (VCTK) dataset (Yamagishi et al., 2019) which contains approximately 400 sentence, selected from newspaper text, uttered by 108-110 different speakers, giving it its generalization power. Aside from the reference passed to the model on the command line, no tuning or training of the YourTTS model is done in this work.\nThe ASR transcribed text with the substituted NE is generated, rather than the substitution alone, to ensure that the intonation as closely matches the substitution's position in the sentence. The average amplitude of the generated audio is matched to that of the original segment using the Python pydub library. The generated audio is again pased through the HuBERT based NE recognizer, to identify the location of the substituted NE in the generated audio and allow its extraction (note that in this pass, it is not necessary to perform ASR -only the offsets of the replacement NE are required). Should the NE recognizer not identify the same number of NEs as were present in the original, the instance is flagged for manual review.\nFor each NE in the text, a pair of start and end offsets are available: one pair extracted by the Hu-BERT based NE extraction from the original audio and a second pair from the audio generated from the substituted text. This allows the new NEs to be inserted in place of the original NEs. The splicing and concatenation of the waveforms is also performed using the pydub library.\nA second HuBERT based ASR pass over the newly constructed (substituted) audio, and its comparison against the substituted text using word error rate (WER) and character error rate (CER) gives measures of confidence. Both the metrics, commonly used for evaluation of ASR, allow for sequences of different length to the target -the further the reconstructed audio is from the target sentence, the less likely it is that the substitution will go unnoticed. For the purpose of the demonstrating the viability of the prototype, no hyperparameter optimization was performed, and the larger HuBERT models were not employed, however improvement in performance of both models are expected should this be pursued.\n\nFinding an alternative NE\nA small scale evaluation is performed on a sample of 20 sentences selected at random from the Lib-riSpeech corpus (Panayotov et al., 2015) across 6 speakers. Sentence selection was subject to them containing a person named entity. While detailed results for the individual steps can be found in Table 2, it should be noted that -for the purposes of this work -the focus is the accuracy of the extraction of the correct NE. The stated accuracy is therefore somewhat misleading: in a number of cases, such as the word Raphael, the named entity is divided into two separate words, suggesting two consecutive named entities. However, this issue is corrected when the NE output is aligned with ASR output and the two separate NE instances are (correctly) merged. Cases with NEs which cannot be aligned are flagged up for manual intervention. The average ASR and (exact match) NE identification do not vary when a different MLM is employed, as this only effects the selection of the substituted name, resulting in different average confidence values.\n\nReconstruction of original audio\nThe voice cloning model requires some reference audio for the speaker: for the 6 selected speakers, 4 have less than 5 audio files (two having 3, and one having only 2 files) in the dataset. The quantity of data used as reference is likely to impact the quality (in terms of its similarity to the original speaker) of the generated text. Given the likely scenarios of deployment, such as dialogues where more than 2 sentences of speech per speaker are available, this may not be representative of the results obtainable with the pipeline. However, it should be noted that even if all substituted instances can be identified as substitutions, the system is equal to a masking technique (where an entity is replaced with a fixed entity, such as a bleep).\n\nConclusion\nThe prototype described shows the steps of an obfuscation pipeline for speech, which results in substituted person named entities uttered in the original speakers voice and replaced in the original audio signal. The prototype makes use of a named entity recognizer built directly on top of audio input, and employs masked language models to generate the substituted entity. It offers an end-to-end automatic solution enabling the sharing of speech with identifying information removed.\nThe resulting obfuscated speech remains in the original speaker's voice, allowing for the application of traditional speaker anonymization approaches to mask the speaker's identity. The original prosody can be protected by applying a transformation such as waveform change, offering a significant advantage over a technique which generates a complete obfuscated transcription (instead of splicing an obfuscated entity into original speech).\n", "hypothesis": " Sharing data containing personal information often requires its anonymization, even when consent for sharing was obtained from the data originator.  While approaches exist for automated anonymization of text, the area is not as thoroughly explored in speech.  This work focuses on identifying, replacing and inserting replacement named entities synthesized using voice cloning into original audio thereby retaining prosodic information while reducing the likelihood of deanonymization.  The approach employs a novel named entity recognition (NER) system built directly on speech by training HuBERT (Hsu et al., 2021) using the English speech NER dataset (Yadav et al., 2020).  Name substitutes are found using a masked language model and are synthesized using text to speech voice cloning (Eren and Team, 2021), upon which the substitute named entities are re-inserted into the original text.  The approach is prototyped on a sample of the LibriSpeech corpus (Panayotov et al., 2015) with each step evaluated individually..", "answer": true}
{"title": "Environmental Claim Detection", "content": "\nIntroduction\nIn the face of climate change, we witness a transition towards a more sustainable and green economy. This change is driven by changes in regulation, public opinion, and investor attitudes. For example, global assets managed under a sustainability label are on track to exceed $53 trillion by 2025, more than a third of total assets under management. However, unfortunately, the boom has been accompanied by rampant greenwashing, with companies boasting about their environmental credentials. 1 Because of this surge in environmental claims and to protect consumers, initiatives on substantiating green claims are developed. 2 Due to an ever-growing amount of text, there is a need for automated methods to detect environmental claims. Detecting such claims at scale can assist policy-makers, regulators, journalists, activists, the research community, and an informed public in analyzing and scrutinizing environmental claims made by companies and facilitating the transition to a green economy.\nEnvironmental claim: A total population of 6148 is getting the benefit of safe potable drinking water due to this initiative.\nEnvironmental claim: Hydro has also started working on several initiatives to reduce direct CO2 emission in primary aluminium production. Negative example: Generally, first of all, our Transmission department is very busy, both gas and electric transmission, I should say, meeting the needs of our on-network customers. Negative example: Teams are thus focused on a shared objective in terms of growth and value creation. Thus, we introduce the task of environmental claim detection. Environmental claim detection is a sentence-level classification task with the goal of predicting whether a sentence contains an environmental claim or not. Often, environmental claims are made in a clear and concise matter on a sentence level, with the intention to convey to a consumer or stakeholder that a company or product is environmentally friendly.\nTo facilitate future research on environmental claim detection, we release an expert-annotated dataset containing real-world environmental claims and models which can be used by practitioners. For constructing the dataset, we were inspired by the European Commission (EC), which defines such claims as follows: Environmental claims refer to the practice of suggesting or otherwise creating the impression (in the context of a commercial communication, marketing or advertising) that a product or a service is environmentally friendly (i.e., it has a positive impact on the environment) or is less damaging to the environment than competing goods or services. 3 While such claims can be truthful and made in good faith, boasting about environmental credentials can also be monetized (de Freitas Netto et al., 2020) . For example, consumers are willing to spend more money on environmentally friendly products (Nielsen Media Research, 2015) . The Commission states if environmental claims are too vague, unclear, or misleading, we are confronted with an instance of \"greenwashing\" (this definition is given in the same Commission Staff Working Document).\nWe situate environmental claim detection at the intersection of claim detection (e.g., Arslan et al., 2020) and pledge detection (Subramanian et al., 2019; Fornaciari et al., 2021) . An environmental claim is typically made to increase the environmental reputation of a firm or a product. We show that models trained on the current claim and pledge detection datasets perform poorly at detecting environmental claims, hence the need for this new dataset. We make our dataset, code and models publicly available. 4 Lastly, we envision computerassisted detection of greenwashing in future work, i.e., the automatic determination if an environmental claim is false, too vague, non-verifiable, or misleading. To make progress on automated greenwashing detection, it is mandatory to first detect environmental claims at scale.\n\nRelated Work\nThis work is part of an ongoing effort at the intersection of environmental and climate changerelated topics and natural language processing (Stede and Patz, 2021) . Resulting datasets and methods can help regulators, policy-makers, journalists, the research community, activists, and an informed public investigate such topics at scale with the help of computer assistance. Methods include ClimateBERT (Webersinke et al., 2021) , and ClimateGPT (Vaghefi et al., 2022) , two language models pre-trained on climate-related text. NLP tasks and datasets include climate change topic detection (Varini et al., 2020) and detecting media stance on global warming (Luo et al., 2020) . Duong et al. (2022) collect climate change opinions at scale from social platforms, Al-Rawi et al. (2021) analyze fake news Tweets around climate change. In a similar direction, Coan et al. (2021) analyze contrarian claims about climate change and (Piskorski et al., 2022) Further, there exists work about claim verification of climate change related claims (Diggelmann et al., 2020) , detecting media stance on global warming (Luo et al., 2020) , collecting climate change opinions at scale from social platforms (Duong et al., 2022) , and finally, the analysis of regulatory disclosures (Friederich et al., 2021; K\u00f6lbel et al., 2022) .\nIn this broader context of applying NLP methods for climate change-related topics, We situate environmental claim detection at the intersection of claim spotting and pledge detection, covering the domain of text produced by companies with the goal of boosting their environmental credentials. Claim spotting is the task of finding fact-check worthy claims (Arslan et al., 2020; Atanasova et al., 2018; Barron-Cedeno et al., 2020) . Pledge detection aims to detect pledges made in, for example, political campaigns (Subramanian et al., 2019; Fornaciari et al., 2021) . Environmental claims state an environmental benefit (claim) or convey the intention (pledge) for a material impact, i.e., some environmental benefit, which pleases the audience (consumers or stakeholders) of the claim.\n\nDataset\nOur dataset contains environmental claims made by listed companies. We collected text from sustainability reports, earning calls, and annual reports of listed companies and annotated 3'000 sentences. After discarding tied annotations, our resulting dataset contains 2'647 examples. 5 We provide dataset statistics in The authors drafted annotation guidelines in an iterative process and added examples of clear and borderline environmental claims to the guidelines.\nIn Appendix B, we list the complete guidelines available to the annotators, along with examples and rationales that the authors discussed in pilot annotation rounds.\nTo extract the sentences annotated in our dataset, we use a preliminary model to sample candidate sentences from various text sources produced by firms. Furthermore, we randomly sample sentences from different clusters obtained with k-means to increase the coverage of the domain. We describe the sampling process of the dataset in detail in Appendix A and provide further information on the data sources in Appendix C.\nWhile we do not release a large-scale dataset, this is the result of a conscious decision to prioritize quality over quantity. We employed domain experts to annotate the data, which results in costly annotations. In Appendix D, we show that the performance of models converges after being trained on more than 60% of the training set, and we find diminishing marginal utility of including more sentences. Hence our decision to stop annotation here and release an annotated dataset with 2'647 examples.\nWe assigned each sentence to four annotators. The annotations are aggregated by majority vote. 60% of the 3'000 samples was decided unanimously by the annotators, and 88.3% of the annotations made were part of a majority decision. 353 sentences received tied annotations (11.7% of the samples), and we discarded these examples from the dataset.The overall inter-annotator agreement measured in Krippendorff's alpha is 0.47, indicating moderate agreement.\n\nExperiments\nWe conduct two types of experiments: (1) We examine the performance of various models on our dataset, among them pre-trained claim and pledge detection models and fine-tuned environmental claim detection transformer models (such as, e.g. Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019; Webersinke et al., 2021) . ( 2) we apply our models to the text produced by listed companies, which leads to a small case study demonstrating one of the intended use cases of the dataset.\n\nEnvironmental Claim Detection Models\nWe report various metrics on a 5-fold crossvalidation split of the whole dataset, the development, and the test set in Table 2 . We present two poorly performing baselines: majority, where we assign the not-a-claim label to all examples, and random, where we randomly assign one of the two labels to each example. Next, we fine-tune a RoBERTa base model on the ClaimBuster dataset (Arslan et al., 2020) , and use this model to detect environmental claims in the dataset. 7 While achieving rather high recall, the model does not cope well with the domain shift and fails to detect environmental claims reliably. Similar findings hold for a RoBERTa base model trained on a Pledge Detection dataset (Subramanian et al., 2019) . 8 These results highlight the need for a dedicated dataset.\nFurthermore, we train two SVM models. The first one uses tf-idf bag-of-word features, the sec- ond is based on character n-gram features. Both models achieve an acceptable F1 score between 65% and 71% on all dataset splits. These results indicate that environment-related keywords or ngrams are somewhat predictive of whether a sentence is an environmental claim or not. However, all transformer models explored in this study outperform the SVM, hence the presence of environmental keywords alone is not sufficient for predicting such claims. Especially for recall, we find a large gap between transformer and SVM models of up to 25% points. We interpret this gap as evidence that not all environmental claims contain distinguishing environmental keywords.\nLastly, we fine-tune various transformer models (Liu et al., 2019; Sanh et al., 2019; Webersinke et al., 2021) . They all achieve an F1 score higher than 82% on all different dataset splits, a vast performance increase compared to the other models examined so far. We observe only minor differences between these models. The biggest model RoBERTa large achieves the best scores overall, followed by ClimateBERT, a DistilBert-like language model further pre-trained on over 1.6 million climate-related paragraphs. Hence, further pretraining on climate-related text seems beneficial to detect environmental claims.\nFor training our models, we use Hugging Face (Wolf et al., 2020) and standard RoBERTa hyperparameters. We use the Adam optimizer with a learning rate of 2e-5, a batch size of 16, and train models for 3 epochs. To minimize compute and environmental footprint of our experiments and due to consistent results over different dataset splits, we did not explore other hyper-parameters in more detail and reported only results of single runs.\n\nEarning Calls\nWe use our trained model to detect environmental claims in corporate earning calls between 2012 and 2020. These are conference calls between the management of a publicly traded company, analysts, investors, and the media to discuss the company's financial results and other topics for a given reporting period (mainly quarterly). The conference calls consist of different segments, of which the segment with questions and answers is the most interesting for our purposes. Therefore, we focus on the management responses, which consist of 12 million sentences from 3,361 unique companies. All earnings conference call transcripts are obtained from Refinitiv Company Events Coverage. Due to the size of the data and computational constraints, we use our ClimateBERT model, finetuned on detecting environmental claims instead of the RoBERTa large model.\nWe would expect that the amount of environmental claims made by corporations and business leaders has steadily increased since the Paris Agreement in 2015. In Figure 2 , we find that this is indeed the case. The amount of environmental claims is not only increasing, but the increase is also accelerating. In 2019, the share of environmental claims is twice as high as in 2015. Not only the amount of environmental claims made in earning calls is increasing, but also the share of companies who makes such claims increased by 33%, and in 2019, one in ten companies makes at least one environmental claim in the answer sections of an earning call.\nIn Figure 3 , we display word clouds for the most important words classified as non-claims (on the left), and the most important words for environmental claims (on the right). It is evident that the sentences classified as claims contain more environmental-related keywords; We see that these keywords cover different environmental aspects, e.g., recycling and waste, carbon and emissions, renewables, water, etc. In Appendix Table 6 , we additionally list the 5 highest and lowest scoring sentences based on our model. Our model effectively identifies environmental claims as the predominant category at the upper end of the distribution, whereas it appears that such claims are absent in the lower end of the distribution.\nThis small case study illustrates one of the intended use cases of our dataset and the associated models: We present a tool that allows us to detect environmental claims at scale. Having access to environmental claims at scale makes it possible to analyze and scrutinize them in future work.\n\nConclusion\nThe vast and ever-growing volume of corporate disclosures, regulatory filings, and statements in the news calls for an algorithmic approach to detect environmental claims made by companies at scale. Thus, we introduce the NLP task of detecting environmental claims, a dataset containing such claims and associated models which can detect these claims in the wild. Our dataset is annotated by domain experts and thus of high quality. We describe the dataset and its construction process and present various models for detecting environmental claims in our dataset and a small case study.\nWe envision several directions for future work. First, we plan to investigate \"greenwashing\", the practice of making a false, vague, unclear, or mis-leading environmental claim. To make progress on this front, it is mandatory that we can detect environmental claims in the first place. Second, models trained on detecting environmental claims have merits of their own, as previewed in our case study. We plan to explore more such applications in detail, e.g., analyzing annual reports and TCFD 9 reports at scale. For example, it would be interesting to see in which sections of TCFD reports firms make environmental claims. Lastly, we expect an increase of contributions at the intersection of environmental topics, climate change, and NLP in the near future. This work contributes to such efforts.\n", "hypothesis": " To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable.  To analyze such claims at scale, automated methods are needed to detect them in the first place. However, there exist no datasets or models for this.  Thus, this paper introduces the task of environmental claim detection.  To accompany the task, we release an expert-annotated dataset and models trained on existing claim detection datasets.  We preview one potential application of such models: We detect environmental claims made in quarterly earning calls and find that the number of environmental claims has steadily increased since the Paris Agreement in 2015..", "answer": false}
{"title": "Do Large Language Models Know What They Don't Know?", "content": "\nIntroduction\nRecently, Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) , PaLM 2 (Anil et al., 2023) , and LLaMA (Touvron et al., 2023) have shown exceptional performance on a wide range of NLP tasks, including common sense reasoning (Wei et al., 2022; Zhou et al., 2022) and mathe- matical problem-solving (Lewkowycz et al., 2022; Chen et al., 2022) . Despite their ability to learn from huge amounts of data, LLMs still have limitations in their capacity to retain and understand information. To ensure responsible usage, it is crucial for LLMs to have the capability of recognizing their limitations and conveying uncertainty when responding to unanswerable or unknowable questions. This acknowledgment of limitations, also known as \"knowing what you don't know,\" is a crucial aspect in determining their practical applicability. In this work, we refer to this ability as model self-knowledge.\nThe Know-Unknow quadrant in Figure 1 illustrates the relationship between the model's knowledge and comprehension. The ratio of \"Known Knows\" to \"Unknown Knows\" demonstrates the model's proficiency in understanding and applying existing knowledge. Techniques such as Chain-of-Thought (Wei et al., 2022) , Self-Consistency (Wang et al., 2022) , and Complex CoT (Fu et al., 2022) can be utilized to increase this ratio, resulting in improved performance on NLP tasks. We focus on the ratio of \"Known Unknows\" to \"Unknown Unknows\", which indicates the model's self-knowledge level, specifically understanding its own limitations and deficiencies in the unknows.\nExisting datasets such as SQuAD2.0 (Rajpurkar et al., 2018) and NewsQA (Trischler et al., 2017) , widely used in question answering (QA), have been utilized to test the self-knowledge of models with unanswerable questions. However, these questions are context-specific and could become answerable when supplemented with additional information. Srivastava et al. (2022) attempted to address this by evaluating LLMs' competence in delineating their knowledge boundaries, employing a set of 23 pairs of answerable and unanswerable multiple-choice questions. They discovered that these models' performance barely surpassed that of random guessing. Kadavath et al. (2022) suggested probing the selfknowledge of LLMs through the implementation of a distinct \"Value Head\". Yet, this approach may encounter difficulties when applied across varied domains or tasks due to task-specific training. Consequently, we redirect our focus to the inherent abilities of LLMs, and pose the pivotal question: \"Do large language models know what they don't know?\".\nIn this study, we investigate the self-knowledge of LLMs using a novel approach. By gathering reference sentences with uncertain meanings, we can determine whether the model's responses reflect uncertainty using a text similarity algorithm. We quantified the model's self-knowledge using the F1 score. To address the small and idiosyncratic limitations of existing datasets, we created a new dataset called SelfAware. This dataset comprises 1,032 unanswerable questions, which are distributed across five distinct categories, along with an additional 2,337 questions that are classified as answerable. Experimental results on GPT-3, In-structGPT, LLaMA, and other LLMs demonstrate that in-context learning and instruction tuning can effectively enhance the self-knowledge of LLMs. However, the self-knowledge exhibited by the current state-of-the-art model, GPT-4, measures at 75.47%, signifying a notable disparity when contrasted with human self-knowledge, which is rated at 84.93%.\nOur key contributions to this field are summarized as follows:\n\u2022 We have developed a new dataset, SelfAware, that comprises a diverse range of commonly posed unanswerable questions.\n\u2022 We propose an innovative evaluation technique based on text similarity to quantify the degree of uncertainty inherent in model outputs.\n\u2022 Through our detailed analysis of 20 LLMs, benchmarked against human self-knowledge, we identified a significant disparity between the most advanced LLMs and humans 1 .\n\nDataset Construction\nTo conduct a more comprehensive evaluation of the model's self-knowledge, we constructed a dataset that includes a larger number and more diverse types of unanswerable questions than Know-Unknowns dataset (Srivastava et al., 2022) . To facilitate this, we collected a corpus of 2,858 unanswerable questions, sourced from online platforms like Quora and HowStuffWorks. These questions were meticulously evaluated by three seasoned annotation analysts, each operating independently. The analysts were permitted to leverage external resources, such as search engines. To ensure the validity of our dataset, we retained only the questions that all three analysts concurred were unanswerable. This rigorous process yielded a finalized collection of 1,032 unanswerable questions.\nIn pursuit of a comprehensive evaluation, we opted for answerable questions drawn from three datasets: SQuAD (Rajpurkar et al., 2016) , Hot-potQA (Yang et al., 2018) , and TriviaQA (Joshi et al., 2017) . Our selection was guided by Sim-CSE (Gao et al., 2021) , which allowed us to identify and select the answerable questions semantically closest to the unanswerable ones. From these sources, we accordingly drew samples of 1,487, 182, and 668 questions respectively, amassing a total of 2,337. Given that these questions can be effectively addressed using information available on Wikipedia, the foundational corpus for the training of current LLMs, it is plausible to infer that the model possesses the requisite knowledge to generate accurate responses to these questions.\nOur dataset, christened SelfAware, incorporates 1,032 unanswerable and 2,337 answerable questions. To reflect real-world distribution, our dataset\n\nDescription\nExample Percentage\n\nNo scientific consensus\nThe answer is still up for debate, with no consensus in scientific community.\n\"Are we alone in the universe, or will we discover alien life at some point?\"\n\n25% Imagination\nThe question are about people's imaginations of the future.\n\"What will the fastest form of transportation be in 2050?\" 15%\nCompletely subjective\nThe answer depends on personal preference.\n\"Would you rather be shot into space or explore the deepest depths of the sea?\"\n\nToo many variables\nThe question with too many variables cannot be answered accurately. contains a proportion of answerable questions that is twice as large as the volume of unanswerable ones. Nevertheless, to ensure the feasibility of testing, we have purposefully capped the number of answerable questions.\n\nDataset Analysis\nTo gain insight into the reasons precluding a certain answer, we undertook a manual analysis of 100 randomly selected unanswerable questions. As tabulated in Table 1 , we have broadly segregated these questions into five distinctive categories. \"No Scientific Consensus\" encapsulates questions that ignite ongoing debates within the scientific community, such as those concerning the universe's origin. \"Imagination\" includes questions involving speculative future scenarios, like envisaged events over the next 50 years. \"Completely Subjective\" comprises questions that are inherently personal, where answers depend heavily on individual predispositions. \"Too Many Variables\" pertains to mathematical problems that become unsolvable owing to the overwhelming prevalence of variables. Lastly, \"Philosophical\" represents questions of a profound, often metaphysical, nature that resist concrete answers. Ideally, upon encountering such questions, the model should express uncertainty instead of delivering conclusive responses.\n\nEvaluation Method\nThis section elucidates the methodology employed for assessing self-knowledge in the generated text.\nIn order to achieve this, we define a similarity function, f sim , to compute the similarity, S, between a given sentence, t, and a collection of reference sentences, U = {u 1 , u 2 , . . . , u n }, endowed with uncertain meanings.\nEQUATION\nWhenever any S i surpasses a pre-determined threshold T , we perceive the text t as encompassing uncertain meanings, thereby eliminating the need for manual evaluation of the response.\nGiven the substantial disparity in the volume of answerable and unanswerable questions in Self-Aware, we adopt the F1 score as a measure of LLMs' self-knowledge. Our focus rests on identifying unanswerable questions, hence we designate them as positive cases and categorize answerable questions as negative cases.\n\nModel\nWe conduct a sequence of experiments to evaluate the degree of self-knowledge manifested by various LLMs, including GPT-3 (Brown et al., 2020) and InstructGPT (Ouyang et al., 2022) series, as well as the recent LLaMA (Touvron et al., 2023) and its derivative models, namely Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023) . Our investigative approach employed three distinct input forms: Direct, Instruction, and In-Context Learning (ICL), which is encapsulated in Appendix A.4. In-Context Learning \n\nSetting\nWe devised the reference sentence set U through a process that combined automated generation by LLMs and manual filtering, detailed further in Appendix A.1. To quantify the similarity between target and reference sentences, we utilized Sim-CSE (Gao et al., 2021) , setting the similarity threshold to 0.75 during our experiments. An exploration of threshold ablation is available in Appendix A.2.\nTo counteract potential errors in similarity calculation induced by varying lengths of the target and reference sentences, we employed a sliding window of length 5 to parse the target sentence into semantic chunks. During the generation process, we set the temperature to 0.7. We selected a random sample of 100 instances for GPT-4, while the remainder of the models were scrutinized using the full SelfAware dataset.\n\nHuman Self-Knowledge\nTo establish a benchmark for human selfknowledge, we engaged two volunteers and selected 100 random samples from the SelfAware dataset. The volunteers has 30 minutes to make judgments on the same set of questions, yielding an average F1 score of 84.93%, which we subsequently adopted as the benchmark for human self-knowledge. Detailed scores are available in Appendix A.3.\n\nAnalysis\nWe evaluate the manifestation of LLMs' selfknowledge, centering our investigation on three fundamental dimensions: the size of the model, the impact of instruction tuning, and the influence exerted by different input forms.\nModel Size. Figure 2 illustrates the correlation between model size and self-knowledge across various LLMs. It is noteworthy that across all three input forms, an augmentation in model parameter size is associated with an elevation in the F1 Score, with the most conspicuous enhancement manifesting in the ICL input form. Therefore, our analysis indicates that an LLM's self-knowledge tends to enhance with increasing model size, a trend consistent with the scaling law. Instruction Tuning. Figure 2 delineates that models from the InstructGPT series exhibit a superior level of self-knowledge compared to their GPT-3 counterparts. Further evidence of model enhancement is provided by Figure 4 , where textdavinci models show significant improvement relative to the base davinci model. An additional comparative analysis, presented in Figure 5 , evaluates LLaMA against its derivative models. The results underscore a notable increase in self-knowledge for Alpaca and Vicuna upon instruction tuning, exceeding their base model performances. Among these, Vicuna-13B outperforms the LLaMA-65B, corroborating the efficacy of instruction tuning for enhancing model self-knowledge.\nL L a M A -7 B A lp a c a -7 B V ic u n a -7 B L L a M A -1 3 B A lp a c a -1 3 B V ic u n a -1 3 B L L a M A -3 0 B L L a M A -\nInput Forms. As shown in Figure 2 , the incorporation of instructions and examples serves to boost the self-knowledge of both the GPT-3 and Instruct-GPT series. Specifically, ICL input form, providing richer contextual information, contributes to a significant enhancement in models' self-knowledge. This impact is particularly noticeable in the davinci model, where ICL facilitates a 27.96% improvement over the direct. Moreover, a comparison between Figure 3 performance to the human benchmark of 84.93%. This underscores the considerable potential that remains for enhancing the self-knowledge level of LLMs.\nAnswerable Questions. Figure 6 traces the performance evolution of the InstructGPT series in addressing answerable questions, adhering to the closed-book question answering paradigm (Touvron et al., 2023) , where output accuracy is contingent on the presence of the correct answer. Our observations underscore a steady enhancement in QA task accuracy corresponding to an increase in model parameter size and continuous learning.\nParticularly, the accuracy of text-davinci-001 experiences a significant ascent, scaling from a meager 2.48% in text-ada-001 to 10.61%, whereas GPT-4 marks an even more striking jump to 42.64%.\n\nConclusion\nThis study investigates the self-knowledge of LLMs by evaluating their ability to identify unanswerable questions. Through the introduction of a novel dataset and an automated method for detecting uncertainty in the models' responses, we are able to accurately measure the self-knowledge of LLMs such as GPT-3, InstructGPT and LLaMA.\nOur results reveal that while these models possess a certain degree of self-knowledge, there is still an apparent disparity in comparison to human selfknowledge. This highlights the need for further research in this area to enhance the ability of LLMs to understand their own limitations on the unknows. Such efforts will lead to more accurate and reliable responses from LLMs, which will have a positive impact on their applications in diverse fields.\n", "hypothesis": " Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks.  Current research focuses on enhancing their performance within their existing knowledge.  Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend.  Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance.  This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions.  We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge.  We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts.  Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models.  Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge.  Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.\n\"True wisdom is knowing what you don't know.\" -Confucius.", "answer": true}
{"title": "Do GPTs Produce Less Literal Translations?", "content": "\nIntroduction\nDespite training only on a language-modeling objective, with no explicit supervision on aligned parallel data (Briakou et al., 2023) , LLMs such as GPT-3 or PaLM (Brown et al., 2020; Chowdhery et al., 2022) achieve close to state-of-the-art translation performance under few-shot prompting (Vilar et al., 2022; Hendy et al., 2023) . Work investigating the output of these models has noted that the gains in performance are not visible when using older surface-based metrics such as BLEU (Papineni et al., 2002a) , which typically show large losses against NMT systems. This raises a question: How do these LLM translations differ qualitatively from those of traditional NMT systems?\nWe explore this question using the property of translation literalness. Machine translation systems have long been noted for their tendency to produce source He survived by the skin of his teeth .\n\nNMT\nIl a surv\u00e9cu par la peau de ses dents . GPT-3 Il a surv\u00e9cu de justesse . Table 1 : An example where GPT-3 produces a more natural (non-literal) translation of an English idiom. When word-aligning these sentences, the source word skin remains unaligned for the GPT-3 translation.\noverly-literal translations (Dankers et al., 2022b) , and we have observed anecdotally that LLMs seem less susceptible to this problem (Table 1 ). We investigate whether these observations can be validated quantitatively. First, we use measures based on word alignment and monotonicity to quantify whether LLMs produce less literal translations than NMT systems, and ground these numbers in human evaluation ( \u00a7 2). Next, we look specifically at idioms, comparing how literally they are translated under both natural and synthetic data settings ( \u00a7 3).\nOur investigations focus on the translation between English and German, Chinese, and Russian, three typologically diverse languages. Our findings are summarized as follows: (1) We find that translations from two LLMs from the GPT series of LLMs are indeed generally less literal than those of their NMT counterparts when translating out of English, and (2) that this is particularly true in the case of sentences with idiomatic expressions.\n\nQuantifying Translation Literalness\nWe compare the state-of-the-art NMT systems against the most capable publicly-accessible GPT models (at the time of writing) across measures designed to capture differences in translation literalness. We conduct both automatic metric-based as well as human evaluations. We explain the evaluation and experimental details below. for evaluation (Barrault et al., 2021) .\n\nMeasures of Quality\nWe use COMET-QE 1 (Rei et al., 2020) as the Quality Estimation (QE) measure (Fomicheva et al., 2020) to quantify the fluency and adequacy of translations. Using QE as a metric presents the advantage that it precludes the presence of any reference bias, which has been shown to be detrimental in estimating the LLM output quality in related sequence transduction tasks (Goyal et al., 2022) . On the other hand, COMET-QE as a metric suffers from an apparent blindness to copy errors (i.e., cases in which the model produces output in the source language) (He et al., 2022) . To mitigate this, we apply a language identifier (Joulin et al., 2017) on the translation output and set the translation to null if the translation language is the same as the source language. Therefore, we name this metric COMET-QE + LID.\n\nMeasures of Translation Literalness\nThere do not exist any known metrics with high correlation geared towards quantifying translation literalness.\nWe propose and consider two automatic measures at the corpus-level:\n1. Unaligned Source Words (USW): Two translations with very similar fluency and adequacy could be differentiated in terms of their literalness by computing word to word alignment between the source and the translation, then measuring the number of source words left unaligned. When controlled for quality, a less literal translation is likely to contain more unaligned source words (as suggested in Figure 1 ).\n\nTranslation Non-Monotonicity (NM):\nAnother measure of literalness is how closely the translation tracks the word order in the source. We use the non-monotonicity metric proposed in Schioppa et al. (2021) , which computes the deviation from the diagonal in the word to word alignment as the non-monotonicity measure.\n1 wmt20-comet-qe-da\nThis can also be interpreted as (normalized) alignment crossings, which has been shown to correlate with translation non-literalness (Schaeffer and Carl, 2014) .\nWe use the multilingual-BERT-based awesomealigner (Devlin et al., 2019; Dou and Neubig, 2021) to obtain the word to word alignments between the source and the translation. Table 2 presents an illustration of translations with different USW and NM scores 2 , obtained from different systems.\n\nSystems Under Evaluation\nWe experiment with the below four systems (NMT and LLMs):\n1. WMT-21-SOTA: The Facebook multilingual system (Tran et al., 2021) won the WMT-21 News Translation task (Barrault et al., 2021) , and thereby represents the strongest NMT system on the WMT'21 test sets.\n2. Microsoft-Translator: MS-Translator is one of the strongest publicly available commercial NMT systems (Raunak et al., 2022) .\n3. text-davinci-002: The text-davinci-002 model is an instruction fine-tuned model in the GPT family (Brown et al., 2020) . It represents one of the strongest publicly-accessible LLMs (Liang et al., 2022) .\n4. text-davinci-003: The text-davinci-003 model further improves upon text-davinci-002 for many tasks 3 (Liang et al., 2022) .\nFor both the GPT models, we randomly select eight samples from the corresponding WMT-21 development set, and use these in the prompt as demonstrations for obtaining all translations from GPTs.\n\nResults\nWe compare the performance of the four systems on the WMT-21 test sets. Figure 1 shows the results of this comparison. A key observation is that while the GPT based translations achieve superior COMET-QE+LID scores than Microsoft Translator across the language pairs (except En-Ru), they The NMT Systems and GPT models achieve similar COMET-QE+LID Scores (Top), there exists a significant gap in the number of unaligned source words (USW) across the datasets (Bottom). Further, GPT translations obtain higher non-monotonicity scores for E-X translations (Middle).\nalso consistently obtain considerably higher number of unaligned source words. This result holds for the comparison between the WMT-21-SOTA and GPT systems as well. Further, GPT translations also consistently show higher non-monotonicity for E\u2192X translations. However, this is not the case for translations into English, wherein the multilingual WMT-21-SOTA system obtains very close non-monotonicity measurements. The combined interpretation of these measurements suggests that GPTs do produce less literal E\u2192X translations.\n\nHuman Evaluation\nWe verify the conclusion from the results in Figure 1 by conducting a human evaluation of translation literalness on 6 WMT-22 language pairs: En-De, En-Ru, En-Zh and De-En, Ru-En, Zh-En. For each language pair, we randomly sample 100 source-translation pairs, with translations obtained from MS-Translator (a strong commercial NMT system) and text-davinci-003 (a strong commercial LLM) (Hendy et al., 2023) . We used zero-shot text-davinci-003 translations for human evaluations in order to eliminate any biases through the use of specific demonstration examples. In each case, we ask a human annotator (bilingual speaker for Zh-En, target-language native plus bilingual speaker otherwise) to annotate 100 translations from both GPT and MS-Translator and select which of the two translations is more literal. The human annotation interface is described in Appendix A. The results in Table 3 show that the annotators rate the GPT translations as less literal.\nLang Experiments on Best WMT-22 NMT Systems Further, we also experiment with the WMT-Best systems on the WMT-22 General Machine Translation task (Kocmi et al., 2022) . We evaluate USW and NM on De-En, Ja-En, En-Zh and Zh-En, since on each of these language pairs, text-davinci-003's few-shot performance is very close to that of the WMT-Best system as per COMET-22 (Rei et al., 2022) , based on the evaluation done in Hendy et al. (2023) . We report our results in Table 4 , which shows our prior findings replicated across the language pairs. For example, text-davinci-003, despite obtaining a 0.2 to 0. \n\nEffects On Figurative Compositionality\nIn this section, we explore whether the less literal nature of E\u2192X translations produced by GPT models could be leveraged to generate higher quality translations for certain inputs. We posit the phenomenon of composing the non-compositional meanings of idioms (Dankers et al., 2022a) with the meanings of the compositional constituents within a sentence as figurative compositionality. Thereby, a model exhibiting greater figurative compositionality would be able to abstract the meaning of the idiomatic expression in the source sentence and express it in the target language non-literally, either through a non-literal (paraphrased) expression of the idiom's meaning or through an equivalent idiom in the target language. Note that greater nonliteralness does not imply better figurative compositionality. Non-literalness in a translation could potentially be generated by variations in translation different from the desired figurative translation.\n\nTranslation with Idiomatic Datasets\nIn this section, we quantify the differences in the translation of sentences with idioms between traditional NMT systems and a GPT model. There do not exist any English-centric parallel corpora dedicated to sentences with idioms. Therefore, we experiment with monolingual (English) sentences with idioms. The translations are generated with the same prompt in Section 2. The datasets with natural idiomatic sentences are enumerated below:\n\u2022 MAGPIE (Haagsma et al., 2020) contains a set of sentences annotated with their idiomaticity, alongside a confidence score. We use the sentences pertaining to the news domain which are marked as idiomatic with cent percent annotator confidence (totalling 3,666 sentences).\n\u2022 EPIE (Saxena and Paul, 2020) contains idioms and example sentences demonstrating their usage. We use the sentences available for static idioms (totalling 1,046 sentences).\n\u2022 The PIE dataset (Zhou et al., 2021) contains idioms along with their usage. We randomly sample 1K sentences from the corpus.\n\nResults\nThe results are presented in Table 5 . We find that text-davinci-002 produces better quality translations than the WMT'21 SOTA system, with greater number of unaligned words as well as with higher non-monotonicity.\nFurther Analysis Note that a direct attribution of the gain in translation quality to better translation of idioms specifically is challenging. Further, similarity-based quality metrics such as COMET-QE themselves might be penalizing non-literalness, even though they are less likely to do this than surface-level metrics such as BLEU or ChrF (Papineni et al., 2002b; Popovi\u0107, 2015) . Therefore, while a natural monolingual dataset presents a useful testbed for investigating figurative compositionality abilities, an explicit comparison of figurative compositionality between the systems is very difficult. Therefore, we also conduct experiments on synthetic data, where we explicitly control the finegrained attributes of the input sentences. We do this by allocating most of the variation among the input sentences to certain constituent expressions in synthetic data generation.\n\nSynthetic Experiments\nFor our next experiments, we generate synthetic English sentences, each containing expressions of specific type(s): (i) names, (ii) random descriptive phrases, and (iii) idioms. We prompt text-davinci-002 in a zero-shot manner, asking it to generate a sentence with different instantiations of each of these types (details are in appendix B). We then translate these sentences using the different systems, in order to investigate the relative effects on our literalness metrics between systems and across types. In each of the control experiments, we translate the synthetic English sentences to German. The results are presented in Table 7 .\nResults Table 6 shows that the percentage of unaligned source words is highest in the case of idioms, followed by random descriptive phrases & named entities. The results are consistent with the hypothesis that the explored GPT models produce less literal E\u2192X translations, since named entities or descriptive phrases in a sentence would admit more literal translations as acceptable, unlike sentences with idioms. Davinci-002 obtains a much higher COMET-QE score in the case of translations of sentences with idioms, yet obtains a higher percentage of unaligned source words. Similarly, the difference in non-monotonicity scores is also considerably higher for the case of idioms. These results provide some evidence that the improved results of the GPT model, together with the lower literalness numbers, stem from correct translation of idiomatic expressions. Table 7 shows that this effect only increases with the number of idioms.\n\nDiscussion\nIn our experiments conducted across different NMT systems and GPT models, we find evidence that GPTs produce translations with greater nonliteralness for E\u2192X in general. There could be a number of potential causes for this; we list two plausible hypotheses below:\nParallel Data Bias NMT models are trained on parallel data, which often contains very literal webcollected outputs. Some of this may even be the output of previous-generation MT systems, which is highly adopted and hard to detect. In addition, even high quality target text in parallel data always contains artifacts that distinguishes it from text originally written in that language, i.e. the 'translationese' effect (Gellerstam, 2005) . These factors could likely contribute to making NMT translations comparatively more literal.\nLanguage Modeling Bias Translation capability in GPTs arises in the absence of any explicit supervision for the task during the pre-training stage. Therefore, the computational mechanism that GPTs leverage for producing translations might be different from NMT models, imparting them greater abstractive abilities. This could have some measurable manifestation in the translations produced, e.g., in the literalness of the translations.\nDifferences in E\u2192X and X\u2192E In E\u2192X, we consistently find that GPT translations of similar quality are less literal and in the X\u2192E direction, we observe a few anomalies. For X\u2192E, in Figure 1 , in all but one comparison (WMT-21-SOTA vs GPTs for De-En) GPTs obtain higher measures for non-literalness. On the other hand, we did not see anomalies in the trend for E\u2192X directions.\n\nVariations in Experimental Setup\nWe also experimented with a variant of USW and NM which doesn't use the alignments pertaining to stopwords. Each of our findings remain the same, with relatively minor changes in magnitudes but not in system rankings. Similarly, we observed a greater tendency towards less literalness in GPT translations in both few-shot and zero-shot settings, when compared across a range of NMT systems.\n\nSummary and Conclusion\nWe investigated how the translations obtained through LLMs from the GPT family are qualitatively different by quantifying the property of translation literalness. We find that for E\u2192X translations, there is a greater tendency towards nonliteralness in GPT translations. In particular, this tendency becomes evident in GPT systems' ability to figuratively translate idioms.\n", "hypothesis": " Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks.  On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs.  However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models.  In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E\u2192X) from GPTs tend to be more literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.", "answer": false}
{"title": "Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios", "content": "\nIntroduction\nReasoning plays a central role in human communication (Frank and Goodman, 2012) . While language models have demonstrated remarkable capacity on downstream tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019) , it remains unclear to what extent predictions generated by language models are consequences of correlation with linguistic heuristics in the context, versus robust reasoning about causal relations grounded on understanding of world knowledge.\nIn this paper we leverage counterfactual conditionals to investigate the capacity of pre-trained LMs (PLMs) to distinguish hypothetical scenarios from reality, and to examine how this interacts with models' use of existing real world knowledge as well as shallower associative cues. Counterfactuals consist of a premise which is false in the real world but true in the hypothetical world (e.g., \"If cats were vegetarians\"), and an imaginary consequence of this premise (\"cats would love cabbages\"). Testing language models with counterfactuals allows us to use language to manipulate what is true and what is hypothetical, and to test models' ability to separate and use this information for predictions. Previous work has established the use of counterfactual scenarios to probe inference ability (Qin et al., 2019; Zellers et al., 2019; Mostafazadeh et al., 2016; Meng et al., 2022; Rajani et al., 2019; Saparov and He, 2022; Frohberg and Binder, 2021; Elazar et al., 2021; Rudinger et al., 2020) , but the datasets lack systematic control of lexical cues and world knowledge, which makes it likely that the performance could be attributable to spurious cues in the datasets (Niven and Kao, 2019) .\nFor our tests we draw on and adapt inputs from existing psycholinguistic experiments. We begin by testing models' ability to override existing world knowledge when the context indicates that the correct completion involves a hypothetical world (e.g., \"if cats were vegetarian, cats would love cabbages/fish\"). We test five popular PLMs, and find that models can increase their preference for counterfactual completions given counterfactual context-however, most models rely strongly on simple lexical cues. Next we control the effect of real world knowledge and lexical triggers, to test models' understanding of what counterfactual language implies about the world state. We find that most models fail to understand real-world implications of counterfactuals and largely rely on lexical triggers-with the exception of GPT-3, which shows greater sophistication, but continues to show non-trivial susceptibility to interfer-ence from lexical-associative cues. We discuss the implications and possible interpretations of these findings with respect to linguistic competence and predictive strategies of these models.\n\nExp1: overriding world knowledge\nOur first experiment investigates whether LMs are able to take a counterfactual scenario and predict a counterfactual-consistent completion that contradicts general world knowledge.\nItems We draw directly on counterfactual stimuli from the psycholinguistic study of Ferguson and Sanford (2008) . There are 128 items from the original psycholinguistic experiments, and we synthetically generate 10,720 additional items (see Appendix A.2 for illustration of data generation process). We match target nouns and syntactic constructions across conditions in order to control lexical properties that influence language models' predictions. Table 1 shows example items from the synthetic large-scale dataset (see Section A.1 for example items from the small-scale dataset).\n\nCond Sentence\nCW If cats were vegetarians, people would love them.\nFamilies would feed cats with fish/cabbages.\n\nRW\nBecause cats are carnivores, people love them.\nFamilies would feed cats with fish/cabbages.\n\nBB\nFamilies would feed cats with fish/cabbages The experiment includes two key conditions: Counterfactual-World (CW) and Real-World (RW) (Fig. 1 ). The CW condition presents a counterfactual scenario, e.g., in which cats are vegetarians. The logical target completion in this example is \"cabbages\", but because in reality cats are more likely to eat fish, this contradicts world knowledge. By contrast, in the RW condition the logical completion is consistent with the real world (\"feed cats with fish\"). We also include one Baseline Bias (BB) condition, for a more direct test of the strength of models' baseline preference for each completion.\nExperiments We test counterfactual reasoning in five pre-trained language models. We include autoregressive transformers in the GPT family (GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) ) and masked language models in the BERT family (BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) and MPNet (Song et al., 2020)) 2 .\nWe test models by comparing the log-probability that each model assigns to the CW-congruent (\"cabbages\") and RW-congruent (\"fish\") completions given the contexts. For all conditions, we compute the percentage of items in which the CW-congruent continuation has a higher probability than the RWcongruent continuation. This means that in RW and BB conditions, lower values reflect better predictions, since the CW-congruent completion is the less logical completion in these conditions. Table 2 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp1. In the CW condition, higher values reflect better predictions. In RW and BB conditions, lower values reflect better predictions.\nResults Table 2 shows the preferences for CWcongruent completions across all models and conditions, for the small-scale hand-designed items from the psycholinguistic experiment, and for the large-scale synthetic items. 3 We see that all mod-els show stronger preference for CW-congruent continuations in the counterfactual (CW) context than in the other conditions (though in the case of BERT on the small-scale data, this difference is negligible). All models show below-chance preference for CW-congruent continuations in the RW condition-which means above-chance preference for the correct RW-congruent continuations. However, though all model preferences for the correct CW-congruent continuation are higher in the CW condition than in the RW condition, even in the CW condition the preference for CW-congruent conditions is at best slightly above chance for most models. The exception is GPT-3, which is the only model to prefer the CW-congruent continuation in greater than 70% of items.\nWe also see that GPT-3 shows exceptionally strong performance on both BB and CW conditions. This suggests, slightly counterintuitively, that stronger grasp of relevant world knowledge may in fact be associated with models more effectively overriding that knowledge in a counterfactual. To investigate this effect further, we examine the impact of world knowledge at the item level. We quantify strength of world knowledge as the difference between models' log-probability of CW-and RW-congruent continuations for a given item in the BB condition, and the strength of counterfactual preference as the difference between log-probability of CW-and RW-congruent continuations for a given item in the CW condition. We then compute the Pearson correlation between these strength measures. We find a significant correlation between the robustness of world knowledge encoding and strength of counterfactual preference in the CW condition across all language models (see Appendix A.3), further supporting a relationship between strength of world knowledge and counterfactual sensitivity. While previous work has suggested that large language models may have difficulty avoiding memorized texts when explicitly prompted to end famous quotes differently (McKenzie et al., 2022) , our results suggest that world knowledge may in fact facilitate reasoning when accompanied with clear structural cues (e.g. \"if\"). To better understand how world knowledge informs language models' predictions and in-CW condition alone. However, to further address this concern, we calculate the proportion of items in which the model shows the correct preference in both CW and RW conditions. The results are presented in Section A.5 and suggest a comparable pattern in terms of relative model strengths.\nference, it will be important to continue expanding the scale of tests and more carefully operationalize definitions of world knowledge in future work.\n\nExp2: impact of cue words in context\nThe first experiment suggests that models can to an extent override world knowledge given a counterfactual, particularly in cases when models have a strong handle on the relevant world knowledge. However, it is possible that in these tests the models were not relying on sophisticated understanding of counterfactuals, but rather on simple lexical triggers in context. Consider, for instance, that models could perform well in Exp1 if they simply increase their preference for \"cabbages\" in the proximity of \"vegetarians\", etc. To test the impact of these lexical triggers, we incorporate an additional condition.\nItems Table 3 and Fig. 2 show a sample item and illustration of experimental set-up with the new added condition. In this Counterfactual-to-Reality (CR) condition, models see the same counterfactual context, but the subsequent sentence references actual reality. So the correct completion is consistent with reality, but inconsistent with the lexical trigger (\"vegetarians\"). We generate sentences in the CR condition by modifying CW sentences to include the discourse connective \"In reality\" and to include present tense in the second sentence.\n\nCR\nIf cats were vegetarians, people would love them.\nIn reality, families feed cats with fish/cabbages. Experiments As above, we calculate percentage of items in which models prefer the CW-congruent continuations. Models relying on information beyond simple lexical triggers should show a sharp drop in preference for the CW-congruent completion in the CR condition, where the correct completion should align with real world information.\nResults Table 4 shows the results. We see that most models show non-zero drop between CW and CR conditions-however, for most models this reduction is minor. It is only GPT-3 that shows a truly substantial drop in CW-congruent preference, and only in the large-scale synthetic dataset. This suggests that most models are largely following simpler lexical triggers, while GPT-3 has somewhat greater sensitivity to more detailed linguistic cues. Note, however that GPT-3's relative success on the synthetic data over the small-scale data may rely on larger distance between lexical triggers and target positions: see Appendix A.4 for evidence on GPT-3's sensitivity to linear distance. Table 4 : Percentage of preference for CW-congruent completion (e.g., \"cabbages\") in Exp2. In the CW condition, higher values reflect better predictions. In the CR condition, lower values reflect better predictions.\n\nExp3: Inferring real world state with counterfactual cues\nThe previous experiments indicate that models can override world knowledge in the face of counterfactual evidence, and that the ability to do this improves with stronger world knowledge-but for most models this performance appears to be driven largely by simple lexical triggers in the context, with the possible exception of GPT-3. In this section we remove the influence of pre-existing world knowledge, and hold constant lexical triggers across conditions, for a more direct test of models' sensitivity to linguistic indicators of counterfactuals, and what they say about the true state of the world. This task is particularly challenging because language models must infer the true state of the world based on the presence of counterfactuals, with lexical cues often being misleading.\nItems We adapt stimuli from a psycholinguistic study with 96 controlled sentences (Ferguson, 2012) . We additionally create a larger-scale synthetic dataset with 12,960 sentences, using the same events as the generated dataset from Section 2. We modify the subject noun phrases such that there is no influence of existing world knowledge. For example, we modify the subject \"cat\" to \"pet\", so that there is no prior knowledge about the subject's preference for \"cabbages\" or \"fish\". As a result, existing world knowledge cannot inform the correct completion-instead, models need to infer based on the counterfactual language that the true state of the world is different from what the counterfactual states. Further, we control the lexical items used across different conditions to minimize effects of lexical cues on condition differences (see Table 5 ).\n\nCond Sentence\nCWC If the pets were vegetarians, people would love them.\nIn fact, people feed the pets with fish/cabbages.\nRWCA Because the pets are vegetarians, people love them.\nIn fact, people feed the pets with fish/cabbages.\nBBC In fact, people feed the pets with fish/cabbages. Fig. 3 shows the set-up of conditions. In the Counterfactual-World Context (CWC) condition, the scenario described in the first sentence is neutral with respect to real world knowledge-it is the use of the counterfactual (\"if...were\") that tips us off that this scenario is not true in reality. The correct completion, then, cannot be informed by world knowledge, and is also misaligned with the lexical trigger (e.g., \"vegetarians\"), so models must rely specifically on this implication from the counterfactual in order to perform well.\nIn the Real-World Context Alternative (RWCA) condition, the context uses the same lexical triggers (\"vegetarians\") as the CWC condition. However, because there is no counterfactual language, the logical completion is now the word associated with the lexical trigger (e.g., \"cabbages\", associated with \"vegetarians\").\nGiven that the logical completions in CWC and RWCA differ, we also compare against a Baseline Bias Context (BBC) condition, to establish default model preference for the target factual completion in the presence of the new subject noun phrase.\nExperiments We compare proportion of CWCcongruent completions across conditions. Good performance will assign high values in the CWC condition and low values in the RWCA condition. Table 6 : Percentage of preference for CWC-congruent completion (e.g., \"fish\") in Exp3. In the CWC condition, higher values reflect better predictions. In the CWCA condition, lower values reflect better predictions. The BBC condition establishes models' default preference for the CWC-congruent completion.\nResults Table 6 shows the results. In the smallscale dataset, most models show a similar preference in CWC and RWCA, suggesting again that their predictions are largely driven by lexical triggers. Only GPT-3 shows substantial difference between CWC and RWCA, indicating finer-grained sensitivity to counterfactual structures. This sensitivity is, however, less pronounced in the largescale dataset. Closer inspection suggests that GPT-3's specific success on the small-scale data may in fact be attributable to canceling out of lexical triggers: in the small-scale dataset, there are lexical triggers supporting both continuations (see A.1 for more illustration of the characteristics of the smallscale dataset), which may cause lexical cues to cancel out, enabling more influence from other linguistic cues. To take one example, the small-scale dataset contains the item \"If Helen had received her student loan, her bank balance would now be in credit. When she checked her bank balance she was worried/happy about her finance.\" In this item, among the lexical triggers (\"student loan\", \"in credit\", \"bank balance\") there are potential associations with both the CWC-congruent completion \"worried\" and the CWC-incongruent completion \"happy\". By contrast, in the large-scale dataset, the major lexical trigger (\"vegetarians\") always favors the CWC-incongruent continuation (\"cabbages\"), causing strong lexical bias against the CWC-congruent continuation (see Appendix A.4 for further analysis on the role of conflicting lexical triggers and other linguistic factors). This suggests that GPT-3 does show real sensitivity to linguistic indicators of counterfactuals, but the effect of superficial lexical cues remains strong.\n\nConclusion\nThe experiments above have shown that when presented with counterfactual situations, PLMs are able to prefer completions that conflict with world knowledge-and counterintuitively, this sensitivity appears better in cases where that world knowledge is stronger. Our results also indicate, however, that models are in large part relying on simple lexical cues to inform these preferences. The only model that shows more sophisticated sensitivity to finegrained linguistic cues separating counterfactuals from reality is GPT-3-which successfully distinguishes conditions based on counterfactual cues, but nonetheless still shows strong influences from lexical associative cues. Why might world knowledge aid counterfactual sensitivity? Does GPT-3 truly understand counterfactuals? One possibility worth considering is that explanations in both of these cases involve volume of exposure. First, models' stronger world knowledge for a given fact suggests that models have encountered that fact more often in training-and this may in turn translate to more exposure to that type of knowledge in counterfactual contexts, enabling more straightforward memorization-based performance. Similarly, while GPT-3 may robustly understand counterfactuals, the massive data exposure for that model may enable a simpler path to success: GPT-3 could simply have developed lower-level knowledge of how linguistic cues like \"If/had\" versus \"Because\" mediate levels of association between nearby lexical cues and later words. We leave investigation of these hypotheses for future work.\n", "hypothesis": " Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world.  We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions.  We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models.  We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge-however, we also find that for most models this effect appears largely to be driven by simple lexical cues.  When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.", "answer": true}
